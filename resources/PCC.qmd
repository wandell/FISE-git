\[1996/06/01\]

# Principles and Consequences of the Initial Visual Encoding

> 'Only infrequently is it possible to subject the manifold phenomena of
> life to simple and strict forms of mathematical treatment without
> forcing the data and encountering contradiction, probably never
> without a certain abandonment of the immense multiplicity of details
> to which those phenomena owe their aesthetic attractiveness.
> Nevertheless, however, it has often proved to be possible and useful
> to establish, for wide fields of biological processes and organic
> arrangements, comparatively simple mathematical formulas which, though
> they are probably not applicable with absolute accuracy, nevertheless
> simulate to a certain approximation a large number of phenomena. Such
> representations not only offer preliminary orientation in a field that
> at first seems completely incomprehensible, but they also often direct
> research into a correct course, inasmuch as first an insight into
> those fundamental formulations is sought, and then the deviations from
> their strict validity, which become apparent here and there, are made
> the subject of special investigations. Among the fields of physiology
> which have permitted the establishment of such guiding formulas the
> theory of visual sensations and of color mixture assumes a
> particularly distinguished position [@vonKries1902Chromatic].'

## Introduction

Vision research has many purposes. Medical investigators aim to diagnose
and repair visual disorders ranging from optical focus to retinal
dysfunction to cortical lesions. Psychologists aim to identify and
quantify the systematic rules of perception, including models of visual
sensitivity, image quality, and the laws that predict percepts such as
brightness, color, motion, size and depth. Systems neuroscientists seek
to relate visual experience and performance to the neural signals in the
visual pathways, and computational investigators seek principles and
models of perceptual and neural processes. Image systems engineers ask
how to design sensors and processing to provide effective artificial
vision systems.

Vision science draws upon findings from many fields, including biology,
computer science, electrical engineering, neuroscience, psychology and
physics. Clear communication among people trained in different
disciplines is not always straightforward. One of the ways that vision
science has flourished is by using the language of mathematics to
communicate core ideas. Vision science uses many types of mathematics;
here we describe methods that have been used for many decades. These are
certain linear methods, descriptions of noise distributions, and
Bayesian inference. Many other linear methods (e.g., principal
components, Fourier and Gabor bases, and independent components
analysis) and nonlinear methods (e.g., linear-nonlinear cascades,
normalization, information theory, and neural networks) can be found
throughout the vision science literature. For this chapter, we focus on
a few core mathematical methods and the complementary role of
computation.

Physics - the field that quantifies the input to the visual system -
provides mathematical representations of the light signal and
definitions of physical units. The field of physiological optics
quantifies the optical and biological properties of the lens. These
properties are summarized as a mathematical transformation that maps the
physical stimulus to the image focused on the retina, generally referred
to as the retinal image. At each retinal location the image is
characterized as the spectral irradiance (power per unit area as a
function of wavelength). Retinal anatomy and electrophysiology identify
the properties of the rod and cone photoreceptors, enabling us to
calculate the photopigment excitations from the retinal image using
linear algebraic methods.

Perhaps the most famous use of mathematics in vision science is at the
intersection of physics and psychology: The laws of color matching
formalize the relationship between the physics of light and certain
aspects of color appearance. The mathematical principles of color
matching are also deeply connected to Thomas Young's biological insight
that there are only three types of cone photopigment
[@Young1802OnTheTheory]. This insight implies a low-dimensional
biological encoding of the high-dimensional spectral light. The linear
algebraic techniques used to describe the laws of color matching were
developed by the mathematician Grassmann. Indeed, he developed vector
spaces in part for this purpose [@Grassmann1853ZurTheorie]. The
mathematics he introduced remains central to color imaging technologies
and throughout science and engineering.

While acknowledging the importance of mathematical foundations, it is
also important to recognize that there is much to be gained by building
computational methods that account for specific system properties. The
added value of computations is clear in many different fields, not just
vision science. The laws of gravity are simple, but predicting the tides
at a particular location on earth is not done via analytic application
of Newton's formulas. Similarly, that color vision is three-dimensional
is a profound principle, yet precise stimulus control requires
accounting for many factors, such as variations of the inert pigments
across the retinal surface [@Whitehead2006Macular; @CIE2007Fundamental]
and the wavelength-dependent blur of chromatic aberration
[@Marimont1994-ys]. The mathematical principles guide, but we need
detailed computations to predict precisely how color matches vary from
central to peripheral vision.

We hope this chapter helps the reader value principles expressed by
equations and the computations embodied in software. Establishing the
principles first provides a foundation for implementing accurate
computations. Historically, our knowledge about vision has been built up
by developing principles, testing them against experiments, and
combining them with computation; this remains a useful and important
approach. Indeed, we believe the goal of vision science includes not
only producing models that account for performance and enable
engineering advances, but also leveraging those models to extract new
principles that help us think about how visual circuits work.

There are competing views: Some would argue that large data sets
combined with analyses using machine learning provide the best way
forward to understanding, and recent years have seen impressive
engineering advances achieved with this approach [@Cox2014-hp]. We are
certainly interested in the performance of such black-box models as a
point of departure, but here we emphasize principles and data-guided
computational implementations of these principles.

This chapter begins by describing the representation of the visual
stimulus, and how light rays in the scene pass through the optics of the
eye and arrive at the retina. Next, we explain how the retinal
photoreceptors (a) transform the retinal spectral irradiance into
photoreceptor excitations, and (b) spatially sample the retinal image.
Each of these steps can be expressed by a crisp mathematical
formulation. To describe the real system with quantitative precision, we
implemented software that models specific features of the scene, optics,
and retina (ISETBio; @Cottaris2019AComputational,
[-@Cottaris2019AComputational]; @Cottaris2020AComputational,
[-@Cottaris2020AComputational];
https://github.com/isetbio/isetbio/wiki), and we illustrate the use of
these models in several examples.

The frontiers of vision science use mathematics to understand visual
percepts, which provide a useful basis for thought and action. The
information provided by light-driven photopigment excitations is used to
create these percepts, but knowledge of the excitations alone falls far
short of describing visual perception. The brain makes inferences about
the external world from the retinal encoding of light, and throughout
the history of vision science many investigators have suggested that the
role of neural computation is to implement the principles that underlie
these inferences. This point was emphasized as early as Helmholtz who
wrote

> "The general rule determining the ideas of vision that are formed
> whenever an impression is made on the eye, is that such objects are
> always imagined as being present in the field of vision as would have
> to be there in order to produce the same impression on the nervous
> mechanism (@Helmholtz1866Handbuch, [-@Helmholtz1866Handbuch]; English
> translation @Helmholtz1896Southall, [-@Helmholtz1896Southall])."

Within psychology this idea is called 'unconscious inference', a phrase
that emphasizes that we are not aware of the neural processes that
produce our conscious experience, an idea was important to Helmholtz.
Perhaps more important in this context is the principle that the
percepts represent critical properties of external objects in the field
of view, such as depth, reflectance, shape, and motion.

The mathematics of perceptual inferences can take many forms, and in
common scientific practice the mathematics of inference depend on what
is known about the input signal. If the scene properties are not
uniquely determined by the sensory measurements, such as when only three
spectral classes of cones sample the spectral irradiance of the retinal
image, probabilistic reasoning about the likely state of the world is
inevitable. In vision science, linear methods combined with the
mathematical tools of probabilistic inference are commonly used to
understand how the brain interprets the mosaic of photoreceptor
excitations to see objects, depth, and color. In the final part of this
chapter we close the loop between sensory measurements and perceptual
inference by introducing the mathematics of such inferences, focusing on
two specific examples relevant to the study of the initial visual
encoding. The principles we introduce, however, apply generally.

## Scene to retinal image

### Light field 

Light is the most important visual stimulus.[^1] The word light means
the electromagnetic 'radiation that is visible to the human eye.'[^2]
The mathematical representation of light has been developed through a
series of famous experiments over many centuries, and these experiments
provide several different ways to think about light. Several properties
of how light is encoded by the eye can be understood by treating light
as comprising rays of many different wavelengths.

<figure id="fig:lightfield">
<img src="lightfield.png" style="width:12cm" />
<p>Figure reproduced from <span class="citation"
data-cites="Ayscough1755-uf"></span>. <span id="fig:lightfield"
data-label="fig:lightfield"></span></p>
<figcaption><strong>Light field geometry</strong>. The complete set of
rays in the environment is the light field. The rays that arrive at the
imaging system, in this figure a large pinhole camera, are the incident
light field. If the imaging system includes a lens, rather than just a
pinhole, the incident light field is described by the positions and
angles of the rays at the lens aperture.</figcaption>
</figure>

In a passage in his 1509 notebook [@Da_Vinci1970-fj], Leonardo da Vinci
noted that an illuminated scene is filled with rays that travel in all
directions[^3]. As evidence, he described a pinhole camera (camera
obscura) made by placing a small hole in a wall of a windowless room
(Figure [1.1](#fig:lightfield){reference-type="ref"
reference="fig:lightfield"}). The wall is adjacent to a brightly
illuminated piazza; an image of the piazza (inverted) appears on a wall
within the room. Leonardo noted that an image is formed wherever the
pinhole is placed, and he concluded that the rays needed to form an
image must be present at all of these positions. Leonardo compared the
space-filling light rays to the traveling waves that arise after
dropping a rock in a pond.

The Russian physicist, Gershun, provided a mathematical representation
of the geometry of these rays, which he called the light field
[@Gershun1939-lu]. The mathematical representation of the light field
quantifies the properties of the light rays at each position in space
(Equation [\[eqn:lightfield\]](#eqn:lightfield){reference-type="ref"
reference="eqn:lightfield"}). Each ray travels from a location (x,y,z)
in a direction ($\alpha,\beta$) and has a wavelength and polarization
($\lambda,\rho$). To know these parameters and the intensity of every
ray is to know the light field at a given moment in time:

$$LF(x,y,z,\alpha,\beta,\lambda,\rho) .
\label{eqn:lightfield}$$

The light field representation does not capture some phenomena of
electromagnetic radiation such as interference (waves) or the Poisson
character of light absorption by the photoreceptors (photons). Even so,
the light field representation provides an excellent model to describe
the ways in which light interacts with surfaces, and the geometric
description of the light field is important in the mathematics of
computer graphics, a technology that is important for illumination
engineering, photography and cinema
[@Pharr2016-yb; @Wald2003-vz; @Wald2006-ej].

### The incident light field

An eye - or a camera - records the small subset of the light field,
those rays arriving at the pupil or entrance aperture. We call these the
incident light field. In Figure
[1.1](#fig:lightfield){reference-type="ref" reference="fig:lightfield"}
the dashed and solid lines are the light field and the solid lines are
the incident light field. The natural parameterization of the incident
light differs from the general light field. We can represent the
incident light field using only the position ($u,v$) and angle
($\alpha,\beta$) of the rays at the entrance aperture of the imaging
system:

$$ILF(u,v,\alpha,\beta,\lambda,\rho,t) .
\label{eqn:ilf}$$

Equation [\[eqn:ilf\]](#eqn:ilf){reference-type="ref"
reference="eqn:ilf"} also represents time ($t$) explicitly, which allows
it to describe effects of motion both in the scene and by the eye.

### Spectral irradiance and the plenoptic function

The eye and most cameras do not measure the incident light field.
Rather, the rays are focused to form an image and the photodetectors
respond to the sum of the rays across all directions. Adelson and Bergen
[-@Adelson1991Plenoptic] introduced the term plenoptic function, a
simplified version of the incident light field, that was chosen to guide
thinking about the computations carried out in the human visual pathways
(their Equation 2). First, they approximated the eye as a pinhole
camera; with this approximation all rays have the same entrance position
$p$, and the retinal/sensor surface faces the scene in a single
direction $d$ through the pinhole. For pinhole optics, specifying the
two angles of a ray at the pinhole is equivalent to specifying the
location where the ray intersects the retinal/sensor surface,
$(r_x,r_y)$. Finally, Adelson and Bergen ignored polarization as
unimportant for human perception. With these restrictions, the plenoptic
function for human vision is no more than the retinal spectral
irradiance, over time (t), with the added parameters defining the
pinhole position (p) and the viewing direction (d):

$$E(r_x,r_y,\lambda,t,p,d).$$

Understanding the progression from light field to incident light field
to plenoptic function/retinal image is useful for understanding how the
information available for visual processing relates to the complete set
of potential information that could be sensed by a visual system.

Adelson and Bergen note that by placing the pinhole at many different
positions and viewing directions, we can estimate the full light field
from the set of spectral irradiances. Thus an array of cameras can
return an estimate of the light field. Similarly, a way to estimate the
incident light field is to use a lens and insert a microlens array over
the photodetector array, placing multiple detectors behind each
microlens. Both cameras and microscopes have using this technology to
measure the incident light field, not just the spectral irradiance.
These specialized devices support depth estimation [@Adelson1992-bt] and
they enable controlling focus and depth of field in post-processing
[@Ng2005-yh].

Such general purpose light-field cameras are not currently in wide use
[@Wikipedia_contributors2021-hz], but many modern cameras do acquire
partial information about the incident light field. Specifically, dual
pixel autofocus technology is designed to distinguish the light rays
incident at different sides of the entrance aperture
[@Canon_USA_Inc2017-xx; @Mlinar2016-ev]. This is accomplished by
inserting a microlens array over pairs of photodetectors, so that rays
from the left and right sides of the lens are captured by different but
adjacent detectors. The images from rays arriving from the two sides of
the lens are compared and used to set the lens focus or estimate depth.

### The initial visual encoding

Computational models of the early visual pathways define a series of
transformations that characterize how the incident light field becomes a
neural response. In this chapter, we introduce the mathematics used to
characterize the initial visual encoding in the context of the first few
of these transformations (Figure
[1.2](#fig:computations){reference-type="ref"
reference="fig:computations"}; see also @Wandell1995Foundations,
[-@Wandell1995Foundations];
@Rodieck1998FirstSteps,[-@Rodieck1998FirstSteps];
@Brainard2010Colorimetry, [-@Brainard2010Colorimetry]; @Packer2003Light,
[-@Packer2003Light]). We focus on the encoding of the spectral radiance
by the photoreceptors - this encoding provides the sensory measurements
upon which subsequent neural processing operates.

A visual scene's light field is generated by the properties and
locations of the light sources and objects, and how the rays from the
light sources are absorbed and reflected by the objects. Here we
consider the special case of scenes presented on a flat display, so that
in the idealized case where the display is the only object and there are
no other light sources, the full light field is determined just by the
spectral radiance emitted at each location of the display. Elsewhere, we
consider the more general case of modeling the formation of the retinal
spectral irradiance, given a description of the light sources and
objects in a three-dimensional scene [@Lian2019Ray].

The optics of the eye collect the incident light field and focus the
rays to produce the spectral irradiance arriving at the retina. Factors
such as diffraction and aberrations in the eye's optics mean that this
image is blurred relative to the displayed image. In addition,
wavelength-selective absorption of short-wavelength light by the lens
and and inert macular pigment also affect the spectral irradiance. Of
note (but not illustrated in Figure
[1.2](#fig:computations){reference-type="ref"
reference="fig:computations"}) the density the macular pigment is high
in the central area of the retina and falls off rapidly with increasing
eccentricity.

Photoreceptors spatially sample the retinal image. Excitations of
photopigment molecules in these photoreceptors provides the information
available to the visual system for making perceptual inferences about
the scene. Here we consider the cone photoreceptors, which operate at
light levels typical of daylight. There are three spectral classes of
cones, each characterized by its own spectral sensitivity. That there
are three classes leads to the trichromatic nature of human color
vision. Figure [1.2](#fig:computations){reference-type="ref"
reference="fig:computations"} illustrates a patch of cone mosaic from
the central region of the human retina. The properties of the mosaic are
quite interesting. For example, there are no S cones in the very center
of the retina, and many properties of the mosaic (e.g., cone density,
cone size, cone photopigment optical density) vary systematically with
eccentricity [@Hofer2014Color; @Brainard2015Color].

![**The initial encoding of light by the visual system**. *Scene*: An
image on a display surface is characterized by the spectral radiance at
each display location. Images of the display spectral radiance are shown
at a few sample wavelengths, along with a rendering of the image.
*Optics*: The incident light field enters the pupil of the eye and a
spectral irradiance image is formed on the retina. The *retinal image*
is blurred relative to the displayed image, and the spectral irradiance
is affected by lens and macular pigment absorptions. *Cone mosaic*: The
retinal image is spatially sampled by the L, M and S cone mosaics. *Cone
excitations*: The retinal image irradiance, spectrally weighted by each
cone photopigment absorptance function, is integrated within the cone's
aperture and temporally integrated over the exposure duration to produce
a pattern of cone excitations. We thank Nicolas Cottaris for the
figure.](computations.pdf){#fig:computations width="12cm"}

Not considered here is a separate mosaic of highly-sensitive rod
photoreceptors that is interleaved with the cone mosaic. The rods
mediate human vision at low light levels [@Rodieck1998FirstSteps]. We
also ignore the melanopsin containing intrinsically-sensitive retinal
ganglion cells
[@Hattar2002Melanopsin; @Van_Gelder2016-dm; @Gamlin2007Human]. The
principles we develop, however, also apply to modeling the excitations
of these receptors.

Computational modeling of the initial linear encoding is
well-understood, and we explain these mathematical principles next,
using a simplified representation of the light stimulus. Advanced
modeling of the subsequent neural processes includes non-linearities,
the mathematical principles and computational methods we introduce are a
fundamental part of the full description. After explaining the
mathematical principles, we illustrate how to extend them through
computational modeling that harnesses the power of computers to
characterize biological reality in more detail than is possible with
analytic calculations alone.

## Mathematical principles

### Linear systems

Linear systems and the tools of linear algebra are the most important
mathematical methods used in vision science. Indeed, when trying to
characterize a system, the scientist and engineer's first hope is that
the system can be approximated as linear. A system, $L$, is linear if it
follows the superposition rule: $$L(x + y) = L(x) + L(y).$$ Here $x$ and
$y$ are two possible inputs to the system and $x+y$ represents their
superposition. The homogeneity rule of linear systems follows from the
superposition rule. Consider that

$$\begin{aligned}
L(x + x) &= L(2x) \nonumber  \\ 
&= L(x) + L(x) \nonumber \\
&= 2L(x). \nonumber
\end{aligned}$$

This is easily generalized for any integer $m$ to show that:[^4]

$$L(m x) = m  L(x).$$

No physical system can be linear over an infinite range - if you put
enough energy into a system it will blow up! But many systems are linear
over a meaningful range of input values.

### Linearity example: Cone excitations and color matching

Vision is initiated when a photopigment molecule absorbs a photon of
light. The absorption can cause the photopigment, a protein, to change
conformation, an event we refer to as a photopigment excitation. The
excitation initiates a molecular cascade inside the photoreceptor that
changes the ionic currents at the photoreceptor membrane. The change in
current modulates the voltage at the photoreceptor's synapse and causes
a release of neurotransmitter [@Rodieck1998FirstSteps].

The transformation from the spectral energy of light, $E(\lambda)$,
incident upon a cone to the number of photopigment excitations, $n$,
produced by that light is an important, early vision, linear system.
Consider two different spectra, denoted by $E_1(\lambda)$ and
$E_2(\lambda)$. Let $L$ represent the system that describes the
transformation between spectra and excitations. This system obeys the
superposition rule: $$\begin{aligned}
L(E_1(\lambda) + E_2(\lambda)) &= L(E_1(\lambda))  + L(E_2(\lambda)) 
\label{eq:linearity}
\end{aligned}$$ This linearity holds well over a wide range of light
levels typical of daylight natural environments [@Burns1987-kt].

An important feature of photopigment excitations is that their effect on
the membrane current and transmitter release does not differ with the
wavelength of the exciting photon. Such differences might have existed
because different wavelengths are preferentially absorbed at different
locations within the cone outer segment, or because photons of different
wavelengths carry different amounts of energy. The observation that all
excitations have the same impact is called the Principle of Univariance.
As Rushton wrote:

> "The output of a receptor depends upon its quantum catch, but not upon
> what quanta are caught [@Rushton1972Pigments]."

The color-typical human retina contains three distinct classes of cones,
which are referred to as the L (long-wavelength sensitive), M
(middle-wavelength sensitive) and S (short-wavelength sensitive) cones.
While the effects of photopigment excitations are univariant, the
probability of a photopigment excitation is wavelength-dependent. The
wavelength-dependent probability that an incident photon leads to an
excitation is characterized by the pigment's spectral absorbtance[^5]
The absorbtance depends on the specific density of the photopigment
within the cone's outer segment, as well as on the outer segment length;
details are elaborated elsewhere (@Rodieck1998FirstSteps,
[-@Rodieck1998FirstSteps]; see also @Pugh1988Vision, [-@Pugh1988Vision];
@Packer2003Light, [-@Packer2003Light]).

It is difficult to measure the light at the retinal surface in the
living eye, but it is straightforward to measure the light incident at
the cornea. Hence, it is typical to specify the absorptance with respect
to the spectrum of the light incident at the cornea. This convention
effectively combines the effects of absorption of light by the lens and
the inert retinal macular pigment with absorption by photopigment. The
cornea-referred absorbtance function is called the cone fundamental.

The three (L-, M-, and S-) cone fundamentals define for each cone type
the probability of excitation given the spectrum of light entering the
eye. The human cone fundamentals have been carefully measured and
tabulated (Figure [1.3](#fig:fundamentals){reference-type="ref"
reference="fig:fundamentals"}; @Stockman1999SpectralShort,
[-@Stockman1999SpectralShort]; @Stockman2000SpectralLongMiddle,
[-@Stockman2000SpectralLongMiddle]; http://www.cvrl.org) and are an
international standard [@CIE2007Fundamental].

To compute the number of cone excitations we use linear formulas.
Suppose that a cone's fundamental is given by $C(\lambda)$. Using
linearity and continuous mathematics, we compute the number of
excitations at a single location as

$$N(r_x,r_y) = \int C(\lambda) E(r_x,r_y,\lambda) d\lambda .$$

The discrete form of this integral, commonly used in computational
methods, is the inner product of the cone fundamental with the
cornea-referred spectral irradiance incident upon a retinal
location:[^6]

$$N(r_x,r_y) = \sum_{\lambda_i} C(\lambda_i) E(r_x,r_y,\lambda_i) \Delta \lambda .$$

Here the $\lambda_i$ are a set of $w$ discretely sampled wavelengths,
and $\Delta \lambda$ is the wavelength sample spacing.

### Matrix formulation of linearity

We can calculate cone excitations by a matrix multiplication. The matrix
$\mathbf{C}$ combines the three discretized cone fundamentals,
$C_L(\lambda_i)$, $C_M(\lambda_i)$, and $C_S(\lambda_i)$ into its rows,
so that its dimension is $3 \times w$. Similarly, we write the spectral
irradiance at a position, $E(r_x,r_y,\lambda)$, as a $w \times 1$ vector
$\mathbf{e}(r_x,r_y)$. The L-, M-, and S-cone excitations available at a
retinal location are described by a $3$-dimensional column vector,

$$\mathbf{n}(r_x,r_y) = \mathbf{C} \mathbf{e}(r_x,r_y).
\label{eqn:conematrix}$$

The vector field $\mathbf{n}(r_x,r_y)$ describes the potential
information available to the visual system from the cones at a moment in
time. This representation replaces the dependence of the spectral
irradiance on wavelength with the excitations of the three classes of
cones. As we describe in more detail below, not all of this potential
information is sensed by the visual system, since the cones discretely
sample $\mathbf{n}(r_x,r_y)$.

It is worth reflecting on the implication of the linearity expressed by
Equation [\[eqn:conematrix\]](#eqn:conematrix){reference-type="ref"
reference="eqn:conematrix"}. If we measure the cone fundamentals at each
of the sample wavelengths $\lambda_i$, we can predict the cone
excitations to any spectrum $E(r_x,r_y,\lambda_i)$. Thus, linearity
implies that we can compute the system response to any input after
making enough measurements to determine the system matrix $\mathbf{C}$.
The ability to delineate the set of measurements required for complete
system characterization is an important consequence of linearity, and
this observation applies to linear systems in general, not just to
computation of cone excitations.

A second implication of Equation
[\[eqn:conematrix\]](#eqn:conematrix){reference-type="ref"
reference="eqn:conematrix"} concerns which spectral radiances appear to
be the same. This was first established quantitatively by the color
matching experiment, which dates back to James Clerk Maxwell
[-@Maxwell1860OnTheTheory]. He built an instrument to measure pairs of
spectral irradiance functions, $\mathbf{e_1}$ and $\mathbf{e_2}$, that
appear the same to humans despite being physically different; these
pairs are called metamers. Thomas Young [-@Young1802OnTheTheory] had
proposed that metamers arise if two lights produce the same set of cone
excitations. This implies that the difference between a metameric pair
is in the null space[^7] of the matrix, $\mathbf{C}$. That is,
$\mathbf{e_1}$ and $\mathbf{e_2}$ must satisfy

$$\begin{aligned}
\mathbf{C} \mathbf{e_1} &= \mathbf{C} \mathbf{e_2}  \nonumber \\
\mathbf{0} &= \mathbf{C}(\mathbf{e_2} - \mathbf{e_1}).
\end{aligned}$$

Wyszecki ([-@Wyszecki1958Evaluation]; see also @WyseckiStiles1982Book,
[-@WyseckiStiles1982Book]) referred to vectors in the null space of
$\mathbf{C}$ as metameric black spectra. Adding a metameric black to any
spectrum produces a metamer.

Displays and printers do not reproduce the original physical stimulus;
rather, they create lights that aspire to be metamers to the original.
Thus, calculation of metamers is central to color reproduction
technologies; knowledge of the matrix $\mathbf{C}$ is sufficient to
compute metamers. Although the dimensionality of the space of metamers
is quite high ($w-3$), metamers produced by devices are constrained by
the spectra that the device can produce. The computation of metamers for
color reproduction applications is discussed in detail elsewhere
(@Hunt2004Reproduction, [-@Hunt2004Reproduction];
@Brainard2010Colorimetry, [-@Brainard2010Colorimetry]).

Notice that the null space of $\mathbf{C}$ is the same as the null space
of $\mathbf{T} = \mathbf{M C}$, for any invertible $3 \times 3$ matrix,
$\mathbf{M}$. Thus any such matrix $\mathbf{T}$ may be used to predict
physically different lights that match perceptually. The rows of
$\mathbf{T}$, when viewed as functions of wavelength, are referred to as
a set of color matching functions. The technology for creating metamers
relies on standardized color-matching functions
[@CIE1986Colorimetry; @CIE2007Fundamental]. How color matching functions
may be obtained directly from perceptual color matching experiments,
without explicit reference to the cone fundamentals, is treated in many
sources
[@WyseckiStiles1982Book; @Wandell1995Foundations; @Brainard2010Colorimetry].
Indeed, high-quality measurements of behavioral color matching (e.g.
@Stiles1959NPL, [-@Stiles1959NPL]) provide key data that constrain
modern estimates of human cone fundamentals.

There are a number of properties of the eye that must be modeled if we
are to compute a true estimate of cone mosaic excitations. For example,
only one type of cone is present at each position, so we must specify a
cone spatial sampling scheme. That is the reason that we use the term
*potential information* to describe cone excitations
$\mathbf{n}(r_x,r_y)$ as a function of retinal location - not all of
that information is sampled by the cone mosaic. Also, as noted above,
the density of both inert pigments and photopigments varies with retinal
location, as does the size of the cone apertures. Enough is known about
these properties to enable us to compute a reasonable approximation to
the cone mosaic excitations across the retina.

<figure id="fig:fundamentals">
<img src="Fundamentals.png" style="width:12cm" />
<p>. <span id="fig:fundamentals"
data-label="fig:fundamentals"></span></p>
<figcaption><strong>Human cone fundamentals</strong>. The left panel
shows estimates of the L-, M- and S-cone fundamentals for foveal
viewing. The fundamentals are the probability of excitation per photon
entering the cone’s entrance aperture, but with pre-retinal absorption
taken into account. Note the large difference between the L- and M-cone
fundamentals compared to the S-cone fundamental. This difference is due
partly to the selective absorption of short-wavelength light by the lens
and macular pigment. The right panel shows estimates for cones at 10°
eccentricity. The S-cone fundamental is relatively higher at 10°,
because there is little or no macular pigment at that eccentricity; and
for the same reason there is a slight change in the relative values of
the L- and M-cone fundamentals. In addition, the cone outer segment
lengths decrease with eccentricity, leading to the lower peak
probability of excitation in the periphery. This reduction, however, is
more than compensated for by an increase in the size of the cone
apertures with eccentricity. The impact of the aperture is not shown in
these plots, but see Figure <a href="#fig:noise"
data-reference-type="ref"
data-reference="fig:noise">1.4</a></figcaption>
</figure>

### Noise in the sensory measurements

Measurement noise is fundamental in the physical sciences and
engineering. Two types of noise are used throughout the sensory
sciences: Gaussian (Normal) noise and Poisson noise. Gaussian noise has
two parameters (a mean and variance) but the Poisson distribution has a
single parameter (the Poisson mean equals its variance). The formulas
for Gaussian and Poisson noise, along with example probability
density/mass functions, are shown in Figure
[1.4](#fig:noise){reference-type="ref" reference="fig:noise"}.

The Gaussian and Poisson distributions can be compared by setting the
Gaussian mean equal to its variance. For small values, the Gaussian has
values below zero. As the Poisson mean increases, the matched Gaussian
is extremely similar (Figure [1.4](#fig:noise){reference-type="ref"
reference="fig:noise"}).

There is an important conceptual difference between how these noise
distributions are used in applications. There are many theorems about
additive Gaussian noise, and thus it is common to introduce noise in a
model with such noise using a fixed mean ($\mu$) and standard deviation
($\sigma$). The added noise has the the same distribution for all values
of the signal (signal-independent noise).

For typical sensor measurements, including the cone excitations, the
noise depends on the signal. Specifically, for the cones and many other
measurement devices, the noise is Poisson distributed and the Poisson
parameter is equal to the mean number of excitations (signal-dependent
noise). The difference between signal-independent and signal-dependent
noise can be quite significant
(Figure [1.4](#fig:noise){reference-type="ref" reference="fig:noise"}).

::: center
![**The number of cone excitations is inescapably noisy, following a
signal-dependent Poisson distribution**. (Top). For mean values greater
than 10, the Poisson distribution is reasonably approximated by a
Gaussian distribution with a mean equal to the variance. For smaller
values, it is necessary to clip the negative values for the Gaussian to
achieve a good approximation. Low excitation rates are common under
low-light conditions and for nearly all conditions when assessing the
S-cones and rods [@Hecht1942Energy; @Baylor1979Responses]. (Bottom) The
signal-dependent nature of Poisson noise is important; simply adding
Gaussian noise with a fixed mean is not a good approximation if there is
a substantial range in the mean excitation values. The images illustrate
the excitations in response to a series of bars spanning a large range
of mean excitation using a signal-independent clipped Gaussian noise
(left) and a Poisson noise (right). The Gaussian distribution added to
the signal has zero mean and variance equal to the number of excitations
in the brightest bar (blue arrows); this approximates Poisson noise for
that bar. The inset trace, which shows excitations across a row of the
image, illustrates that the Gaussian noise is far too large for the dark
bars. Had the variance been set to match the noise at the dark bar, the
clipped Gaussian would be far too small for the brightest bar. The
simulation was created for an array of M-cones in the central fovea, a 2
ms exposure duration, achromatic bars of increasing intensity, and the
brightest bar with a luminance of 300 cd/m^2^.](noise.png){#fig:noise
width="12cm"}
:::

### Image formation

The linear system principles described for one-dimensional spectral
functions can be extended to two-dimensional functions, such as images.
We use linear system methods to analyze how the cornea and lens form the
retinal image. An important, but simple, case occurs for an image
confined to a plane, such as a visual display or an optometrist's eye
chart. For such images we can estimate the spectral irradiance at the
cone apertures using a two-dimensional linear system computation.

The image emitted from a visual display is a function of position
$(x,y)$ and wavelength $\lambda$. As a first approximation, the display
emits the same density of rays over a wide angle, which is why the
display appears to be approximately the same when seen from different
positions. The image from the display is called the spectral radiance,
$I(x,y,\lambda)$, and it has units of $W/sr/{m^2}/nm$.

The spectral irradiance at the retina, $E(r_x,r_y,\lambda)$, is formed
from the cone of rays that are captured by the pupil. In this case,
$r_x$ and $r_y$ specify retinal location and the units of the image are
those of spectral irradiance, $W/m^2/nm$, which result from integration
over the solid angle of the pupil.

The key linear system idea
(Equation [\[eq:linearity\]](#eq:linearity){reference-type="ref"
reference="eq:linearity"}) holds for retinal image formation
[@Wandell1995Foundations]. If two input images $I_1(x,y,\lambda)$ and
$I_2(x,y,\lambda)$, produce two retinal images $E_1(r_x,r_y,\lambda)$
and $E_2(r_x,r_y,\lambda)$, then the superposition of the input images,
$I_1(x,y,\lambda) = I_1(x,y,\lambda) + I_2(x,y,\lambda)$, produces the
superposition of the retinal images,

$$E(r_x,r_y,\lambda) = E_1(r_x,r_y,\lambda) + E_2(r_x,r_y,\lambda).$$

It follows that if the input image is the weighted sum of two input
images,
$I(x,y,\lambda) = \alpha I_1(x,y,\lambda) + \beta I_2(x,y,\lambda)$, the
output retinal image will be the weighted sum of the two corresponding
retinal images
$$E(r_x,r_y,\lambda) = \alpha E_1(r_x,r_y,\lambda) + \beta E_2(r_x,r_y,\lambda)$$

As noted above, an important consequence of linearity is that it tells
us how to generalize. When we know the response to an image $I_k$,
measuring the response to a second image, $I_j$, enables us to predict
the responses to an entire class of new images, all images of the form
$\alpha I_k + \beta I_j$.

### Shift-invariance and convolution

We saw in our treatment of color matching (Equation
[\[eqn:conematrix\]](#eqn:conematrix){reference-type="ref"
reference="eqn:conematrix"}) that a discrete linear system may be
characterized as a matrix multiplication. This is also true for retinal
image formation, but in this case the number of measurements required to
determine the requisite matrix is very large for typical image sizes.
For this reason, we consider an additional special and simplifying
property linear systems can have: shift-invariance. These are linear
systems such that shifting the position of the input stimulus
correspondingly shifts the position of the output, without changing its
form[^8]. It is possible to measure whether a system is shift-invariant
by a simple experiment. Take an input image, say $I(x,y,\lambda)$, and
measure the retinal image $E(r_x,r_y,\lambda)$. Then shift the input,
$I(x-\delta x, y- \delta y,\lambda)$ and measure the retinal image
again. If for all choices of $I(x,y,\lambda)$ and $\delta$, the output
corresponding to $I(x-\delta x, y- \delta y,\lambda)$ is a shifted
version of $E(r_x,r_y,\lambda)$,
$E(r_x-\delta r_x, r_y- \delta r_y, \lambda)$, then the system is
shift-invariant.

We can express linearity and shift-invariance using the convolution
formula. For simplicity, we choose one wavelength and suppress
$\lambda$. Suppose $P(r_x,r_y)$ is the retinal image from an image that
is just a single point. The image $P(r_x,r_y)$ plays a central role in
the characterization of optical systems: it is called the point spread
function.[^9] The point spread function is all we need to compute the
retinal image for any input image. The idea is to treat the input image
as a set of points, and to add shifted copies of the point spread
function, each weighted by the input image intensity:

$$E(r_x,r_y) = \int_u \int_v I(u,v)P(r_x-u,r_y-v) du dv .$$

The importance of linear shift-invariance is that we characterize the
system fully by one measurement, $P(r_x,r_y)$. We use the convolution
formula and this measurement to compute the responses of a linear
shift-invariant system to any input.

While shift-invariance and convolution are important concepts, the eye's
optics deviates significantly from this ideal. Shift-invariance is a
good approximation of human retinal image formation in local regions,
say spanning a few degrees of visual angle and a change in wavelength of
20-50 nm. Properties of the photoreceptor sampling mosaic further limit
the accuracy of the shift-invariant approximation of the visual encoding
(see Figure [1.6](#fig:sampling){reference-type="ref"
reference="fig:sampling"}). Thus the convolutional approximation is
helpful for thinking about encoding over small regions, but it is not an
accurate depiction when one considers a larger field of view. A
realistic approximation requires computational modeling.

## Computational model of the initial encoding

The mathematical principles described above tell us how to compute the
retinal image and the noisy cone excitations from a displayed image; the
calculations are straightforward for a single retinal location. But an
accurate model of the visual system must account for variations in the
optics, pigments, and sampling properties of the cone mosaic with visual
field location. These are substantial and impact the information
available to the brain for making perceptual inferences about the visual
scene. Parameters with significant spatial variation across the visual
field include the optical point spread function, density and size of the
cones in the mosaic, the distribution of different cone types within the
overall mosaic, and the cone fundamentals. To make a realistic
calculation requires implementing a computational model of the visual
transformations.

### The value of computational modeling

Carefully validated computer simulation of the initial visual encoding
has the potential to support advances in understanding many aspects of
visual function. We use image-computable models to build upon the
mathematical characterizations - earned through 400 years of
experimental and theoretical work in vision science - and estimate the
initial visual signals. Such knowledge is an essential foundation to use
when modeling less well understood visual processes. The models help us
separate effects attributable to known factors of the initial encoding
from effects of factors that arise in later processing. For example,
understanding cortical visual processing requires representing the input
to the cortex. Without accurate modeling of the input, we risk
attributing features of the cortical signals to the wrong neural
mechanisms.

Because of the central role computational modeling plays in
understanding vision, we have invested in developing a set of freely
available software tools to model retinal image formation and cone
excitations (Image Systems Engineering Tools for Biology - ISETBio;
https://github.com/isetbio/isetbio.git; @Cottaris2019AComputational,
[-@Cottaris2019AComputational]; @Cottaris2020AComputational,
[-@Cottaris2020AComputational]). The tools can be used for images
presented on planar displays and for a full three-dimensional
descriptions of the objects and light sources in the scene (Image
Systems Engineering Tools 3D; https://github.com/iset/iset3d.git;
@Lian2019Ray, [-@Lian2019Ray]). In this section we briefly illustrate
some basic calculations enabled by ISETBio. We are not advocating
uniquely for our implementation, but we do believe that the field needs
to develop trusted open-science tools for computational modeling.

### Shift-varying and wavelength-dependent pointspreads

The point spread functions from a single subject, measured at different
retinal locations and wavelengths, differ significantly
(Figure [1.5](#fig:pointspread){reference-type="ref"
reference="fig:pointspread"}). The variation with retinal location
occurs because the optical aberrations depend on the direction of the
rays incident at the retina. The ISETBio tools explicitly represent the
full incident light field and can calculate these effects from a model
eye [@Lian2019Ray]. Improvement of eye models is an active area of
investigation, and in some cases ISETBio uses empirical measurements of
the eye's optics to estimate the point spread function for a range of
retinal field locations [@Jaeken2012-bc; @Polans2015-de].

The point spread function varies with pupil diameter and wavelength in
addition to visual field position. The dependence on pupil diameter,
which varies with the light level of the scene, occurs for two reasons.
As the pupil opens the aberrations vary because more of the
imperfectly-shaped corneal and lens surfaces refract the light. As the
pupil closes, diffraction starts to be a significant factor. The
wavelength dependence is explained by the refractive indices of the
cornea and lens. These chromatic aberrations are the largest of all the
aberrations [@Thibos1990-sq; @Wandell1995Foundations].

![**The human point spread function**. The diagram at the left shows
simple ways to estimate degrees of visual angle: Hold your hand at arms
length and make the gesture. The images in the top row show the point
spread functions at 550 nm from a typical subject measured at three
different visual eccentricities. The point spread increases with
eccentricity. The bottom images show the point spread but for light at
450 nm. The human eye cannot focus these two wavelengths at the same
time because the index of refraction in the lens and cornea is
wavelength-dependent. For many people, chromatic aberration is the
largest aberration. Diagram on the left reproduced from @Branwyn2016-sc
[-@Branwyn2016-sc].](pointspread.png){#fig:pointspread width="12cm"}

### Shift-varying sampling

Figure [1.6](#fig:sampling){reference-type="ref"
reference="fig:sampling"} shows the spatial arrangement of cones at
different locations within the retina. The cone density is highest in
the central fovea where the cones are tightly packed. Moving away from
the center, cone density falls off and the cone apertures become larger.
As cone density decreases, rod photoreceptors (the smaller receptors in
the peripheral images) appear and fill the gaps between the cones. In
addition, not apparent in the figure, cones become shorter away from the
fovea. The shortening affects the spectral absorptance.

The sampling density reduction means that less spatial information about
the retinal image is extracted at retinal locations away from the
central fovea. The relative density of the different cone types also
varies with eccentricity. Indeed, as noted above, there are no S-cones
in the very central fovea [@Williams1981Foveal], so that vision in this
small retinal region is dichromatic rather than trichromatic. Perhaps
this region is specialized for high-resolution vision and omitting a few
S-cones, which see a blurry retinal image at short wavelengths because
of the chromatic aberrations, maximizes the information transmitted to
the brain about spatial structure
[@Williams1991Cost; @Hofer2014Color; @Brainard2015Color; @Garrigan2010Design; @Zhang2021Reconstruction].

![**Human cone and rod sampling mosaics**. The en face images show the
photoreceptor inner segments, where light enters the cones, at four
retinal eccentricities. In the central region, all of the receptors are
cones. At 4 deg and beyond, the large apertures are the cones and the
smaller apertures are the rods. The cone sampling density and cone
aperture sizes differ substantially between the central fovea and other
visual eccentricities. The reduced sampling density limits the spatial
resolving power of the eye. The larger cone apertures increase the rate
of photon excitations per cone. Scale bar is 10 um. Reproduced from
Curcio [-@Curcio1990Photoreceptor].](sampling.png){#fig:sampling
width="12cm"}

The impact of the cone size and density, along with variations in the
inert pigments described above, mean that calculating the cone
excitations is shift-varying: The calculation is linear, but the
parameters change with eccentricity. These eccentricity-dependent
calculations are included in the ISETBio simulations. There is little
value in expressing the full complexity of these calculations in pure
mathematical form.

The impact of the several eccentricity-dependent factors on the cone
excitations is substantial and illustrated in
Figure [1.7](#fig:excitations){reference-type="ref"
reference="fig:excitations"}. The images in the left column illustrate
calculations in central fovea and the images in the right column
illustrate the same calculations at 10 deg in the periphery. The top
image shows the differences in the size and density of the cone
photoreceptor apertures. Also, notice the absence of S-cones in the
small region of the very central fovea. The images inset in the top show
the size of the point spread function for an infocus wavelength: there
are many more cones within the foveal point spread than within the 10
deg point spread.

The images in the middle row represent the number of cone excitations in
response to a relatively low frequency grating pattern. There are more
excitations per cone at 10 deg than in the fovea, and there are many
more cones representing the stimulus in the fovea. The third row shows
the effect of increasing the stimulus spatial frequency. The foveal
mosaic samples densely enough to preserve the regular pattern, but at 10
deg the spatial samples look like a wobbly form of the stimulus.

![**Excitation calculations**. The two columns represent two retinal
eccentricities, each about 1 deg^2^. (Top): The interleaved L, M and S
cone mosaics are shown as red, green and blue dots, are shown at the
top. The inset shows an expanded view of the point spread function in
the same region. The rods are not represented. (Middle and Bottom): The
gray level n these images shows the estimated cone excitations for a 6
c/deg harmonic and a 12 c/deg harmonic. The scale for the foveal
location runs between 0 and 250, while that for the peripheral location
runs from 0 to 1000. Peripheral cones have more excitations to the same
stimulus because the cone apertures are larger. Courtesy of Nicolas
Cottaris.](excitations.pdf){#fig:excitations width="12cm"}

Finally, notice that there are many cones that have a very low
excitation level to this achromatic stimulus. These cones appear as the
quasi-regular array of black dots that are easy to see at 10 deg. They
are also present, but harder to see in excitations for the central
location. These cones are the S-cones, which absorb many fewer photons
than the L- and M-cones. This lower excitation rate is partly due the
spectral transmission of the lens (and in the central region the macular
pigment), which absorbs a great deal of short-wavelength light.

In summary, the principles of linearity and shift-invariance are useful
guides for thinking about cone excitations. These principles were part
of our toolkit as we built a specific model of the human eye, and so
they would be for any model. However, the deviations from
shift-invariance in the human eye are substantial. In addition, there
are significant differences between people that may be important for
explaining between subject differences. For these reasons, a
computational model is essential for applications that aim to create
realistic estimates of the cone excitations.

Finally, an essential ingredient for building a computational model is
data sets that quantify the critical model parameters (e.g. how the
optical PSF and cone density vary with visual field position) and how
these parameters are likely to vary across individuals. The
computational implementation has benefited enormously from data
collected and shared by many investigators. Conversely, the exercise of
building computational models often highlights the need for data sets
that do not yet exist (e.g. across individuals, do are optical quality
and cone density independent, or do they covary in some systematic
way?).

At this point in the chapter, the reader might find it useful to re-read
the quote at the start of this chapter which was written by Von Kries,
Helmholtz' greatest disciple [@Cahan1993-mm], more than a century ago.

### Spatial derivatives of the cone excitations mosaic

In addition to describing the plenotpic function, Adelson and Bergen
[-@Adelson1991Plenoptic] observed that the partial derivatives of the
spectral irradiance correspond to computations performed by neurons in
the early visual system.
Figure [1.8](#fig:derivativeslater){reference-type="ref"
reference="fig:derivativeslater"} illustrates these derivatives for
several cases that they highlight: derivatives with respect to spatial
position, wavelength, and viewpoint (i.e. across the viewpoints provided
by the left and right eyes). Receptive fields that respond to these
derivatives include neurons that are pattern-selective
[@shapley1985spatial; @priebe2016mechanisms], cone-opponent
[@Solomon2997Machinery; @Shevell2017Color], and stereo
disparity-selective [@cumming2001physiology]. Partial derivatives with
respect to time describe motion-selective neurons
[@wei2018neural; @pasternak2020linking].

The emphasis that Adelson and Bergen [-@Adelson1991Plenoptic] place on
derivatives is consistent with the generally-accepted idea that it is
the local change (contrast) in the spectral irradiance, not the absolute
level of that irradiance, that provides the critical information used
for perception [@Shapley1986Importance]. Later in the chapter, we
analyze psychophysical measurements of contrast sensitivity, which
characterize quantitatively how small changes in spatial contrast are
encoded by human vision.

An additional advantage of representations based on derivatives is that
they are a highly compressible representation of naturally occurring
spectral irradiance. The reason for this is that natural radiances tend
to vary slowly, and thus many of the partial derivatives are small. A
distribution with many small values may be compressed by coding the
small values with tokens specified with a small number of bits,
reserving tokens specified with a large number of bits for rarely
occurring larger values [@Cover1991Elements; @Wandell1995Foundations].

![**Derivatives of the retinal image**. A scene (top) is represented as
spectral irradiance hypercubes for the left and right eye. The responses
of neurons that compute the local differences, as indicated by several
oval pairs with $\pm$, approximate local partial derivatives.
Differences can be taken across spatial location, across wavelength,
across the spectral radiance measured by the two eyes, and across time
(not shown). The original color image was kindly provided by David
Sparks.](derivatives.png){#fig:derivativeslater width="12cm"}

## Perceptual inference

### Ambiguity and Perceptual Processing

An important and consistent take-away from the analysis of sensory
encoding is that the information available to the brain about the state
of the external world is ambiguous: many different physical
configurations produce the same sensory representation. A classic
example is metamerism: There are only three classes of cone
photoreceptors and different spectra produce identical triplets of
responses in the L, M and S cones. Another well-known example is depth
reconstruction: The three spatial dimensions of the light field are
projected onto a two dimensional retina, and many 3D shapes produce the
same retinal image. Such many-to-one mappings are a reason why Helmholtz
([-@Helmholtz1866Handbuch]; [-@Helmholtz1896Southall]) emphasized
perceptual inference: The brain decodes the sensory representation to
produce perceptions that are a likely guess about the state of the
external world. Perception is an unconscious inference.

### Mathematical principles of inference

The mathematical formulation of perceptual inference can be developed
within a Bayesian probabilistic framework. Suppose $\mathbf{x}$ is a
vector that describes some aspect of a scene. The entries of
$\mathbf{x}$ might represent the spectral power density of a light
entering the eye at a set of discretely sampled wavelengths, the pixel
values of a displayed stimulus image, the optical flow vectors
corresponding to a viewed dynamic scene, or a full 3D scene description
input to a computer graphics package. Now, suppose $\mathbf{y}$ is the
sensory representation at some stage of the visual system produced when
an observer views the scene described by $\mathbf{x}$. The entries of
$\mathbf{y}$ might describe the retinal image, the excitations of each
cone in the retinal mosaic, or the action potentials in a class of
retinal ganglion cells.

Because sensory measurements are noisy, the relation between
$\mathbf{y}$ and $\mathbf{x}$ is described by a conditional probability
distribution, $p(\mathbf{y} | \mathbf{x})$. This distribution is
referred to as the likelihood function. The likelihood can be estimated
using a forward model that relates the scene parameters $\mathbf{x}$ to
the sensory representation $\mathbf{y}$.

Within the Bayesian framework, the perceptual representation results
from a choice the brain makes about the most likely scene given the
observed sensory representation. Indeed, we can reverse the likelihood
function, $p(\mathbf{y} | \mathbf{x})$, to obtain a conditional
probability distribution $p(\mathbf{x} | \mathbf{y})$, which is called
the posterior distribution. The posterior defines which are the more or
less likely scenes, given the sensory measurements. To obtain the
posterior, we use Bayes Rule [@Lee1989Bayeisan; @Bishop2006Pattern]
$$p(\mathbf{x} | \mathbf{y}) = K( \mathbf{y}) p( \mathbf{y} |  \mathbf{x}) p(\mathbf{x}),
\label{eqn:bayesrule}$$

where $K( \mathbf{y})$ is a normalizing factor that depends on
$\mathbf{y}$ but not $\mathbf{x}$. This factor ensures that the
posterior integrates to 1 for any value of $\mathbf{y}$. For many
applications, our interest is in how the posterior depends on
$\mathbf{x}$, and it is not necessary to compute $K(\mathbf{y})$.

Critically, $p(\mathbf{x})$ is a prior distribution that describes the
statistical regularities of the scenes; how likely it is *a priori* that
the world is in the state $\mathbf{x}$. A prior is essential because
many scenes might have produced the same sensory measurements. Bayes
Rule specifies how to combine the prior with the likelihood. Sometimes
little is known about the prior. In these cases, using the Bayesian
formulation directs our attention to learn more about it. The Bayesian
formulation also forces us to make the forward model explicit in the
form of the likelihood.

The posterior is a distribution over possible $\mathbf{x}$. We need a
means of selecting a specific one, say $\hat{\mathbf{x}}$, to generate
the percept. One common way to make a choice is to select a value
$\hat{\mathbf{x}}$ that is most likely: the maximum a posterior (MAP)
estimate. Other possibilities, such as the mean of the posterior, are
also commonly used. The interested reader is referred to the literature
on Bayesian decision theory (e.g., @Berger1985Statistical,
[-@Berger1985Statistical]) for more on this topic.

It is helpful to consider a simple example. Above we explained that the
mean cone excitations at a location are a linear function of the
radiance of a displayed image. Suppose we treat the spectral radiance on
a display as the state of the world $\mathbf{x}$, with the entries of
$\mathbf{x}$ appropriately ordered, and we denote the noisy cone
excitations as $\mathbf{y}$. Then,
$$\mathbf{y} = \mathbf{C} \mathbf{x} + \mathbf{\epsilon}$$

for an appropriately arranged matrix $\mathbf{C}$. and where the noise
in cone excitations is represented by the random variable
$\mathbf{\epsilon}$. If we approximate $\mathbf{\epsilon}$ as with
signal-independent zero-mean Gaussian distribution, we have:

$$p(\mathbf{y} | \mathbf{x}) = norm(\mathbf{C} \mathbf{x} , \sigma_y^2 \mathbf{I}_y) .$$

where $norm()$ denotes the multivariate Gaussian distribution,
$\sigma_y^2$ is the variance of the noise added to each mean cone
excitation, which will be equal to the mean for the Poisson
approximation. The symbol $\mathbf{I}_y$ denotes the identity matrix
with the same dimensionality as the vector $\mathbf{y}$.

We can also use a Gaussian distribution to describe a prior over $x$

$$p(\mathbf{x}) = norm(\mathbf{\boldsymbol{\mu}}_x,\mathbf{\Sigma}_x)$$

where the vector $\boldsymbol{\mu}_x$ and matrix $\mathbf{\Sigma_x}$
represent the mean and covariance of the prior.

Given the Gaussian likelihood and prior, the posterior is also Gaussian;
its mean and covariance matrix may be computed analytically from the
mean and covariance matrices of the likelihood and prior. This result
follows from a standard identity that the product of two multivariate
Gaussian distibutions is also a multivariate Gaussian (see
@RasmussenGaussian2006, [-@RasmussenGaussian2006]; @Brainard1995AnIdeal,
[-@Brainard1995AnIdeal] provides the derivation in the context of the
Bayesian posterior). In the case where the posterior is a multivariate
Gaussian, its mean $\boldsymbol{\mu}_{x|y}$ provides the estimate of
$\mathbf{x}$ that corresponds to both the posterior mean and the MAP
estimate.

![**Bayes reconstruction**. See description in text. For the prior and
posterior, probability is given as the probability mass for a region of
size $0.01^2$ in the pixel radiance plane. Matlab code to produce this
figure is available at
https://github.com/DavidBrainard/BrainardFigListings.git (subdirectory
scripts/MathPsychChapter/FigLinBayesExample, script
Example.m).](bayesexample.png){#fig:bayesexample width="12cm"}

Figure [1.9](#fig:bayesexample){reference-type="ref"
reference="fig:bayesexample"} illustrates the idea for a simple example
case. Suppose that the display has only two pixels and emits at only one
wavelength. Then $\mathbf{x} = [x_1, x_2]^T$. We will assume that the
radiance at each pixel of the display can range between 0 and 1. For
natural images, there is a strong correlation between the radiance at
neighboring pixels at the same wavelength
[@Burton1987Color; @Tkacik2011Natural]. A bivariate Gaussian prior
distribution with this property is illustrated in the left panel of
Figure [1.9](#fig:bayesexample){reference-type="ref"
reference="fig:bayesexample"}. The mean of the prior is
$\mathbf{x} = [0.5, 0.5]$ while the covariance matrix
$\mathbf{\Sigma}_x$ corresponds to standard deviation of 0.127 and a
correlation across the two pixels of 0.89. The strong correlation in the
prior restricts the best guesses about the values of $\mathbf{x}$
relative to the full available range.

To compute a likelihood we need to know the nature of the sensory
measurements. We suppose that there is just one cone and that it is
equally sensitive to the radiance at the two display pixels. This gives
us $\mathbf{C} = [0.5, 0.5]$. We assume that the mean excitation of the
cone is perturbed by zero-mean Gaussian noise with standard deviation
$\sigma_y = 0.01$. The middle panel of Figure
[1.9](#fig:bayesexample){reference-type="ref"
reference="fig:bayesexample"} illustrates the likelihood for the
specific cone excitation $\mathbf{y} = 0.3$: the likelihood
$p(\mathbf{y} = 0.3 | \mathbf{x})$ is plotted as a function of $x_1$ and
$x_2$. This likelihood is highest along the ridge where the weighted sum
of the pixel radiances sums to the observed cone excitation of $0.3$.
The likelihood falls off away from this ridge, with the rate of falloff
determined by the magnitude of the noise. If the noise were smaller, the
falloff would be faster and the likelihood ridge thinner, and conversely
if the noise were larger. The likelihood alone tells us that
$\mathbf{x}$ is unlikely to lie far from that ridge. At the same time,
the likelihood makes explicit the ambiguity about $\mathbf{x}$ remaining
after observing $\mathbf{y}$, with many values of $\mathbf{x}$ equally
likely.

Bayes' rule specifies that the prior and likelihood should be combined
using point-by-point multiplication over the pixel radiance plane
(Equation [\[eqn:bayesrule\]](#eqn:bayesrule){reference-type="ref"
reference="eqn:bayesrule"}), and then normalized, to form the posterior.
The right panel of Figure [1.9](#fig:bayesexample){reference-type="ref"
reference="fig:bayesexample"} illustrates the result of this
multiplication. The same result may be obtained directly by application
of the analytic formulae for the posterior.

The posterior makes intuitive sense: It is large where both the prior
and likelihood are large, and the resulting distribution is more
concentrated than either the prior or likelihood alone. Although there
is still uncertainty remaining in the posterior, it captures what we
know about the scene when we combine the statistical regularities of the
displayed images with the sensory measurement provided by the cone
excitation.

### Thresholds and ideal observer theory

In this and the next sections, we show how ideas of perceptual inference
as implemented through Bayes rule help us understand perceptual
processing. We begin with analysis of threshold measurements. A
threshold is the minimum difference required for an observer to
correctly discriminate between two stimuli, and threshold measurements
are fundamental to the psychophysical approach to characterizing
perceptual performance and making inferences about the mechanisms
underlying this performance.

Consider, for example, discrimination between a uniform field and a
contrast grating (see Figure [1.10](#fig:csf){reference-type="ref"
reference="fig:csf"}). In a typical experiment, the observer is shown
the uniform field and the grating in sequence, with the order randomized
on each trial. The observer's task is to indicate which was presented
first. In the experiment the stimulus contrast is titrated to a level at
which the observer is correct, say, 80% of the time. The resulting
contrast is the threshold.

Threshold measurements quantify the information needed by the visual
system to make a basic perceptual decision: namely, that two stimuli
differ. They inovlve small perturbations of the visual stimulus, and
they may be thought of as assessing sensitivity to derivatives of the
retinal image. In this way, thresholds are connected to the ideas
introduced above about the importance of derivatives of the spectral
radiance as a basis for visual processing.

Figure [1.10](#fig:csf){reference-type="ref" reference="fig:csf"} shows
the threshold for contrast gratings measured as a function of grating
spatial frequency, in the form of the spatial contrast sensitivity
function (CSF). When measured with static or very slowly moving
gratings, the human CSF has an inverted U-shape: the highest contrast
sensitivity is between 3-6 cycles per degree, with lower sensitivity at
higher and lower spatial frequencies. Because any image may be
synthesized by a weighted superposition of sinusoidal gratings
[@Bracewell1978Fourier], the CSF characterizes the sensitivity to basic
stimulus components. Because the visual system as a whole is neither
shift-invariant nor linear, however, the CSF is a useful but not
complete description of sensitivity.

We would like to understand how the human contrast sensitivity function
is limited by the properties of the visual components described in this
chapter. Bayes' rule provides a way to build this understanding, by
linking the initial encoding to performance on the psychophysical
threshold detection task. Analyses of this sort are called ideal
observer theory [@Geisler1989Sequential]. Ideal observer theory allows
us to understand the extent to which observer performance is limited by
the factors in early visual encoding that we have outlined above. These
include blurring by the eye's optics, which reduces the retinal contrast
of a grating stimulus, spatial sampling by the cone mosaic, and the
Poisson variability in the cone excitations. Of particular interest is
separating aspects of visual performance that are tightly coupled to
these factors from aspects that are limited by processes not
incorporated into the ideal observer calculation.

![ **Modeling the human contrast sensitivity function**. A. Sensitivity,
defined as the inverse of threshold contrast, is plotted as a function
of spatial frequency. The stimuli were small, equal-sized patches of
contrast gratings. Reproduced from De Valois et al.
[-@de1974psychophysical]. The thumbnails below the graph illustrate
contrast grating patches at different spatial frequencies. B. Triangles
and black line: Human contrast sensitivity function for two observers,
data from Banks et al. [-@Banks1987Physical]. Grey circles/line:
Contrast sensitivity of an ideal observer implemented at the level of
the Poisson limited cone excitations, from Banks et al.
[-@Banks1987Physical]. Red circles/line: Ideal observer CSF with recent
estimates of optics and mosaic properties. Blue circles/line:
Computational observer CSF with decision rule determined using
supervised machine learning. Green circles/line: Computational observer
CSF additionally accounting for fixational drift. Purple circles/line:
Computational observer CSF additionally incorporating a model of the
transformation from excitations to photocurrent. After Figure 6 of
Cottaris et al. [-@Cottaris2020AComputational].](csf.pdf){#fig:csf
width="12cm"}

So, how do we use Bayes to predict performance in the two-interval
forced choice task described above? We'll use the terms reference
stimulus and comparison stimulus to describe the two stimuli being
discriminated. In this example the reference stimulus will be a
spatially uniform field and the comparison stimulus will be a patch of
contrast grating with known spatial frequency, orientation, size, and
contrast; but, the ideas we develop here apply to any two stimuli being
discriminated.

Using the computational methods described in this chapter, we can
compute the mean cone excitations to the reference and comparison
stimuli. Let $\mathbf{u}_r$ be the vector of mean cone mosaic
excitations in response to the reference stimulus and let $\mathbf{u}_c$
be the vector of mean cone excitations in response to the comparison
stimulus. In the two-interval forced choice task, the observer must
indicate whether the reference came first followed by the comparison, or
the other way around. We thus form two concatenated vectors,
$\mathbf{u}_1 = [\mathbf{u}_r, \mathbf{u}_c]$ and
$\mathbf{u}_2 = [\mathbf{u}_c, \mathbf{u}_r]$.

To apply Bayes' rule to this problem, we can think of the scene as
described by the binary random variable. This variable, $x$, can take on
value $1$ or $2$. These values represent the reference first and
reference second possibilities that can occur on each trial. The prior
probability $p(x)$ is given by

$$p(x = 1) = 0.5; \hspace{1mm} p(x = 2) = 0.5.$$

The data available to the observer to make a response of $x = 1$ or
$x = 2$ are the pattern of observed cone excitations across the two
intervals, which we will denote by $\mathbf{y}$. We know that for
$x = 1$, each entry of $\mathbf{y}$ is an independent Poisson random
variable with mean given by the corresponding entry of $\mathbf{u}_1$,
while for $x = 2$ the means are given by the corresponding entries of
$\mathbf{u}_2$. From this, we have for the posterior:

$$p(x = 1 | \mathbf{y}) = p(\mathbf{y} | x = 1) p(x = 1) = \prod_{i} { (p(y_i | x = 1)}  p(x = 1)$$

where $y_i$ denotes the $i_{th}$ entry of $\mathbf{y}$ and we have
explicitly expressed the joint distribution of independent random
variables as the product of their individual distributions.

We substitute the expression for the probability mass function of a
Poisson random variable and the value of $p(x = 1)$ to obtain

$$p(x = 1 | \mathbf{y}) = \prod_{i} { \frac {{{u_1}_i}^{y_i}  e^{-{{u_1}_i}} } { {y_i}! } } 0.5$$

where ${u_1}_i$ denotes the $i^{th}$ entry of $\mathbf{u}_1$. Similarly,
we have

$$p(x = 2 | \mathbf{y}) = \prod_{i} { \frac {{{u_2}_i}^{y_i}  e^{-{{u_2}_i}} } { {y_i}! } } 0.5.$$

To maximize percent correct on the task, the observer should compare
$p(x = 1 | \mathbf{y})$ with $p(x = 2 | \mathbf{y})$ and indicate $1$ or
$2$ according to which is larger. It is instructive to implement this
comparison in terms of the difference of the logs of
$p(x = 1 | \mathbf{y})$ and $p(x = 2 | \mathbf{y})$, with a response of
$1$ corresponding to a difference greater than or equal to $0$ and a
response of $2$ corresponding to a difference less than $0$. Writing the
difference of logs explicitly and simplifying, we have decision variable
$$\delta = \sum_{i} {y_i {log( \frac {{u_1}_i} {{u_2}_i}}) } + \sum_{i} ({ {u_2}_i - {u_1}_i)  }$$
An observer who responds according to the sign of $\delta$ will maximize
percent correct. The value of percent correct depends on how $\delta$ is
distributed when $x = 1$ and $x = 2$. Geisler [-@Geisler1984Physical]
provides a Gaussian approximation to these distributions, which may be
used to obtain the corresponding percent correct. As with the human
psychophysical experiment, contrast may be titrated to find the ideal
observer threshold contrast, that which leads to the ideal observer
having the criterion percent correct.

Figure [1.10](#fig:csf){reference-type="ref" reference="fig:csf"}B shows
the ideal observer contrast sensitivity for human foveal viewing (gray
circles/line), along with psychophysical measurements of human contrast
sensitivity at spatial frequencies increasing from 5 cpd, and with the
measurements (triangles/black line) made with stimuli matched to those
used in the ideal observer calculations [@Banks1987Physical]. As with
the human data at higher spatial frequencies, the ideal observer
contrast sensitivity function falls off as spatial frequency increases,
and indeed the slope of this falloff closely resembles that of the human
observer. This correspondence suggests that the factors that cause the
human falloff share basic features with those included in the ideal
observer calculation. Here the primary factor is blur from the eye's
optics and cone apertures, both of which reduce the contrast captured by
spatial variation in the cone excitations.

The ideal observer CSF differs from the human measurements. One
differeince is that the overall sensitivity of the ideal observer is
markedly higher than that of the human observer. The Poisson noise in
the cone excitations limits the ideal observer sensitivity. The fact
that incorporating only this noise source leads to an ideal observer
more sensitive than the human tells us that additional factors limit
human sensitivity, and motivates study of what these additional factors
are.

In many cases, a single \"efficiency\" parameter representing an omnibus
loss of information by the actual visual system relative to an ideal
observer calculation is sufficient to bring ideal observer predictions
into alignment with measured human performance [@Burge2020ARVSReview],
as is true in the case of the ideal and human CSF rolloff at high
spatial frequencies. The efficiency parameter can be thought of as
capturing the effect of additional noise in the human visual system, not
included in the ideal observer calculation, whose effect on performance
is stimulus-independent.

It is important to note, however, that it is not always possible to
account for a difference between ideal and human with a single
efficiency parameter. For example, the ideal observer CSF does not roll
off at low spatial frequencies but the human CSF does. The factors that
produce the measured low-spatial-frequency roll off are not included in
the ideal observer calculations presented here. As with the difference
in overall sensitivity, the difference between ideal and human CSF at
low spatial frequencies motivates investigation of what additional
factors in the human visual system account for the difference.

### Computational observers

The ideal observer calculation used by Banks et al.
[-@Banks1987Physical] employed a simplified model of the eye's point
spread function and cone mosaic, and this simplification enabled
efficient computation of ideal observer performance. In two recent
papers, Cottaris et al. ([-@Cottaris2019AComputational];
@Cottaris2020AComputational, [-@Cottaris2020AComputational]) employed
computational methods to examine the effect of more recent estimates of
the point spread function [@Thibos2002Statistical] and a more detailed
model of the foveal mosaic on performance. These had only a modest
effect on the predictions (Figure [1.10](#fig:csf){reference-type="ref"
reference="fig:csf"}B, red circles/line).

The ideal observer developed above has full knowledge of mean cone
excitations and Poisson structure of the noise, so that the observer's
performance is not degraded by stimulus uncertainty
[@Pelli1985Uncertainty; @Geisler2018Psychometric]. Cottaris et al.
[-@Cottaris2019AComputational] relaxed this assumption by replacing the
ideal observer decision rule with a decision rule based on a trained
linear classifier [@Manning2008Introduction; @scholkopf2002learning].
The classifier measured the match of the data to a template that had the
same spatial structure as the stimuli. The decision boundary was
optimized in the presence of noise. The need to partially learn the
decision rule reduced the absolute level of ideal observer performance
while retaining the same CSF shape (blue circles/line in Figure
[1.10](#fig:csf){reference-type="ref" reference="fig:csf"}B). Cottaris
et al. [-@Cottaris2020AComputational] then introduced a computational
model of fixational eye movements (@Mergenthaler2007Modeling,
[-@Mergenthaler2007Modeling]; see also @Engert2004Microsaccades,
[-@Engert2004Microsaccades]) and showed that an approach to handling the
stimulus blur introduced by these movements further reduced performance
(green circles/line). Finally, Cottaris et al.
[-@Cottaris2020AComputational] introduced a computational model of the
transformation from excitations to electrical photocurrent, which
included both gain control and additional noise. Accounting for this
transformation brought computational observer performance into
approximate alignment with the human measurements at the higher spatial
frequncies (purple circles/line).

This analysis outlines a set of factors that together provide an account
of the high-spatial frequency limb of the human spatial CSF, capturing
both the shape and absolute level of this important measure of
performance. For purposes of the present chapter, we emphasize less the
specific elements of the account, which will surely be refined by future
research, but rather the way the mathematical principles are combined
with computational modeling with the goal of accounting for the full
richness of the visual system. The combination of principles and
computations accounts for factors that are beyond what is possible using
analytic calculations alone.

### Image reconstruction

The ideal observer and computational observer development above applies
Bayesian inference to the analysis of threshold measurements. Thresholds
characterize the limits of visual performance, and the analyses
illustrate how threshold performance can be linked to quantitative
measurements of physiological optics, retinal anatomy, and retinal
physiology. Not all vision is threshold vision, however. Sometimes we
are interested in predicting what clearly visible stimuli look like
(e.g., \"that apple looks red\") or how similar easily distinguishable
objects appear (e.g., \"the color of the apple appears more similar to
the color of the tomato than it does to the color of the banana\").
There are a number of methods for studying suprathreshold vision. These
include asymmetric matching
[@Burnham1957; @Brainard1992Asymmetric; @Wandell1995Foundations] and
various scaling techniques
[@Knoblauch2012; @MaloneyMLDS2003; @CoxMDS2001]. We will not treat these
methods here. Below, however, we illustrate how Bayesian methods can be
used to understand how the initial visual encoding shapes the perceptual
inferences that can be made about supra-threshold stimuli.

In our introduction to Bayes Rule, we illustrated the core ideas by
considering reconstruction of a two pixel image from the excitations of
a single cone, using both a Gaussian prior and a Gaussian likelihood. As
computer power has increased, these same Bayesian principles have been
applied to increasingly large perceptual problems. As we illustrate
here, it is now possible to reconstruct an estimate of a full displayed
color image from a realistic model of cone excitations using the Poisson
likelihood [@Zhang2021Reconstruction].

The forward computation starts with the displayed image, $\mathbf{x}$
and computes the cone excitations $\mathbf{y}$. The vector $\mathbf{x}$
can be thought of as the concatenation of the linearized and rasterized
pixel values for each of the red, green, and blue channels of the
display. Using the Poisson noise model of the cone excitations, we
compute the likelihood of observed cone excitations
$p(\mathbf{y} | \mathbf{x})$. Here the vector $\mathbf{y}$ is simply a
list of the excitations of each cone in the mosaic. Because the mean
cone excitations are a linear function of of the display pixel values,
we can write for these mean excitations

$$\mathbf{\bar{y}} = \mathbf{R} \mathbf{x}$$

for some matrix $\mathbf{R}$. Each column of this matrix may be computed
as the vector of cone excitations produced when one pixel is at its
maximum value for one color channel, with the display values for all
other pixels and color channels are set to zero, and these computations
may be implemented in software such as ISETBio to determine explicitly
the matrix $\mathbf{R}$ [@Zhang2021Reconstruction]. This yields for the
likelihood

$$p(\mathbf{y} | \mathbf{x}) = Poisson(\mathbf{R} \mathbf{x}),$$

where Poisson() denotes the result of Poisson noise applied
independently to its vector argument by taking each entry of the
argument as corresponding Poisson mean.

Next, we specify a prior distribution $p(\mathbf{x})$ for natural
images. Natural images have a great deal of structure
[@Simoncelli2005Statistical], and a full statistical description of this
structure is not currently available. There are two robust regularities
of natural images, however, that can be described by a multivariate
Gaussian. The first is that within a single wavelength band, the
spectral radiances at nearby image locations are highly correlated
[@Pratt1978Digital; @Field1987Relations; @Ruderman1988Statistics]. The
second regularity is that at a single position, values in nearby
wavelength bands are highly correlated
[@Burton1987Color; @Tkacik2011Natural]. This is a consequence of the
relatively smooth spectral functions one observes in nature
[@Cohen1964Dependency; @Maloney1986Evaluation; @Vrhel1994Measurement].
These two observations may be used to construct a covariance matrix for
a multivariate Gaussian that describes the second order statistics of
natural images. Together with the average image, these provide a
Gaussian image prior.

With the likelihood and prior, we can construct an estimate of the image
given a vector of cone excitations. As with many calculations described
in this chapter, the principles of Bayesian estimation guide the way,
but once we introduce the Poisson likelihood, numerical computational
methods are used to find the solution.

We used ISETBio to reconstruct images from cone excitations, with the
Poisson likelihood and Gaussian image prior described above. We
reconstructed images for retinal patches at various visual field
eccentricities. As visual field eccentricity increases, the point spread
of the retinal image becomes more blurred and the density with which the
cones sample the image decreases(Figures
[1.5](#fig:pointspread){reference-type="ref"
reference="fig:pointspread"}, [1.6](#fig:sampling){reference-type="ref"
reference="fig:sampling"}, and
[1.7](#fig:excitations){reference-type="ref"
reference="fig:excitations"}). Thus, less information becomes available
to the visual system in the peripheral visual field. The effect of this
loss for reconstruction depends on the prior. Although the information
loss means that two images whose cone excitations are different in the
fovea can produce the same cone excitations in the periphery, this
ambiguity need not degrade the reconstructions if the probability that
one of the two images will occur is small.

The reconstructions in
Figure [1.11](#fig:reconstructions){reference-type="ref"
reference="fig:reconstructions"} show the effect of information loss at
the level of the cone excitations, in the context of the Gaussian image
prior. The reconstructed image quality in the periphery is worse than in
the fovea, but many objects remain recognizable from the peripheral
reconstructions. Moreover, there are interesting interactions between
the likelihood and prior. For example, the recovery of color can be
better in the fovea and more peripheral locations than it is in the
mid-periphery (see images of strawberries in
Figure [1.11](#fig:reconstructions){reference-type="ref"
reference="fig:reconstructions"}, for example). Zhang et al.
[-@Zhang2021Reconstruction] describe the reconstruction approach to
analyzing the initial visual encoding in more detail, extending the
ideas to a more realistic prior than the Gaussian, and showing a number
of calculations that use image reconstruction to examine how prior and
likelihood interact to support both color and spatial vision (see also
@Brainard2008Trichromatic, [-@Brainard2008Trichromatic]).

Image reconstruction computations provide useful insights about how
statistical regularities in natural scenes interact with the sensory
measurements to guide perception. But, it is important to bear in mind
that reconstruction of displayed images is not the task for which visual
perception evolved. Rather, we view the task of perception to
reconstruct the properties and positions of objects in the
three-dimension environment. The Bayesian ideas presented here have
applicability to this task as well [@KnillRichards1996], but a
computational solution that is as effective as human vision currently
remains elusive. This is an area where recent progress in machine
learning and deep neural networks may provide new insights.

### Optimizing sensory measurements

Earlier in this chapter, we explained that the visual system appears to
extract information about motion, color, and pattern from the pattern of
cone excitations by estimating the local derivatives of various
quantities [@Adelson1991Plenoptic]. The Bayesian framework provides a
quantitative framework for addressing how to optimize which signals
should be transduced by a sensory system when the goal is reconstruction
of the state of the environment, as well as how the sensory signals
should be summarized (e.g., in the form of local derivatives) for
further processing. Indeed, the Bayesian image reconstruction methods
developed here point towards the ingredients required for a full
analysis of such questions. To know what measurements we should make, we
first need to know the prior distribution over the environmental states
that an organism will encounter. We then need a parameterized set of
candidate likelihood functions, each of which describes a feasible
arrangement of the sensory apparatus and (if desired) associated early
processing. This information allows us to compute the posterior over the
environmental states for any candidate likelihood function, and we can
ask how well different sensory measurements constrain the posterior,
averaging this information over the environmental states described by
the prior. Developing a parameterized set of candidate likelihood
functions requires an understanding of what biological constraints apply
to the sensory system. Also required is an understanding of the cost of
different types of error in the resultant perceptual representation (the
loss function; @Berger1985Statistical, [-@Berger1985Statistical]), as
well as how the cost of error should be balanced against the energetic
cost of making and processing the sensory measurements
[@Laughlin2001Energy; @Balasubramanian2001Metabolically; @Koch2004Efficiency].
A number of authors have pursued questions of optimizing sensory
measurements in this manner
[@Levin2008Understanding; @Manning2009Optimal; @Garrigan2010Design; @Zhang2021Reconstruction].[^10]
It would be interesting to compare the results of an analysis of this
sort to the Adelson/Bergen conjecture that approximations to local
derivatives represent an optimal measurement set.

![**Image reconstructions from cone excitations at three retinal
eccentricities**. Each row shows reconstructions of 7 images using the
Bayesian method and Poisson likelihood and multivariate Gaussian prior.
The reconstructions at 1 degree eccentricity are close to veridical,
with increasing distortions seen at the 10 and 18 degree locations. Each
original and reconstructed image was represented at a pixel resolution
of 128 by 128, and the extent of each image on the retina was 1 degree
by 1 degree. The mean excitation of the cones was $~10^5$ excitations
per cone, so the simulation corresponds to a relatively high
signal-to-noise regime. The parameters of the Gaussian prior were fit to
16 by 16 pixel patches of images from the ImageNet ILSVRC dataset
(https://www.image-net.org), and extended in an overlapping block-wise
fashion to the higher image pixel resolution. Figure courtesy Lingqi
Zhang. See Zhang et al. [-@Zhang2021Reconstruction] for a more extended
discussion of Bayesian image reconstruction and the general methods used
to produce this figure.](reconstruction.png){#fig:reconstructions
width="12cm"}

## Summary and conclusions

To focus on the mathematics of the initial visual encoding, we introduce
vision science from the point of view of a forward calculation: physics
of the stimulus, image formation, and quantitative system modeling. The
key mathematical principles are linear algebra, shift-invariant linear
systems, and specification of sensory noise. The mathematics of vision
science shares much in common with the mathematics of many fields of
science and engineering.

After expressing and implementing the forward calculations, we explore
the mathematics of Helmholtz' hypothesis: People see a stimulus that is
the most likely explanation of the cone excitations. We use Bayesian
inference methods to clarify the uncertainty about the encoded signal.
This approach requires that we confront the problem of establishing
priors on the signal. There is a close connection between Helmholtz'
unconscious inference and Bayesian inference; the latter may be thought
of as a quantitative implementation of Helmholtz's idea.

The approach we describe has a long and accomplished tradition. But, it
is not the only valid way to make progress in vision science; several
other approaches are important. A quantitative study of behavioral rules
can be very informative. For example, color appearance matching was a
largely behavioral exploration at first; an understanding of the physics
of the signal and the biological underpinnings followed later. Also,
neurobiological measures can be helpful. Anatomical and functional
measurements that characterize the properties of multiple pathways
within the visual system - including multiple types of retinal ganglion
cells and multiple pathways through the visual cortex - are useful
guides to understanding visual specializations and computations,
particularly for stages of vision beyond the initial encoding.. Finally,
engineering work to build functional artificial visual systems continues
to be very helpful in understanding vision: A classic principle states
that the best way to demonstrate you understand a system is to build one
that does the same thing. Engineering efforts continue to clarify
features that we might look for in the nervous system as well as why
certain behavioral patterns emerge.

The field of vision science is large and vigorous enough that there is
no need to choose a single approach. We are inspired by the fact that
different investigators adopt different approaches, all seeking to gain
some understanding. To the student thinking about how to approach vision
science, we offer advice from an American philosopher who commented
about making difficult decisions: 'When you come to a fork in the road,
take it (Yogi Berra).'

## Further Reading

This chapter introduces key mathematical and computational approaches to
understanding the initial visual encoding. A number of the mathematical
ideas we present here are developed in more detail by Wandell
[-@Wandell1995Foundations], and the classic treatment of visual
perception by Cornsweet [-@Cornsweet1970] remains a valuable
introduction to the field, as does Rodieck [-@Rodieck1998FirstSteps].
Principles of ray tracing are introduced in many computer graphics texts
(e.g. @Pharr2016-yb, [-@Pharr2016-yb]); similarly many texts introduce
optics (e.g., Hecht, [-@Hech2017Optics]). In the context of the retinal
image and cone excitations specifically, Yellott et al.
[-@Yellot1984Beginnings], Pugh [-@Pugh1988Vision], and Packer and
Williams [-@Packer2003Light] are useful. Brainard and Stockman
[-@Brainard2010Colorimetry] elaborate in more detail on using linear
algebra in support of colorimetric applications. Although we do not
treat the Fourier transform and frequency domain representations in this
chapter, the reader who wishes to specialize in this field will want to
learn about these ideas. Two useful sources are Bracewell
[-@Bracewell1978Fourier] and Pratt [-@Pratt1978Digital]. Useful
introductions to statistical inference include Bishop
[-@Bishop2006Pattern] and Duda, Hart and Stork [-@DudaHart2001].

## Acknowledgments

We thank Nicolas Cottaris and Lingqi Zhang for providing figures for
this chapter. We also thank Amy Ni, Nicolas Cottaris, Lingqi Zhang,
Joyce Farrell, Eline Kupers, Heiko Schutt, and Greg Ashby for useful
comments on the manuscript.

------------------------------------------------------

[^1]: Mechanical force on the retina (pressure phosphenes) and injecting
    current into the retina or brain (electrical phosphenes) can also
    cause a visual sensation.

[^2]: https://www.merriam-webster.com/dictionary/light

[^3]: From the section "prove how all objects, placed in one position,
    are all everywhere"

[^4]: It is an exercise for the reader to show that real-valued systems
    that follow the superposition rule also obey the homogeneity rule,
    not just for integers, but for any real scalar. The converse is not
    true. A real-valued system can obey the homogeneity rule but not the
    superposition rule.

[^5]: Strictly speaking, the absorptance spectrum tells us the
    probability that a photon is absorbed. Not all absorbed photons lead
    to an excitation, so an additional factor specifying the quantal
    efficiency (probability of excitation given absorption) needs to be
    included in the calculation. Current estimates put the quantal
    efficiency of human cone photopigment at about 67%. In addition, the
    calculation of cone excitations from spectral irradiance requires
    taking into account the size of the cone's light collecting
    aperture.

[^6]: In this mathematical formulation, we do not make the spatial
    extent of the cone acceptance aperture explicit. This aperture
    introduces additional blur into the retinal image. Computational
    models such as the one shown in Figure
    [1.7](#fig:excitations){reference-type="ref"
    reference="fig:excitations"}) do take this factor, which is
    significant, into account.

[^7]: The null space of a matrix $\mathbf{C}$ is the space of vectors
    $\mathbf{v}$ such that $\mathbf{C} \mathbf{v} = 0$. If a matrix has
    column dimension $n$ and rank $r$, its null space has dimension
    $n-r$.

[^8]: When describing optics a shift-invariant region within the visual
    field is called an isoplanatic region.

[^9]: The point spread function is the spatial analog of the impulse
    response function used to characterize time-invariant linear
    systems.

[^10]: The formalism used in these analyses is interestingly similar to
    that underlying Bayesian adaptive psychophysical procedures
    [@Watson1983QUEST; @Watson2017QUEST].
