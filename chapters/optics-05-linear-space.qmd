# Linear optics - space {#sec-optics-linear-space}
Geometric optics helps us understand how lenses work by focusing on things like the shape of the lens, what it’s made of, and how far it is from the sensor. This approach is great for designing lenses and figuring out their basic properties.

But there’s another way to look at optical systems: as signal processors. In this view, the system takes in light from the scene and transforms it into the light that arrives at the sensor. This signal processing perspective is powerful because it lets us think about what the system does, without worrying too much about the details of how it’s built. It’s also a practical way to simulate and analyze optical systems.

This approach is especially useful because many optical systems are close to being linear—they follow the **Principle of Superposition**. You can actually test this property in the lab. There are lots of tools for working with linear systems, and you’ll see them used again and again in science and engineering. In this section, we’ll introduce these ideas using a simple optical system as an example.

## Principle of Superposition
A system $L$ is linear if it satisfies the superposition rule:

$$
L(x + y) = L(x) + L(y).
$$ {#eq-superposition}

Here, $x$ and $y$ are two possible inputs, and $L(x)$ is the system's response to the input $x$. The input variable can be a scalar, vector, matrix, or tensor. For example, in an optical system, $x$ might represent the incident light field at the entrance aperture, and $L(x)$ could be the spectral irradiance measured at the sensor .

### Real-World Linearity 

No real system is perfectly linear over all possible signals. Extreme inputs with a massive amount of energy may behave unpredictably or even break! However, many systems are linear over an input range that covers all the likely use cases. 

### Local Linearity 
Some systems are only **locally linear**.  This means they behave linearly within a limited region, such as near a particular input $x_1$.  A system might be approximately linear in different ways depending on the input region. For example, the linear approximation near $x_1$ could be different from the one near $x_2$. This idea of local linearity is a fundamental concept in mathematics, physics, and engineering.[^taylor-series]

 [^taylor-series]: If you’ve seen the Taylor series expansion in calculus, you may recognize the principle of local linearity in mathematics!

### Principle of Homogeneity
A special case of superposition is **homogeneity**:

$$
L(\alpha x) = \alpha L(x)
$$ {#eq-homogeneity}

Homogeneity follows from superposition. For example, adding an input to itself:

$$\begin{aligned}
L(x + x) &= L(2x) \nonumber  \\ 
&= L(x) + L(x) \nonumber \\
&= 2L(x). \nonumber
\end{aligned}
$$

This generalizes to any integer $m$:

$$
L(m x) = m L(x).
$$

::: {.callout-note title="Extension to Rational Numbers" collapse="true"}

For real-valued systems that obey superposition, homogeneity holds for any rational number. If $\alpha = \frac{p}{q}$ is rational:

$$
L(\alpha x) = L\left(\frac{p}{q}x\right) = p L\left(\frac{x}{q}\right).
$$

Let $x' = x/q$, so

$$
L(x) = L(q x') = q L(x') \implies L(x') = \frac{1}{q} L(x).
$$

Therefore,

$$
L(\alpha x) = p L(x') = p \frac{1}{q} L(x) = \frac{p}{q} L(x) = \alpha L(x).
$$

To extend this to all real numbers, continuity and far more extensive proof is required. But how about we just work with the rationals and be engineers for now?
:::

It is important to note that a system can be homogeneous without being linear. For example, $f(x) = |x|$ and the vector length function

$$
f(\mathbf{x}) = \|\mathbf{x}\|_2 = \sqrt{x_1^2 + x_2^2 + \cdots + x_n^2}
$$

are homogeneous but not linear. The function $ReLU(x) = \max(0, x)$, widely used in neural networks, is also homogeneous for $\alpha > 0$ but not linear:

$$
1 = ReLU(2 + -1) \neq ReLU(2) + ReLU(-1) = 2 + 0.
$$


## Point spread functions
Linearity is the critical foundation for modeling how optics turns an input scene into a sensor image. We imagine the scene as a collection of points, $\mathbf{p} = (p_x, p_y, p_z)$, each with its own intensity $I(\mathbf{p})$. The optics spreads out the light from each point in a certain way—this is called the point spread function, or $PSF_\mathbf{p}(x, y)$, where $(x, y)$ are positions in the image.

If we know how the light from each point is spread, and superposition holds, we can calculate the output image, $O(x,y)$ for any input scene: We just add up the contribution from every point. The formula looks like this:

$$
O(x, y) = \sum_{\mathbf{p}} I(\mathbf{p})\, PSF_\mathbf{p}(x, y)
$$ {#eq-pointspread-1}

Here, $O(x, y)$ is the intensity at image position. Each scene point contributes a scaled PSF, and the output image is just the sum of all those PSFs.

::: {.callout-note title="A limitation of PSFs" collapse="true"}
There’s a catch: in real scenes, objects can block each other (occlusion). That means the point spread from one spot might be blocked or changed by something else in the scene, so the formula above isn’t always perfect.

But if all the points are on a flat surface, or if everything is far away (so nothing blocks anything else), this approach works well. The main idea is that you can build up the whole image by adding up the point spreads from each part of the scene. It’s a great starting point for understanding how images form.
:::

## Shift-invariant linear systems
A practical limitation of characterizing a general linear systems is that it can have many different PSFs - one for every point! But many real systems are much simpler. These systems have essentially the same point spread function across many points, with the only difference being that the point spread function is shifted between points. Because the shape of the point spread function remains the same -it is only shifted- we say such a system is **shift invariant**. 

Across the many branches of science and engineering, there are a large number of physical phenomena and engineering devices that are well-approximated as shift invariant linear systems. These systems are relatively straightforward to calibrate because we only need to estimate a single point spread function. Also, there are many computational tools that help us simulate shift invariant systems.  In imaging the point spread function is the key measurement that characterizes the linear system. In applications to systems that characterize responses over time, the key measurement is the response to a short temporal input. This is called the **impulse response**, and it is the equivalent of the point spread function in an imaging system. 

The simplicity derived from shift invariance is so useful that some courses entitled 'linear systems' focus almost entirely on shift-invariant linear systems. 

::: {.callout-note title="Shift invariance notation" collapse="true"}
A little mathematical notation may be useful. Suppose we write image translation as a function, $T()$. If we shift an image, $x$, by an amount, $\delta$, we write it as $T(x,\delta)$. A shift invariant system means that the output of a linear system, $L()$, to a translated image is the translated linear response

$$
L(T(x,\delta)) \approx T(L(x),\alpha \delta).
$$ {#eq-shiftinvariance-defined}

Notice that @eq-shiftinvariance-defined allows the size of the image shift, $\delta$, to differ from the size of the output shift, $\alpha \delta$. So if we displace an object by an amount $\delta \text{m}$, the image will be displaced by an amount $\alpha \delta \text{m}$ on the sensor. This scale factor depends on the image magnfication. 
:::

The image formation component (optics) of most image systems are typically linear, but they are not globally shift-invariant. Rather, they are shift invariant over some part, but not all, of the full visual field. Usually there is a region near the optical axis that is shift invariant, and the system is also locally shift-invariant at off-axis points. This local shift invariance is similar to a system that is locally linear. In optics the shift invariant region is called an **isoplanatic** region. 

## Common PSFs in simulation

When simulating optical systems, a few point spread functions (PSFs) that come up again and again (@fig-psf-graph).

- **Airy pattern** is the only one of these PSFs that is based on the physics of light waves. This is the PSF we measure for a perfect, diffraction-limited lens imaging a point at a distance (@sec-airy-pattern and @fig-blur-comparison). The Airy pattern depends on wavelength and represents a best case limit. 
- **Gaussian** is popular because it’s easy to work with in calculations. 
- **Lorentzian** (also called Cauchy) has higher tails than the Gaussian. This makes it better for modeling systems where light gets scattered a lot—like if the lens is scratched or there are many internal reflections from a multi-element lens assembly.
- **Pillbox** A square region with constant values that averages the values over the region. Who doesn't like a simple average?

A graph of the cross-section through these PSFs is a useful way to compare them. 

![Cross-section of four  point spread functions that are widely used in simulation. These point spreads were applied for a lens and sensor configuration with an $f/\#$ of 4 and a focal length of 3.9mm. The radius of the Airy disk for this configuration is 3.4 microns (see the zero-crossing of the Airy pattern).](images/optics/optics-siKernels-graph.png){#fig-psf-graph width="60%"}

We can further visualize how the four PSF shapes transform a spectral radiance image using simulation. @fig-psf-image compares the optical images created by the four PSFs for a scene comprising a set of grid lines. The images were calculated using the ISETCam script <a href="../code/fise_opticsPSF.html" target="_blank">fise_opticsPSF.</a>  The Airy and Gaussian PSFs were implemented with point spread functions that vary with wavelength. The Pillbox and Lorentzian PSFs were implemented as wavelength-indepedent.

![The Airy pattern shows the image for a diffraction limited lens with an $f/\# =4$. The Gaussian has a spread that is roughly consistent with the Airy Disk size and a similar wavelength dependency; long-wavelengths spread more than short wavelengths. The high tail of the Lorentzian PSF produces a veiling glare. The Pillbox size is about equal to the Airy disk, but it introduces much more blur because it has a relatively high level over the entire width.](images/optics/optics-siKernels-image.png){#fig-psf-image width="90%"}

Here is a description of the different PSF formulae along with a brief descrition of their properties.

### The Airy pattern
The Airy pattern is circularly symmetric, so you only need to know the distance from the center, $r$, to describe it. The intensity at a distance $r$ from the center is:

$$
I(r) = I_0 \left[ \frac{2 J_1\left( \frac{\pi r}{\lambda N} \right)}{ \frac{\pi r}{\lambda N} } \right]^2
$$

Where:

- $I_0$ is the peak intensity at the center
- $J_1$ is the first-order Bessel function
- $N$ is the $f/\#$ of the lens
- $\lambda$ is the wavelength of light

The first dark ring defines the Airy disk.  It is the diameter where the intensity drops to zero, and defines the smallest spot size.

$$
d = \arcsin(2.44 \, \lambda N) \approx 2.44 \, \lambda N
$$

For $f/\# = 4$ the diameter ranges from 3.9 $\mu\text{m}$ (400 nm) to 6.83 $\mu\text{m}$ (700nm).  See the ISETCam function <a href="https://github.com/ISET/isetcam/blob/main/opticalimage/optics/airyDisk.m" target="_blank">`airyDisk`</a>

### Gaussian
The Gaussian is the classic “bell curve.” It’s simple and often a used as an approximation for optical systems.
$$
I(r) = I_0 \exp\left( -\frac{r^2}{2\sigma^2} \right)
$$

Here, $\sigma$ sets how spread out the blur is. Larger $\sigma$ means a blurrier image.

### Lorentzian (Cauchy)
The Lorentzian function appears in physics and probability theory. It’s famous for its “fat tails”—meaning it doesn’t drop off as quickly as a Gaussian. This makes it handy for modeling blur when there’s a lot of stray light or scattering.

$$
I(r) = \frac{I_0}{1 + \left( \frac{r}{\gamma} \right)^2}
$$

Here, $\gamma$ controls how wide the central peak is. A bigger $\gamma$ means a wider, flatter blur with more light in the tails.

All of these PSFs are implemented in <a href="https://github.com/ISET/isetcam/blob/main/opticalimage/optics/shiftinvariant/siSynthetic.m" target="_blank">`siSynthetic.m`.</a>

## Line spread functions
When I created @fig-psf-image, I decided to illustrate the effect of the PSFs using grid lines rather than points. It was my judgment that the differences would be easier to visualize the impact of the point spread this way. When we calculate the image of a line, we are calculating the **line spread function (LSF)**.

If the PSF is circularly symmetric, the same LSF is measured for any line orientation. To find the PSF we simply sum along the direction (see <a href="https://github.com/ISET/isetcam/blob/main/opticalimage/wavefront/psf/psf2lsf.m" target="_blank">`psf2lsf.m`.</a>. We will calculate a different LSF if the PSF is not circularly symmetric. The formula is very simple.

$$
LSF = \sum_x PSF(x,y)
$$

Here are the linespread functions of the Gaussian and Lorenzian.

Given an LSF, it is possible to recover a unique circularly symmetric PSF using <a href="https://github.com/ISET/isetcam/blob/main/opticalimage/wavefront/psf/lsf2circularpsf.m" target="_blank">`lsf2circularpsf.m`.</a>. I am not plotting the Pillbox because it is not circularly symmetric. In has the same LSF for the horizontal and vertical directions, but its LSF differs in other directions. 

## Chromatic aberration types {#sec-transverse-chromatic-aberration}

*Not sure where this should go*

In @sec-chromatic-aberration I explained the idea that because the index of refraction of a lens might vary with wavelength, the best focus distance would also vary with wavelength. In addition, even for an ideal lens, there will be a wavelength dependence arising from the wave nature of light. In this section we captured this variation by creating wavelength-dependent PSFs for the Airy and the Gaussian PSFs.  This type of chromatic aberration, a defocus of the image, is called **longitudinal chromatic aberration**.

There is a second type of chromatic aberration, called **transverse chromatic aberration** that arises in real imaging systems. The transverse aberration is somewhat related to the longitudinal aberration, but it has a different physical basis.





