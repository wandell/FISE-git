# Linear optics - space {#sec-optics-linear-space}
Geometric optics helps us understand how lenses work by focusing on things like the shape of the lens, what it’s made of, and how far it is from the sensor. This approach is great for designing lenses and figuring out their basic properties.

But there’s another way to look at optical systems: as signal processors. In this framing, an optical system takes in light from the scene and transforms it into light at the sensor. The signal processing perspective is powerful because it lets us think about what the system does, without worrying too much about the details of how it’s built. It’s a practical way to develop image systems simulations and to analyze optical systems.

This approach is especially useful because many optical systems are close to being linear —they satisfy the **Principle of Superposition**. You can test this property in the lab. There are lots of mathematical tools for working with linear systems, and they are taught in many context across science, technology, engineering and mathematics.

Because many readers will be familiar with the ideas, I put an introduction to linear systems into a linear systems appendix, @sec-appendix-linear-systems.  If you are familiar with linear systems principles and tools, use that appendix to understand the notation and thinking for the image systems engineering simulations in this book.

## Point spread functions {#sec-pointspread-2}
In @sec-pointspread I introduced the idea of the point spread function (PSF). It is simply the image of a point of light. If we know the point spread for each location in the scene, and superposition holds, we can predict how any input scene becomes an image. 

The scene is a collection of scene points, $\mathbf{p} = (p_x, p_y, p_z)$, each with its own intensity $S(\mathbf{p})$. Write the point spread function of a scene point as $PSF_\mathbf{p}(x, y)$, where $(x, y)$ are positions in the image. Using superposition, we calculate the output image, $O(x,y)$ by adding up the weighted point spreads from every scene point. The formula looks like this:

$$
O(x, y) = \sum_{\mathbf{p}} s(\mathbf{p})\, \text{PSF}_\mathbf{p}(x, y)
$$ {#eq-pointspread-1}

Here, $O(x, y)$ is the intensity at each image position. Each point in the scene contributes a scaled version of its PSF. The output image is the weighted sum of all those PSFs.

Point spread functions are an intuitive way to understand the performance of the optical system. They are widely used and discussed. There are even sections of conferences devoted to **point spread function engineering**.  This refers to understanding how to design optics that produce point spread functions with specific characteristics, and I give some examples below in @sec-optics-psf-engineering.

::: {.callout-note title="A limitation of PSFs" collapse="true"}
Point spread functions (PSFs) are an excellent tool for modeling optical systems when the scene can be approximated as a flat surface, or when all objects are far enough away that occlusion is negligible. In these cases, each point in the scene contributes independently to the image, and the total image can be predicted by summing the PSFs from each point.

However, in three-dimensional scenes where objects can block light from those behind them (**occlusion**), the situation becomes more complex. The contribution of a point to the image may be partially or completely blocked by closer objects. As a result, the effective point spread depends on the arrangement of objects in the scene, and cannot be characterized by a single, scene-independent PSF. Accurately modeling these effects requires more advanced techniques, such as those used in computer graphics, and often involves recalculating the image formation for each specific scene.

In summary, PSFs provide a powerful and practical framework for image prediction when occlusion can be ignored, but their direct application is limited in scenes with significant three-dimensional structure and occlusion.
:::

## Common PSFs in simulation {#sec-pointspread-common}

In optical system simulation, a few PSFs come up again and again (@fig-psf-graph).

- **Airy pattern** is the only one of these PSFs that is based on the physics of light waves. This is the PSF we measure for a perfect, diffraction-limited lens imaging a point at a distance (@sec-airy-pattern and @fig-blur-comparison). The Airy pattern depends on wavelength and represents a best case limit. 
- **Gaussian** is popular because it’s easy to work with in calculations. 
- **Lorentzian** (also called Cauchy) has higher tails than the Gaussian. This makes it better for modeling systems where light gets scattered a lot—like if the lens is scratched or there are many internal reflections from a multi-element lens assembly.
- **Pillbox** A square region with constant values that averages the values over the region. Who doesn't like a simple average?

A graph of the cross-section through these PSFs is a useful way to compare them. 

![Cross-section of four  point spread functions that are widely used in simulation. These point spreads were applied for a lens and sensor configuration with an $f/\#$ of 4 and a focal length of 3.9mm. The radius of the Airy disk for this configuration is 3.4 microns (see the zero-crossing of the Airy pattern).](images/optics/optics-siKernels-graph.png){#fig-psf-graph width="60%"}

We can visualize how the four PSF shapes transform a spectral radiance image using simulation. @fig-psf-image compares the optical images created by the four PSFs for a scene comprising a set of grid lines. The images were calculated using the ISETCam script <a href="../code/fise_opticsPSF.html" target="_blank">fise_opticsPSF.</a>  The Airy and Gaussian PSFs were implemented with point spread functions that vary with wavelength. The Pillbox and Lorentzian PSFs were implemented as wavelength-indepedent.

![The Airy pattern shows the image for a diffraction limited lens with an $f/\# =4$. The Gaussian has a spread that is roughly consistent with the Airy Disk size and a similar wavelength dependency; long-wavelengths spread more than short wavelengths. The high tail of the Lorentzian PSF produces a veiling glare. The Pillbox size is about equal to the Airy disk, but it introduces much more blur because it has a relatively high level over the entire width.](images/optics/optics-siKernels-image.png){#fig-psf-image width="90%"}

Here is a description of the different PSF formulae along with a brief descrition of their properties.

### The Airy pattern
The Airy pattern is circularly symmetric, so you only need to know the distance from the center, $r$, to describe it. The intensity at a distance $r$ from the center is:

$$
I(r) = I_0 \left[ \frac{2 J_1\left( \frac{\pi r}{\lambda N} \right)}{ \frac{\pi r}{\lambda N} } \right]^2
$$

Where:

- $I_0$ is the peak intensity at the center
- $J_1$ is the first-order Bessel function
- $N$ is the $f/\#$ of the lens
- $\lambda$ is the wavelength of light

The first dark ring defines the Airy disk.  It is the diameter where the intensity drops to zero, and defines the smallest spot size.

$$
d = \arcsin(2.44 \, \lambda N) \approx 2.44 \, \lambda N
$$

For $f/\# = 4$ the diameter ranges from 3.9 $\mu\text{m}$ (400 nm) to 6.83 $\mu\text{m}$ (700nm).  See the ISETCam function <a href="https://github.com/ISET/isetcam/blob/main/opticalimage/optics/airyDisk.m" target="_blank">`airyDisk`</a>

### Gaussian
The Gaussian is the classic “bell curve.” It’s simple and often a used as an approximation for optical systems.
$$
I(r) = I_0 \exp\left( -\frac{r^2}{2\sigma^2} \right)
$$

Here, $\sigma$ sets how spread out the blur is. Larger $\sigma$ means a blurrier image.

### Lorentzian (Cauchy)
The Lorentzian function appears in physics and probability theory. It’s famous for its “fat tails”—meaning it doesn’t drop off as quickly as a Gaussian. This makes it handy for modeling blur when there’s a lot of stray light or scattering.

$$
I(r) = \frac{I_0}{1 + \left( \frac{r}{\gamma} \right)^2}
$$

Here, $\gamma$ controls how wide the central peak is. A bigger $\gamma$ means a wider, flatter blur with more light in the tails.

All of these PSFs are implemented in <a href="https://github.com/ISET/isetcam/blob/main/opticalimage/optics/shiftinvariant/siSynthetic.m" target="_blank">`siSynthetic.m`.</a>

## Line spread functions
When I created @fig-psf-image, I decided to illustrate the effect of the PSFs using grid lines rather than points. It was my judgment that the differences would be easier to visualize the impact of the point spread this way. When we calculate the image of a line, we are calculating the **line spread function (LSF)**.

If the PSF is circularly symmetric, the same LSF is measured for any line orientation. To find the PSF we simply sum along the direction (see <a href="https://github.com/ISET/isetcam/blob/main/opticalimage/wavefront/psf/psf2lsf.m" target="_blank">`psf2lsf.m`.</a>. We will calculate a different LSF if the PSF is not circularly symmetric. The formula is very simple.

$$
LSF = \sum_x PSF(x,y)
$$

Here are the linespread functions of the Gaussian and Lorenzian.

Given an LSF, it is possible to recover a unique circularly symmetric PSF using <a href="https://github.com/ISET/isetcam/blob/main/opticalimage/wavefront/psf/lsf2circularpsf.m" target="_blank">`lsf2circularpsf.m`.</a>. I am not plotting the Pillbox because it is not circularly symmetric. In has the same LSF for the horizontal and vertical directions, but its LSF differs in other directions. 

## Chromatic aberration types {#sec-transverse-chromatic-aberration}
In @sec-chromatic-aberration I explained the idea that because the index of refraction of a lens might vary with wavelength, the best focus distance would also vary with wavelength. In addition, even for an ideal lens, there will be a wavelength dependence arising from the wave nature of light and the size of the aperture or ($\text{f}/\#$). 

A typical consequence for optical systems is that we can achieve a very good focus (small PSF) for one wavelength, but less precise for others.  change in shape of the PSF as we vary the wavelength is called This type of chromatic aberration, a defocus of the image, is called **longitudinal chromatic aberration**.

There is a second type of chromatic aberration, called **transverse chromatic aberration** that arises in real imaging systems. The transverse aberration is somewhat related to the longitudinal aberration, but it has a different physical basis.

## Point spread engineering {#sec-optics-psf-engineering}

See if we can find the example of the astronomer looking for exoplanets.  He wanted a point spread that was black in the center!

Based on your description, the astronomer you're looking for is Jeffrey Chilcote.

He is an expert in building astronomical instruments and is known for his work on instruments designed to directly image exoplanets. He was part of the team that built the Coronagraphic High Angular Resolution Imaging Spectrograph (CHARIS), which operates on the Subaru Telescope in Hawaii. This instrument uses a coronagraph, which is a type of point spread function engineering that is "black in the middle" to block out the light from the central star, allowing for the detection of fainter, nearby planets.

While he worked in Hawaii, he is now an associate professor at the University of Notre Dame. He is also involved in the Gemini Planet Imager (GPI) and is part of a project to upgrade it and relocate it to the Gemini North Observatory in Hawaii.

Jeff Kuhn also fits that description. He is an astronomer at the University of Hawaiʻi's Institute for Astronomy and has a long history of working on instruments and technologies for both solar observation and exoplanet detection.

He is a key figure in the development of a special type of telescope called the Exo-Life Finder (ELF), which is designed to directly image exoplanets. The technology behind this, as you describe, involves creating a coronagraphic point spread function (PSF) that effectively blocks the light from the central star, allowing the much fainter light from nearby planets to be detected. This is a form of PSF engineering with a "black in the middle" design, specifically for the purpose of finding exoplanets.

He has also been involved in other projects with similar goals, such as the Kermit camera for the Lyot Project Coronagraph, which was designed for "companion searches close to bright stars."

https://www.light-am.com/article/doi/10.37188/lam.2025.033


Dr. Rafael Piestun, a professor at the University of Colorado Boulder.

He and his research group developed the Double-Helix Point Spread Function (DH-PSF). This innovative optical method encodes the 3D position of a point source of light into a unique 2D image. The image consists of two lobes that appear to rotate around a central point, and the angle of this rotation directly corresponds to the depth (z-position) of the original point.

Dr. Piestun also co-founded a company based on this technology called Double Helix Optics, which commercializes 3D imaging and sensing systems.