# Linear optics - space {#sec-optics-linear-space}
Geometric optics helps us understand how lenses work by focusing on things like the shape of the lens, what it’s made of, and how far it is from the sensor. This approach is great for designing lenses and figuring out their basic properties.

But there’s another way to look at optical systems: as signal processors. In this framing, an optical system takes in light from the scene and transforms it into light at the sensor. The signal processing perspective is powerful because it lets us think about what the system does, without worrying too much about the details of how it’s built. It’s a practical way to develop image systems simulations and to analyze optical systems.

This approach is especially useful because many optical systems are close to being linear —they satisfy the **Principle of Superposition**. You can test this property in the lab. There are lots of mathematical tools for working with linear systems, and they are taught in many context across science, technology, engineering and mathematics.

Because many readers will be familiar with the ideas, I put an introduction to linear systems into a linear systems appendix, @sec-appendix-linear-systems.  If you are familiar with linear systems principles and tools, use that appendix to understand the notation and thinking for the image systems engineering simulations in this book.

## Point spread functions {#sec-pointspread-2}
In @sec-pointspread I introduced the idea of the point spread function (PSF). It is simply the image of a point of light. If we know the point spread for each location in the scene, and superposition holds, we can predict how any input scene becomes an image. 

The scene is a collection of scene points, $\mathbf{p} = (p_x, p_y, p_z)$, each with its own intensity $S(\mathbf{p})$. Write the point spread function of a scene point as $\text{PSF}_\mathbf{p}(x, y)$, where $(x, y)$ are positions in the image. Using superposition, we calculate the output image, $O(x,y)$ by adding up the weighted point spreads from every scene point. The formula looks like this:

$$
O(x, y) = \sum_{\mathbf{p}} s(\mathbf{p})\, \text{PSF}_\mathbf{p}(x, y)
$$ {#eq-pointspread-1}

Here, $O(x, y)$ is the intensity at each image position. Each point in the scene contributes a scaled version of its PSF. The output image is the weighted sum of all those PSFs.

Point spread functions are an intuitive way to understand the performance of the optical system, andthey are widely used in applications. In @sec-optics-psf-engineering I will describe how scientists and engineers design optics to achieve specific PSF characteristics to estimate depth or to reveal hard-to-see objects.

In the @sec-appendix-linear-systems I describe how the point spread function is used to create the linear system matrix: each column of the linear system matrix is a point spread.  When the system is shift-invariant (@sec-ls-shiftinvariance), the columns of the matrix are shifted copies of that point spread.  We also explain that harmonics are eigenvectors of such matrices.  This section further explores the analysis of optics using PSFs, and  @sec-optics-harmonics we explores applications using harmonics.

::: {.callout-note title="A limitation of PSFs" collapse="false"}
Point spread functions (PSFs) are an excellent tool for modeling optical systems when the scene can be approximated as a flat surface, or when all objects are far enough away that occlusion is negligible. In these cases, each point in the scene contributes independently to the image, and the total image can be predicted by summing the PSFs from each point.

However, in three-dimensional scenes where objects can block light from those behind them (**occlusion**), the situation becomes more complex. The contribution of a point to the image may be partially or completely blocked by closer objects. As a result, the effective point spread depends on the arrangement of objects in the scene, and cannot be characterized by a single, scene-independent PSF. Accurately modeling these effects requires more advanced techniques, such as those used in computer graphics, and often involves recalculating the image formation for each specific scene.

In summary, PSFs provide a powerful and practical framework for image prediction when occlusion can be ignored, but their direct application is limited in scenes with significant three-dimensional structure and occlusion. In those cases, computer graphics techniques based on ray tracing are more appropriate.
:::

## Common PSFs in simulation {#sec-pointspread-common}
The PSFs of real systems can be difficult to model using a simple formula. In ISETCam we make it possible to provide an empirical PSF when it is known.  But there are many cases in which the PSF is not yet known, say for a system that is under design and the optics have not yet been selected. In such optical system simulations, a few PSFs are routinely used to approximate image system performance prior to implementation (@fig-psf-graph).

- **Airy pattern** is the only one of these PSFs that is based on the physics of light waves. This is the PSF we measure for a perfect, diffraction-limited lens imaging a point at a distance (@sec-airy-pattern and @fig-blur-comparison). The Airy pattern depends on wavelength and represents a best case limit. The optics you use will not be better than this.
- **Gaussian** is popular because it’s easy to work with in calculations. 
- **Lorentzian** (also called Cauchy) has higher tails than the Gaussian. This makes it better for modeling systems where light gets scattered a lot—like if the lens is scratched or there are many internal reflections from a multi-element lens assembly.
- **Pillbox** A square region with constant values that averages the values over the region. Who doesn't like a simple average?  Well, I don't - but apart from me?

A graph of the cross-section through these PSFs is a useful way to compare them. 

![Cross-section of four  point spread functions that are widely used in simulation. These point spreads were applied for a lens and sensor configuration with an $f/\#$ of 4 and a focal length of 3.9mm. The radius of the Airy disk for this configuration is 3.4 microns (see the zero-crossing of the Airy pattern).](images/optics/optics-siKernels-graph.png){#fig-psf-graph width="60%"}

We can simulate how the four PSF shapes transform a spectral radiance (@fig-psf-image). We compare the optical images four each of the PSFs for a scene comprising a set of grid lines. The images were calculated using the ISETCam script <a href="../code/fise_opticsPSF.html" target="_blank">fise_opticsPSF.</a>  The Airy and Gaussian PSFs were implemented with wavelength-dependent point spread functions. The Pillbox and Lorentzian PSFs were implemented as wavelength-indepedent.

![The Airy pattern shows the image for a diffraction limited lens with an $f/\# =4$. The Gaussian has a spread that is roughly consistent with the Airy Disk size and a similar wavelength dependency; long-wavelengths spread more than short wavelengths. The high tail of the Lorentzian PSF produces a veiling glare. The Pillbox size is about equal to the Airy disk, but it introduces much more blur because it has a relatively high level over the entire width.](images/optics/optics-siKernels-image.png){#fig-psf-image width="90%"}

These PSFs functions are valuable because they have simple parameters that control the amount of blur or light scatter. Here is a description of the different PSF formulae along with a brief descrition of their properties.

### The Airy pattern
The Airy pattern is circularly symmetric, so you only need to know the distance from the center, $r$, to describe it. The intensity at a distance $r$ from the center is:

$$
I(r) = I_0 \left[ \frac{2 J_1\left( \frac{\pi r}{\lambda N} \right)}{ \frac{\pi r}{\lambda N} } \right]^2
$$

Where:

- $I_0$ is the peak intensity at the center
- $J_1$ is the first-order Bessel function
- $N$ is the $f/\#$ of the lens
- $\lambda$ is the wavelength of light

The first dark ring defines the Airy disk (@fig-blur-comparison , @sec-airy-pattern).  It is the diameter where the intensity drops to zero, and defines the smallest spot size.

$$
d = \arcsin(2.44 \, \lambda N) \approx 2.44 \, \lambda N
$$ {#eq-airydisk3}

For $f/\# = 4$ the diameter ranges from 3.9 $\mu\text{m}$ (400 nm) to 6.83 $\mu\text{m}$ (700nm).  See the ISETCam function <a href="https://github.com/ISET/isetcam/blob/main/opticalimage/optics/airyDisk.m" target="_blank">`airyDisk`</a>

### Gaussian
The Gaussian is the classic “bell curve.” It’s simple and often a used as an approximation for optical systems.
$$
I(r) = I_0 \exp\left( -\frac{r^2}{2\sigma^2} \right)
$$

Here, $\sigma$ sets how spread out the blur is. Larger $\sigma$ means a blurrier image.

### Lorentzian (Cauchy)
The Lorentzian function appears in physics and probability theory. It’s famous for its “fat tails”—meaning it doesn’t drop off as quickly as a Gaussian. This makes it handy for modeling blur when there’s a lot of stray light or scattering.

$$
I(r) = \frac{I_0}{1 + \left( \frac{r}{\gamma} \right)^2}
$$

Here, $\gamma$ controls how wide the central peak is. A bigger $\gamma$ means a wider, flatter blur with more light in the tails.

All of these PSFs are simulated in <a href="https://github.com/ISET/isetcam/blob/main/opticalimage/optics/shiftinvariant/siSynthetic.m" target="_blank">`siSynthetic.m`.</a>

::: {.callout-note title="Linespread functions" collapse="true"}
In @fig-psf-image, I illustrated the effect of the PSFs using grid lines rather than points. It seemed to me that the differences are  easier to visualize the impact of the point spread this way. The image created by a very thin line is called the **line spread function (LSF)**.

To find the LSF from the PSF, we simply sum along the direction (see <a href="https://github.com/ISET/isetcam/blob/main/opticalimage/wavefront/psf/psf2lsf.m" target="_blank">`psf2lsf.m`.</a>. We will calculate the same LSF for any direction when the PSF is cicularly symmetric.  If it is not, then we will find a different LSF for each direction. 

The formula to calculate the PSF in a direction, in this case the $y$ direction, is very simple. We just sum across the $x$ direction.

$$
LSF = \sum_x PSF(x,y)
$$

Here are the linespread functions of the Gaussian and Lorenzian.

Given an LSF, it is possible to recover a unique circularly symmetric PSF using <a href="https://github.com/ISET/isetcam/blob/main/opticalimage/wavefront/psf/lsf2circularpsf.m" target="_blank">`lsf2circularpsf.m`.</a>. The Airy pattern is circularly symmetric; the Gaussian and Lorentz are usually implemented as circularly symmetric.  The pillbox is not.
:::

One of the challenges in image system simulations is to select the wavelength dependence of the PSF.  This dependence is formally specified for the Airy pattern, based on the wavelength of light and the size of the circular, pinhole aperture.  But for the Gaussian and Lorentzian, we must make a choice.

One way to make a decision is to measure the wavelength dependence for an existing system, say a system that uses materials with known properties, and use those measurements to guide the simulation. The variations in the PSF caused by these wavelength-dependences are called chromatic aberrations. There are two types of chromatic aberrations, and we describe them and how they might be measured in the next section.

## Chromatic aberration types {#sec-transverse-chromatic-aberration}
Because the index of refraction of a lens might vary with wavelength, the best focus distance also varies with wavelength (@sec-chromatic-aberration). Even for an ideal lens there is a wavelength dependence arising from the wave nature of light and the size of the aperture or $\text{f}/\#$ (@eq-airy1). 

### Longitudinal chromatic aberration
A consequence of this wavelength-dependence is that for a given image distance will be in best focus (small PSF) for one wavelength, but not all. The wavelength-dependent defocus is called **longitudinal chromatic aberration (LCA)**.

There is noticeable chromatic aberration for many types of lenses and materials.  The most surprising optical system with a large amount of chromatic aberration is the human eye!  The line spread and point spread functions measured at the human retina both vary considerably as we change the stimulus wavelength.

:::{.panel-tabset}
## Line Spread Function
![Typical human line spread functions. The dashed white lines are the relative retinal illuminance.](images/optics/linespread.png){#fig-optics-linespread}

## Point Spread Function
![Typical human point spread functions. The large spread for the blue is striking.  Notice that the orientation of the blur differs for this example subject, which is typical.](images/optics/pointspread.png){#fig-optics-pointspread}
:::

<figcaption>
Click on the tabs at the top to see typical human line spread functions or point spread functions for stimuli with equal energy, 425 nm, 525 nm, and 625 nm. See <a href="../code/02Optics/fise_humanLSF.html" target="_blank">Human LSF</a> for how ISETBio/ISETCam implements that calculation.

</figcaption>


The very large LCA of the human eye has consequences for many aspects of the human visual system and image systems engineering technology.  We will describe it quantitatively and explain why it matters to technology in @sec-human.

### Transverse chromatic aberration
A second type of chromatic aberration, called **transverse chromatic aberration (TCA)**, is also common in real imaging systems. While it is related to LCA, its physical cause is different.

TCA occurs when the image blur interacts with the position and size of the aperture. The two panels of the figure below show short wavelength (blue) and long wavelength (red) rays, originating from a point at the lower left. The blue rays are in focus, forming a tight spot on the image plane. The red rays, however, come to focus behind the image plane, creating a larger, blurred red circle.

:::{.panel-tabset}
## Longitudinal chromatic aberration
![With this aperture, there is only longitudinal chromatic aberration (LCA) and no transverse chromatic aberration (TCA).](images/optics/chromatic-notransverse.png){#fig-optics-notransverse width=70%}

## Transverse chromatic aberration
![With this pupil aperture, some of the rays from this point will be blocked from entering the lower part of the lens. The absence of these rays has a differential effect on the short- and long-wavelength light. The of the center of the red blur circle will be shifted so it no longer aligns with the center of the blue blur circle (TCA).](images/optics/chromatic-transverse.png){#fig-optics-transverse width=70%}
:::

<figcaption>
The images illustrate short wavelength (blue) and long wavelength (red) rays a single point. The blue rays are in good focus, but the red rays have a large blur circle. The two panels illustrate how changing the aperture position interacts with defocus to create transverse chromatic aberration. See the text for details.


</figcaption>
<!-- keep the empty lines for spacing in the rendered image -->

Now, imagine reducing the aperture size and moving it in front of the lens. This new aperture position blocks some of the rays from both wavelengths. For the blue rays, which were already converging to a tight focus, the effect is minimal —the spot gets a little dimmer. But for the red rays, which form a large blur circle already, the new aperture blocks rays that would have formed the lower part of their blur circle. As a result, the red blur circle is clipped at the bottom, and its center shifts upward. The red and blue circles are no longer aligned. Th displacement of the red blur circle is transverse chromatic aberration.

The size and position of the aperture determines the amount of TCA. The physical factors that cause the transverse shift are different from those that produce LCA. Both effects are related to the different wavelengths focus.  Were there no LCA, there would be no TCA.

## Point spread engineering {#sec-optics-psf-engineering}
The usual goal of optical design for consumer photography is to create a lens that has a very compact point spread function (sharp image) that is the same for all wavelengths (no chromatic aberration).  This makes sense because people want their photographs to be sharp and free of the kind of chromatic fringing you can see in @fig-optics-linespread and @fig-optics-pointspread.

The goal of optical design can differ, of course, for different applications. In the sections below, I describe two cases that I found to be very interesting.  The first case designs optics for telescopes that are looking for planets in other star systems (exoplanets).  The second case designs optics that can be helpful in estimating the depth of objects in an image. For example, in microscopy we might be interested to measure the tissue layer where a particular glowing molecule resides. These two examples are drawn from the literature of PSF engineering.

### Exoplanets and PSF engineering {#sec-psf-expolanets}
When we see an eclipse of the sun, the moon blocks out the bright central region.  The edge of the sun, its corona, becomes visible. Its properties have intrigued people for centuries, but waiting for a solar eclipse to study it was quite limiting.  Thus, there was considerable interest when Bernard Lyot, in the 1930s, introduced an optical technique to view and measure the sun's corona — the coronagraph. The basic idea has been extended over the years and modern space telescopes include [coronograph instrumentation](https://jwst-docs.stsci.edu/jwst-mid-infrared-instrument/miri-instrumentation/miri-coronagraphs#gsc.tab=0){target=_blank}.

A coronagraph is an optical instrument designed to block the light from a bright central source (like the sun or a star), making it possible to observe faint objects nearby, such as the solar corona.  The key to a coronagraph’s effectiveness is engineering the point spread function (PSF) so that light from the central source is suppressed in the image.  This allows much dimmer features to be detected. Modern coronagraphs use carefully designed apertures and phase masks to create a “dark spot” in the PSF at the location of the bright source, dramatically improving the ability to see faint companions or structures close to it. This is a good example of **PSF engineering**:** designing optics to implement PSF that achieves a specific goal. 

::: {.callout-note title="Lyot coronagraph" collapse="false"}

Image from the star evenly illuminates the telescope pupil and is brought to a focus at a point behind the first lens.  This point is then reimaged with a second lens that passes through a 'Lyot stop'.  The new image 

![JWST description of the Lyot coronagraph. The 2nd image is black because the bright light at the center is blocked.  I don't know why there is a black disk in the first image.  The 3rd image shows the diffraction pattern, arising from the light at the margin of the occulting spot. The 4th image, following the slightly smaller pupil, shows the corona.  The light level is about 1/200th of the original light level because of the masks.](images/optics/optics-lyot-coronagraph.png){#fig-optics-lyot width="80%"}

[ChartGPT description of the optical path.  The picture isn't very good, but the text and math work.](https://chatgpt.com/share/6892d701-54d0-8002-8137-ec17287ab549)

:::

<!-- https://www.spiedigitallibrary.org/conference-proceedings-of-spie/12182/121820G/The-small-ELF-project--toward-an-ultra-large-coronagraphic/10.1117/12.2629645.full 

Jeff describes the Exo-Life Finder (ELF) project.  The quotes below are about the Small ELF (SELF) prototype.

Kuhn is an astronomer at the University of Hawaiʻi's Institute for Astronomy and has a long history of working on instruments and technologies for both solar observation and exoplanet detection.

Kuhn is a key figure in the development of a special type of telescope called the Exo-Life Finder (ELF), which is designed to directly image exoplanets. The technology behind this, as you describe, involves creating a coronagraphic point spread function (PSF) that effectively blocks the light from the central star, allowing the much fainter light from nearby planets to be detected. This is a form of PSF engineering with a "black in the middle" design, specifically for the purpose of finding exoplanets.

He has also been involved in other projects with similar goals, such as the Kermit camera for the Lyot Project Coronagraph, which was designed for "companion searches close to bright stars."

https://www.light-am.com/article/doi/10.37188/lam.2025.033
-->

In recent years, coronograph techniques have been applied to discovering **exoplanets**; these are planets that orbit a star outside the solar system. One such project, the Exo-Life Finder (ELF), is using PSF engineering to search for life on exoplanets. 

> The telescope we’d like to  design  and  build  targets  the  problem  of  finding  life around planets outside the solar system. To do this requires separating  the  reflected  starlight  off  the  planet  from  the star.  Astronomers  call  this “direct  imaging”.  This  means we’re interested in optical systems that can have a narrow field-of-view  (FOV),  perhaps  just  a  few  arcseconds,  and
enormous  photometric  dynamic  range  capability.  A laudable  goal  is  to  achieve  raw  sensitivity  of  $10^−8$ of  the
stellar  flux  in  a  region  within  about  1  arcsecond  of  the central  star.  A  dedicated  telescope  with  this  capability
could  make  images  of  the  surface  of  nearby  exoplanets using  data  inversion  techniques,  perhaps  sufficient  to  see even signs of advanced exolife civilizations (@kuhn2025-astronomy).

The paper by @kuhn2025-astronomy describes the challenges in building a large optical instrument that can suppress the central light from a star to reveal the light reflected from the near by circling exoplanet.  Building such an instrument involves mechanical engineering, material science, optics, signal processing, financial and political considerations.

![Optics design for a point spread with a dark spot in the middle, where the star is located.  That would be panel e.  The design of the pupil aperture with these multiple little circles and some of them delaying the phase, is shown in a-d.](images/optics/optics-psfengineering-kuhn.png){#fig-psfengineering-kuhn width=70%}

### Depth and PSF engineering {#sec-psf-depth}
Biologists frequently use  fluorescent markers to tag specific molecules. These tags identify a single molecule, and when stimulated they emit enough photons to be easily visible in a dark field microscope.  Because each of the molecules is so small, they are effectively points.  The image that they form through the microscope is a point spread function.

For certain purposes the only information the investigator measures is the $(x,y)$ position of the tagged molecule. This is possible when the biological tissue is sparsely labeled, because each molecule produces a point spread image. As long as the labeled molecules are sparsely sampled, their images will be separated. To find the position of these molecules and count them, it is only necessary to count the point spreads in the image. The position of the molecule is assumed to be at the centroid of the point spread. In this way, the location is determined at high precision compared to the size of the point spread. This approach is called **localization microscopy**. Some say this method localizes the molecules beyond the diffraction limit, which seems magical. It is not magical, nor is this my favorite comment. The technique is useful, not magical.

When the biological sample has significant thickness, it would be very nice to know its depth in addition to its position. YOu can see how this might be possible because the point spread function will depend on the distance of the molecule from the optics. But the usual dependence is just a change in the size of the blur circle. This is hard to measure.  

![Double-helix point spread from Piestun. Figure 1 from the PNAS article with Moerner.  SLM is a spatial light modulator to control the phase. Probably this figure should be redrawn for here.](images/optics/piestun-dh-1.png){#fig-optics-double-helix width=70%}

Rafael Piestun and his collaborators engineered an optical system that has an oriented point spread function, comprising a pair of lines. They implemented optics so that the orientation of the point spread lines varies with depth (Figure ref here). The image consists of two lobes that appear to rotate around a central point, and the angle of this rotation directly corresponds to the depth (z-position) of the original point. If one plots the point spread as a function of z-position in the biological substrate, the pattern forms a double-helix. In practical applications with sparsely labeled molecules one can measure the centroid of the separated point spreads to localize position, and the orientation of the point spread to measure depth.  

::: {.callout-note title="Prizes and companies" collapse="false"}
A number of different groups have developed methods for localizing the position of these fluorophores, including Xiaowei Zhuang, W.E. Moerner (Localization microscopy) ,  Betzsig (Localization microscopy) Stefan Hell (STED) and.  The latter three were [awarded the Nobel Prize](https://www.theguardian.com/science/2014/oct/08/nobel-prize-chemistry-trio-microscopy-betzig-moerner-hell#:~:text=Eric%20Betzig%20and%20William%20Moerner,more%20than%2010%20years%20old), and Zhuang has won numerous other prizes for her work.  Who was the 3rd Nobel Laureate with Moerner?

nanoscopy, rather than microscopy

<!--
The woman scientist you're thinking of is Xiaowei Zhuang, a biophysicist at Harvard University.

The super-resolution technique she developed is called Stochastic Optical Reconstruction Microscopy, which is more commonly known by its "cute" acronym, STORM.-->

Dr. Piestun co-founded a company based on this technology called [Double Helix Optics](https://www.doublehelixoptics.com/), which commercializes 3D imaging and sensing systems.
:::
