# Linear optics - space {#sec-optics-linear-space}
Geometric optics helps us understand how lenses work by focusing on things like the shape of the lens, what it’s made of, and how far it is from the sensor. This approach is great for designing lenses and figuring out their basic properties.

But there’s another way to look at optical systems: as signal processors. In this framing, an optical system takes in light from the scene and transforms it into light at the sensor. The signal processing perspective is powerful because it lets us think about what the system does, without worrying too much about the details of how it’s built. It’s a practical way to develop image systems simulations and to analyze optical systems.

This approach is especially useful because many optical systems are close to being linear —they satisfy the **Principle of Superposition**. You can test this property in the lab. There are lots of mathematical tools for working with linear systems, and they are taught in many context across science, technology, engineering and mathematics.

Because many readers will be familiar with the ideas, I put an introduction to linear systems into a linear systems appendix, @sec-appendix-linear-systems.  If you are familiar with linear systems principles and tools, use that appendix to understand the notation and thinking for the image systems engineering simulations in this book.

## Point spread functions {#sec-pointspread-2}
In @sec-pointspread I introduced the idea of the point spread function (PSF). It is simply the image of a point of light. If we know the point spread for each location in the scene, and superposition holds, we can predict how any input scene becomes an image. 

The scene is a collection of scene points, $\mathbf{p} = (p_x, p_y, p_z)$, each with its own intensity $S(\mathbf{p})$. Write the point spread function of a scene point as $\text{PSF}_\mathbf{p}(x, y)$, where $(x, y)$ are positions in the image. Using superposition, we calculate the output image, $O(x,y)$ by adding up the weighted point spreads from every scene point. The formula looks like this:

$$
O(x, y) = \sum_{\mathbf{p}} s(\mathbf{p})\, \text{PSF}_\mathbf{p}(x, y)
$$ {#eq-pointspread-1}

Here, $O(x, y)$ is the intensity at each image position. Each point in the scene contributes a scaled version of its PSF. The output image is the weighted sum of all those PSFs.

Point spread functions are an intuitive way to understand the performance of the optical system, andthey are widely used in applications. In @sec-optics-psf-engineering I will describe how scientists and engineers design optics to achieve specific PSF characteristics to estimate depth or to reveal hard-to-see objects.

In the @sec-appendix-linear-systems I describe how the point spread function is used to create the linear system matrix: each column of the linear system matrix is a point spread.  When the system is shift-invariant (@sec-ls-shiftinvariance), the columns of the matrix are shifted copies of that point spread.  We also explain that harmonics are eigenvectors of such matrices.  This section further explores the analysis of optics using PSFs, and  @sec-optics-harmonics we explores applications using harmonics.

::: {.callout-note title="A limitation of PSFs" collapse="true"}
Point spread functions (PSFs) are an excellent tool for modeling optical systems when the scene can be approximated as a flat surface, or when all objects are far enough away that occlusion is negligible. In these cases, each point in the scene contributes independently to the image, and the total image can be predicted by summing the PSFs from each point.

However, in three-dimensional scenes where objects can block light from those behind them (**occlusion**), the situation becomes more complex. The contribution of a point to the image may be partially or completely blocked by closer objects. As a result, the effective point spread depends on the arrangement of objects in the scene, and cannot be characterized by a single, scene-independent PSF. Accurately modeling these effects requires more advanced techniques, such as those used in computer graphics, and often involves recalculating the image formation for each specific scene.

In summary, PSFs provide a powerful and practical framework for image prediction when occlusion can be ignored, but their direct application is limited in scenes with significant three-dimensional structure and occlusion. In those cases, computer graphics techniques based on ray tracing are more appropriate.
:::

## Common PSFs in simulation {#sec-pointspread-common}
The PSFs of real systems can be difficult to model using a simple formula. In ISETCam we make it possible to provide an empirical PSF when it is known.  But there are many cases in which the PSF is not yet known, say for a system that is under design and the optics have not yet been selected. In such optical system simulations, a few PSFs are routinely used to approximate image system performance prior to implementation (@fig-psf-graph).

- **Airy pattern** is the only one of these PSFs that is based on the physics of light waves. This is the PSF we measure for a perfect, diffraction-limited lens imaging a point at a distance (@sec-airy-pattern and @fig-blur-comparison). The Airy pattern depends on wavelength and represents a best case limit. The optics you use will not be better than this.
- **Gaussian** is popular because it’s easy to work with in calculations. 
- **Lorentzian** (also called Cauchy) has higher tails than the Gaussian. This makes it better for modeling systems where light gets scattered a lot—like if the lens is scratched or there are many internal reflections from a multi-element lens assembly.
- **Pillbox** A square region with constant values that averages the values over the region. Who doesn't like a simple average?  Well, I don't - but apart from me?

A graph of the cross-section through these PSFs is a useful way to compare them. 

![Cross-section of four  point spread functions that are widely used in simulation. These point spreads were applied for a lens and sensor configuration with an $f/\#$ of 4 and a focal length of 3.9mm. The radius of the Airy disk for this configuration is 3.4 microns (see the zero-crossing of the Airy pattern).](images/optics/optics-siKernels-graph.png){#fig-psf-graph width="60%"}

We can simulate how the four PSF shapes transform a spectral radiance (@fig-psf-image). We compare the optical images four each of the PSFs for a scene comprising a set of grid lines. The images were calculated using the ISETCam script <a href="../code/fise_opticsPSF.html" target="_blank">fise_opticsPSF.</a>  The Airy and Gaussian PSFs were implemented with wavelength-dependent point spread functions. The Pillbox and Lorentzian PSFs were implemented as wavelength-indepedent.

![The Airy pattern shows the image for a diffraction limited lens with an $f/\# =4$. The Gaussian has a spread that is roughly consistent with the Airy Disk size and a similar wavelength dependency; long-wavelengths spread more than short wavelengths. The high tail of the Lorentzian PSF produces a veiling glare. The Pillbox size is about equal to the Airy disk, but it introduces much more blur because it has a relatively high level over the entire width.](images/optics/optics-siKernels-image.png){#fig-psf-image width="90%"}

These PSFs functions are valuable because they have simple parameters that control the amount of blur or light scatter. Here is a description of the different PSF formulae along with a brief descrition of their properties.

### The Airy pattern
The Airy pattern is circularly symmetric, so you only need to know the distance from the center, $r$, to describe it. The intensity at a distance $r$ from the center is:

$$
I(r) = I_0 \left[ \frac{2 J_1\left( \frac{\pi r}{\lambda N} \right)}{ \frac{\pi r}{\lambda N} } \right]^2
$$

Where:

- $I_0$ is the peak intensity at the center
- $J_1$ is the first-order Bessel function
- $N$ is the $f/\#$ of the lens
- $\lambda$ is the wavelength of light

The first dark ring defines the Airy disk (@fig-blur-comparison , @sec-airy-pattern).  It is the diameter where the intensity drops to zero, and defines the smallest spot size.

$$
d = \arcsin(2.44 \, \lambda N) \approx 2.44 \, \lambda N
$$ {#eq-airydisk3}

For $f/\# = 4$ the diameter ranges from 3.9 $\mu\text{m}$ (400 nm) to 6.83 $\mu\text{m}$ (700nm).  See the ISETCam function <a href="https://github.com/ISET/isetcam/blob/main/opticalimage/optics/airyDisk.m" target="_blank">`airyDisk`</a>

### Gaussian
The Gaussian is the classic “bell curve.” It’s simple and often a used as an approximation for optical systems.
$$
I(r) = I_0 \exp\left( -\frac{r^2}{2\sigma^2} \right)
$$

Here, $\sigma$ sets how spread out the blur is. Larger $\sigma$ means a blurrier image.

### Lorentzian (Cauchy)
The Lorentzian function appears in physics and probability theory. It’s famous for its “fat tails”—meaning it doesn’t drop off as quickly as a Gaussian. This makes it handy for modeling blur when there’s a lot of stray light or scattering.

$$
I(r) = \frac{I_0}{1 + \left( \frac{r}{\gamma} \right)^2}
$$

Here, $\gamma$ controls how wide the central peak is. A bigger $\gamma$ means a wider, flatter blur with more light in the tails.

All of these PSFs are simulated in <a href="https://github.com/ISET/isetcam/blob/main/opticalimage/optics/shiftinvariant/siSynthetic.m" target="_blank">`siSynthetic.m`.</a>

::: {.callout-note title="Linespread functions" collapse="true"}
In @fig-psf-image, I illustrated the effect of the PSFs using grid lines rather than points. It seemed to me that the differences are  easier to visualize the impact of the point spread this way. The image created by a very thin line is called the **line spread function (LSF)**.

To find the LSF from the PSF, we simply sum along the direction (see <a href="https://github.com/ISET/isetcam/blob/main/opticalimage/wavefront/psf/psf2lsf.m" target="_blank">`psf2lsf.m`.</a>. We will calculate the same LSF for any direction when the PSF is cicularly symmetric.  If it is not, then we will find a different LSF for each direction. 

The formula to calculate the PSF in a direction, in this case the $y$ direction, is very simple. We just sum across the $x$ direction.

$$
LSF = \sum_x PSF(x,y)
$$

Here are the linespread functions of the Gaussian and Lorenzian.

Given an LSF, it is possible to recover a unique circularly symmetric PSF using <a href="https://github.com/ISET/isetcam/blob/main/opticalimage/wavefront/psf/lsf2circularpsf.m" target="_blank">`lsf2circularpsf.m`.</a>. The Airy pattern is circularly symmetric; the Gaussian and Lorentz are usually implemented as circularly symmetric.  The pillbox is not.
:::

One of the challenges in pure simulation studies is to determine how to specify the wavelength dependence of the PSF.  This is only formally specified for the Airy pattern, based on the wavelength of light and the size of the circular, pinhole aperture.  But for the Gaussian and Lorentzian, we must make a choice.

It is possible to measure the wavelength dependence for an existing system, say a system that uses materials with known properties. The variations in the PSF caused by these wavelength-dependences are called chromatic aberrations.  We describe the two types of aberrations and how they might be measured in the next section.

## Chromatic aberration types {#sec-transverse-chromatic-aberration}
Because the index of refraction of a lens might vary with wavelength, the best focus distance also varies with wavelength (@sec-chromatic-aberration). Even for an ideal lens there is a wavelength dependence arising from the wave nature of light and the size of the aperture or $\text{f}/\#$ (@eq-airy1). 

### Longitudinal aberration
A consequence of this wavelength-dependence is that for a given image distance will be in best focus (small PSF) for one wavelength, but not all. The wavelength-dependent defocus is called **longitudinal chromatic aberration**.

There is noticeable chromatic aberration for many types of lenses and materials.  The most surprising optical system with a large amount of chromatic aberration is the human eye!  The line spread and point spread functions measured at the human retina both vary considerably as we change the stimulus wavelength.

:::{.panel-tabset}
## Line Spread Function
![Typical human line spread functions. The dashed white lines are the relative retinal illuminance.](images/optics/linespread.png)

## Point Spread Function
![Typical human point spread functions. The large spread for the blue is striking.  Notice that the orientation of the blur differs for this example subject, which is typical.](images/optics/pointspread.png)
:::

<figcaption>
Click on the tabs at the top to see typical human line spread functions or point spread functions for stimuli with equal energy, 425 nm, 525 nm, and 625 nm. See <a href="../code/02Optics/fise_humanLSF.html" target="_blank">Human LSF</a> for how ISETBio/ISETCam implements that calculation.

</figcaption>


The very large longitudinal chromatic aberration of the human eye has consequences for many aspects of the human visual system and image systems engineering technology.  We will describe it quantitatively and explain why it matters to technology in @sec-human.

### Transverse aberration
A second type of chromatic aberration, called **transverse chromatic aberration**, is also common in real imaging systems. While it is related to longitudinal chromatic aberration, its physical origin is different.

Transverse chromatic aberration occurs when the image blur interacts with the position and size of the aperture. The top panel in the figure below shows blue and red rays from a point on the left. The blue rays are in focus, forming a tight spot on the image plane. The red rays, however, come to focus behind the image plane, creating a larger, blurred red circle.

![Transverse chromatic aberration. (A) Typical longitudinal chromatic aberration.  (B). Shifting the pupil aperture and size introduces transverse chromatic aberration.](images/optics/chromatic-transverse.png){#fig-optics-transverse width=70%}

Now, imagine reducing the aperture size and moving it in front of the lens. This new aperture position blocks some of the rays from both colors. For the blue rays, which were already converging to a tight focus, the effect is minimal—perhaps the spot gets a little dimmer. But for the red rays, which were more spread out, the shifted aperture blocks rays that would have formed the lower part of the red blur circle. As a result, the red blur circle is clipped at the bottom and its center shifts upward, so it no longer lines up with the blue spot. This lateral displacement is transverse chromatic aberration.

By adjusting the size and position of the aperture, we can control the amount and direction of transverse chromatic aberration. The physical factors that cause this shift are different from those that produce longitudinal chromatic aberration, even though both effects are related to how different wavelengths focus.

## Point spread engineering {#sec-optics-psf-engineering}

Interesting developments in PSF engineering here.  These are two I know about.  Maybe look a little deeper.

We don't have to be passive consumers of the psf.  We can try to make one that we like.

### Finding exoplanets through PSF engineering {#sec-psf-expolanets}

See if we can find the example of the astronomer looking for exoplanets.  He wanted a point spread that was black in the center!

Based on your description, the astronomer you're looking for is Jeffrey Chilcote.

He is an expert in building astronomical instruments and is known for his work on instruments designed to directly image exoplanets. He was part of the team that built the Coronagraphic High Angular Resolution Imaging Spectrograph (CHARIS), which operates on the Subaru Telescope in Hawaii. This instrument uses a coronagraph, which is a type of point spread function engineering that is "black in the middle" to block out the light from the central star, allowing for the detection of fainter, nearby planets.

While he worked in Hawaii, he is now an associate professor at the University of Notre Dame. He is also involved in the Gemini Planet Imager (GPI) and is part of a project to upgrade it and relocate it to the Gemini North Observatory in Hawaii.

Jeff Kuhn also fits that description. He is an astronomer at the University of Hawaiʻi's Institute for Astronomy and has a long history of working on instruments and technologies for both solar observation and exoplanet detection.

He is a key figure in the development of a special type of telescope called the Exo-Life Finder (ELF), which is designed to directly image exoplanets. The technology behind this, as you describe, involves creating a coronagraphic point spread function (PSF) that effectively blocks the light from the central star, allowing the much fainter light from nearby planets to be detected. This is a form of PSF engineering with a "black in the middle" design, specifically for the purpose of finding exoplanets.

He has also been involved in other projects with similar goals, such as the Kermit camera for the Lyot Project Coronagraph, which was designed for "companion searches close to bright stars."

https://www.light-am.com/article/doi/10.37188/lam.2025.033

### Estimating Depth PSF engineering {#sec-psf-depth}

Dr. Rafael Piestun, a professor at the University of Colorado Boulder.

He and his research group developed the Double-Helix Point Spread Function (DH-PSF). This innovative optical method encodes the 3D position of a point source of light into a unique 2D image. The image consists of two lobes that appear to rotate around a central point, and the angle of this rotation directly corresponds to the depth (z-position) of the original point.

Dr. Piestun also co-founded a company based on this technology called Double Helix Optics, which commercializes 3D imaging and sensing systems.