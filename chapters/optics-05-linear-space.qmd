# Linear optics - space {#sec-optics-linear-space}
Geometric optics helps us understand how lenses work by focusing on things like the shape of the lens, what it’s made of, and how far it is from the sensor. This approach is great for designing lenses and figuring out their basic properties.

But there’s another way to look at optical systems: as signal processors. In this framing, an optical system takes in light from the scene and transforms it into light at the sensor. The signal processing perspective is powerful because it lets us think about what the system does, without worrying too much about the details of how it’s built. It’s a practical way to develop image systems simulations and to analyze optical systems.

This approach is especially useful because many optical systems are close to being linear —they satisfy the **Principle of Superposition**. You can test this property in the lab. There are lots of mathematical tools for working with linear systems, and they are taught in many context across science, technology, engineering and mathematics.

Because many readers will be familiar with the ideas, I put an introduction to linear systems into a linear systems appendix, @sec-appendix-linear-systems.  If you are familiar with linear systems principles and tools, use that appendix to understand the notation and thinking for the image systems engineering simulations in this book.

## Point spread functions {#sec-pointspread-2}
In @sec-pointspread I introduced the idea of the point spread function (PSF). It is simply the image of a point of light. If we know the point spread for each location in the scene, and superposition holds, we can predict how any input scene becomes an image. 

The scene is a collection of scene points, $\mathbf{p} = (p_x, p_y, p_z)$, each with its own intensity $S(\mathbf{p})$. Write the point spread function of a scene point as $\text{PSF}_\mathbf{p}(x, y)$, where $(x, y)$ are positions in the image. Using superposition, we calculate the output image, $O(x,y)$ by adding up the weighted point spreads from every scene point. The formula looks like this:

$$
O(x, y) = \sum_{\mathbf{p}} s(\mathbf{p})\, \text{PSF}_\mathbf{p}(x, y)
$$ {#eq-pointspread-1}

Here, $O(x, y)$ is the intensity at each image position. Each point in the scene contributes a scaled version of its PSF. The output image is the weighted sum of all those PSFs.

Point spread functions are an intuitive way to understand the performance of the optical system, andthey are widely used in applications. In @sec-optics-psf-engineering I will describe how scientists and engineers design optics to achieve specific PSF characteristics to estimate depth or to reveal hard-to-see objects.

In the @sec-appendix-linear-systems I describe how the point spread function is used to create the linear system matrix: each column of the linear system matrix is a point spread.  When the system is shift-invariant (@sec-ls-shiftinvariance), the columns of the matrix are shifted copies of that point spread.  We also explain that harmonics are eigenvectors of such matrices.  This section further explores the analysis of optics using PSFs, and  @sec-optics-harmonics we explores applications using harmonics.

::: {.callout-note title="A limitation of PSFs" collapse="false"}
Point spread functions (PSFs) are an excellent tool for modeling optical systems when the scene can be approximated as a flat surface, or when all objects are far enough away that occlusion is negligible. In these cases, each point in the scene contributes independently to the image, and the total image can be predicted by summing the PSFs from each point.

However, in three-dimensional scenes where objects can block light from those behind them (**occlusion**), the situation becomes more complex. The contribution of a point to the image may be partially or completely blocked by closer objects. As a result, the effective point spread depends on the arrangement of objects in the scene, and cannot be characterized by a single, scene-independent PSF. Accurately modeling these effects requires more advanced techniques, such as those used in computer graphics, and often involves recalculating the image formation for each specific scene.

In summary, PSFs provide a powerful and practical framework for image prediction when occlusion can be ignored, but their direct application is limited in scenes with significant three-dimensional structure and occlusion. In those cases, computer graphics techniques based on ray tracing are more appropriate.
:::

## Common PSFs in simulation {#sec-pointspread-common}
The PSFs of real systems can be difficult to model using a simple formula. In ISETCam we make it possible to provide an empirical PSF when it is known.  But there are many cases in which the PSF is not yet known, say for a system that is under design and the optics have not yet been selected. In such optical system simulations, a few PSFs are routinely used to approximate image system performance prior to implementation (@fig-psf-graph).

- **Airy pattern** is the only one of these PSFs that is based on the physics of light waves. This is the PSF we measure for a perfect, diffraction-limited lens imaging a point at a distance (@sec-airy-pattern and @fig-blur-comparison). The Airy pattern depends on wavelength and represents a best case limit. The optics you use will not be better than this.
- **Gaussian** is popular because it’s easy to work with in calculations. 
- **Lorentzian** (also called Cauchy) has higher tails than the Gaussian. This makes it better for modeling systems where light gets scattered a lot—like if the lens is scratched or there are many internal reflections from a multi-element lens assembly.
- **Pillbox** A square region with constant values that averages the values over the region. Who doesn't like a simple average?  Well, I don't - but apart from me?

A graph of the cross-section through these PSFs is a useful way to compare them. 

![Cross-section of four  point spread functions that are widely used in simulation. These point spreads were applied for a lens and sensor configuration with an $f/\#$ of 4 and a focal length of 3.9mm. The radius of the Airy disk for this configuration is 3.4 microns (see the zero-crossing of the Airy pattern).](images/optics/optics-siKernels-graph.png){#fig-psf-graph width="60%"}

We can simulate how the four PSF shapes transform a spectral radiance (@fig-psf-image). We compare the optical images four each of the PSFs for a scene comprising a set of grid lines. The images were calculated using the ISETCam script <a href="../code/fise_opticsPSF.html" target="_blank">fise_opticsPSF.</a>  The Airy and Gaussian PSFs were implemented with wavelength-dependent point spread functions. The Pillbox and Lorentzian PSFs were implemented as wavelength-indepedent.

![The Airy pattern shows the image for a diffraction limited lens with an $f/\# =4$. The Gaussian has a spread that is roughly consistent with the Airy Disk size and a similar wavelength dependency; long-wavelengths spread more than short wavelengths. The high tail of the Lorentzian PSF produces a veiling glare. The Pillbox size is about equal to the Airy disk, but it introduces much more blur because it has a relatively high level over the entire width.](images/optics/optics-siKernels-image.png){#fig-psf-image width="90%"}

These PSFs functions are valuable because they have simple parameters that control the amount of blur or light scatter. Here is a description of the different PSF formulae along with a brief descrition of their properties.

### The Airy pattern
The Airy pattern is circularly symmetric, so you only need to know the distance from the center, $r$, to describe it. The intensity at a distance $r$ from the center is:

$$
I(r) = I_0 \left[ \frac{2 J_1\left( \frac{\pi r}{\lambda N} \right)}{ \frac{\pi r}{\lambda N} } \right]^2
$$

Where:

- $I_0$ is the peak intensity at the center
- $J_1$ is the first-order Bessel function
- $N$ is the $f/\#$ of the lens
- $\lambda$ is the wavelength of light

The first dark ring defines the Airy disk (@fig-blur-comparison , @sec-airy-pattern).  It is the diameter where the intensity drops to zero, and defines the smallest spot size.

$$
d = \arcsin(2.44 \, \lambda N) \approx 2.44 \, \lambda N
$$ {#eq-airydisk3}

For $f/\# = 4$ the diameter ranges from 3.9 $\mu\text{m}$ (400 nm) to 6.83 $\mu\text{m}$ (700nm).  See the ISETCam function <a href="https://github.com/ISET/isetcam/blob/main/opticalimage/optics/airyDisk.m" target="_blank">`airyDisk`</a>

### Gaussian
The Gaussian is the classic “bell curve.” It’s simple and often a used as an approximation for optical systems.
$$
I(r) = I_0 \exp\left( -\frac{r^2}{2\sigma^2} \right)
$$

Here, $\sigma$ sets how spread out the blur is. Larger $\sigma$ means a blurrier image.

### Lorentzian (Cauchy)
The Lorentzian function appears in physics and probability theory. It’s famous for its “fat tails”—meaning it doesn’t drop off as quickly as a Gaussian. This makes it handy for modeling blur when there’s a lot of stray light or scattering.

$$
I(r) = \frac{I_0}{1 + \left( \frac{r}{\gamma} \right)^2}
$$

Here, $\gamma$ controls how wide the central peak is. A bigger $\gamma$ means a wider, flatter blur with more light in the tails.

All of these PSFs are simulated in <a href="https://github.com/ISET/isetcam/blob/main/opticalimage/optics/shiftinvariant/siSynthetic.m" target="_blank">`siSynthetic.m`.</a>

::: {.callout-note title="Linespread functions" collapse="true"}
In @fig-psf-image, I illustrated the effect of the PSFs using grid lines rather than points. It seemed to me that the differences are  easier to visualize the impact of the point spread this way. The image created by a very thin line is called the **line spread function (LSF)**.

To find the LSF from the PSF, we simply sum along the direction (see <a href="https://github.com/ISET/isetcam/blob/main/opticalimage/wavefront/psf/psf2lsf.m" target="_blank">`psf2lsf.m`.</a>. We will calculate the same LSF for any direction when the PSF is cicularly symmetric.  If it is not, then we will find a different LSF for each direction. 

The formula to calculate the PSF in a direction, in this case the $y$ direction, is very simple. We just sum across the $x$ direction.

$$
LSF = \sum_x PSF(x,y)
$$

Here are the linespread functions of the Gaussian and Lorenzian.

Given an LSF, it is possible to recover a unique circularly symmetric PSF using <a href="https://github.com/ISET/isetcam/blob/main/opticalimage/wavefront/psf/lsf2circularpsf.m" target="_blank">`lsf2circularpsf.m`.</a>. The Airy pattern is circularly symmetric; the Gaussian and Lorentz are usually implemented as circularly symmetric.  The pillbox is not.
:::

One of the challenges in image system simulations is to select the wavelength dependence of the PSF.  This dependence is formally specified for the Airy pattern, based on the wavelength of light and the size of the circular, pinhole aperture.  But for the Gaussian and Lorentzian, we must make a choice.

One way to make a decision is to measure the wavelength dependence for an existing system, say a system that uses materials with known properties, and use those measurements to guide the simulation. The variations in the PSF caused by these wavelength-dependences are called chromatic aberrations. There are two types of chromatic aberrations, and we describe them and how they might be measured in the next section.

## Chromatic aberration types {#sec-transverse-chromatic-aberration}
Because the index of refraction of a lens might vary with wavelength, the best focus distance also varies with wavelength (@sec-chromatic-aberration). Even for an ideal lens there is a wavelength dependence arising from the wave nature of light and the size of the aperture or $\text{f}/\#$ (@eq-airy1). 

### Longitudinal chromatic aberration
A consequence of this wavelength-dependence is that for a given image distance will be in best focus (small PSF) for one wavelength, but not all. The wavelength-dependent defocus is called **longitudinal chromatic aberration (LCA)**.

There is noticeable chromatic aberration for many types of lenses and materials.  The most surprising optical system with a large amount of chromatic aberration is the human eye!  The line spread and point spread functions measured at the human retina both vary considerably as we change the stimulus wavelength.

:::{.panel-tabset}
## Line Spread Function
![Typical human line spread functions. The dashed white lines are the relative retinal illuminance.](images/optics/linespread.png){#fig-optics-linespread}

## Point Spread Function
![Typical human point spread functions. The large spread for the blue is striking.  Notice that the orientation of the blur differs for this example subject, which is typical.](images/optics/pointspread.png){#fig-optics-pointspread}
:::

<figcaption>
Click on the tabs at the top to see typical human line spread functions or point spread functions for stimuli with equal energy, 425 nm, 525 nm, and 625 nm. See <a href="../code/02Optics/fise_humanLSF.html" target="_blank">Human LSF</a> for how ISETBio/ISETCam implements that calculation.

</figcaption>


The very large LCA of the human eye has consequences for many aspects of the human visual system and image systems engineering technology.  We will describe it quantitatively and explain why it matters to technology in @sec-human.

### Transverse chromatic aberration
A second type of chromatic aberration, called **transverse chromatic aberration (TCA)**, is also common in real imaging systems. While it is related to LCA, its physical cause is different.

TCA occurs when the image blur interacts with the position and size of the aperture. The two panels of the figure below show short wavelength (blue) and long wavelength (red) rays, originating from a point at the lower left. The blue rays are in focus, forming a tight spot on the image plane. The red rays, however, come to focus behind the image plane, creating a larger, blurred red circle.

:::{.panel-tabset}
## Longitudinal chromatic aberration
![With this aperture, there is only longitudinal chromatic aberration (LCA) and no transverse chromatic aberration (TCA).](images/optics/chromatic-notransverse.png){#fig-optics-notransverse width=70%}

## Transverse chromatic aberration
![With this pupil aperture, some of the rays from this point will be blocked from entering the lower part of the lens. The absence of these rays has a differential effect on the short- and long-wavelength light. The of the center of the red blur circle will be shifted so it no longer aligns with the center of the blue blur circle (TCA).](images/optics/chromatic-transverse.png){#fig-optics-transverse width=70%}
:::

<figcaption>
The images illustrate short wavelength (blue) and long wavelength (red) rays a single point. The blue rays are in good focus, but the red rays have a large blur circle. The two panels illustrate how changing the aperture position interacts with defocus to create transverse chromatic aberration. See the text for details.


</figcaption>
<!-- keep the empty lines for spacing in the rendered image -->

Now, imagine reducing the aperture size and moving it in front of the lens. This new aperture position blocks some of the rays from both wavelengths. For the blue rays, which were already converging to a tight focus, the effect is minimal —the spot gets a little dimmer. But for the red rays, which form a large blur circle already, the new aperture blocks rays that would have formed the lower part of their blur circle. As a result, the red blur circle is clipped at the bottom, and its center shifts upward. The red and blue circles are no longer aligned. Th displacement of the red blur circle is transverse chromatic aberration.

The size and position of the aperture determines the amount of TCA. The physical factors that cause the transverse shift are different from those that produce LCA. Both effects are related to the different wavelengths focus.  Were there no LCA, there would be no TCA.

## Point spread engineering {#sec-optics-psf-engineering}
The usual goal of optical design for consumer photography is to create a lens that has a very compact point spread function (sharp image) that is the same for all wavelengths (no chromatic aberration).  This makes sense because people want their photographs to be sharp and free of the kind of chromatic fringing you can see in @fig-optics-linespread and @fig-optics-pointspread.

The goal of optical design can differ, of course, for different applications. In the sections below, I describe two cases that I found to be very interesting.  The first case designs optics for telescopes that are looking for planets in other star systems (exoplanets).  The second case designs optics that can be helpful in estimating the depth of objects in an image. For example, in microscopy we might be interested to measure the tissue layer where a particular glowing molecule resides. These two examples are drawn from the literature of PSF engineering.

### Exoplanets {#sec-psf-expolanets}
When we see an eclipse of the sun, the moon blocks out the bright central region.  The edge of the sun, its *corona*, becomes visible. Its properties have intrigued people for centuries, but waiting for a solar eclipse to study it was quite limiting.  Thus, there was considerable interest when Bernard Lyot, in the 1930s, introduced an optical technique to view and measure the sun's corona — the coronagraph.

Lyot's optics are designed to block the light from a bright central source (like the sun or a star), making it possible to observe the light nearby, such as the solar corona.  The key to a coronagraph’s effectiveness is engineering the point spread function (PSF) so that light from the central source is suppressed in the image.  This allows much dimmer features to be detected. This is a good example of **PSF engineering**: designing optics to implement PSF that achieves a specific goal.  Lyot's design remains relevant, and it is implemented on a modern space telescope [coronograph instrumentation](https://jwst-docs.stsci.edu/jwst-mid-infrared-instrument/miri-instrumentation/miri-coronagraphs#gsc.tab=0){target=_blank}.  The basic design is sketched in @fig-optics-lyot. [^lyot-math] 

[^lyot-math]: The relationship between the images at the pupil, first image, reimaged pupil, and final image can be described using a series of Fourier Transform operations. Thus, we can predict how effectively this system will work. These calculations will be developed in later sections. [Link to the math, maybe expressed in an ISETCam script. Class project?](https://chatgpt.com/share/6892d701-54d0-8002-8137-ec17287ab549){target=_blank}

The principle of the instrumentation is shown in @fig-optics-lyot. The rays from the sun, which is very far away, are parallel at the optical aperture. The light from the sun (or another star) are brought into focus by the first lens, forming a spot. The center of this image is blocked by a small stop, called an occulting disk. This prevents most of the light from the bright part of the star from continuing. This spot also introduces diffraction from the rays passing near the edge of the disk.  A second lens, placed a focal length from this spot, reimages the entrance pupil creating a new pupil plane with parallel rays. The Lyot stop, which is somewhat smaller than the original aperture is placed in this plane. This rejects much of the diffracted light. Finally, a new image is formed.  This image formation process blocks the rays from the core of the star and much of the effect of diffraction. The contrast in the final image reveals properties mainly from the star's corona.  The total amount of light that makes it through the system is usually about ${1/200}^{th}$ of the light at the first aperture.

![James Webb Space Telescope's Lyot coronagraph system.](images/optics/optics-lyot-coronagraph.png){#fig-optics-lyot width="80%"}

Modern coronagraphs use apertures with optical components that adjust the phase of the surrounding light to create a very dark spot in the PSF.  In recent years, coronograph techniques have been applied to discovering **exoplanets**; these are planets that orbit a star outside the solar system. One such project, the Exo-Life Finder (ELF), uses coronagraphic style PSF engineering to search for life on exoplanets (@kuhn2025-astronomy).  

> The telescope we’d like to  design  and  build  targets  the  problem  of  finding  life around planets outside the solar system. To do this requires separating  the  reflected  starlight  off  the  planet  from  the star.  Astronomers  call  this “direct  imaging”.  This  means we’re interested in optical systems that can have a narrow field-of-view  (FOV),  perhaps  just  a  few  arcseconds,  and enormous  photometric  dynamic  range  capability.  A laudable  goal  is  to  achieve  raw  sensitivity  of  $10^{−8}$ of  the stellar  flux  in  a  region  within  about  1  arcsecond  of  the central  star.  A  dedicated  telescope  with  this  capability could  make  images  of  the  surface  of  nearby  exoplanets using  data  inversion  techniques,  perhaps  sufficient  to  see even signs of advanced exolife civilizations (@kuhn2025-astronomy).

![Optics design for a point spread with a dark spot in the middle, where the star is located.  That would be panel e.  The design of the pupil aperture with these multiple little circles and some of them delaying the phase, is shown in a-d.](images/optics/optics-psfengineering-kuhn.png){#fig-psfengineering-kuhn width=70%}

The implementation described in @kuhn2025-astronomy replaces the stops with optical components that act on the phase of the wavefront (@fig-psfengineering-kuhn). This advance in blocking light from the central region is important in the search for exoplanets; astronmers believe the light reduction of the central region must be at least eight orders of magnitude, or preferably 10! In addition to PSF engineering, @kuhn2025-astronomy describes the challenges in building a very large optical instrument to reveal the light reflected from the near by circling exoplanet. These include problems of mechanical engineering, material science, optics, signal processing, financial and political considerations.

<!-- https://www.spiedigitallibrary.org/conference-proceedings-of-spie/12182/121820G/The-small-ELF-project--toward-an-ultra-large-coronagraphic/10.1117/12.2629645.full 
Jeff Kuhn describes the Exo-Life Finder (ELF) project.  The quotes below are about the Small ELF (SELF) prototype. Kuhn is an astronomer at the University of Hawaiʻi's Institute for Astronomy and has a long history of working on instruments and technologies for both solar observation and exoplanet detection. Kuhn is a key figure in the development of a special type of telescope called the Exo-Life Finder (ELF), which is designed to directly image exoplanets. The technology behind this, as you describe, involves creating a coronagraphic point spread function (PSF) that effectively blocks the light from the central star, allowing the much fainter light from nearby planets to be detected. This is a form of PSF engineering with a "black in the middle" design, specifically for the purpose of finding exoplanets. He has also been involved in other projects with similar goals, such as the Kermit camera for the Lyot Project Coronagraph, which was designed for "companion searches close to bright stars."  https://www.light-am.com/article/doi/10.37188/lam.2025.033
-->

### Depth estimation {#sec-psf-depth}
This second example of PSF engineering applies takes us from astronomical scale to microscopic scale.  

Biologists frequently use fluorescent markers to tag specific molecules and investigate the biological substrate of many types of tissues. These fluorescent tags can be attached to specific cells or molecules. When the tags are stimulated by photons at one wavelength (typically short wavelength), they emit photons at a longer wavelength.  Enough photons are emitted so that tags visible in a microscope. Single molecules are very small, and thus they are point sources. The image formed by a single fluorescent tag, through the microscope optics, is a point spread function.

For many purposes in biology, investigators are primarily interesetd in the $(x,y)$ position and number of tags. When the biological tissue is sparsely labeled, each tagged molecule produces a point spread image that is well-separated fom the image of the other nearby tagged molecules. To find the position of these molecules and count them, it is only necessary to count the point spreads in the image. The $(x,y)$ position of the molecule is assumed to be at the centroid of the point spread. There will be some variance in the position estimate because of photon noise and other imperfections in the optics.  But even so, the location of the centroid can be determined at high precision compared to the size of the point spread. The general approach is called Single Molecule Localization Microscopy (**SMLM**). Some say SMLM methods localize molecules beyond the diffraction limit, by which they mean at a finer resolution than the size of the Airy Disk.  This seems magical, but it is not. It is also not my favorite comment. The technique is useful, not magical.

![Double-helix point spread from Piestun. Figure 1 from the PNAS article with Moerner.  SLM is a spatial light modulator to control the phase. Probably this figure should be redrawn for here.](images/optics/piestun-dh-1.png){#fig-optics-double-helix width=70%}

When the biological sample has some thickness, it can be important to know its depth, $z$, in addition to its position, $(x,y)$. Rafael Piestun recognized that this is possible in principle, because in the near field the point spread function depends on distance (@sec-pointspread). The usual dependence, so called Fresnel diffraction, is mainly a change in the size of the blur circle which is hard to measure.  Piestun and his collaborators engineered an optical system with an oriented point spread function that depends on depth (@pavani2008-3d-dh-tracking). The PSF consists of two lobes that rotate around a central point as the depth (z-position) of the point source changes (@fig-optics-double-helix). If one plots the point spread as a function of z-position in the biological substrate, the pattern forms a double-helix. In applications with sparsely labeled molecules, this enables investigators to measure the PSF centroid to localize position $(x,y)$ and the orientation to measure depth $z$. Piestun co-founded a company based on this technology called [Double Helix Optics](https://www.doublehelixoptics.com/), which commercializes 3D imaging and sensing systems.

::: {.callout-note title="Prizes and companies" collapse="false"}
<!--
The woman scientist you're thinking of is Xiaowei Zhuang, a biophysicist at Harvard University. The super-resolution technique she developed is called Stochastic Optical Reconstruction Microscopy, which is more commonly known by its "cute" acronym, STORM.
-->
Several groups developed SMLM techniques that are now widely used in microscopy. The spatial resolution of SMLM techniques is sometimes called nanoscopy to emphasize its very fine spatial scale.

W.E. Moerner showed that light could be detected from single fluorophores, and further that the ones used broadly in biology could be turned on and off (@moerner1989-singlemolecule, @dickson1997-gfp-moerner). Betzig introduced Photoactivated light microscopy (PALM), a method based on this observation to identify the position of the individual fluorophores. Xiaowei Zhuang introduced a parallel method Stochastic optical reconstruction for microscopy (called STORM), that also relied on Moerner's observation.

The Moerner, Betzig, and Hell[^hell-sted] [were awarded the Nobel Prize](https://www.theguardian.com/science/2014/oct/08/nobel-prize-chemistry-trio-microscopy-betzig-moerner-hell#:~:text=Eric%20Betzig%20and%20William%20Moerner,more%20than%2010%20years%20old). Zhuang has won numerous other prizes for her work.

[^hell-sted]: Stefan Hell stimulates the fluorescence in a way that depletes the surrounding fluorescence, narrowing the emission and producing high spatial resolution images.

<!--
Gemini agrees about PALM and STORM both being part of **Single Molecule Localization Microscopy (SMLM)**  https://g.co/gemini/share/97d52c76d6b1  
-->
:::
