# Linear optics - space {#sec-optics-linear-space}
Geometric optics helps us understand how lenses work by focusing on things like the shape of the lens, what it’s made of, and how far it is from the sensor. This approach is great for designing lenses and figuring out their basic properties.

But there’s another way to look at optical systems: as signal processors. In this view, the system takes in light from the scene and transforms it into the light that arrives at the sensor. This signal processing perspective is powerful because it lets us think about what the system does, without worrying too much about the details of how it’s built. It’s also a practical way to simulate and analyze optical systems.

This approach is especially useful because many optical systems are close to being linear—they follow the **Principle of Superposition**. You can actually test this property in the lab. There are lots of tools for working with linear systems, and you’ll see them used again and again in science and engineering. In this section, we’ll introduce these ideas using a simple optical system as an example.

## Principles of Superposition and Homogeneity
A system $L$ is linear if it satisfies the superposition rule:

$$
L(x + y) = L(x) + L(y).
$$ {#eq-superposition}

Here, $x$ and $y$ are two possible inputs, and $L(x)$ is the system's response to the input $x$. The input variable can be a scalar, vector, matrix, or tensor. For example, in an optical system, $x$ might represent the incident light field at the entrance aperture, and $L(x)$ could be the spectral irradiance measured at the sensor .

No real system is perfectly linear over all possible signals. Extreme inputs with a massive amount of energy may behave unpredictably or even break the system! However, many systems are linear over an input range that covers most of the likely use cases. 

In other cases the system will have many linear regimes, each much less than the entire input range. These systems are called **locally linear**. The linear approximation will differ between regions, so that the linear approximation in a region near $x_1$ will differ from the linear approximation near $x_2$. The principle of local linearity is a fundamental concept in mathematics, physics, and engineering.[^taylor-series]

 [^taylor-series]: If you’ve seen the Taylor series expansion in calculus, you may recognize the principle of local linearity in mathematics!

A special case of superposition is **homogeneity**:

$$
L(\alpha x) = \alpha L(x)
$$ {#eq-homogeneity}

Homogeneity follows from superposition. For example, adding an input to itself:

$$\begin{aligned}
L(x + x) &= L(2x) \nonumber  \\ 
&= L(x) + L(x) \nonumber \\
&= 2L(x). \nonumber
\end{aligned}
$$

This generalizes to any integer $m$:

$$
L(m x) = m L(x).
$$

::: {.callout-note title="Extension to Rational Numbers" collapse="true"}

For real-valued systems that obey superposition, homogeneity holds for any rational number. If $\alpha = \frac{p}{q}$ is rational:

$$
L(\alpha x) = L\left(\frac{p}{q}x\right) = p L\left(\frac{x}{q}\right).
$$

Let $x' = x/q$, so

$$
L(x) = L(q x') = q L(x') \implies L(x') = \frac{1}{q} L(x).
$$

Therefore,

$$
L(\alpha x) = p L(x') = p \frac{1}{q} L(x) = \frac{p}{q} L(x) = \alpha L(x).
$$

To extend this to all real numbers, continuity and far more extensive proof is required. But how about we just work with the rationals and be engineers for now?
:::

Do remember that a system can be homogeneous without being linear. For example the absolute value, $f(x) = |x|$, and the vector length function

$$
f(\mathbf{x}) = \|\mathbf{x}\|_2 = \sqrt{x_1^2 + x_2^2 + \cdots + x_n^2}
$$

are homogeneous but not linear. The function $ReLU(x) = \max(0, x)$, widely used in neural networks, is also homogeneous for $\alpha > 0$ but not linear:

$$
1 = ReLU(2 + -1) \neq ReLU(2) + ReLU(-1) = 2 + 0.
$$

Remembering that these cases exist may be of use to you in the future.


## Discrete linear systems {#sec-linear-discrete}
Many of the practical computations we describe in the following sections use **discrete** representations of linear systems. 

We have already used discretization to represent a spatial stimulus at sample points or a spectral stimulus at sample wavelengths. In these discrete representations, a continuous stimulus, say $s(x)$, is sampled to form a vector of values, $\mathbf{s}$. We will write the entries as $s[x_i]$, where $i$ is an integer index, or simply $s[u]$, with square brackets to remind us that the signal is discretely sampled.

Discrete linear systems can be described by a matrix, $\mathbf{L}$, which maps the input vector to an output vector, $\mathbf{o}$:

$$
\mathbf{o} = \mathbf{L} \mathbf{s}
$$ {#eq-linear-matrix}

This can also be written as a summation

$$
o[i] = \sum_j L_{i,j} s[j] 
$$ {#eq-linear-matrix-summation}

I find it helpful to visualize this as a matrix tableau: the $j^{th}$ column of $\mathbf{L}$ is scaled by the $j^{th}$ entry of $s[j]$. The output vector is the weighted sum of the columns of $\mathbf{L}$.

**PUT IN A MATRIX TABLEAU IMAGE HERE**

The transformations of any discrete linear system can be represented as matrix. Sometimes, the input is naturally an image (a matrix), and we need to "flatten" it into a vector; other times, we might use more clever tricks to keep the image structure. But the key idea to keep in mind is that, in the discrete world, linear systems are matrices that map input vectors to output vectors.

::: {.callout-note collapse="true" title='History - linear algebra and matrices'}
When I was an assistant professor, I had the privilege of joining faculty meetings with senior colleagues I deeply admired—Joe Goodman, Ron Bracewell, Tom Cover, and others. In these meetings, we often discussed how best to teach key mathematical ideas to engineering students. The more senior faculty generally favored starting with integrals and continuous functions, emphasizing that these are the true foundation of the mathematics. The younger faculty, who spent much of their time programming, leaned toward teaching with matrices and vectors. I remember Ron Bracewell remarking that continuous functions are the reality, and we should introduce those first—then, only later, show students the "vulgar" discrete approximations that we use because our computational tools require them. Good times.

We all agreed that discrete representations are extremely useful—and today, they are essential. Mathematics framed in terms of vectors and matrices is at the heart of modern neural networks and much of computational science and engineering. That’s the approach I use throughout this book. Still, I remember Bracewell’s advice, and sometimes feel a bit of guilt for not starting with the continuous case.

It helps to know that representing problems with vectors and matrices is not a recent development. The importance of linear algebra and matrix methods was recognized nearly 200 years ago. If you’re interested in the history of these ideas, you can read more here: [History of matrices and linear algebra](resources/history-linear-algebra.html){target=_blank}
:::

## Spatial basis functions {#sec-optics-spatial-basis}
It is natural to think of images as a set of points, and in @sec-pointspread we introduced the idea of the point spread function.  This is a very important system characvterization, but it isn't always helpful for system characterization. It was an important insight to realize that images can be described in many other ways. Some of these alternatives are very valuable for analyzing and characterizing optical performance of image systems.

Linear models are an important method for exploring alternative image representations. We introduced linear models in @sec-linear-models, and we applied them to characterizing the spectral properties of light fields. Here we use linear models again, but this time to as models of the image intensity, $I(x,y)$. A linear model of the image intensity can be written this way:

$$
I(x,y) = \sum_{i} w_{i} B_{i}(x,y)
$${#eq-linear-model}

The functions $B_{i}(x,y)$ are the **spatial basis functions**, and the scalar values $w_{i}$ are the weights applied to each basis function. The image is equal to the weighted sum of these basis functions. 

## Orthogonal image transforms
For a linear model to be useful, we need a method for finding the weights from the image. A function that maps $I(x,y)$ into the weights, $w_i$ is called an **image transform**.  If we choose a basis functions that are orthogonal to one another, the transform is easy to find.

$$
0 = \sum_{x,y} B_i (x,y) B_j (x,y),
$$ {#eq-spatial-orthogonal}

The simplicity of the transform can be seen by representing @eq-linear-model in discrete form an using matrices. Discrete representations using vectors and matrices is the way we compute, and thus a more practical if less pure.  To emphasize the discrete representation we will shift notation just a bit so that $I(x)$ becomes $I[n]$, and $B(x,y)$ becomes $B[n,m]$. 

We can convert the basis functions, which are matrices, into a column vector by stacking the columns on top of one another. We then group these columns into the columns of a single matrix, $\mathbf{B}$. We similarly stack the columns of the image into a long column vector $\mathbf{I}$.  The weights are already a vector which we write as, $\mathbf{w}$. 

We can write @eq-linear-model as a matrix equation this way

$$
\mathbf{I} = \mathbf{B w} .
$$ {#eq-linear-model-matrix}

The $i^{th}$ weight in the vector $\mathbf{w}$ multiples the $i^{th}$ column of the matrix $\mathbf{B}$.  All of these products are summed, and thus this matrix equation is the same as the @eq-linear-model.

When the basis functions are orthogonal, $\mathbf{B^t B}$ is the identity matrix.  This means we can transform from the image data to the weights quite simply,

$$
\mathbf{w} = \mathbf{B^t I} .
$$ {#eq-linear-model-inverse}

So, that's simple! Now, there are instances when we might make linear models that are not orthogonal. In that case, we have to find the inverse of $\mathbf{B}$ to move back and forth between the image and its transform. Computers are pretty big these days, so we can live with that.  But it is very nice to have the basis functions be orthogonal.

## Point spreads and other representations
The principles of linear models are important for image systems and science and engineering broadly. Of the many linear models, two stand out as the most important. 

The first is the representation as points.  When we represent an image as a set of points, we don't even notice that we are using an orthogonal basis. The points are the basis functions, $B_i$ is zero everywhere except except at one $(x,y)$ point.  The weights are the image intensity at that point, $I(x,y)$. Also, the point representation is orthogonal.  The inner product of two point basis functions is zero!  

The point representation is used very widely in many fields.  When used with dynamic systems, the point is usually called the **impulse**.  Or, in general, the **Dirac delta function** to honor a physicist who developed formal theory using the representation.  The use of points to represent an image is so natural that this observation is rarely made. We make it here mainly to emphasize that there are other linear model representations that have these key properties:  the stimulus is the weighted sum of the basis functions and the basis functions are orthogonal to one another.  Those two properties make the point representation valuable, and they also make the alternative representations valuable.

 The second most important orthogonal representation are the **harmonic functions** . We are heading towards an explanation of these. The way will get there is through eigenfunctions.

## System eigenfunctions

An **eigenfunction** of a system is an input that whose output is the same as the input, up to a scale factor.  Harmonics are almost eigenfunctions. This is a very useful fact to know about a system because it means when we use an eigenfunction as input, we know almost everything about the output.  In symbols, if $\mathbf{L}$ is a matrix that represents a discrete linear system, the column vector $\mathbf{e}_i$ is an eigenfunction if

$$
\alpha_i \mathbf{e}_i = \mathbf{L} \mathbf{e}_i 
$$ {#eq-eigenfunction-definition}

The scalar values, $\alpha_i$ are the **eigenvalues** of the linear transformation. 

Some linear systems -and of course the ones we will be studying- have a set of eigenvectors that serve as an orthogonal basis for the input. For these systems, we can represent the stimulus as the weighted sum of its eigenfunctions.  Notice how simple this makes calculatin the output, $\mathbf{o}$ to an input $\mathbf{s}$. 

$$
\begin{aligned}
\mathbf{o} & = \mathbf{L} ~ \mathbf{s} \\
  & = \mathbf{L} ~ (\sum_i \sigma_i \mathbf{e}_i ) \\
  & = \sum_i \sigma_i \mathbf{L} \mathbf{e}_i \\
  &  = \sum_i \sigma_i \alpha_i  \mathbf{e}_i
 \end{aligned}
$$ {#eq-linear-eigencalculations}

In words, we express the stimulus as the weighted sum of the eigenfunctions.  Then we use linearity to apply the transformation to each of the eigenfunctions. This brings up the eigenvalue times the eigenfunction. So in the end, the output is the sum of the eigenfunctions, with each scaled by a weight corresponding to the stimulus itself ($\sigma_i$) and a second weight associated with the linear transformation ($\alpha_i$).  This can become even simpler if we select our stimulus to be an eigenfunction!  

Harmonic functions are a tool to help with system characterization. Their value arises from this simple and important fact:  When the input to an optical system is a harmonic function at spatial frequency $f$ and unit contrast, in each local region the image will also be a harmonic function at the same frequency.  The relative phase and amplitude of the output will differ from the input, but we only need to measure these two parameters.  

This chapter explains the general principle and then provides several ways in which harmonics are used to characterize optical systems as well as the human visual system.


If the input is a harmonic of frequency $f$, the output will be a scaled version of that harmonic.  It is not a perfect eigenfunction only because there can also be a phase shift. This input-output simplicity makes the harmonics very useful tools for system characterization.

The mathematics 

## Shift-invariant linear systems
A practical limitation of characterizing a general linear systems is that it can have many different PSFs - one for every point! But many real systems are much simpler. These systems have essentially the same point spread function across many points, with the only difference being that the point spread function is shifted between points. Because the shape of the point spread function remains the same -it is only shifted- we say such a system is **shift invariant**. 

Across the many branches of science and engineering, there are a large number of physical phenomena and engineering devices that are well-approximated as shift invariant linear systems. These systems are relatively straightforward to calibrate because we only need to estimate a single point spread function. Also, there are many computational tools that help us simulate shift invariant systems.  In imaging the point spread function is the key measurement that characterizes the linear system. In applications to systems that characterize responses over time, the key measurement is the response to a short temporal input. This is called the **impulse response**, and it is the equivalent of the point spread function in an imaging system. 

The simplicity derived from shift invariance is so useful that some courses entitled 'linear systems' focus almost entirely on shift-invariant linear systems. 

::: {.callout-note title="Shift invariance notation" collapse="true"}
A little mathematical notation may be useful. Suppose we write image translation as a function, $T()$. If we shift an image, $x$, by an amount, $\delta$, we write it as $T(x,\delta)$. A shift invariant system means that the output of a linear system, $L()$, to a translated image is the translated linear response

$$
L(T(x,\delta)) \approx T(L(x),\alpha \delta).
$$ {#eq-shiftinvariance-defined}

Notice that @eq-shiftinvariance-defined allows the size of the image shift, $\delta$, to differ from the size of the output shift, $\alpha \delta$. So if we displace an object by an amount $\delta \text{m}$, the image will be displaced by an amount $\alpha \delta \text{m}$ on the sensor. This scale factor depends on the image magnfication. 
:::

The image formation component (optics) of most image systems are typically linear, but they are not globally shift-invariant. Rather, they are shift invariant over some part, but not all, of the full visual field. Usually there is a region near the optical axis that is shift invariant, and the system is also locally shift-invariant at off-axis points. This local shift invariance is similar to a system that is locally linear. In optics the shift invariant region is called an **isoplanatic** region. 

## Point spread functions {#sec-pointspread-2}
In @sec-pointspread we introduced the idea of the point spread function. It is a measurement of how the optics transforms the light from the point source in the scene into an image. When superposition holds, we can combine use the point spread functions to predict how the input scene becomes a sensor image. 

We describe the scene as a collection of points, $\mathbf{p} = (p_x, p_y, p_z)$, each with its own intensity $I(\mathbf{p})$.   We write the point spread function for each point as $PSF_\mathbf{p}(x, y)$, where $(x, y)$ are positions in the image. By superposition, we can calculate the output image, $O(x,y)$ by adding up the point spreads from every scene point. The formula looks like this:

$$
O(x, y) = \sum_{\mathbf{p}} I(\mathbf{p})\, PSF_\mathbf{p}(x, y)
$$ {#eq-pointspread-1}

Here, $O(x, y)$ is the intensity at each image position. Each point in the scene contributes a scaled version of its PSF. The output image is the weighted sum of all those PSFs.

Point spread functions are an intuitive way to understand the performance of the optical system. But, this framing is not easy to use as for quantifying image quality. The point spread function can be quantified using a few parameters only in a few cases (@sec-airy-pattern, @sec-pointspread-common). But in general we do not have high quality numerical approximations of point spread functions for many real lenses, multielement lenses, or measurements of the human eye.  The following sections describes other methods of thinking about the linear transforms that provide us with alternative representations that we can use to characterize and understand system performance.  All of these representations still rely on linearity (superposition).

::: {.callout-note title="A limitation of PSFs" collapse="true"}
Point spread functions (PSFs) are an excellent tool for modeling optical systems when the scene can be approximated as a flat surface, or when all objects are far enough away that occlusion is negligible. In these cases, each point in the scene contributes independently to the image, and the total image can be predicted by summing the PSFs from each point.

However, in three-dimensional scenes where objects can block light from those behind them (**occlusion**), the situation becomes more complex. The contribution of a point to the image may be partially or completely blocked by closer objects. As a result, the effective point spread depends on the arrangement of objects in the scene, and cannot be characterized by a single, scene-independent PSF. Accurately modeling these effects requires more advanced techniques, such as those used in computer graphics, and often involves recalculating the image formation for each specific scene.

In summary, PSFs provide a powerful and practical framework for image prediction when occlusion can be ignored, but their direct application is limited in scenes with significant three-dimensional structure and occlusion.
:::

## Common PSFs in simulation {#sec-pointspread-common}

When simulating optical systems, a few point spread functions (PSFs) that come up again and again (@fig-psf-graph).

- **Airy pattern** is the only one of these PSFs that is based on the physics of light waves. This is the PSF we measure for a perfect, diffraction-limited lens imaging a point at a distance (@sec-airy-pattern and @fig-blur-comparison). The Airy pattern depends on wavelength and represents a best case limit. 
- **Gaussian** is popular because it’s easy to work with in calculations. 
- **Lorentzian** (also called Cauchy) has higher tails than the Gaussian. This makes it better for modeling systems where light gets scattered a lot—like if the lens is scratched or there are many internal reflections from a multi-element lens assembly.
- **Pillbox** A square region with constant values that averages the values over the region. Who doesn't like a simple average?

A graph of the cross-section through these PSFs is a useful way to compare them. 

![Cross-section of four  point spread functions that are widely used in simulation. These point spreads were applied for a lens and sensor configuration with an $f/\#$ of 4 and a focal length of 3.9mm. The radius of the Airy disk for this configuration is 3.4 microns (see the zero-crossing of the Airy pattern).](images/optics/optics-siKernels-graph.png){#fig-psf-graph width="60%"}

We can further visualize how the four PSF shapes transform a spectral radiance image using simulation. @fig-psf-image compares the optical images created by the four PSFs for a scene comprising a set of grid lines. The images were calculated using the ISETCam script <a href="../code/fise_opticsPSF.html" target="_blank">fise_opticsPSF.</a>  The Airy and Gaussian PSFs were implemented with point spread functions that vary with wavelength. The Pillbox and Lorentzian PSFs were implemented as wavelength-indepedent.

![The Airy pattern shows the image for a diffraction limited lens with an $f/\# =4$. The Gaussian has a spread that is roughly consistent with the Airy Disk size and a similar wavelength dependency; long-wavelengths spread more than short wavelengths. The high tail of the Lorentzian PSF produces a veiling glare. The Pillbox size is about equal to the Airy disk, but it introduces much more blur because it has a relatively high level over the entire width.](images/optics/optics-siKernels-image.png){#fig-psf-image width="90%"}

Here is a description of the different PSF formulae along with a brief descrition of their properties.

### The Airy pattern
The Airy pattern is circularly symmetric, so you only need to know the distance from the center, $r$, to describe it. The intensity at a distance $r$ from the center is:

$$
I(r) = I_0 \left[ \frac{2 J_1\left( \frac{\pi r}{\lambda N} \right)}{ \frac{\pi r}{\lambda N} } \right]^2
$$

Where:

- $I_0$ is the peak intensity at the center
- $J_1$ is the first-order Bessel function
- $N$ is the $f/\#$ of the lens
- $\lambda$ is the wavelength of light

The first dark ring defines the Airy disk.  It is the diameter where the intensity drops to zero, and defines the smallest spot size.

$$
d = \arcsin(2.44 \, \lambda N) \approx 2.44 \, \lambda N
$$

For $f/\# = 4$ the diameter ranges from 3.9 $\mu\text{m}$ (400 nm) to 6.83 $\mu\text{m}$ (700nm).  See the ISETCam function <a href="https://github.com/ISET/isetcam/blob/main/opticalimage/optics/airyDisk.m" target="_blank">`airyDisk`</a>

### Gaussian
The Gaussian is the classic “bell curve.” It’s simple and often a used as an approximation for optical systems.
$$
I(r) = I_0 \exp\left( -\frac{r^2}{2\sigma^2} \right)
$$

Here, $\sigma$ sets how spread out the blur is. Larger $\sigma$ means a blurrier image.

### Lorentzian (Cauchy)
The Lorentzian function appears in physics and probability theory. It’s famous for its “fat tails”—meaning it doesn’t drop off as quickly as a Gaussian. This makes it handy for modeling blur when there’s a lot of stray light or scattering.

$$
I(r) = \frac{I_0}{1 + \left( \frac{r}{\gamma} \right)^2}
$$

Here, $\gamma$ controls how wide the central peak is. A bigger $\gamma$ means a wider, flatter blur with more light in the tails.

All of these PSFs are implemented in <a href="https://github.com/ISET/isetcam/blob/main/opticalimage/optics/shiftinvariant/siSynthetic.m" target="_blank">`siSynthetic.m`.</a>

## Line spread functions
When I created @fig-psf-image, I decided to illustrate the effect of the PSFs using grid lines rather than points. It was my judgment that the differences would be easier to visualize the impact of the point spread this way. When we calculate the image of a line, we are calculating the **line spread function (LSF)**.

If the PSF is circularly symmetric, the same LSF is measured for any line orientation. To find the PSF we simply sum along the direction (see <a href="https://github.com/ISET/isetcam/blob/main/opticalimage/wavefront/psf/psf2lsf.m" target="_blank">`psf2lsf.m`.</a>. We will calculate a different LSF if the PSF is not circularly symmetric. The formula is very simple.

$$
LSF = \sum_x PSF(x,y)
$$

Here are the linespread functions of the Gaussian and Lorenzian.

Given an LSF, it is possible to recover a unique circularly symmetric PSF using <a href="https://github.com/ISET/isetcam/blob/main/opticalimage/wavefront/psf/lsf2circularpsf.m" target="_blank">`lsf2circularpsf.m`.</a>. I am not plotting the Pillbox because it is not circularly symmetric. In has the same LSF for the horizontal and vertical directions, but its LSF differs in other directions. 

## Chromatic aberration types {#sec-transverse-chromatic-aberration}

*Not sure where this should go*

In @sec-chromatic-aberration I explained the idea that because the index of refraction of a lens might vary with wavelength, the best focus distance would also vary with wavelength. In addition, even for an ideal lens, there will be a wavelength dependence arising from the wave nature of light. In this section we captured this variation by creating wavelength-dependent PSFs for the Airy and the Gaussian PSFs.  This type of chromatic aberration, a defocus of the image, is called **longitudinal chromatic aberration**.

There is a second type of chromatic aberration, called **transverse chromatic aberration** that arises in real imaging systems. The transverse aberration is somewhat related to the longitudinal aberration, but it has a different physical basis.





