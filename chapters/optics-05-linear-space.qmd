# Linear optics - space {#sec-optics-linear-space}
Geometric optics helps us understand how lenses work by focusing on things like the shape of the lens, what it’s made of, and how far it is from the sensor. This approach is great for designing lenses and figuring out their basic properties.

But there’s another way to look at optical systems: as signal processors. In this framing, an optical system takes in light from the scene and transforms it into light at the sensor. The signal processing perspective is powerful because it lets us think about what the system does, without worrying too much about the details of how it’s built. It’s a practical way to develop image systems simulations and to analyze optical systems.

This approach is especially useful because many optical systems are close to being linear —they satisfy the **Principle of Superposition**. You can test this property in the lab. There are lots of mathematical tools for working with linear systems, and they are taught in many context across science, technology, engineering and mathematics.

Because many readers will be familiar with the ideas, I put an introduction to linear systems into a linear systems appendix, @sec-appendix-linear-systems.  If you are familiar with linear systems principles and tools, use that appendix to understand the notation and thinking for the image systems engineering simulations in this book.

## Point spread functions {#sec-pointspread-2}
In @sec-pointspread I introduced the idea of the point spread function (PSF). It is simply the image of a point of light. If we know the point spread for each location in the scene, and superposition holds, we can predict how any input scene becomes an image. 

The scene is a collection of scene points, $\mathbf{p} = (p_x, p_y, p_z)$, each with its own intensity $S(\mathbf{p})$. Write the point spread function of a scene point as $\text{PSF}_\mathbf{p}(x, y)$, where $(x, y)$ are positions in the image. Using superposition, we calculate the output image, $O(x,y)$ by adding up the weighted point spreads from every scene point. The formula looks like this:

$$
O(x, y) = \sum_{\mathbf{p}} s(\mathbf{p})\, \text{PSF}_\mathbf{p}(x, y)
$$ {#eq-pointspread-1}

Here, $O(x, y)$ is the intensity at each image position. Each point in the scene contributes a scaled version of its PSF. The output image is the weighted sum of all those PSFs.

Point spread functions are an intuitive way to understand the performance of the optical system, andthey are widely used in applications. In @sec-optics-psf-engineering I will describe how scientists and engineers design optics to achieve specific PSF characteristics to estimate depth or to reveal hard-to-see objects.

In the @sec-appendix-linear-systems I describe how the point spread function is used to create the linear system matrix: each column of the linear system matrix is a point spread.  When the system is shift-invariant (@sec-ls-shiftinvariance), the columns of the matrix are shifted copies of that point spread.  We also explain that harmonics are eigenvectors of such matrices.  This section further explores the analysis of optics using PSFs, and  @sec-optics-harmonics we explores applications using harmonics.

::: {.callout-note title="A limitation of PSFs" collapse="false"}
Point spread functions (PSFs) are an excellent tool for modeling optical systems when the scene can be approximated as a flat surface, or when all objects are far enough away that occlusion is negligible. In these cases, each point in the scene contributes independently to the image, and the total image can be predicted by summing the PSFs from each point.

However, in three-dimensional scenes where objects can block light from those behind them (**occlusion**), the situation becomes more complex. The contribution of a point to the image may be partially or completely blocked by closer objects. As a result, the effective point spread depends on the arrangement of objects in the scene, and cannot be characterized by a single, scene-independent PSF. Accurately modeling these effects requires more advanced techniques, such as those used in computer graphics, and often involves recalculating the image formation for each specific scene.

In summary, PSFs provide a powerful and practical framework for image prediction when occlusion can be ignored, but their direct application is limited in scenes with significant three-dimensional structure and occlusion. In those cases, computer graphics techniques based on ray tracing are more appropriate.
:::

## Common PSFs in simulation {#sec-pointspread-common}
The PSFs of real systems can be difficult to model using a simple formula. In ISETCam we make it possible to provide an empirical PSF when it is known.  But there are many cases in which the PSF is not yet known, say for a system that is under design and the optics have not yet been selected. In such optical system simulations, a few PSFs are routinely used to approximate image system performance prior to implementation (@fig-psf-graph).

- **Airy pattern** is the only one of these PSFs that is based on the physics of light waves. This is the PSF we measure for a perfect, diffraction-limited lens imaging a point at a distance (@sec-airy-pattern and @fig-blur-comparison). The Airy pattern depends on wavelength and represents a best case limit. The optics you use will not be better than this.
- **Gaussian** is popular because it’s easy to work with in calculations. 
- **Lorentzian** (also called Cauchy) has higher tails than the Gaussian. This makes it better for modeling systems where light gets scattered a lot—like if the lens is scratched or there are many internal reflections from a multi-element lens assembly.
- **Pillbox** A square region with constant values that averages the values over the region. Who doesn't like a simple average?  Well, I don't - but apart from me?

A graph of the cross-section through these PSFs is a useful way to compare them. 

![Cross-section of four  point spread functions that are widely used in simulation. These point spreads were applied for a lens and sensor configuration with an $f/\#$ of 4 and a focal length of 3.9mm. The radius of the Airy disk for this configuration is 3.4 microns (see the zero-crossing of the Airy pattern).](images/optics/optics-siKernels-graph.png){#fig-psf-graph width="60%"}

We can simulate how the four PSF shapes transform a spectral radiance (@fig-psf-image). We compare the optical images four each of the PSFs for a scene comprising a set of grid lines. The images were calculated using the ISETCam script <a href="../code/fise_opticsPSF.html" target="_blank">fise_opticsPSF.</a>  The Airy and Gaussian PSFs were implemented with wavelength-dependent point spread functions. The Pillbox and Lorentzian PSFs were implemented as wavelength-indepedent.

![The Airy pattern shows the image for a diffraction limited lens with an $f/\# =4$. The Gaussian has a spread that is roughly consistent with the Airy Disk size and a similar wavelength dependency; long-wavelengths spread more than short wavelengths. The high tail of the Lorentzian PSF produces a veiling glare. The Pillbox size is about equal to the Airy disk, but it introduces much more blur because it has a relatively high level over the entire width.](images/optics/optics-siKernels-image.png){#fig-psf-image width="90%"}

These PSFs functions are valuable because they have simple parameters that control the amount of blur or light scatter. Here is a description of the different PSF formulae along with a brief descrition of their properties.

### The Airy pattern
The Airy pattern is circularly symmetric, so you only need to know the distance from the center, $r$, to describe it. The intensity at a distance $r$ from the center is:

$$
I(r) = I_0 \left[ \frac{2 J_1\left( \frac{\pi r}{\lambda N} \right)}{ \frac{\pi r}{\lambda N} } \right]^2
$$

Where:

- $I_0$ is the peak intensity at the center
- $J_1$ is the first-order Bessel function
- $N$ is the $f/\#$ of the lens
- $\lambda$ is the wavelength of light

The first dark ring defines the Airy disk (@fig-blur-comparison , @sec-airy-pattern).  It is the diameter where the intensity drops to zero, and defines the smallest spot size.

$$
d = \arcsin(2.44 \, \lambda N) \approx 2.44 \, \lambda N
$$ {#eq-airydisk3}

For $f/\# = 4$ the diameter ranges from 3.9 $\mu\text{m}$ (400 nm) to 6.83 $\mu\text{m}$ (700nm).  See the ISETCam function <a href="https://github.com/ISET/isetcam/blob/main/opticalimage/optics/airyDisk.m" target="_blank">`airyDisk`</a>

### Gaussian
The Gaussian is the classic “bell curve.” It’s simple and often a used as an approximation for optical systems.
$$
I(r) = I_0 \exp\left( -\frac{r^2}{2\sigma^2} \right)
$$

Here, $\sigma$ sets how spread out the blur is. Larger $\sigma$ means a blurrier image.

### Lorentzian (Cauchy)
The Lorentzian function appears in physics and probability theory. It’s famous for its “fat tails”—meaning it doesn’t drop off as quickly as a Gaussian. This makes it handy for modeling blur when there’s a lot of stray light or scattering.

$$
I(r) = \frac{I_0}{1 + \left( \frac{r}{\gamma} \right)^2}
$$

Here, $\gamma$ controls how wide the central peak is. A bigger $\gamma$ means a wider, flatter blur with more light in the tails.

All of these PSFs are simulated in <a href="https://github.com/ISET/isetcam/blob/main/opticalimage/optics/shiftinvariant/siSynthetic.m" target="_blank">`siSynthetic.m`.</a>

::: {.callout-note title="Linespread functions" collapse="true"}
In @fig-psf-image, I illustrated the effect of the PSFs using grid lines rather than points. It seemed to me that the differences are  easier to visualize the impact of the point spread this way. The image created by a very thin line is called the **line spread function (LSF)**.

To find the LSF from the PSF, we simply sum along the direction (see <a href="https://github.com/ISET/isetcam/blob/main/opticalimage/wavefront/psf/psf2lsf.m" target="_blank">`psf2lsf.m`.</a>. We will calculate the same LSF for any direction when the PSF is cicularly symmetric.  If it is not, then we will find a different LSF for each direction. 

The formula to calculate the PSF in a direction, in this case the $y$ direction, is very simple. We just sum across the $x$ direction.

$$
LSF = \sum_x PSF(x,y)
$$

Here are the linespread functions of the Gaussian and Lorenzian.

Given an LSF, it is possible to recover a unique circularly symmetric PSF using <a href="https://github.com/ISET/isetcam/blob/main/opticalimage/wavefront/psf/lsf2circularpsf.m" target="_blank">`lsf2circularpsf.m`.</a>. The Airy pattern is circularly symmetric; the Gaussian and Lorentz are usually implemented as circularly symmetric.  The pillbox is not.
:::

One of the challenges in image system simulations is to select the wavelength dependence of the PSF.  This dependence is formally specified for the Airy pattern, based on the wavelength of light and the size of the circular, pinhole aperture.  But for the Gaussian and Lorentzian, we must make a choice.

One way to make a decision is to measure the wavelength dependence for an existing system, say a system that uses materials with known properties, and use those measurements to guide the simulation. The variations in the PSF caused by these wavelength-dependences are called chromatic aberrations. There are two types of chromatic aberrations, and we describe them and how they might be measured in the next section.

## Chromatic aberration types {#sec-transverse-chromatic-aberration}
Because the index of refraction of a lens might vary with wavelength, the best focus distance also varies with wavelength (@sec-chromatic-aberration). Even for an ideal lens there is a wavelength dependence arising from the wave nature of light and the size of the aperture or $\text{f}/\#$ (@eq-airy1). 

### Longitudinal aberration
A consequence of this wavelength-dependence is that for a given image distance will be in best focus (small PSF) for one wavelength, but not all. The wavelength-dependent defocus is called **longitudinal chromatic aberration (LCA)**.

There is noticeable chromatic aberration for many types of lenses and materials.  The most surprising optical system with a large amount of chromatic aberration is the human eye!  The line spread and point spread functions measured at the human retina both vary considerably as we change the stimulus wavelength.

:::{.panel-tabset}
## Line Spread Function
![Typical human line spread functions. The dashed white lines are the relative retinal illuminance.](images/optics/linespread.png){#fig-optics-linespread}

## Point Spread Function
![Typical human point spread functions. The large spread for the blue is striking.  Notice that the orientation of the blur differs for this example subject, which is typical.](images/optics/pointspread.png){#fig-optics-pointspread}
:::

<figcaption>
Click on the tabs at the top to see typical human line spread functions or point spread functions for stimuli with equal energy, 425 nm, 525 nm, and 625 nm. See <a href="../code/02Optics/fise_humanLSF.html" target="_blank">Human LSF</a> for how ISETBio/ISETCam implements that calculation.

</figcaption>


The very large LCA of the human eye has consequences for many aspects of the human visual system and image systems engineering technology.  We will describe it quantitatively and explain why it matters to technology in @sec-human.

### Transverse aberration
A second type of chromatic aberration, called **transverse chromatic aberration (TCA)**, is also common in real imaging systems. While it is related to LCA, its physical cause is different.

TCA occurs when the image blur interacts with the position and size of the aperture. The two panels of the figure below show short wavelength (blue) and long wavelength (red) rays, originating from a point at the lower left. The blue rays are in focus, forming a tight spot on the image plane. The red rays, however, come to focus behind the image plane, creating a larger, blurred red circle.

:::{.panel-tabset}
## Longitudinal chromatic aberration
![Longitudinal chromatic aberration (LCA) with no transverse chromatic aberration (TCA).](images/optics/chromatic-notransverse.png){#fig-optics-notransverse}

## Transverse chromatic aberration
![Shifting the pupil aperture and reducing its size blocks rays from entering the lower part of the lens. Their absence shifts the center of the red blur circle.](images/optics/chromatic-transverse.png){#fig-optics-transverse}
:::

<figcaption>
The images illustrate short wavelength (blue) and long wavelength (red) rays a single point. The blue rays are in good focus, but the red rays have a large blur circle. The two panels illustrate how changing the aperture position interacts with defocus to create transverse chromatic aberration. See the text for details.


</figcaption>

Now, imagine reducing the aperture size and moving it in front of the lens. This new aperture position blocks some of the rays from both wavelengths. For the blue rays, which were already converging to a tight focus, the effect is minimal —the spot gets a little dimmer. But for the red rays, which form a large blur circle already, the new aperture blocks rays that would have formed the lower part of their blur circle. As a result, the red blur circle is clipped at the bottom, and its center shifts upward. The red and blue circles are no longer aligned. Th displacement of the red blur circle is transverse chromatic aberration.

The size and position of the aperture determines the amount of TCA. The physical factors that cause the transverse shift are different from those that produce LCA. Both effects are related to the different wavelengths focus.  Were there no LCA, there would be no TCA.

## Point spread engineering {#sec-optics-psf-engineering}
The usual goal of optical design for consumer photography is to create a lens that has a very compact point spread function (sharp image) that is the same for all wavelengths (no chromatic aberration).  This makes sense because people want their photographs to be sharp and free of the kind of chromatic fringing you can see in @fig-optics-linespread and @fig-optics-pointspread.

The goal of optical design can differ, of course, for different applications. In the sections below, I describe two cases that I found to be very interesting.  The first case designs optics for telescopes that are looking for planets in other star systems (exoplanets).  The second case designs optics that can be helpful in estimating the depth of objects in an image. For example, in microscopy we might be interested to measure the tissue layer where a particular glowing molecule resides. These two examples are drawn from the literature of PSF engineering.

### Exoplanets and PSF engineering {#sec-psf-expolanets}

See if we can find the example of the astronomer looking for exoplanets.  He wanted a point spread that was black in the center!

Based on your description, the astronomer you're looking for is Jeffrey Chilcote.

He is an expert in building astronomical instruments and is known for his work on instruments designed to directly image exoplanets. He was part of the team that built the Coronagraphic High Angular Resolution Imaging Spectrograph (CHARIS), which operates on the Subaru Telescope in Hawaii. This instrument uses a coronagraph, which is a type of point spread function engineering that is "black in the middle" to block out the light from the central star, allowing for the detection of fainter, nearby planets.

While he worked in Hawaii, he is now an associate professor at the University of Notre Dame. He is also involved in the Gemini Planet Imager (GPI) and is part of a project to upgrade it and relocate it to the Gemini North Observatory in Hawaii.

Jeff Kuhn also fits that description. He is an astronomer at the University of Hawaiʻi's Institute for Astronomy and has a long history of working on instruments and technologies for both solar observation and exoplanet detection.

He is a key figure in the development of a special type of telescope called the Exo-Life Finder (ELF), which is designed to directly image exoplanets. The technology behind this, as you describe, involves creating a coronagraphic point spread function (PSF) that effectively blocks the light from the central star, allowing the much fainter light from nearby planets to be detected. This is a form of PSF engineering with a "black in the middle" design, specifically for the purpose of finding exoplanets.

He has also been involved in other projects with similar goals, such as the Kermit camera for the Lyot Project Coronagraph, which was designed for "companion searches close to bright stars."

https://www.light-am.com/article/doi/10.37188/lam.2025.033

### Depth and PSF engineering {#sec-psf-depth}
Biologists frequently use  fluorescent markers to tag specific molecules. These tags identify a single molecule, and when stimulated they emit enough photons to be easily visible in a dark field microscope.  Because each of the molecules is so small, they are effectively points.  The image that they form through the microscope is a point spread function.

For certain purposes the only information the investigator measures is the $(x,y)$ position of the tagged molecule. This is possible when the biological tissue is sparsely labeled, because each molecule produces a point spread image. As long as the labeled molecules are sparsely sampled, their images will be separated. To find the position of these molecules and count them, it is only necessary to count the point spreads in the image. The position of the molecule is assumed to be at the centroid of the point spread. In this way, the location is determined at high precision compared to the size of the point spread. This approach is called **localization microscopy**. Some say this method localizes the molecules beyond the diffraction limit, which seems magical. It is not magical, nor is this my favorite comment. The technique is useful, not magical.

When the biological sample has significant thickness, it would be very nice to know its depth in addition to its position. YOu can see how this might be possible because the point spread function will depend on the distance of the molecule from the optics. But the usual dependence is just a change in the size of the blur circle. This is hard to measure.  

![Double-helix point spread from Piestun. Figure 1 from the PNAS article with Moerner.](images/optics/piestun-dh-1.png){#fig-optics-double-helix width=70%}

Rafael Piestun and his collaborators engineered an optical system that has an oriented point spread function, comprising a pair of lines. They implemented optics so that the orientation of the point spread lines varies with depth (Figure ref here). The image consists of two lobes that appear to rotate around a central point, and the angle of this rotation directly corresponds to the depth (z-position) of the original point. If one plots the point spread as a function of z-position in the biological substrate, the pattern forms a double-helix. In practical applications with sparsely labeled molecules one can measure the centroid of the separated point spreads to localize position, and the orientation of the point spread to measure depth.  

::: {.callout-note title="Prizes and companies" collapse="false"}
A number of different groups have developed methods for localizing the position of these fluorophores, including Xiaowei Zhuang, W.E. Moerner, and Betzsig.  The latter two were awarded the Nobel Prize, and Zhuang has won numerous other prizes for her work.
<!--
The woman scientist you're thinking of is Xiaowei Zhuang, a biophysicist at Harvard University.

The super-resolution technique she developed is called Stochastic Optical Reconstruction Microscopy, which is more commonly known by its "cute" acronym, STORM.-->

Dr. Piestun co-founded a company based on this technology called [Double Helix Optics](https://www.doublehelixoptics.com/), which commercializes 3D imaging and sensing systems.
:::
