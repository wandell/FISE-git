# Linear systems {#sec-appendix-linear-systems}

## Linear systems overview
Linear systems theory is a foundational topic in science, engineering, and mathematics. Each discipline field explains the importance of linear systems a bit differently, and they use it at varying levels of depth. In this appendix I introduce the core ideas of linear systems for image systems engineering.  I have two goals.  First, I hope to provide readers an accessible and intuitive description from the point of view of image systems engineering. Second, I will introduce the general notation used throughout this book. My hope is that both newcomers and those with prior exposure will find this chapter a useful reference for the concepts and tools we use in image systems engineering.

## Principles of Superposition and Homogeneity {#sec-ls-superposition}
A system $L$ is linear if it satisfies the superposition rule:

$$
L(x + y) = L(x) + L(y).
$$ {#eq-superposition}

Here, $x$ and $y$ are two possible inputs, and $L(x)$ is the system's response to the input $x$. The input variable can be a scalar, vector, matrix, or tensor. For example, in an optical system, $x$ might represent the incident light field at the entrance aperture, and $L(x)$ could be the spectral irradiance measured at the sensor. For modeling an image sensor, $x$ could be the irradiance at the sensor and $L(x)$ would be the electrons accumulated in the pixels.  For human vision, $x$ might be the spectral power distribution of the incident light field and $L(x)$ the photon absorptions in the three types of cone photoreceptors.

**Homogeneity** is special case of superposition:

$$
L(\alpha x) = \alpha L(x)
$$ {#eq-homogeneity}

Homogeneity follows from superposition. For example, adding an input to itself,

$$\begin{aligned}
L(x + x) &= L(2x) \nonumber  \\ 
&= L(x) + L(x) \nonumber \\
&= 2L(x) . \nonumber
\end{aligned}
$$

Perhaps you can see how this generalizes to any integer $m$:

$$
L(m x) = m L(x).
$$

Homogeneity is also noted as a special case because a system can be homogeneous without being linear. For example the absolute value, $f(x) = |x|$, and the vector length function

$$
f(\mathbf{x}) = \|\mathbf{x}\|_2 = \sqrt{x_1^2 + x_2^2 + \cdots + x_n^2}
$$

are both homogeneous but not linear. The function $ReLU(x) = \max(0, x)$, widely used in neural networks, is also homogeneous for $\alpha > 0$ but not linear:

$$
1 = ReLU(2 + -1) \neq ReLU(2) + ReLU(-1) = 2 + 0.
$$

Remembering that such cases exist may be of use to you in the future.

::: {.callout-note title="Extension to Rational Numbers" collapse="false"}

For real-valued systems that obey superposition, homogeneity holds for any rational number. If $\alpha = \frac{p}{q}$ is rational:

$$
L(\alpha x) = L\left(\frac{p}{q}x\right) = p L\left(\frac{x}{q}\right).
$$

Let $x' = x/q$, so

$$
L(x) = L(q x') = q L(x') \implies L(x') = \frac{1}{q} L(x).
$$

Therefore,

$$
L(\alpha x) = p L(x') = p \frac{1}{q} L(x) = \frac{p}{q} L(x) = \alpha L(x).
$$

To extend this to all real numbers, continuity and far more extensive proof is required. But how about we just work with the rationals and be engineers for now?
:::

## Discrete linear systems {#sec-ls-discrete}
The ISETCam simulations rely on **discrete** representations of linear systems.

We convert a continuous stimulus—such as a spatial pattern or a spectrum—by sampling it at a set of points. In general, a continuous signal $s(x)$ becomes a vector of sampled values, $\mathbf{s}$, by sampling it at a set of values, $x_i$.  We might write the entries as $s[x_i]$, where $i$ is an integer index, or simply $s[i]$. The square brackets are a reminder that the signal is discretely sampled.

Any discrete linear system can be represented as a matrix, $\mathbf{L}$, which maps the input vector to an output vector, $\mathbf{o}$:

$$
\mathbf{o} = \mathbf{L} \mathbf{s}
$$ {#eq-linear-matrix}

This can also be written as a summation:

$$
o[i] = \sum_j L[i,j] s[j] 
$$ {#eq-linear-matrix-summation}

It’s helpful to visualize this as a matrix tableau: each column of $\mathbf{L}$ is scaled by the corresponding entry in $\mathbf{s}$, and the output vector is the sum of these scaled columns.

![Visualization of a matrix multiplication.  This is called a matrix tableau.  It emphasizes the matrix as a set of columns, and the signal ($s$) and output ($o$) as two column vectors. The linear transformation, $L$, in this case is a square matrix ($N \times N$).](images/linearsystems/matrixTableau-1.png){#sec-ls-matrix-tableau width=60%}

Some matrices have an inverse, and for these we can estimate the input by measuring the output. Square matrices are particularly simple to work with, and a square matrix means we are matching the sample spacing of the inputs with the sample spacing of the outputs. Some matrices are not invertible, and the best we can do from the measurements is constrain the set of possible inputs.

The notation we use for the vectors and matrices may vary—sometimes the input is naturally an image (a matrix), and we “flatten” it into a vector; other times, we use tricks to keep the image structure intact and still carry out the computation. But the key idea is always the same: in the discrete world, linear systems are matrices that map input vectors to output vectors.

Throughout this volume, I am trying to keep the mathematical typography consistent with this table.

::: {.callout-note title="Some matrix notation" collapse="false"}

| Notation | Meaning |
|----------|---------|
| **Bold capital letter** ($\mathbf{B}$) | Matrix |
| **Bold lower case** ($\mathbf{s}$) | Vector (column, $N \times 1$) |
| *Ordinary lower case* ($\alpha$, $s$) | Scalar (Latin or Greek) |
| *Ordinary upper case* ($L(u,v)$) | 2D matrix or image (continuous) |
| $L[\text{row},\text{col}]$ | Discrete image or matrix (row, col ordering) |
| $\mathbf{B^t}$ | Matrix or vector transpose (superscript $t$) |

:::

## Optics: Pointspread matrix {#sec-ls-point-matrix}
Image systems engineers often examine system performance by measuring the response to a point of light—this is the point spread function (PSF) introduced in @sec-pointspread.  A point of light, $p_i$, is represented by a vector that has $1$ in a single location and zeros elsewhere.  Each column of the linear transformation, $\mathbf{L}$, is the point spread function corresponding to the sampled point, $\text{PSF}_i$.

![Matrix tableau of an optical system with the point spreads in the columns.](images/linearsystems/psfmatrix.png){#fig-ls-matrix-psf width=80%}

I drew this tableau for a one-dimensional point spread, to make the idea clear. For two-dimensional images, we would have to flatten the image data into columns. But only complexity would be added, and the principles would be unchanged.

## Spatial basis functions {#sec-ls-spatial-basis}
The PSF is a fundamental tool, but it has limitations: using a point of light involves very few photons, and PSFs can vary in complex ways that are hard to summarize with just a few parameters. A key insight is that we can use linear models to describe images in many different ways, not just as a collection of points. Some alternative representations are especially useful for analyzing the optical performance of imaging systems.  Here, we use linear models to create alternative representations of the spatial intensity of images, $S[u,v]$. 

A linear model for spatial intensity looks like this:

$$
S[u,v] = \sum_{i} w[i] B_i[u,v]
$${#eq-linear-model}

The functions $B_i[u,v]$ are called **spatial basis functions**, and the scalars $w[i]$ are the weights for each basis function. The image intensity $S[u,v]$ is just a weighted sum of these basis functions.

If we agree on a set of basis functions, then we can describe any image simply by specifying the weights. In fact, when we describe an image as a grid of intensity values, we’re implicitly using a set of point basis functions—each one nonzero at a single location and zero everywhere else.


## Image transforms {#sec-ls-orthogonal-transforms}
For a linear model to be useful, we need a way to find the basis weights from the image. A function that maps an image, $S[u,v]$ to the weights $w[i]$ is called an **image transform**. If we choose **orthogonal basis functions** this transform becomes especially simple. Orthogonal means

$$
0 = \sum_{u,v} B_i[u,v] ~ B_j[u,v]
$$ {#eq-spatial-orthogonal}

for any two pair of different basis functions $B_i$ and $B_j$.

To work with these two-dimensional images mathematically, we often “flatten” each basis function $B_i$ (which is a matrix) into a long column vector by stacking its columns. We do the same for the image $S[u,v]$, turning it into a vector $\mathbf{s}$. If we collect all the basis vectors into the columns of a matrix $\mathbf{B}$, and the weights into a vector $\mathbf{w}$, we can write the linear model from @eq-linear-model as a matrix equation:

$$
\mathbf{s} = \mathbf{B w}
$$ {#eq-linear-model-matrix}

Here, the $i^{th}$ entry in $\mathbf{w}$ multiplies the $i^{th}$ column of $\mathbf{B}$, and the sum gives us the image vector $\mathbf{s}$. This is just a compact way of writing the weighted sum from before.

Now, if the basis functions are orthogonal, then $\mathbf{B^t B}$ is the identity matrix. This makes it easy to recover the weights from the image:

$$
\begin{aligned}
\mathbf{s} &= \mathbf{B w} \\
\mathbf{B^t s} &= \mathbf{B^t B w} \\
\mathbf{B^t s} &= \mathbf{w}
\end{aligned}
$$ {#eq-linear-model-inverse}

In words: just multiply the image vector by the transpose of the basis matrix to calculate the weights. That’s all there is to it!

If the basis functions aren’t orthogonal, we have to work a bit harder and use the inverse of $\mathbf{B}$ to solve for the weights. That’s not usually a problem for computers, but it’s much more convenient when the basis is orthogonal and we can just use the transpose.


:::{.callout-note title="Point basis" collapse="false"}
This seems almost too trivial to mention.  But here we go.

When we describe images as points, $S[u,v]$, we are implicitly using the **point basis**. The points form an orthogonal basis functions—each basis function, $B_i[u,v]$, is zero everywhere except at a single $(u,v)$ location. The weight for each basis function is simply the image intensity at that point, $S[u,v]$. 

$$
S[u,v] = \sum_{u,v} S[u,v] B_i[u,v]
$${#eq-linear-model}

Because each point basis function is nonzero at only one location, the inner product of any two different point basis functions is zero—they are orthogonal by construction.
:::

## Harmonics {#sec-ls-points-harmonics}
Linear models are central to science, engineering, and mathematics. Among the many possible linear models, two orthogonal basis sets are especially important. 

We have repeatedly used the point representation of the image. The point (or “impulse”) representation is so natural that we often take it for granted.  The second is the **harmonic basis** —that is, sine and cosine waves at different spatial frequencies. In the next sections, I explain why harmonics are so valuable for analyzing certain types of linear systems. 

::: {.callout-note title="Dynamic systems" collapse="false"}
In systems that respond over time (like audio or electronics), the equivalent measurement is the **impulse response**—the system’s output when given a very brief input. The impulse response plays the same role as the PSF, just in the time domain. In dynamic systems, the point basis is called the **impulse** and the point spread is the **impulse response**. The mathematical formulation of the impulse or point is often called the **Dirac delta function**, to honor the theoretical physicist Paul Dirac. 
:::

## Eigenfunctions {#sec-ls-eigenfunctions}
An **eigenfunction** (or, in the discrete case, an **eigenvector**) of a system is a special input that produces an output that is simply a scaled version of itself. In other words, when you feed an eigenfunction into a linear system, the system just scales it—nothing else changes. This property is incredibly useful, because it means we can predict exactly what will happen to these inputs.

For a discrete linear system represented by a matrix $\mathbf{L}$, a column vector $\mathbf{e}_i$ is an eigenvector if

$$
\mathbf{L} \mathbf{e}_i = \alpha_i \mathbf{e}_i
$$ {#eq-eigenfunction-definition}

where $\alpha_i$ is the **eigenvalue** associated with $\mathbf{e}_i$. The eigenvalue tells us how much the eigenvector is stretched or shrunk by the transformation.

Many image systems we describe in this book have a **complete set** of orthogonal eigenvectors. By a complete set, I mean (a) the eigenvectors of the system are a basis for the input (@sec-ls-spatial-basis), and (b) the eigenvectors are orthogonal to one another (@sec-ls-orthogonal-transforms). This means we can express any input vector (stimulus) as a weighted sum of the image system's eigenvectors. The power of this observation is that measuring the system response to an eigenvector input is very simple:

$$
\begin{aligned}
\mathbf{o} & = \mathbf{L} \mathbf{s} \\
       & = \mathbf{L} \left(\sum_i \sigma_i \mathbf{e}_i \right) \\
       & = \sum_i \sigma_i \mathbf{L} \mathbf{e}_i \\
       & = \sum_i \sigma_i \alpha_i \mathbf{e}_i
\end{aligned}
$$ {#eq-linear-eigencalculations}

Here, $\mathbf{s}$ is the input, written as a sum of eigenvectors with weights $\sigma_i$. The output $\mathbf{o}$ is just the sum of the same eigenvectors, but now each one is scaled by its eigenvalue $\alpha_i$. This makes analyzing and understanding the system much easier.

Not all linear systems have a complete set of eigenvectors. Only discrete linear systems represented by **normal matrices** are guaranteed to have an orthogonal set of eigenvectors.  The definition of a normal matrix is[^conjugate-transpose].

$$
\mathbf{L L^* = L^* L} .
$$ {#eq-normal-matrix-definition}


[^conjugate-transpose]: The $*$-operator means **conjugate transpose**.
This means take the complex conjugate of each entry ($a + ib \rightarrow a - ib$), and transpose the matrix. For a real matrix, the $*$-operator amounts to the usual matrix transpose. 

## Shift-invariant linear (SIL) systems  {#sec-ls-shiftinvariance}
A practical challenge in characterizing general linear systems is that each input point could have a different point spread function (PSF). That’s a lot to measure! Fortunately, many real systems are much simpler: their PSF is essentially the same, except that it’s shifted to match the shift in input location. In other words, the shape of the PSF doesn’t change—it just translates. We call such systems **shift-invariant**.

Shift-invariant linear systems are common across science and engineering. Many physical devices and phenomena can be well-approximated as shift-invariant, at least over a useful range. This property makes them much easier to calibrate: we only need to measure a single PSF -the response to a single point!- rather than measure every possible point. You can see why for a shift-invariant system the PSF is the key measurement that characterizes the system. 

Because shift invariance simplifies both analysis and computation, many courses on “linear systems” focus almost entirely on the shift-invariant case.

::: {.callout-note title="Shift invariance notation" collapse="true"}
Let’s make the idea of shift invariance a bit more precise. Suppose we use $T()$ to represent a shift (translation) operation. If we shift an image $x$ by an amount $\delta$, we write $T(x, \delta)$. A system $L()$ is shift-invariant if shifting the input and then applying the system gives the same result as applying the system first and then shifting the output (possibly by a scaled amount):

$$
L(T(x, \delta)) \approx T(L(x), \alpha \delta).
$$ {#eq-shiftinvariance-defined}

Here, $\alpha$ is a scale factor that accounts for magnification or other effects. For example, if you move an object by $\delta$ meters, its image might move by $\alpha \delta$ meters on the sensor.
:::

## SIL system matrix
The matrix representation of an SIL, $\mathbf{L}$ is simple and can be visualized.  If the input stimulus is a point, then it is a vector that has a $1$ in a single entry and $0$ elsewhere. The point spread function is the column of the matrix, $\mathbf{L}$.  Since the point spread functions are all the same, except for a shift, the columns are all the same except for a shift. 

If we arrange the sample spacing of the input and output measurements carefully, we can shift the input point by one row and the corresponding column, representing a point spread measurement, will also shift down one row. The entries of the matrix, shown in the tableau, will look like this @fig-ls-circulant-matrix.

![Circulant matrix.](images/linearsystems/circulant-matrix.png){#fig-ls-circulant-matrix width=70%}

For computer simulations, there are many details to keep track of, especially issues relating to the edge and wrapping. For example, we often treat the point spread functions as 'circular', so when a value shifts below the bottom of the matrix, we do not throw it away but we copy it into the missing entry at the top of the column. In @fig-ls-circulant-matrix, the edges of the point spread are all zero, and that made the diagram simpler. 

Have a good look at @fig-ls-circulant-matrix. Feel how simple an SIL system matrix is compared to a general linear system (@sec-ls-matrix-tableau). Linear systems in general are great to work with.  If you are working with an SIL, you are particularly fortunate.  The next section gives another reason why.

## Harmonics:  Eigenvectors for shift-invariance {#sec-ls-harmonics-sil}
We have reached the observation that is the basis of many calculations in the main chapters in the book: the relationship between harmonics and shift invariant linear systems. These ideas are introduced in the main text in @sec-optics-linear-harmonics, where we develop the Discrete Fourier Series (@sec-optics-fourier-series). The ideas continue to be used in many subsequent chapter.

Haromonic functions are *almost* eigenfunctions of shift-invariant linear systems. By almost, I mean that when a harmonic is the input, the output is a scaled harmonic with a possible shift in position (phase). It is not exactly an eigenfunction because two parameters, the amplitude and phase, can change.

The mathematics of this near-miss is managed by expressing the signal using complex instead of real numbers. For complex numbers, which inherently have two parameters, we can say without any guilt that the complex exponentials are eigenvectors of a shift-invariant linear system. Moreover, the complex exponentials are a complete, orthogonal set of eigenfunctions; any input stimulus can be described as the weighted sum of complex exponentials. We relate the complex exponentials back to the real harmonics, using Euler's formula (@eq-Eulers-formula). [^harmonics-FOV].

[^harmonics-FOV]: I explain the relationship between harmonics and shift-invariant linear systems in detail, staying away from complex exponentials, in the Appendix to [Foundations of Vision](https://wandell.github.io/FOV-1995/appendix.html){target=_blank}.

Taken together this means shift-invariant linear systems and harmonics can take advantage of the full power and simplicity of the eigenvector calculations (@eq-linear-eigencalculations). When the input to an optical system is a harmonic function at spatial frequency $f$ and unit contrast, the output image will be a harmonic function at the same frequency. The amplitude and of the output will differ from the input, but we only need to measure these two parameters. This input-output simplicity makes the harmonics very useful tools for system characterization.

::: {.callout-note collapse="true" title='History - linear algebra and matrices'}
When I was an assistant professor, I had the privilege of joining faculty meetings with senior colleagues I deeply admired—Joe Goodman, Ron Bracewell, Tom Cover, and others. In these meetings, we often discussed how best to teach key mathematical ideas to engineering students. The more senior faculty generally favored starting with integrals and continuous functions, emphasizing that these are the true foundation of the mathematics. The younger faculty, who spent much of their time programming, leaned toward teaching with matrices and vectors. I remember Ron Bracewell remarking that continuous functions are the reality, and we should introduce those first—then, only later, show students the "vulgar" discrete approximations that we use because our computational tools require them. Good times.

We all agreed that discrete representations are extremely useful—and today, they are essential. Mathematics framed in terms of vectors and matrices is at the heart of modern neural networks and much of computational science and engineering. That’s the approach I use throughout this book. Still, I remember Bracewell’s advice, and sometimes feel a bit of guilt for not starting with the continuous case.

It helps to know that representing problems with vectors and matrices is not a recent development. The importance of linear algebra and matrix methods was recognized nearly 200 years ago. If you’re interested in the history of these ideas, you can read more here: [History of matrices and linear algebra](resources/history-linear-algebra.html){target=_blank}
:::

## Local linearity {#sec-ls-local-linearity}
No real system is linear for all possible inputs. If you push a system with extremely large signals—say, by shining a laser into a camera or overloading an amplifier—it may behave unpredictably or even break! But for most practical purposes, many systems are linear over a useful range of inputs.

When a system is not linear everywhere, it may still be well-approximated as linear within a limited region of its input space. In other words, the system is **locally linear**: if your keep your signals similar enough to a particular operating point, the system’s behavior is linear, even if it isn’t globally so.

For a locally linear system, the best linear approximation depends on where you are in the input space. The linear model near $x_1$ might look quite different from the linear model near $x_2$. So, when modeling or analyzing a locally linear system, it’s important to know which region you’re working in.

In practice, the optics of most imaging systems are linear over a large range, but they are shift-invariant over a smaller input range, only within a limited region of the field of view. This is called **local shift invariance**. Such a region is known as an **isoplanatic** patch. The shift-invariant analysis applies well to such local regions, and degrades if we measure over larger regions.

This principle of local linearity, or local shift-invariance, is fundamental in mathematics, physics, and engineering.[^taylor-series] It’s the reason why linear models are so widely used, even for systems that aren’t truly linear everywhere.

[^taylor-series]: If you’ve seen the Taylor series expansion in calculus, you’ve already encountered local linearity. Near a point $x$, you can approximate a function by its value at $x$ plus a correction that depends on how far you are from $x$ and the derivative at $x$.
