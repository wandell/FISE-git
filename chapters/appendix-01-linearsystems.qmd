# Linear systems {#sec-appendix-linear-systems .unnumbered}

## Linear systems overview

Many courses in science, technology, engineering and mathematics cover linear systems theory.  Each of the fields has their own way of approaching the topic, and there are many different levels. There are two purposes for this appendix.  The first is to introduce linear systems ideas to people who have never been exposed to them before.  I hope that the framing here is at an introductory level so that people new to the idea of linear systems can see the main points.  The second is to provide more experienced readers with the framing that I use in this book. This chapter explains the ideas and notation that I think are useful for image systems engineering simulations.

## Principles of Superposition and Homogeneity {#sec-ls-superposition}
A system $L$ is linear if it satisfies the superposition rule:

$$
L(x + y) = L(x) + L(y).
$$ {#eq-superposition}

Here, $x$ and $y$ are two possible inputs, and $L(x)$ is the system's response to the input $x$. The input variable can be a scalar, vector, matrix, or tensor. For example, in an optical system, $x$ might represent the incident light field at the entrance aperture, and $L(x)$ could be the spectral irradiance measured at the sensor .

No real system is perfectly linear over all possible signals. Extreme inputs with a massive amount of energy may behave unpredictably or even break the system! However, many systems are linear over an input range that covers most of the likely use cases. 

In other cases the system will have many linear regimes, each much less than the entire input range. These systems are called **locally linear**. The linear approximation will differ between regions, so that the linear approximation in a region near $x_1$ will differ from the linear approximation near $x_2$. The principle of local linearity is a fundamental concept in mathematics, physics, and engineering.[^taylor-series]

 [^taylor-series]: If you’ve seen the Taylor series expansion in calculus, you may recognize the principle of local linearity in mathematics!

A special case of superposition is **homogeneity**:

$$
L(\alpha x) = \alpha L(x)
$$ {#eq-homogeneity}

Homogeneity follows from superposition. For example, adding an input to itself:

$$\begin{aligned}
L(x + x) &= L(2x) \nonumber  \\ 
&= L(x) + L(x) \nonumber \\
&= 2L(x). \nonumber
\end{aligned}
$$

This generalizes to any integer $m$:

$$
L(m x) = m L(x).
$$

Do remember that a system can be homogeneous without being linear. For example the absolute value, $f(x) = |x|$, and the vector length function

$$
f(\mathbf{x}) = \|\mathbf{x}\|_2 = \sqrt{x_1^2 + x_2^2 + \cdots + x_n^2}
$$

are homogeneous but not linear. The function $ReLU(x) = \max(0, x)$, widely used in neural networks, is also homogeneous for $\alpha > 0$ but not linear:

$$
1 = ReLU(2 + -1) \neq ReLU(2) + ReLU(-1) = 2 + 0.
$$

Remembering that these cases exist may be of use to you in the future.

::: {.callout-note title="Extension to Rational Numbers" collapse="false"}

For real-valued systems that obey superposition, homogeneity holds for any rational number. If $\alpha = \frac{p}{q}$ is rational:

$$
L(\alpha x) = L\left(\frac{p}{q}x\right) = p L\left(\frac{x}{q}\right).
$$

Let $x' = x/q$, so

$$
L(x) = L(q x') = q L(x') \implies L(x') = \frac{1}{q} L(x).
$$

Therefore,

$$
L(\alpha x) = p L(x') = p \frac{1}{q} L(x) = \frac{p}{q} L(x) = \alpha L(x).
$$

To extend this to all real numbers, continuity and far more extensive proof is required. But how about we just work with the rationals and be engineers for now?
:::

## Discrete linear systems {#sec-ls-discrete}
The ISETCam simulations we perform in the book use **discrete** representations of linear systems. 

We have already used discretization to represent a spatial stimulus at sample points or a spectral stimulus at sample wavelengths. In these discrete representations, a continuous stimulus, say $s(x)$, is sampled to form a vector of values, $\mathbf{s}$. We will write the entries as $s[x_i]$, where $i$ is an integer index, or simply $s[i]$, with square brackets to remind us that the signal is discretely sampled.

Discrete linear systems can be described by a matrix, $\mathbf{L}$, which maps the input vector to an output vector, $\mathbf{o}$:

$$
\mathbf{o} = \mathbf{L} \mathbf{s}
$$ {#eq-linear-matrix}

This can also be written as a summation

$$
o[i] = \sum_j L[i,j] s[j] 
$$ {#eq-linear-matrix-summation}

I find it helpful to visualize this as a matrix tableau: the $j^{th}$ column of $\mathbf{L}$ is scaled by the $j^{th}$ entry, $s[j]$. The visualization helps see that the output vector is the weighted sum of the columns of the matrix $\mathbf{L}$.

**PUT IN A MATRIX TABLEAU IMAGE HERE**

The action of any discrete linear system can be represented as matrix. There may be some notation or representations that change, but the principle always hold.  For example, sometimes the input is naturally thought of as an image (a matrix), and we need to "flatten" it into a vector; other times, we might use other clever tricks to do the linear transformation while keeping the image structure. But the key idea to keep in mind is that, in the discrete world, linear systems are matrices that map input vectors to output vectors.

::: {.callout-note collapse="true" title='History - linear algebra and matrices'}
When I was an assistant professor, I had the privilege of joining faculty meetings with senior colleagues I deeply admired—Joe Goodman, Ron Bracewell, Tom Cover, and others. In these meetings, we often discussed how best to teach key mathematical ideas to engineering students. The more senior faculty generally favored starting with integrals and continuous functions, emphasizing that these are the true foundation of the mathematics. The younger faculty, who spent much of their time programming, leaned toward teaching with matrices and vectors. I remember Ron Bracewell remarking that continuous functions are the reality, and we should introduce those first—then, only later, show students the "vulgar" discrete approximations that we use because our computational tools require them. Good times.

We all agreed that discrete representations are extremely useful—and today, they are essential. Mathematics framed in terms of vectors and matrices is at the heart of modern neural networks and much of computational science and engineering. That’s the approach I use throughout this book. Still, I remember Bracewell’s advice, and sometimes feel a bit of guilt for not starting with the continuous case.

It helps to know that representing problems with vectors and matrices is not a recent development. The importance of linear algebra and matrix methods was recognized nearly 200 years ago. If you’re interested in the history of these ideas, you can read more here: [History of matrices and linear algebra](resources/history-linear-algebra.html){target=_blank}
:::

## Spatial basis functions {#sec-optics-spatial-basis}
It is natural to think of images as a set of points, and in @sec-pointspread we introduced the idea of the point spread function.  This is a very important system characterization, but it isn't always helpful for system characterization. It was an important insight  realize that images can be described in many other ways. Some of these alternatives are very valuable for analyzing and characterizing optical performance of image systems.

Linear models are an important method for exploring alternative image representations. We introduced linear models in @sec-linear-models, and we applied them to characterizing the spectral properties of light fields. Here we use linear models again, but this time to as models of the image intensity, $I(x,y)$. A linear model of the image intensity can be written this way:

$$
O(x,y) = \sum_{i} w_{i} B_{i}(x,y)
$${#eq-linear-model}

The functions $B_{i}(x,y)$ are the **spatial basis functions**, and the scalar values $w_{i}$ are the weights applied to each basis function. The image is equal to the weighted sum of these basis functions. 

## Orthogonal image transforms {#sec-ls-orthogonal-bases}
For a linear model to be useful, we need a method for finding the weights from the image. A function that maps $I(x,y)$ into the weights, $w_i$ is called an **image transform**.  If we choose a basis functions that are orthogonal to one another, the transform is easy to find.

$$
0 = \sum_{x,y} B_i (x,y) B_j (x,y),
$$ {#eq-spatial-orthogonal}

The simplicity of the transform can be seen by representing @eq-linear-model in discrete form an using matrices. Discrete representations using vectors and matrices is the way we compute, and thus a more practical if less pure.  To emphasize the discrete representation we will shift notation just a bit so that $I(x)$ becomes $I[n]$, and $B(x,y)$ becomes $B[n,m]$. 

We can convert the basis functions, which are matrices, into a column vector by stacking the columns on top of one another. We then group these columns into the columns of a single matrix, $\mathbf{B}$. We similarly stack the columns of the image into a long column vector $\mathbf{I}$.  The weights are already a vector which we write as, $\mathbf{w}$. 

We can write @eq-linear-model as a matrix equation this way

$$
\mathbf{o} = \mathbf{B w} .
$$ {#eq-linear-model-matrix}

The $i^{th}$ entry in the vector $\mathbf{w}$ multiples the $i^{th}$ column of the matrix $\mathbf{B}$.  The weighted columns are summed. This matrix equation is the same as the @eq-linear-model.

When the basis functions are orthogonal, $\mathbf{B^t B}$ is the identity matrix.  In this case we can transform from the image data to the basis function weights quite simply,

$$
\mathbf{w} = \mathbf{B^t o} .
$$ {#eq-linear-model-inverse}

So, that's simple! 

There are instances when we create linear models with bases that are not orthogonal. In that case, we have to find the inverse of $\mathbf{B}$ to move back and forth between the image and its transform. Computers are pretty big these days, so we can live with that.  But it is very nice to have the basis functions be orthogonal.

The principles of linear models are important for image systems and science and engineering broadly. Of the many linear models, two stand out as the most important.  The first is the representation as points.  When we represent an image as a set of points, we don't even notice that we are using an orthogonal basis. The points are the basis functions, $B_i$ is zero everywhere except except at one $(x,y)$ point.  The weights are the image intensity at that point, $I(x,y)$. Also, the point representation is orthogonal.  The inner product of two point basis functions is zero!  

The point representation is used very widely in many fields.  When used with dynamic systems, the point is usually called the **impulse**.  Or, in general, the **Dirac delta function** to honor a physicist who developed formal theory using the representation.  The use of points to represent an image is so natural that this observation is rarely made. We make it here mainly to emphasize that there are other linear model representations that have these key properties:  the stimulus is the weighted sum of the basis functions and the basis functions are orthogonal to one another.  Those two properties make the point representation valuable, and they also make the alternative representations valuable.

 The second important orthogonal representation uses **harmonic functions** as basis functions. We are heading towards an explanation of why these are so important. The way will get there is through eigenfunctions and shift-invariant linear systems, below.

## Eigenfunctions {#sec-ls-eigenfunctions}

An **eigenfunction** of a system is an input that whose output is the same as the input, up to a scale factor.  Harmonics are almost eigenfunctions. This is a very useful fact to know about a system because it means when we use an eigenfunction as input, we know almost everything about the output.  In symbols, if $\mathbf{L}$ is a matrix that represents a discrete linear system, the column vector $\mathbf{e}_i$ is an eigenfunction if

$$
\alpha_i \mathbf{e}_i = \mathbf{L} \mathbf{e}_i 
$$ {#eq-eigenfunction-definition}

The scalar values, $\alpha_i$ are the **eigenvalues** of the linear transformation. 

Some linear systems -and of course the ones we will be studying- have a set of eigenvectors that serve as an orthogonal basis for the input. For these systems, we can represent the stimulus as the weighted sum of its eigenfunctions.  Notice how simple this makes calculatin the output, $\mathbf{o}$ to an input $\mathbf{s}$. 

$$
\begin{aligned}
\mathbf{o} & = \mathbf{L} ~ \mathbf{s} \\
  & = \mathbf{L} ~ (\sum_i \sigma_i \mathbf{e}_i ) \\
  & = \sum_i \sigma_i \mathbf{L} \mathbf{e}_i \\
  &  = \sum_i \sigma_i \alpha_i  \mathbf{e}_i
 \end{aligned}
$$ {#eq-linear-eigencalculations}


## Shift-invariant linear systems {#sec-ls-shiftinvariance}
A practical limitation of characterizing a general linear systems is that it can have many different PSFs - one for every point! But many real systems are much simpler. These systems have essentially the same point spread function across many points, with the only difference being that the point spread function is shifted between points. Because the shape of the point spread function remains the same -it is only shifted- we say such a system is **shift invariant**. 

Across the many branches of science and engineering, there are a large number of physical phenomena and engineering devices that are well-approximated as shift invariant linear systems. These systems are relatively straightforward to calibrate because we only need to estimate a single point spread function. Also, there are many computational tools that help us simulate shift invariant systems.  In imaging the point spread function is the key measurement that characterizes the linear system. In applications to systems that characterize responses over time, the key measurement is the response to a short temporal input. This is called the **impulse response**, and it is the equivalent of the point spread function in an imaging system. 

The simplicity derived from shift invariance is so useful that some courses entitled 'linear systems' focus almost entirely on shift-invariant linear systems. 

::: {.callout-note title="Shift invariance notation" collapse="true"}
A little mathematical notation may be useful. Suppose we write image translation as a function, $T()$. If we shift an image, $x$, by an amount, $\delta$, we write it as $T(x,\delta)$. A shift invariant system means that the output of a linear system, $L()$, to a translated image is the translated linear response

$$
L(T(x,\delta)) \approx T(L(x),\alpha \delta).
$$ {#eq-shiftinvariance-defined}

Notice that @eq-shiftinvariance-defined allows the size of the image shift, $\delta$, to differ from the size of the output shift, $\alpha \delta$. So if we displace an object by an amount $\delta \text{m}$, the image will be displaced by an amount $\alpha \delta \text{m}$ on the sensor. This scale factor depends on the image magnfication. 
:::

The image formation component (optics) of most image systems are typically linear, but they are not globally shift-invariant. Rather, they are shift invariant over some part, but not all, of the full visual field. Usually there is a region near the optical axis that is shift invariant, and the system is also locally shift-invariant at off-axis points. This local shift invariance is similar to a system that is locally linear. In optics the shift invariant region is called an **isoplanatic** region. 

## Harmonics are almost eigenfunctions of SILs {#sec-ls-almost-eigen}
In words, we express the stimulus as the weighted sum of the eigenfunctions.  Then we use linearity to apply the transformation to each of the eigenfunctions. This brings up the eigenvalue times the eigenfunction. So in the end, the output is the sum of the eigenfunctions, with each scaled by a weight corresponding to the stimulus itself ($\sigma_i$) and a second weight associated with the linear transformation ($\alpha_i$).  This can become even simpler if we select our stimulus to be an eigenfunction!  

Harmonic functions are a tool to help with system characterization. Their value arises from this simple and important fact:  When the input to an optical system is a harmonic function at spatial frequency $f$ and unit contrast, in each local region the image will also be a harmonic function at the same frequency.  The relative phase and amplitude of the output will differ from the input, but we only need to measure these two parameters.  

This chapter explains the general principle and then provides several ways in which harmonics are used to characterize optical systems as well as the human visual system.

If the input is a harmonic of frequency $f$, the output will be a scaled version of that harmonic.  It is not a perfect eigenfunction only because there can also be a phase shift. This input-output simplicity makes the harmonics very useful tools for system characterization.

