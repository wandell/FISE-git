# Linear systems {#sec-appendix-linear-systems .unnumbered}
<!-- MORE TODO.  But going back to real optics for a while.  Need to finish the eigenfunction, shift-invariant, and so forth at the bottom.
-->

## Linear systems overview
Linear systems theory is a foundational topic in science, engineering, and mathematics, but each field tends to approach it a bit differently and at varying levels of depth. This appendix has two main goals. First, it aims to introduce the core ideas of linear systems to readers who may not have seen them before, using an accessible and intuitive approach. Second, it provides a consistent framework and notation for more experienced readers, matching the perspective used throughout this book. My hope is that both newcomers and those with prior exposure will find this chapter a useful reference for the concepts and tools we use in image systems engineering.

## Principles of Superposition and Homogeneity {#sec-ls-superposition}
A system $L$ is linear if it satisfies the superposition rule:

$$
L(x + y) = L(x) + L(y).
$$ {#eq-superposition}

Here, $x$ and $y$ are two possible inputs, and $L(x)$ is the system's response to the input $x$. The input variable can be a scalar, vector, matrix, or tensor. For example, in an optical system, $x$ might represent the incident light field at the entrance aperture, and $L(x)$ could be the spectral irradiance measured at the sensor .

A special case of superposition is **homogeneity**:

$$
L(\alpha x) = \alpha L(x)
$$ {#eq-homogeneity}

Homogeneity follows from superposition. For example, adding an input to itself:

$$\begin{aligned}
L(x + x) &= L(2x) \nonumber  \\ 
&= L(x) + L(x) \nonumber \\
&= 2L(x). \nonumber
\end{aligned}
$$

Perhaps you can see how this generalizes to any integer $m$:

$$
L(m x) = m L(x).
$$

Finally, remember that a system can be homogeneous without being linear. For example the absolute value, $f(x) = |x|$, and the vector length function

$$
f(\mathbf{x}) = \|\mathbf{x}\|_2 = \sqrt{x_1^2 + x_2^2 + \cdots + x_n^2}
$$

are both homogeneous but not linear. The function $ReLU(x) = \max(0, x)$, widely used in neural networks, is also homogeneous for $\alpha > 0$ but not linear:

$$
1 = ReLU(2 + -1) \neq ReLU(2) + ReLU(-1) = 2 + 0.
$$

Remembering that such cases exist may be of use to you in the future.

::: {.callout-note title="Extension to Rational Numbers" collapse="false"}

For real-valued systems that obey superposition, homogeneity holds for any rational number. If $\alpha = \frac{p}{q}$ is rational:

$$
L(\alpha x) = L\left(\frac{p}{q}x\right) = p L\left(\frac{x}{q}\right).
$$

Let $x' = x/q$, so

$$
L(x) = L(q x') = q L(x') \implies L(x') = \frac{1}{q} L(x).
$$

Therefore,

$$
L(\alpha x) = p L(x') = p \frac{1}{q} L(x) = \frac{p}{q} L(x) = \alpha L(x).
$$

To extend this to all real numbers, continuity and far more extensive proof is required. But how about we just work with the rationals and be engineers for now?
:::

## Discrete linear systems {#sec-ls-discrete}
The ISETCam simulations we use throughout this book rely on **discrete** representations of linear systems.

We have already seen how to discretize a continuous stimulus—such as a spatial pattern or a spectrum—by sampling it at a set of points. For example, a continuous signal $s(x)$ becomes a vector of sampled values, $\mathbf{s}$. We’ll write the entries as $s[x_i]$, where $i$ is an integer index, or simply $s[i]$. The square brackets are a reminder that the signal is discretely sampled.

A discrete linear system can be described by a matrix, $\mathbf{L}$, which maps the input vector to an output vector, $\mathbf{o}$:

$$
\mathbf{o} = \mathbf{L} \mathbf{s}
$$ {#eq-linear-matrix}

This can also be written as a summation:

$$
o[i] = \sum_j L[i,j] s[j] 
$$ {#eq-linear-matrix-summation}

It’s helpful to visualize this as a matrix tableau: each column of $\mathbf{L}$ is scaled by the corresponding entry in $\mathbf{s}$, and the output vector is the sum of these scaled columns.

![Visualization of a matrix multiplication.  This is called a matrix tableau.  It emphasizes the matrix as a set of columns, and the signal ($s$) and output ($o$) as two column vectors. The linear transformation, $L$, in this case is a square matrix ($N \times N$).](images/linearsystems/matrixTableau-1.png){#sec-appendix-matrix-tableau width=60%}

Any discrete linear system can be represented as a matrix. The notation may vary—sometimes the input is naturally an image (a matrix), and we “flatten” it into a vector; other times, we use tricks to keep the image structure intact. But the key idea is always the same: in the discrete world, linear systems are matrices that map input vectors to output vectors.

::: {.callout-note title="Some matrix notation" collapse="false"}

| Notation | Meaning |
|----------|---------|
| **Bold capital letter** ($\mathbf{B}$) | Matrix |
| **Bold lower case** ($\mathbf{s}$) | Vector (column, $N \times 1$) |
| *Ordinary lower case* ($\alpha$, $s$) | Scalar (Latin or Greek) |
| *Ordinary upper case* ($L(u,v)$) | 2D matrix or image (continuous) |
| $L[\text{row},\text{col}]$ | Discrete image or matrix (row, col ordering) |
| $\mathbf{B^t}$ | Matrix or vector transpose (superscript $t$) |

:::

## Spatial basis functions {#sec-ls-spatial-basis}
Image systems engineers need effective ways to characterize system performance. One common approach is to examine how the system responds to a point of light—this is the point spread function (PSF) introduced in @sec-pointspread. The PSF is a fundamental tool, but it has limitations: using a point of light involves very few photons, and PSFs can vary in complex ways that are hard to summarize with just a few parameters.

A key insight is that images can be described in many ways, not just as a collection of points. Some alternative representations are especially useful for analyzing and characterizing the optical performance of imaging systems. Linear models provide a powerful framework for defining and exploring these alternative image representations. We first used linear models to describe the spectral properties of light fields (@sec-linear-models). Here, we use linear models in a different context, applying them to the spatial intensity of images, $S[u,v]$. A linear model for spatial intensity looks like this:

$$
S[u,v] = \sum_{i} w[i] B_i[u,v]
$${#eq-linear-model}

The functions $B_i[u,v]$ are called **spatial basis functions**, and the scalars $w[i]$ are the weights for each basis function. The image intensity $S[u,v]$ is just a weighted sum of these basis functions.

If we agree on a set of basis functions, then we can describe any image simply by specifying the weights. In fact, when we describe an image as a grid of intensity values, we’re implicitly using a set of point basis functions—each one nonzero at a single location and zero everywhere else.

## Image transforms {#sec-ls-orthogonal-transforms}
For a linear model to be useful, we need a way to find the basis weights from the image itself. A function that maps an image, $S(u,v)$ to the weights $w[i]$ is called an **image transform**. If we choose **orthogonal basis functions** this transform becomes especially simple. Orthogonal means

$$
0 = \sum_{u,v} B_i[u,v] ~ B_j[u,v]
$$ {#eq-spatial-orthogonal}

for any two pair of different basis functions $B_i$ and $B_j$.

To work with these two-dimensional images mathematically, we often “flatten” each basis function $B_i$ (which is a matrix) into a long column vector by stacking its columns. We do the same for the image $S[u,v]$, turning it into a vector $\mathbf{s}$. If we collect all the basis vectors into the columns of a matrix $\mathbf{B}$, and the weights into a vector $\mathbf{w}$, we can write the linear model from @eq-linear-model as a matrix equation:

$$
\mathbf{s} = \mathbf{B w}
$$ {#eq-linear-model-matrix}

Here, the $i^{th}$ entry in $\mathbf{w}$ multiplies the $i^{th}$ column of $\mathbf{B}$, and the sum gives us the image vector $\mathbf{s}$. This is just a compact way of writing the weighted sum from before.

Now, if the basis functions are orthogonal, then $\mathbf{B^t B}$ is the identity matrix. This makes it easy to recover the weights from the image:

$$
\begin{aligned}
\mathbf{s} &= \mathbf{B w} \\
\mathbf{B^t s} &= \mathbf{B^t B w} \\
\mathbf{B^t s} &= \mathbf{w}
\end{aligned}
$$ {#eq-linear-model-inverse}

In words: just multiply the image vector by the transpose of the basis matrix to calculate the weights. That’s all there is to it!

If the basis functions aren’t orthogonal, we have to work a bit harder and use the inverse of $\mathbf{B}$ to solve for the weights. That’s not usually a problem for computers, but it’s much more convenient when the basis is orthogonal and we can just use the transpose.

## Points and harmonics {#sec-ls-points-harmonics}
Linear models are central to science, engineering, and mathematics. Among the many possible linear models, two orthogonal basis sets are especially important.

The first is the **point basis**. When we represent an image as a grid of points, we are implicitly using a set of orthogonal basis functions—each basis function, $B_i[u,v]$, is zero everywhere except at a single $(u,v)$ location. The weight for each basis function is simply the image intensity at that point, $S[u,v]$. Because each point basis function is nonzero at only one location, the inner product of any two different point basis functions is zero—they are orthogonal by construction.

Take note of the linear system matrix when we are working with points. A point of light, $p$, is represented as an input that has $1$ in a single location and zeros elsewhere.  Each column of the linear transformation, $\mathbf{L}$ is the point spread function of that point, $\text{PSF}_p$.

![Point spreads in the columns](){#fig-ls-matrix-psf}

This point (or “impulse”) representation is so natural that we often take it for granted. In dynamic systems, the point basis is called the **impulse** or, more formally, the **Dirac delta function** (named after physicist Paul Dirac). The key idea is that the stimulus can be written as a weighted sum of orthogonal basis functions, with the weights given by the image values themselves. This property makes the point basis both powerful and convenient—but it’s not the only useful choice.

The second is the **harmonic basis** —that is, sine and cosine waves at different spatial frequencies. In the next sections, I explain why harmonics are so valuable for analyzing certain types of linear systems. 

## Eigenfunctions {#sec-ls-eigenfunctions}
An **eigenfunction** (or, in the discrete case, an **eigenvector**) of a system is a special input that produces an output that is simply a scaled version of itself. In other words, when you feed an eigenfunction into a linear system, the system just scales it—nothing else changes. This property is incredibly useful, because it means we can predict exactly what will happen to these inputs.

For a discrete linear system represented by a matrix $\mathbf{L}$, a column vector $\mathbf{e}_i$ is an eigenvector if

$$
\mathbf{L} \mathbf{e}_i = \alpha_i \mathbf{e}_i
$$ {#eq-eigenfunction-definition}

where $\alpha_i$ is the **eigenvalue** associated with $\mathbf{e}_i$. The eigenvalue tells us how much the eigenvector is stretched or shrunk by the transformation.

Many image systems we describe in this book have a **complete set** of orthogonal eigenvectors. By a complete set, I mean (a) the eigenvectors of the system are a basis for the input (@sec-ls-spatial-basis), and (b) the eigenvectors are orthogonal to one another (@sec-ls-orthogonal-transforms). This means we can express any input vector (stimulus) as a weighted sum of the image system's eigenvectors. The power of this observation is that measuring the system response to an eigenvector input is very simple:

$$
\begin{aligned}
\mathbf{o} & = \mathbf{L} \mathbf{s} \\
       & = \mathbf{L} \left(\sum_i \sigma_i \mathbf{e}_i \right) \\
       & = \sum_i \sigma_i \mathbf{L} \mathbf{e}_i \\
       & = \sum_i \sigma_i \alpha_i \mathbf{e}_i
\end{aligned}
$$ {#eq-linear-eigencalculations}

Here, $\mathbf{s}$ is the input, written as a sum of eigenvectors with weights $\sigma_i$. The output $\mathbf{o}$ is just the sum of the same eigenvectors, but now each one is scaled by its eigenvalue $\alpha_i$. This makes analyzing and understanding the system much easier.

Not all linear systems have a complete set of eigenvectors. Only discrete linear systems represented by **normal matrices** are guaranteed to have an orthogonal set of eigenvectors.  The definition of a normal matrix is

$$
\mathbf{L L^* = L^* L} .
$$ {#eq-normal-matrix-definition}

The $*$-operator means **conjugate transpose**[^conjugate-transpose].

[^conjugate-transpose]: This means take the complex conjugate of each entry ($a + ib \rightarrow a - ib$), and transpose the matrix.  For a real matrix, the $*$-operator is amounts to the usual transpose. 

## Shift-invariant linear (SIL) systems  {#sec-ls-shiftinvariance}
A practical challenge in characterizing general linear systems is that, in principle, each point in the input could have its own unique point spread function (PSF). That’s a lot to measure! Fortunately, many real systems are much simpler: their PSF is essentially the same everywhere, except that it’s shifted to match the input location. In other words, the shape of the PSF doesn’t change—it just moves around. We call such systems **shift-invariant**.

Shift-invariant linear systems are common across science and engineering. Many physical devices and phenomena can be well-approximated as shift-invariant, at least over a useful range. This property makes them much easier to calibrate: we only need to measure a single PSF, rather than one for every possible input location. In imaging, the PSF is the key measurement that characterizes the system. In systems that respond over time (like audio or electronics), the equivalent measurement is the **impulse response**—the system’s output when given a very brief input. The impulse response plays the same role as the PSF, just in the time domain.

Because shift invariance simplifies both analysis and computation, many courses on “linear systems” focus almost entirely on the shift-invariant case.

::: {.callout-note title="Shift invariance notation" collapse="true"}
Let’s make the idea of shift invariance a bit more precise. Suppose we use $T()$ to represent a shift (translation) operation. If we shift an image $x$ by an amount $\delta$, we write $T(x, \delta)$. A system $L()$ is shift-invariant if shifting the input and then applying the system gives the same result as applying the system first and then shifting the output (possibly by a scaled amount):

$$
L(T(x, \delta)) \approx T(L(x), \alpha \delta).
$$ {#eq-shiftinvariance-defined}

Here, $\alpha$ is a scale factor that accounts for magnification or other effects. For example, if you move an object by $\delta$ meters, its image might move by $\alpha \delta$ meters on the sensor.
:::

## SIL system matrix
The matrix representation of an SIL, $\mathbf{L}$ is simple and can be visualized.  If the input stimulus is a point, then it is a vector that has a $1$ in a single entry and $0$ elsewhere. The point spread function is the column of the matrix, $\mathbf{L}$.  Since the point spread functions are all the same, except for a shift, the columns are all the same except for a shift. 

If we arrange the sample spacing on the input and output properly, we can shift the input point by one row and the point spread column will shift down one row.  The entries of the matrix, shown in the tableau, will be

![Circulant matrix](){#fig-ls-circulant-matrix}

There is one detail.  We treat the point spread functions as 'circular'.  When a value shifts below the bottom of the matrix, we copy it into the missing entry at the top of the column.  Hopefully you can see what I mean from the @fig-ls-circulant-matrix.

## Harmonics:  Eigenvectors for shift-invariance {#sec-ls-harmonics-sil}
Finally, we are at the main point I have been working towards.  It is the abstraction behind the use of harmonics with shift invariant linear systems in @sec-optics-linear-harmonics, where we develop the Discrete Fourier Series (@sec-optics-fourier-series).

Haromnic functions are (almost) eigenfunctions of shift-invariant linear systems. If we move from purely real stimuli to complex stimuli, the complex exponentials, through Euler's formula (@eq-Eulers-formula), are the complex harmonics. They are genuinely eigenfunctions of shift-invariant linear systems.  And the are a complete, orthogonal set of eigenfunctions so that any input stimulus can be described as the weighted sum of complex exponentials.


*Harmonic functions (like sines and cosines) are almost eigenfunctions for many image systems; this makes them very useful stimuli for characterization and design. I explain this idea in the Appendix to [Foundations of Vision](https://wandell.github.io/FOV-1995/appendix.html). The key ideas are explained below, from a slightly different point of view.*


the stimulus as the weighted sum of the eigenfunctions.  Then we use linearity to apply the transformation to each of the eigenfunctions. This brings up the eigenvalue times the eigenfunction. So in the end, the output is the sum of the eigenfunctions, with each scaled by a weight corresponding to the stimulus itself ($\sigma_i$) and a second weight associated with the linear transformation ($\alpha_i$).  This can become even simpler if we select our stimulus to be an eigenfunction!  


Harmonic functions are a tool to help with system characterization. Their value arises from this simple and important fact:  When the input to an optical system is a harmonic function at spatial frequency $f$ and unit contrast, in each local region the image will also be a harmonic function at the same frequency.  The relative phase and amplitude of the output will differ from the input, but we only need to measure these two parameters.  

This chapter explains the general principle and then provides several ways in which harmonics are used to characterize optical systems as well as the human visual system.

If the input is a harmonic of frequency $f$, the output will be a scaled version of that harmonic.  It is not a perfect eigenfunction only because there can also be a phase shift. This input-output simplicity makes the harmonics very useful tools for system characterization.


::: {.callout-note collapse="true" title='History - linear algebra and matrices'}
When I was an assistant professor, I had the privilege of joining faculty meetings with senior colleagues I deeply admired—Joe Goodman, Ron Bracewell, Tom Cover, and others. In these meetings, we often discussed how best to teach key mathematical ideas to engineering students. The more senior faculty generally favored starting with integrals and continuous functions, emphasizing that these are the true foundation of the mathematics. The younger faculty, who spent much of their time programming, leaned toward teaching with matrices and vectors. I remember Ron Bracewell remarking that continuous functions are the reality, and we should introduce those first—then, only later, show students the "vulgar" discrete approximations that we use because our computational tools require them. Good times.

We all agreed that discrete representations are extremely useful—and today, they are essential. Mathematics framed in terms of vectors and matrices is at the heart of modern neural networks and much of computational science and engineering. That’s the approach I use throughout this book. Still, I remember Bracewell’s advice, and sometimes feel a bit of guilt for not starting with the continuous case.

It helps to know that representing problems with vectors and matrices is not a recent development. The importance of linear algebra and matrix methods was recognized nearly 200 years ago. If you’re interested in the history of these ideas, you can read more here: [History of matrices and linear algebra](resources/history-linear-algebra.html){target=_blank}
:::

## Local linearity {#sec-ls-local-linearity}
No real system is perfectly linear for all possible inputs. If you push a system with extremely large signals—say, by shining a laser into a camera or overloading an amplifier—it may behave unpredictably or even break! But for most practical purposes, many systems are linear over a useful range of inputs.

When a system is not linear everywhere, it may still be well-approximated as linear within a limited region of its input space. In other words, the system is **locally linear**: if your keep your signals similar enough to a particular operating point, the system’s behavior is linear, even if it isn’t globally so.

For a locally linear system, the best linear approximation depends on where you are in the input space. The linear model near $x_1$ might look quite different from the linear model near $x_2$. So, when modeling or analyzing a locally linear system, it’s important to know which region you’re working in.

In practice, the optics of most imaging systems are linear over a large range, but not shift-invariant. Instead, they are shift-invariant only within certain regions of the field of view. This is called **local shift invariance**. In optics, such a region is known as an **isoplanatic** patch. Within an isoplanatic region, the PSF is (approximately) the same everywhere, so the shift-invariant analysis applies locally.

This principle of local linearity is fundamental in mathematics, physics, and engineering.[^taylor-series] It’s the reason why linear models are so widely used, even for systems that aren’t truly linear everywhere.

[^taylor-series]: If you’ve seen the Taylor series expansion in calculus, you’ve already encountered local linearity. Near a point $x$, you can approximate a function by its value at $x$ plus a correction that depends on how far you are from $x$ and the derivative at $x$.
