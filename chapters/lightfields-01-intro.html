<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-10-15">
<meta name="description" content="An integrated overview of image systems, from physical scene formation through sensors, optics, and human vision.">

<title>2&nbsp; Light and seeing – Foundations of Image Systems Engineering</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/lightfields-02-measurement.html" rel="next">
<link href="../chapters/part-scenes.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles/hover.css">
<link rel="stylesheet" href="../styles/callouts.css">
</head>

<body class="nav-sidebar floating slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/part-scenes.html">Scenes</a></li><li class="breadcrumb-item"><a href="../chapters/lightfields-01-intro.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Light and seeing</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Foundations of Image Systems Engineering</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Preface</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Scenes</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/part-scenes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part I: Scenes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/lightfields-01-intro.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Light and seeing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/lightfields-02-measurement.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Measuring light</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/lightfields-03-properties.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Light field properties</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/lightfields-04-properties-spectral.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Spectral regularities</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/lightfields-05-properties-spatial.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Spatial regularities</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Optics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/part-optics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part II: Optics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/optics-01-geometric.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Geometric optics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/optics-02-lenses.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Lens principles</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/optics-03-thinlens.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Thin lenses</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/optics-04-morelenses.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Lenses and ray transfer</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/optics-05-linear-space.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Spatial domain</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/optics-06-linear-transform.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Transform domain</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/optics-07-wavefront.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Wavefronts</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Sensors</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/part-sensors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part III: Sensors</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/sensors-01-photoelectric.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Photons and Electrons</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/sensors-02-pixels.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Pixels and sensors</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/sensors-03-parameters.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Sensor parameters</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/sensors-04-components.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">System components</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/sensors-05-control.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Control systems</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/sensors-06-characterization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Image system modeling</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/sensors-07-innovations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Sensor innovations</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Human</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/part-human.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part IV: Human Vision</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/human-01-seeing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Seeing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/human-02-encoding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Human visual encoding</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/human-03-metrics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Human visual metrics</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Displays</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/part-displays.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part V: Displays</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/displays-01-principles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Display Principles</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Image processing</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/part-processing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part VI: Image processing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/imgprocessing-01-hardware.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Hardware processing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/imgprocessing-02-rendering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Rendering</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendix</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/part-appendix.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part VII: Appendix</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/appendix-01-linearsystems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Linear systems</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/appendix-02-spaceinvariance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Linear Space-Invariant (LSI) Systems</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/appendix-03-isetcam.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Image Systems Simulation</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-light-overview" id="toc-sec-light-overview" class="nav-link active" data-scroll-target="#sec-light-overview"><span class="header-section-number">2.1</span> Light and seeing overview</a></li>
  <li><a href="#sec-light-em-spectrum" id="toc-sec-light-em-spectrum" class="nav-link" data-scroll-target="#sec-light-em-spectrum"><span class="header-section-number">2.2</span> The electromagnetic spectrum</a></li>
  <li><a href="#sec-light" id="toc-sec-light" class="nav-link" data-scroll-target="#sec-light"><span class="header-section-number">2.3</span> Light</a></li>
  <li><a href="#sec-lightfields-types" id="toc-sec-lightfields-types" class="nav-link" data-scroll-target="#sec-lightfields-types"><span class="header-section-number">2.4</span> The environmental light field</a></li>
  <li><a href="#sec-incident-lightfield" id="toc-sec-incident-lightfield" class="nav-link" data-scroll-target="#sec-incident-lightfield"><span class="header-section-number">2.5</span> The incident light field</a></li>
  <li><a href="#sec-optical-lightfield" id="toc-sec-optical-lightfield" class="nav-link" data-scroll-target="#sec-optical-lightfield"><span class="header-section-number">2.6</span> Optical light field</a></li>
  <li><a href="#sec-lightfield-measurement" id="toc-sec-lightfield-measurement" class="nav-link" data-scroll-target="#sec-lightfield-measurement"><span class="header-section-number">2.7</span> Measuring the light field</a></li>
  <li><a href="#interpreting-the-light-field" id="toc-interpreting-the-light-field" class="nav-link" data-scroll-target="#interpreting-the-light-field"><span class="header-section-number">2.8</span> Interpreting the light field</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/part-scenes.html">Scenes</a></li><li class="breadcrumb-item"><a href="../chapters/lightfields-01-intro.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Light and seeing</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-light" class="quarto-section-identifier"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Light and seeing</span></span></h1>
</div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">October 15, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Work in Progress
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>The book is still taking shape, and your feedback is an important part of the process. Suggestions of all kinds are welcome—whether it’s fixing small errors, raising bigger questions, or offering new perspectives. I’ll do my best to respond, but please keep in mind that the text will continue to change significantly over the next two years.</p>
<p>You can share comments through <a href="https://github.com/wandell/FISE-git/issues" target="_blank">GitHub Issues</a>.</p>
<p>Feel free to open a new issue or join an existing discussion. To make feedback easier to address, please point to the section you have in mind—by section number or a short snippet of text. Adding a label characterizing your issue would also be helpful.</p>
<p>Last updated: October 15, 2025</p>
</div>
</div>
</div>
<!--
::: {.callout-warning collapse="false"}
## Work in Progress
This is a **draft** version of *Foundations of Image Systems Engineering*. The content is actively evolving, and your feedback is valuable. Please share your comments or corrections via [GitHub Issues](https://github.com/wandell/FISE-git/issues).

When making a suggestion, kindly specify the section within the chapter where you'd like to see the change or additional material. Providing a section number or a snippet of searchable text will help me locate the relevant part more efficiently.

Last updated: October 15, 2025
:::
-->
<section id="sec-light-overview" class="level2 page-columns page-full" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="sec-light-overview"><span class="header-section-number">2.1</span> Light and seeing overview</h2>
<p>Electromagnetic radiation fills the world around us, providing a rich and reliable stream of information. Mobile organisms — whether a hawk, a rabbit, or a human — have implemented ways to sense and interpret this information. They use the information to make decisions and interact with their surroundings. We call the sensing and interpretation of the radiation <strong>visual perception</strong> or more simply, <strong>seeing</strong>.</p>
<p>Visual perception is fundamental for guiding movement, enabling animals to locate resources, avoid threats, and navigate their environments. The use of radiation for mobility and decision-making by mobile organisms is in contrast to immobile life forms, such as trees and plants. For these organisms the ambient radiation serves primarily as a source of energy, rather than information.</p>
<p>In robotics and most computer applications, visual sensing serves a similar purpose: to enable actions and movement within dynamic and complex environments. Robots with imaging systems identify and manipulate parts. Cars with image systems navigate through their surroundings. Medical imaging provides diagnostic information to guide interventions. Image systems provide information that enables goal-oriented actions for both artificial and biological systems.</p>
<div class="callout callout-style-default callout-note callout-titled page-columns page-full" title="Who needs a brain?">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Who needs a brain?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse show page-columns page-full">
<div class="callout-body-container callout-body page-columns page-full">
<p>The deep connection between mobility, visual sensing, and the brain is illustrated by the remarkable life path of the <a href="https://movementum.co.uk/journal/sea-squirts" target="_blank">sea squirt</a>. It begins life as tadpole-like larvae, equipped with a brain and a tail for swimming. Once the tadpole finds a suitable spot to settle, it attaches to a hard surface like a rock or coral and becomes immobile.</p>
<p>As soon as the sea squirt fixes itself to the rock, with no more navigation or swimming, its brain is no longer needed. In a truly bizarre twist, the sea squirt digests its own brain. This adaptation reallocates energy resources from the now unnecessary brain to more vital functions.</p>
<div id="fig-seasquirt" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-seasquirt-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/lightfields/01-lightfields/seasquirt.png" class="img-fluid figure-img" style="width:50.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig margin-caption" id="fig-seasquirt-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.1: A sea squirt image from <a href="https://goodheartextremescience.wordpress.com/2010/01/27/meet-the-creature-that-eats-its-own-brain/">this nice site</a>. This one is from Papua New Guinea. Plenty more images (and sea squirts) over there.
</figcaption>
</figure>
</div>
<p>My Stanford colleague, Irv Weissmann, and his group wrote many papers about neurodegeneration using the sea squirt.</p>
</div>
</div>
</div>
<p>Consumer photography - a large and important image system application - is an exception. Consumer photographs are created to store memories and evoke emotions, not to guide navigation or immediate decisions. Because of photography’s deep connection to human perception, the technologies for evaluating consumer photography differ from the engineering metrics that are appropriate for robots, cars, and medical imaging. For this reason, we will devote considerable attention to the properties of the human visual system (<a href="part-human.html#sec-human-topics" class="quarto-xref"><span>Section 1</span></a>).</p>
<p>In summary, some image systems measure electromagnetic radiation because the signal is useful for movement or interacting with objects. Other image systems measure radiation to record a moment, enabling us to reproduce the signals at a later time. And finally, some image systems measure electromagnetic radiation at wavelengths we do not see, enabling us to explore domains that are beyond our senses <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. The first section of the book is devoted to understanding the signal.</p>
</section>
<section id="sec-light-em-spectrum" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="sec-light-em-spectrum"><span class="header-section-number">2.2</span> The electromagnetic spectrum</h2>
<p>One of the great achievements of science is the realization that seemingly different phenomena are deeply connected. By the early 1800s, scientists had uncovered links between electricity and magnetism: <span class="citation" data-cites="Øersted1820-electricity">Øersted (<a href="references.html#ref-Øersted1820-electricity" role="doc-biblioref">1820</a>)</span> showed that electric currents generate magnetic fields, and <span class="citation" data-cites="Faraday1832-electricity">Faraday (<a href="references.html#ref-Faraday1832-electricity" role="doc-biblioref">1832</a>)</span> demonstrated that changing magnetic fields induce electric currents.</p>
<p>James Clerk Maxwell unified these discoveries in a set of mathematical equations, simplified by Heaviside, and now known as <a href="https://en.wikipedia.org/wiki/Maxwell%27s_equations" target="_blank"><strong>Maxwell-Heaviside equations</strong></a>. He demonstrated that oscillating electric and magnetic fields behave as waves that propagate through space. Remarkably, the predicted speed of these waves matched the measured speed of light. Maxwell concluded that light itself is an electromagnetic wave, thereby uniting electricity, magnetism, and optics into a single theoretical framework: electromagnetic radiation spanning a range of wavelengths.</p>
<div class="callout callout-style-default callout-note callout-titled" title="About Maxwell">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
About Maxwell
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>We will encounter Maxwell’s work multiple times in this book. In addition to this remarkable unifying set of equations, we will see his insights about optics and his fundamental demonstrations of the properties of human color vision.</p>
<div id="fig-james-clerk-maxwell" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-james-clerk-maxwell-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/lightfields/01-lightfields/James_Clerk_Maxwell_big.jpg" class="img-fluid figure-img" style="width:50.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-james-clerk-maxwell-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.2: James Clerk Maxwell. Image from <a href="https://commons.wikimedia.org/wiki/File:James_Clerk_Maxwell_big.jpg">Wikimedia Commons</a>.
</figcaption>
</figure>
</div>
<p>Einstein kept images of three scientists in his office —Isaac Newton, James Clerk Maxwell, and Michael Faraday. These captured the lineage of thought that led to his own work. Newton’s laws of motion laid the foundation for classical mechanics. Maxwell unified electricity, magnetism, and light with his equations, building upon the experimental work of Faraday, who originated the concept of electric and magnetic “fields.” Einstein’s theories of relativity were a direct response to Maxwell’s equations, and he saw himself as a successor to this great tradition. I have (several) friends who remind me that <a href="http://bit.ly/46hRipe">two of these three scientists were Scottish</a></p>
<p>James Clerk Maxwell died in 1879 at the age of 48 from abdominal cancer. His mother, Frances Clerk Maxwell, also died from the same disease at the age of 48 when he was just eight years old.</p>
</div>
</div>
</div>
</section>
<section id="sec-light" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="sec-light"><span class="header-section-number">2.3</span> Light</h2>
<p>Not all electromagnetic wavelengths are equally useful for seeing. The human eye detects electromagnetic radiation within a specific range, approximately 380 nm to 770 nm. This range is referred to as <em>light</em>—electromagnetic radiation that is visible to the human eye. Radiation in this range is particularly useful for seeing because it is strongly absorbed and reflected by objects in the environment, making it ideal for detecting and interpreting the world around us.</p>
<p>In contrast, high-energy radiation, such as X-rays (0.01–10 nm) and gamma rays (&lt; 0.01 nm), tends to pass through most objects without significant interaction. On the other end of the spectrum, low-frequency radiation (&gt; 1500 nm) provides lower spatial resolution and is heavily influenced by thermal effects, limiting its utility for detailed imaging. The visible spectrum occupies a unique and advantageous position for vision and perception.</p>
</section>
<section id="sec-lightfields-types" class="level2 page-columns page-full" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="sec-lightfields-types"><span class="header-section-number">2.4</span> The environmental light field</h2>
<p>Electromagnetic radiation fills the environment, traveling in all directions and interacting with the objects that surround us. We call the environmental radiation within the visible wavelength band the <strong>environmental light field</strong>.</p>
<p>Leonardo Da Vinci’s notebook <span class="citation" data-cites="davinciNotebooksLeonardoVinci1970">(<a href="references.html#ref-davinciNotebooksLeonardoVinci1970" role="doc-biblioref">Da Vinci 1970</a>)</span> describes why he concluded that light fills the environment. He placed a small hole in a wall of a windowless room that was adjacent to a brightly illuminated piazza (<a href="#fig-ayscough" class="quarto-xref">Figure&nbsp;<span>2.3</span></a>). This produced an (inverted) image of the piazza on the wall within the room. Leonardo observed that one can place the pinhole anywhere in the wall and an image of the same objects is produced. He concluded that the light field is “all everywhere and all in each part”<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</p>
<div id="fig-ayscough" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full" data-fig-align="center" alt="James Ayscough's drawing of Da Vinci's concept.">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-ayscough-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/lightfields/01-lightfields/Ayscough-1755-small-wikipedia.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%" alt="James Ayscough's drawing of Da Vinci's concept.">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig margin-caption" id="fig-ayscough-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.3: <span class="citation" data-cites="ayscoughShortAccountEye1755">Ayscough (<a href="references.html#ref-ayscoughShortAccountEye1755" role="doc-biblioref">1755</a>)</span> (“A short account of the eye and nature of vision. Chiefly … to illustrate the use and advantage of spectacles”) opens with this illustration of Da Vinci’s pinhole camera. I superimposed the colored lines to indicate the courtyard light rays, emitted in all directions. The solid lines are selected by the pinhole and form the image in the dark room. The full collection of rays, solid and dotted, represents the <em>environment light field</em>. <a href="https://en.wikipedia.org/wiki/Camera_obscura#/media/File:1755_james_ayscough.jpg">Source: Wikipedia Camera Obscura</a>.
</figcaption>
</figure>
</div>
<p>For the moment, it is convenient to consider the light field as comprising a large set of rays. We can describe the light field in the environment, <span class="math inline">\(L_E\)</span>, by the intensity of each ray, expressing the intensity as a function of its various parameters. Rays cross through each point in the volume of space, <span class="math inline">\((x,y,z)\)</span>, in many directions. We specify these directions by two direction angles <span class="math inline">\((\alpha, \beta)\)</span>, the azimuth and elevation. Each ray has a wavelength <span class="math inline">\(\lambda\)</span> and a polarization <span class="math inline">\(\rho\)</span>. The ray intensities are described by a function of seven parameters:</p>
<p><span id="eq-lightfield-environment"><span class="math display">\[
L_E(x,y,z,\alpha,\beta,\lambda,\rho)
\tag{2.1}\]</span></span></p>
<p>People in many fields are familiar with the concept of the light field, but they use different terminology. Physicists often describe the environmental light field as the spectral radiance of the environment (see this <a href="https://youtu.be/FjHJ7FmV0M4?si=wi2twZVYjF9KJGRY" target="_blank">lovely description from Feynman</a>). The <em>light field</em> terminology was introduced into optical engineering by <span class="citation" data-cites="gershunLightField1939">Gershun (<a href="references.html#ref-gershunLightField1939" role="doc-biblioref">1939</a>)</span> as part of his work in understanding how to design lighting environments, such as the lighting in school rooms and public places<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>. Ted Adelson and Jim Bergen <span class="citation" data-cites="adelson-plenoptic-1991">(<a href="references.html#ref-adelson-plenoptic-1991" role="doc-biblioref">1991</a>)</span> used the term <em>plenoptic function</em> to describe the same idea<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>. In computer graphics, the value of the light field concept was explained by Marc Levoy and Pat Hanrahan <span class="citation" data-cites="Levoy-1996-light-field">(<a href="references.html#ref-Levoy-1996-light-field" role="doc-biblioref">1996</a>)</span>. I use the light field terminology because I find it less intimidating than spectral radiance, electromagnetic radiation, or plenoptic.</p>
</section>
<section id="sec-incident-lightfield" class="level2 page-columns page-full" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="sec-incident-lightfield"><span class="header-section-number">2.5</span> The incident light field</h2>
<p>An imaging system—whether an eye, a camera, or a simple pinhole—can only capture the portion of the environmental light field that reaches its aperture, or <em>entrance pupil</em> (<a href="#fig-incident-lightfield" class="quarto-xref">Figure&nbsp;<span>2.4</span></a>). We call this subset of rays the <strong>incident light field</strong>.</p>
<div id="fig-incident-lightfield" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-incident-lightfield-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/lightfields/01-lightfields/01-incident-lightfield.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig margin-caption" id="fig-incident-lightfield-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.4: The incident light field is the subset of environmental light field rays that arrive at the imaging system’s entrance pupil.
</figcaption>
</figure>
</div>
<p>Because the incident light field is defined on the two-dimensional surface of the entrance pupil, we can describe it with fewer parameters than the full environmental light field. Instead of a 3D position in space <span class="math inline">\((x,y,z)\)</span>, we use a 2D position <span class="math inline">\((u,v)\)</span> on the pupil plane. The complete description of the imaging system also requires its position in space, <span class="math inline">\(\mathbf{p}\)</span>, and its viewing direction, <span class="math inline">\(\mathbf{n}\)</span>. We treat these as fixed conditions for a given measurement.</p>
<p>The incident light field, <span class="math inline">\(L_I\)</span>, can then be expressed as a function of six variables, plus the system’s position and orientation:</p>
<p><span id="eq-lightfield-incident"><span class="math display">\[
L_I(u,v,\alpha,\beta,\lambda,\rho; ~\mathbf{p},\mathbf{n})
\tag{2.2}\]</span></span></p>
<p>To capture the full environmental light field, we must follow Leonardo’s insight and measure the incident light field from multiple positions and directions. Many animals have two eyes, providing two simultaneous measurements. This redundancy not only offers a backup if one eye is damaged but also provides more information about the scene. Prey animals, like rabbits, often have eyes pointing in different directions to achieve a wide field of view. Predators, on the other hand, typically have forward-facing eyes with overlapping fields, which enables stereoscopic vision for depth perception. We also gather more information about the light field over time as we move our heads and our eyes rotate (motion parallax).</p>
<p>Engineers have built camera arrays that capture many incident light fields simultaneously. By measuring from multiple positions and directions, these systems can create immersive displays that reproduce the experience of moving and looking around within a scene. If the scene is static, a single moving camera can achieve the same result.</p>
<p>A large camera array built at the Stanford Graphics Lab illustrates this principle , <a href="#fig-camera-array" class="quarto-xref">Figure&nbsp;<span>2.5</span></a>. The array shown at the top of the figure captures views from many positions, all pointing in the same direction. This setup allows a viewer to experience moving side-to-side or forward and backward. The arrays at the bottom, with cameras pointing outward from a central point, capture the light field from many directions. This allows a viewer to experience looking around from a fixed position. By combining these approaches, systems can capture enough data to simulate both movement and changes in gaze direction <span class="citation" data-cites="thatte-2017-6dof">(<a href="references.html#ref-thatte-2017-6dof" role="doc-biblioref">Thatte et al. 2017</a>)</span>. These camera arrays capture a coarse but powerful sample of the environmental light field, enabling the rendering of novel viewpoints through interpolation.</p>
<div id="fig-camera-array" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-camera-array-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/lightfields/01-lightfields/01-camera-array.png" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig margin-caption" id="fig-camera-array-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.5: The Stanford Multi-Camera Array (top) used 128 cameras to sample the light field from many positions, enabling the recreation of motion parallax. Other array configurations (bottom) capture the light field from many directions at a single point, enabling a viewer to look around.
</figcaption>
</figure>
</div>
</section>
<section id="sec-optical-lightfield" class="level2 page-columns page-full" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="sec-optical-lightfield"><span class="header-section-number">2.6</span> Optical light field</h2>
<p>The optics of an imaging system—whether a pinhole, a simple lens, or a complex multi-element lens—transform the incident light field into a new light field inside the camera. This <em>optical light field</em> exists in the space between the exit pupil of the optics and the sensor. The journey of light begins in the environment, is narrowed to the incident light field at the entrance pupil, and is then transformed by the optics into the optical light field. The sensor, whether a digital chip or the retina, measures this optical light field<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>.</p>
<div id="fig-optical-lightfield" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-optical-lightfield-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/lightfields/01-lightfields/01-optical-lightfield.png" class="img-fluid figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig margin-caption" id="fig-optical-lightfield-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.6: The rays emerging from the exit pupil of the optics form the optical light field, <span class="math inline">\(L_O\)</span>. The sensor captures a portion of these rays.
</figcaption>
</figure>
</div>
<p>While the environmental and incident light fields are naturally described by a ray’s position and angle, the optical light field is often described differently. Because we are interested in the rays that travel from the optics to the sensor, it is convenient to parameterize them by their start and end points. We can define a ray by its position <span class="math inline">\((u,v)\)</span> on the exit pupil plane and its destination <span class="math inline">\((r,c)\)</span> on the sensor plane. This is often called the two-plane parameterization.</p>
<p>Using this convention, we can describe the optical light field as a function of these four spatial parameters, along with wavelength and polarization:</p>
<p><span id="eq-lightfield-optical"><span class="math display">\[
L_O(u,v,r,c,\lambda,\rho)
\tag{2.3}\]</span></span></p>
<p>This parameterization is particularly useful because it directly relates to what a light field sensor measures. Of course, it is possible to convert between this two-plane representation and the position-angle representation used for the other light fields.</p>
<p>The data recorded by the sensor is all we have to create an image or interpret the scene. The ability of any system to derive information from the light field is ultimately limited by what portion of the environmental light field passes through the optics to become the optical light field and is finally captured by the sensor.</p>
</section>
<section id="sec-lightfield-measurement" class="level2" data-number="2.7">
<h2 data-number="2.7" class="anchored" data-anchor-id="sec-lightfield-measurement"><span class="header-section-number">2.7</span> Measuring the light field</h2>
<p>How do we measure a light field? A pinhole camera offers a simple conceptual answer. An image from a pinhole camera is a measurement of the incident light field at the pinhole’s location. The intensity at each image point corresponds to the intensity of a small bundle of rays arriving from a single direction. To learn more about the environmental light field, we can follow Leonardo’s lead and move the pinhole to different locations, accumulating information with each measurement.</p>
<p>The camera arrays in <a href="#fig-camera-array" class="quarto-xref">Figure&nbsp;<span>2.5</span></a> are a practical implementation of this idea. They sample the environmental light field by measuring the incident light field at multiple camera positions. However, these cameras use lenses, not pinholes. A lens gathers light from across its entire aperture, and a conventional sensor integrates all of this light. For any single point on the sensor, the rays in the optical light field arrive from many positions within the lens’s exit pupil. A conventional sensor sums the energy from all these rays, losing the information about where each ray originated in the pupil. Consequently, a conventional camera measures an integrated version of the light field, not the full angular distribution of light at each point.</p>
<p>While a conventional camera array does not capture the complete light field, it captures enough information for its intended purpose: rendering a scene from various viewpoints by interpolating between the captured images. The light field concept is a powerful theoretical guide for the design and interpretation of such systems, even when the full light field is not measured.</p>
<p>In many scientific and clinical applications, a more precise measurement of the light field is essential. A key application is characterizing how an optical system transforms an incident light field into an optical light field (<a href="#eq-lightfield-optical" class="quarto-xref">Equation&nbsp;<span>2.3</span></a>). For example, astronomers measure the light from a distant star to understand how parallel rays are distorted by the atmosphere and their telescope optics. They use this information to build adaptive optics systems that correct for these distortions, ensuring that rays from a point source converge to a sharp point in the final image.</p>
<p>A second example is measuring the optics of the human eye. Imperfections in a person’s cornea and lens can be corrected with spectacles or laser surgery. To design these corrections, we must first measure how the eye’s optics transform incoming light rays. These measurements are central to the fields of ophthalmology and vision science, as we will explore further in <a href="human-02-encoding.html" class="quarto-xref"><span>Chapter 22</span></a>.</p>
<p>A sensor design that directly captures the optical light field was invented by <span class="citation" data-cites="adelson-wang-1992">Adelson and Wang (<a href="references.html#ref-adelson-wang-1992" role="doc-biblioref">1992</a>)</span>. They placed a lenslet array in front of a standard camera sensor. Each lenslet covers a small group of pixels. As <a href="#fig-lightfield-pixel" class="quarto-xref">Figure&nbsp;<span>2.7</span></a> illustrates—using a pinhole array rather than lenslet for simplicity—rays arriving from different angles are captured by different pixels behind the pinhole. This sensor records both the position of a ray (which lenslet or pinhole) and its angle (which subpixel). <span class="citation" data-cites="adelson-wang-1992">Adelson and Wang (<a href="references.html#ref-adelson-wang-1992" role="doc-biblioref">1992</a>)</span> describe how to use this information to estimate depth. The ideas was further developed by Ren Ng and his colleagues. Ng formed company, Lytro, that commercialized light field cameras <span class="citation" data-cites="ng2005-lightfield">(<a href="references.html#ref-ng2005-lightfield" role="doc-biblioref">Ng et al. 2005</a>)</span>. These cameras allow users to perform novel operations, such as refocusing an image after it has been captured (<a href="sensors-07-innovations.html#sec-sensor-lightfield" class="quarto-xref"><span>Section 20.3</span></a>).</p>
<div id="fig-lightfield-pixel" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lightfield-pixel-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/lightfields/01-lightfields/Gemini_lightfieldPixel-Circle.png" class="img-fluid figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lightfield-pixel-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.7: A light field sensor captures both the position and angle of incoming light rays. The sensor is composed of an array of “macropixels” (bottom). Each macropixel consists of a microlens (or a pinhole, as simplified here) and a group of pixels beneath it. The microlens or pinhole location on the sensor determines the ray’s position. The subpixels are below the pinhole. Which of these detects the light determines the ray’s angle. By capturing the position and angle data across the entire sensor, a light field camera can perform powerful computations, such as refocusing an image after it has been taken. (Image by me and Gemini 2.5 Pro.)
</figcaption>
</figure>
</div>
<p>The principles of the light field camera continue to be used in modern sensors. While full depth estimation is not common in consumer devices, many cameras use a simplified version of this technology for autofocus (<a href="sensors-05-control.html#sec-focus-control" class="quarto-xref"><span>Section 18.2</span></a>). We will return to light field cameras in <a href="sensors-07-innovations.html#sec-sensor-lightfield" class="quarto-xref"><span>Section 20.3</span></a> and discuss the parallel developments of Shack-Hartmann wavefront sensing in <a href="human-02-encoding.html#sec-wavefront-sensor" class="quarto-xref"><span>Section 22.4.3</span></a>.</p>
</section>
<section id="interpreting-the-light-field" class="level2" data-number="2.8">
<h2 data-number="2.8" class="anchored" data-anchor-id="interpreting-the-light-field"><span class="header-section-number">2.8</span> Interpreting the light field</h2>
<p>Encoding the light field data is the first step in seeing. The instrumentation for encoding is a critical bottleneck because it limits what we can infer about the scene. For example, if an instrument does not record information about certain wavelengths, we can only guess at the missing information, for instance, by choosing the most likely value based on context.</p>
<p>Knowledge of the light field encoding is always connected to the next step in an image system: interpreting the measurement. The invention of algorithms that interpret the light field—to recognize objects or measure distances—is central to building computers that see. Interpreting the light field is also important for consumer photography, where we may wish to distinguish a surface marking from a shadow, or the color of an object from the color of the ambient lighting. In vision science, a primary goal is to discover how the brain interprets the signal encoded by the retina.</p>
<p>We will review many algorithms for interpreting images, but one overarching principle has endured for centuries. The principle is that image systems, whether artificial or biological, should use the encoded signal to build an internal model of the scene. The great 19th-century scientist Hermann von Helmholtz described it this way:</p>
<blockquote class="blockquote">
<p>The general rule determining the ideas of vision that are formed whenever an impression is made on the eye, is that <em>such objects are always imagined as being present in the field of vision as would have to be there in order to produce the same impression on the nervous mechanism</em>. [Italics in the original; from <em>Treatise on Physiological Optics</em>, Vol. III, 1867]</p>
</blockquote>
<p>Understanding the light field—how it interacts with objects, how it is measured by instruments, and how it is interpreted—is fundamental to achieving that goal. We are still working on the project assigned to us by Helmholtz.</p>



<div id="refs" class="references csl-bib-body hanging-indent" role="list" style="display: none">
<div id="ref-adelson-plenoptic-1991" class="csl-entry" role="listitem">
Adelson EH, Bergen JR (1991) The plenoptic function and the elements of early vision. In: Landy MS, Movshon JA (eds) Computational <span>Models</span> of <span>Visual Processing</span>. MIT Press, Cambridge, pp 3–20
</div>
<div id="ref-adelson-wang-1992" class="csl-entry" role="listitem">
Adelson EH, Wang JYA (1992) Single lens stereo with a plenoptic camera. IEEE Transactions on Pattern Analysis and Machine Intelligence 14:99–106
</div>
<div id="ref-ayscoughShortAccountEye1755" class="csl-entry" role="listitem">
Ayscough J (1755) <span>A short account of the eye and nature of vision. Chiefly designed to illustrate the use and advantage of spectacles. ... By James Ayscough, optician. ... - The fourth edition. 1755</span>
</div>
<div id="ref-davinciNotebooksLeonardoVinci1970" class="csl-entry" role="listitem">
Da Vinci L (1970) The <span>Notebooks</span> of <span>Leonardo Da Vinci</span>. Dover, New York
</div>
<div id="ref-Faraday1832-electricity" class="csl-entry" role="listitem">
Faraday M (1832) <a href="http://dx.doi.org/10.1098/rstl.1832.0006"><span>V</span>. Experimental researches in electricity</a>. Philos Trans R Soc Lond 122:125–162
</div>
<div id="ref-gershunLightField1939" class="csl-entry" role="listitem">
Gershun A (1939) The <span>Light Field</span>. Journal of Mathematical Physics 18:51–151
</div>
<div id="ref-Levoy-1996-light-field" class="csl-entry" role="listitem">
Levoy M, Hanrahan P (1996) <a href="https://doi.org/10.1145/237170.237199">Light field rendering</a>. Association for Computing Machinery, New York, NY, USA, pp 31–42
</div>
<div id="ref-ng2005-lightfield" class="csl-entry" role="listitem">
Ng R, Levoy M, Brédif M, et al (2005) <a href="'https://hal.science/hal-02551481'">Light field photography with a hand-held plenoptic camera</a>. PhD thesis, Stanford university
</div>
<div id="ref-Øersted1820-electricity" class="csl-entry" role="listitem">
Øersted HC (1820) Experiments on the effect of a current of electricity on the magnetic needle. Annals of Philosophy
</div>
<div id="ref-thatte-2017-6dof" class="csl-entry" role="listitem">
Thatte J, Lian T, Wandell B, Girod B (2017) <a href="http://dx.doi.org/10.1109/vcip.2017.8305085">Stacked omnistereo for virtual reality with six degrees of freedom</a>. In: 2017 IEEE visual communications and image processing (VCIP). IEEE
</div>
<div id="ref-Wandell2021-Principles-Ashby" class="csl-entry" role="listitem">
Wandell BA, Brainard D (2021) Principles and consequences of the initial visual encoding. In: F. Gregory Ashby HC, Dzhafarov EN (eds) New handbook of mathematical psychology
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>There are instruments that use different wavebands of the electromagnetic spectrum. <a href="resources/lightfields-wavebands.html" target="_blank">A brief introduction is here.</a>. More examples of instruments will be presented in later chapters.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p><strong>PROVE HOW ALL OBJECTS, PLACED IN ONE POSITION, ARE ALL EVERYWHERE AND ALL IN EACH PART</strong> <br> “I say that if the front of a building—or any open piazza or field—which is illuminated by the sun has a dwelling opposite to it, and if, in the front which does not face the sun, you make a small round hole, all the illuminated objects will project their images through that hole and be visible inside the dwelling on the opposite wall which may be made white; and there, in fact, they will be upside down, and if you make similar openings in several places in the same wall you will have the same result from each. Hence the images of the illuminated objects are all everywhere on this wall and all in each minutest part of it. The reason, as we clearly know, is that this hole must admit some light to the said dwelling, and the light admitted by it is derived from one or many luminous bodies. If these bodies are of various colours and shapes the rays forming the images are of various colours and shapes, and so will the representations be on the wall. (Leonardo’s 1509 Notebook, curated by John Paul Richter)”<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>I enjoyed the first sentence in the 1939 translation of Gershun’s <span class="citation" data-cites="gershunLightField1939">(<a href="references.html#ref-gershunLightField1939" role="doc-biblioref">1939</a>)</span> paper. The translators, Moon and Timoshenko, express frustration with the pace of advances in photometry. “Theoretical photometry constitutes a case of ‘arrested development’, and has remained basically unchanged since 1760 while the rest of physics has swept triumphantly ahead.” Gershun himself expresses the same sentiment: “The problems of theoretical photometry were pushed aside from the main path of the development of physics.”<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p><span class="citation" data-cites="adelson-plenoptic-1991">Adelson and Bergen (<a href="references.html#ref-adelson-plenoptic-1991" role="doc-biblioref">1991</a>)</span> introduced the plenoptic function for a pinhole camera and a sensor, making it closely related to the incident light field. (<a href="#sec-incident-lightfield" class="quarto-xref"><span>Section 2.5</span></a>). Using a pinhole camera model, the two parameters describing the aperture position are not needed. With these restrictions the plenoptic function becomes equivalent to the spectral irradiance at the image sensor <span class="citation" data-cites="Wandell2021-Principles-Ashby">(<a href="references.html#ref-Wandell2021-Principles-Ashby" role="doc-biblioref">Wandell and Brainard 2021</a>)</span>. The authors clearly had the idea of the environmental and incident light fields in mind.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Ren Ng used the phrase ‘in-camera light field’ in his Ph.D.&nbsp;thesis <span class="citation" data-cites="ng2005-lightfield">(<a href="references.html#ref-ng2005-lightfield" role="doc-biblioref">Ng et al. 2005</a>)</span>. I like that expression, too.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../chapters/part-scenes.html" class="pagination-link" aria-label="Part I:  Scenes">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Part I: Scenes</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/lightfields-02-measurement.html" class="pagination-link" aria-label="Measuring light">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Measuring light</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>