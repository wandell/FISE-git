# Pixels and sensors {#sec-pixels}
Having established the fundamental physics of light-to-electron conversion, we now examine how these principles are implemented in the design of image sensor pixels. To understand how the photoelectric effect is harnessed in practical devices, we next review the behavior of electrons in semiconductors—the foundation of modern image sensor pixels.

## Semiconductors and electrons {#sec-semiconductor-physics}
Atoms in a silicon semiconductor are arranged in a crystalline structure, and their electrons occupy specific energy levels grouped into two main bands: the **valence band** (lower energy) and the **conduction band** (higher energy). Under normal conditions, most electrons remain in the valence band. When a silicon atom absorbs a photon, an electron can be excited from the valence band to the conduction band (@fig-sensor-bandgap).

<!--
A particularly clear explanation with figures and a video.
https://scientificimaging.com/knowledge-base/photoelectric-effect/ 
-->

### Bandgap {#sec-bandgap-photon-absorption}
 The minimum energy required for this transition is called the **bandgap**, and it is measured in electron volts (eV). For silicon at room temperature ($21 \deg C$), the bandgap is 1.12 eV. Only photons with energy equal to or greater than this value can generate free electrons in silicon.

![Energy bands in silicon semiconductors. A photon can excite an electron in the Valence band.  If the photon has enough energy, the electron can shift to the conduction band, becoming a free electron that can be stored and counted. ](images/sensors/03-sensor-bandgap.png){#fig-sensor-bandgap width=50%}

Without special circuitry, the hole and electron will re-combine. To make an image sensor, we need a method to measure the number of electrons. In a CCD an electric field between the bands prevents recombination. In CMOS electrons are trapped in capacitors placed in the silicon. We explain how this is done in @sec-pixels.

### Wavelength effects in silicon sensors {#sec-wavelength-effects}
There are several wavelength-dependent effects worth noting about silicon sensors. First, we can calculate the energy in Joules of a photon of wavelength $\lambda$ using this the photon energy formula:

We can convert from Joules (J) to electron volts (eV) with this formula

$$
1~\text{eV} = {1.602 \times 10^{-19}}~\text{J} 
$$ {#eq-joules-ev}

A wavelength of 1100 nm has just enough energy (1.12 eV) to transition an electron across the bandgap: Thus, silicon sensors are insensitive to electromagnetic radiation beyond 1100 nm and infrared sensors must use a different material. Second, photons with wavelengths less than 1100 nm have enough energy to excite electrons across the bandgap, and silicon will absorb radiation at these shorter wavelengths.  It is notable that the eye does not encode wavelengths below about 400 nm. 

Third, the chance that a photon excites an electron that crosses the bandgrap depends on the photon's energy level. Higher energy (shorter wavelength) photons are more likely to cause the electron to change bands. For this reason, different wavelengths are absorbed at different spatial depths in the material. Short-wavelength photons are much more likely to generate electrons near the surface compared to long-wavelength photons.  Conversely, long-wavelength photons are relatively more likely to generate electrons deeper in the material. This depth-dependence on wavelength has been used to create color image sensors (@sec-sensor-foveon).

## CMOS Pixel: Front-side illuminated {#sec-cmos-pixel-fsi}
CMOS image sensors use photodiodes—and the photoelectric effect—to measure and store the optical light field at the sensor surface. The key technical breakthrough that enabled CMOS sensors was the invention of circuitry to store and then transmit the photon-generated electrons from the photodiode to the computer.

The photodiode and this circuitry are critical parts of the pixel; there are other essential components as well (@fig-pixel-overview). For example, effective image sensors place a small lens (microlens) above each pixel.  Color image sensors place a color filter between the microlens and the photodiode that passes certain wavelengths and blocks others. 

![Schematic of the early CMOS pixel architecture.  This image shows a microlens and color filter, which are typically part of the pixel. The microlens is centered over the pixel here, but for pixels at the edge of the sensor array it will be positioned a bit to the side. This color filter passes long wavelength (red-appearing) light, but different pixels will have different color filters. The first layer of silicon, with its photodiode and classic circuitry sharing space, are shown. Source: <a href="https://evidentscientific.com/en/microscope-resource/knowledge-hub/digital-imaging/cmosimagesensors" target="_blank">Evident Scientific.</a>](images/sensors/03-pixel-overview.png){#fig-pixel-overview width="60%"}

 Figure @fig-pixel-overview is a schematic that omits many details. For example, notice the gap between the photodiode and the color filter. In early CMOS imagers, this gap contained many metal lines that carried control and data signals. The metal lines were organized into layers and they are illustrated in  Figure @fig-pixel-layers. In the first generation of image sensors the distance from the microlens at the top to the photodiode at the bottom was fairly large compared to the size of the photodiode. In these early pixels, light from the main lens had to travel through what was effectively a tunnel to reach the photodiode.

![Color imagers use an array of color filters to create pixels with different wavelength selectivity. The color filters are typically arranged in a repeating pattern, and the smallest repeating unit of the color filter array is called a **super pixel**. The image illustrates a super pixel for a sensor with cyan, magenta, and yellow color filters. This image emphasizes the metal lines, omitted in @fig-pixel-overview. In the original CMOS imagers, light from the main imaging lens had to pass through these metal layers before reaching the photodiode.](images/sensors/03-pixel-overview2.png){#fig-pixel-layers width="50%}

<!-- fig-pixel-layers:  not sure where this image comes from -->

Pixel architecture has evolved significantly over the years, and there continue to be many novel designs. The most straightforward change is that technology has scaled, enabling pixel sizes to shrink and thus increasing spatial sampling resolution. In addition, the placement of the circuitry and metal lines has changed to below the photodiode; light no longer passes through a deep tunnel. There are other improvements to the circuitry, some experimental, that we will review later in @sec-sensor-innovations. It is helpful to first understand the concepts behind the classic circuitry, so that we can better appreciate these innovations.

The logical flow of the circuitry is as follows:

-   Before image capture, the sensor circuitry resets each pixel, clearing residual charge from previous exposures.
-   During image acquisition, the photodiode array collects electrons generated by incoming light; these electrons are stored within the photodiode (3T) or in a nearby capacitor (4T).
-   During readout, the circuitry transfers the stored charge to an analog-to-digital converter (ADC).
-   The digital image array records the amount of light captured by each pixel.

The role of the other essential pixel components, the color filter arrays and microlenses, is explained in Section @sec-sensors.  In @sec-sensors-characterization I describes how to calibrate and model sensor performance, from the scene light field to image capture.

## CMOS circuits {#sec-sensor-circuits}

### Three-Transistor (3T) Pixel Design {#sec-3T-pixel-design}

![The original CMOS 3-transistor (3T) circuit design (@Fossum1997-early).](images/sensors/03-3T-circuit.png){#fig-sensor-3T-circuit width="60%"}

The original CMOS pixel design uses a three-transistor (3T) circuit to store and read out the charge collected by the photodiode (@fig-sensor-3T-circuit). Each pixel contains three key transistors: $M_{rst}$ (reset), $M_{sel}$ (select), and $M_{sf}$ (source follower). The $M_{rst}$ transistor resets the photodiode by connecting it to the supply voltage (Vdd), clearing any residual charge before image capture. The $M_{sel}$ transistor selects a specific row of pixels, connecting them to the readout circuitry. The $M_{sf}$ transistor acts as a buffer, transferring the stored charge to the analog-to-digital converter (ADC) for measurement. In most sensors, each column has its own ADC, but some designs use a single ADC shared among multiple columns—a technique known as multiplexing (mux).

Ideally, the photodiode’s response to light is linear, with the number of generated electrons following Poisson statistics. However, the surrounding circuitry introduces additional sources of noise and nonlinearity. For example, the storage capacitor has a limited capacity, so it can saturate at high light levels. The transistors used for readout can also introduce noise and small nonlinearities. Furthermore, variations in pixel properties across the sensor array can cause fixed-pattern noise. In modern CMOS sensors, these nonlinearities and noise sources are typically small—on the order of a few percent [@Wang2017-CMOSLinearity] -but they can still affect image quality. Careful characterization and calibration are necessary to minimize these effects and produce high-quality images.

```{=html}
<!-- 
 (http://encyclopedia2.thefreedictionary.com/Vdd also the drain voltage), Vrst is the reset voltage Vee is the negative supply voltage Ground is ground
http://en.wikipedia.org/wiki/Active_pixel_sensor , http://encyclobeamia.solarbotics.net/articles/vxx.html , 
http://en.wikipedia.org/wiki/Active_pixel_sensor , http://encyclobeamia.solarbotics.net/articles/vxx.html , 
Vdd is positive supply voltage  (http://encyclopedia2.thefreedictionary.com/Vdd also the drain voltage), Vrst is the reset voltage Vee is the negative supply voltage Ground is ground
-->
```

### Four-Transistor (4T) pixel design {#sec-4T-pixel-design}

![The 4T pixel circuit introduces a pinned photodiode and an additional transistor. When activated, the new transistor transfers the accumulated charge from the photodiode to a storage capacitor (the floating diffusion node, $C_{fd}$). The rest of the circuit, including readout and row select, is similar to the 3T design.](images/sensors/03-4T-circuit.png){#fig-sensor-4T-circuit width="60%"}

Since about 2000, most CMOS image sensors have used a four-transistor (4T) pixel circuit [@Fossum2014-pinnedpd]. The 4T pixel adds two important features: a pinned photodiode (PPD) and a transfer gate transistor (@fig-sensor-4T-circuit). The pinned photodiode improves charge storage and reduces noise, while the transfer gate allows precise movement of the collected charge from the photodiode to a storage node called the floating diffusion, which has a capacitance $C_{fd}$. The other circuit elements—reset, row select, and source follower—remain the same as in the 3T design.

The transfer gate and floating diffusion give the circuit better control over when and how charge is transferred and measured, which helps reduce noise and improve image quality.

Today, the 4T pixel is the standard in almost all high-performance CMOS image sensors, including those found in smartphones, industrial cameras, cars, and scientific instruments.

<!--
There is also Theuwissen diagram, with five transistors that he calls a 4T circuit.  In the paper 'Linearity analysis ...' @Wang2017-CMOSLinearity.  He says the source follower is shared by multiple rows of pixels. It is Figure 2.  

The text: "Figure 2 shows the schematic of a typical voltage mode 4T pixel. The pixel circuit consists of a pinned photodiode, a charge
transfer switch (M1), a reset switch (M2), a source follower (M3) and a row select switch (M4). The current source transistor (M5) is shared by multiple rows of pixels. VDD is the power supply while VPIX is the output voltage of pixel. VLN provides an adjustable bias current. In the following analysis, the nonlinearity caused by the row select transistor (M4) is omitted to simplify the analysis."
-->
## Sensor evolution {#sec-sensor-evolution}

There has been a large engineering effort to reduce various sources of noise, including temporal and fixed pattern noise, as well as other limitations in the sensor design relating to color and dynamic range. The engineering effort was supported by the enormous demand for CMOS sensors. In 2020, approximately 6.5 billion sensors were shipped.  In 2025, we expect about 13 billion sensors to be produced. That's a lot of sensors and the demand motivates a lot of engineering. This section covers the evolution of the pixel and sensor design.

The sensor architecture I have described is known as **front-side illumination (FSI)**. The architecture has the obvious challenge of placing multiple metal layers above the semiconductor substrate (@fig-pixel-layers). Even at the time of the original implementation, it was clear that requiring light to pass through the metal layers to reach the light-sensitive photodiode created many problems. 

Additionally, both the photodiodes and their associated circuitry share the same substrate. Requiring the circuitry and photodiode to share space reduced the sensitivity the sensor. Finally, the original pixels were fairly large, $6 \mu \text{m}$ or more. As technology scaled and pixels became smaller -many CMOS imagers have pixels as small as $0.8 \mu \text{m}$ a reduction in area of $50x$- additional problems arose. Different opportunities arose as well.

These challenges, and the huge success of CMOS sensors, were met by a massive, world-wide, engineering effort to improve the system. The evolution of image sensor pixel technology that came from this effort enabled dramatic improvements in image quality, sensitivity, and device functionality. The following sections describe the key milestones that were achieved through academic and industry partnerships. For an authoritative review, consult @Oike2022-stackedevolution.

<!--# https://www.pcmag.com/how-to/whats-the-difference-between-cmos-bsi-cmos-and-stacked-cmos 
https://news.skhynix.com/evolution-of-pixel-technology-in-cmos-image-sensor
-->

:::{.callout-note title="Pixel Evolution timeline" collapse="true"}
<!-- See Research doc 'The Genesis and Evolution of Stacked Pixel Structures ....' -->

Here is a timeline for the key milestones described in the sections below.

| Technology                | Commercialization | Key Benefit                                 |
|---------------------------|------------------|----------------------------------------------|
| **Front-Side Illumination (FSI)**   | 1990s–2000s      | Simplicity, but limited by wiring obstruction|
| **Back-Side Illumination (BSI)**    | ~2007–2010       | Higher QE, better low-light, smaller pixels  |
| **Deep Trench Isolation (DTI)**     | ~2010s           | Reduced crosstalk, higher pixel density      |
| **Deep Photodiode**                 | ~2015+           | Higher sensitivity, dynamic range            |
| **Stacked Sensor**                  | ~2015+           | Advanced features, compact design            |

<!--
- **Front-Side Illumination (FSI):** Early standard; light passes through wiring before reaching the photodiode.
- **Back-Side Illumination (BSI):** Introduced ~2010; light enters from the back, improving efficiency for small pixels.
- **Deep Trench Isolation (DTI):** Early 2010s; insulating trenches reduce crosstalk between pixels.
- **Deep Photodiode:** Mid-2010s; deeper photodiodes increase sensitivity and dynamic range.
- **Stacked Sensor:** Mid-2010s onward; sensor and circuitry are stacked for advanced features and compact design.


- **Front-Side Illumination (FSI) (1970s–2000s)**
    - Standard architecture for early CCD and CMOS sensors.
    - Light enters from the front, passing through metal wiring and transistors before reaching the photodiode.
    - Wiring blocks some light, reducing quantum efficiency, especially as pixel sizes shrink.

- **Back-Side Illumination (BSI) (Late 2000s, commercialization ~2007–2010)**
    - Sensor wafer is thinned and flipped so light enters from the back, directly reaching the photodiode.
    - Dramatically improves quantum efficiency, especially for small pixels.
    - First widely adopted in smartphone cameras (e.g., iPhone 4 in 2010).

- **Deep Trench Isolation (DTI) (Early 2010s)**
    - Introduced to reduce electrical and optical crosstalk between pixels.
    - Trenches filled with insulating material are etched between pixels, confining photo-generated electrons.
    - Enables higher pixel density and better color fidelity.

- **Deep Photodiode (Mid 2010s)**
    - Photodiodes are engineered to extend deeper into the silicon substrate.
    - Increases the volume for photon absorption, improving sensitivity and full-well capacity.
    - Particularly beneficial for small pixels and high dynamic range imaging.

- **Stacked Sensor (3D Stacking) (Mid–Late 2010s)**
    - Sensor and circuitry are fabricated on separate silicon layers and then stacked (bonded) together.
    - Allows for more complex circuitry (e.g., faster readout, on-chip memory, AI processing) without sacrificing pixel area.
    - Now common in high-end smartphone and professional sensors.
  
  -->
:::

## Back-side illuminated (BSI) {#sec-sensor-bsi}
After years of incremental improvements, engineers developed a new architecture to overcome the limitations of front-side illuminated (FSI) sensors. The resulting design, known as **Back-Side illuminated (BSI)** CMOS, rearranges the device layers to improve sensitivity. In BSI sensors, the photodiode and wiring layers are flipped; the light no longer needs to pass through the wires.  

![The back-side illuminated pixel. (a) The front-side illuminated pixel has metal lines above the silicon substrate. (b) The back side illuminated sensor flips the metal lines and silicon substrate.  Source: <a href="https://scientificimaging.com/knowledge-base/front-side-illuminated-and-back-side-illuminated-imagers" target="_blank">Scientific Imaging.</a>](images/sensors/03-sensor-bsi2.png){#fig-sensor-bsi2 width="60%"}

In addition to flipping the order of the silicon and wires, it was necessary to change the thickness of the substrate (@fig-sensor-bsi2). In the FSI pixel, the photodiode is embedded in a relatively thick layer of silicon. Flipping this structure would require the light to pass through a substantial amount of silicon prior to reaching the photodiode. This is problematic because the silicon absorbs light, and particularly short-wavelength light (@sec-sensor-wavelength). Thus, it was necessary to find a way to thin the silicon substrate and still have a working photodiode and circuitry. 

::: {#sensor-bsi .callout-note collapse="true" title="BSI technology development"}

The concept of back-side illuminated (BSI) CMOS image sensors was first proposed by Eric Fossum in 1994 to address the limitations caused by metal wiring obstructing incoming light and to improve sensitivity. While the benefits of rearranging the sensor layers were clear to many, developing a practical manufacturing process proved challenging. It took more than a decade for BSI technology to become commercially viable. OmniVision Technologies was among the first to introduce a commercial BSI sensor, releasing the OV8810 (1.4 μm pixels, 8 megapixels) in September 2008. Sony soon followed, launching the first widely adopted BSI sensors in 2009.

[Wikipedia article on BSI](https://en.wikipedia.org/wiki/Back-illuminated_sensor)

:::

## Deep photodiodes {#sec-sensor-deepdiode}
@fig-sensor-skhynix illustrates a further improvement in the BSI design. SK Hynix, Omnivision and other manufacturers recognized that simply flipping the sensor (BSI) improved light capture. But as pixel sizes continued to shrink there was relatively little ability to store electrons.  Thus the well capacity was reduced and which limited the pixel's dynamic range (@sec-pixel-wellcapacity). Moreover, the sensitivity to longer wavelengths, which penetrates deeper into silicon, became a challenge (@sec-sensor-wavelength). 

To overcome these limitations, many vendors invented methods to build photodiodes that extend significantly deeper into the silicon (deep photodiode technology). Increasing the volume of the photodiode has two benefits.  First, the photodiode can gather more charge before saturating.  Second, the photodiode has higher long-wavelength sensitivity. 

![Deep photodiodes in the BSI. (a) A super pixel of a front-side illuminated CMOS sensor, including the microlenses, color filters, metal lines, and silicon. (b) A back-side illuminated super pixel with deep photodiode technology. Source: <a href="https://news.skhynix.com/evolution-of-pixel-technology-in-cmos-image-sensor/" target="_blank">Skhynix.</a>](images/sensors/03-sensor-bsi.png){#fig-sensor-skhynix width="70%"}

## Stacked sensors {#sec-sensor-stacked}
In the original planar technology, photodiodes are adjacent to the transistor circuits within the silicon substrate. Thus, photodiodes must compete for space with the transistors.  Reducing the photodiode area means less light efficiency; reducing the transistor size means noisier performance. Youse pays yer money and yer makes yer choice.

The **stacked sensor** architecture overcomes this limitation by physically separating the photodiode layer from the readout and processing circuitry. In a stacked sensor, the photodiodes occupy one silicon layer, while the supporting electronics are fabricated on a separate layer beneath. This structure maximizes the light-sensitive area (fill factor) for each pixel and enables the use of more advanced, lower-noise circuitry without sacrificing pixel size.

![Pixels in the stacked sensor are based on die stacking technology. Several layers of silicon that are bonded and connected using Through-Silicon Via (TSV) technology. The technology began with 2-layers, but when have you ever known Silicon Valley to stop?](images/sensors/03-pixel-stacked.png){#fig-sensor-stacked width="50%"}

The key enabling technology is called **die stacking**. The silicon layers are bonded together and electrically connected using two main methods. The original approach used **Through-Silicon Vias (TSVs)**—tiny holes etched through the silicon and filled with a conductive material (such as copper or tungsten) to create vertical electrical connections. 

More recently, **Cu-Cu (copper-to-copper)** direct bonding has become common. In this method, patterned copper pads on each wafer are precisely aligned and bonded under heat and pressure, allowing for dense, fine-pitch interconnects between the pixel and logic layers. Cu-Cu bonding is ideal for high-resolution, high-speed sensors, enabling features like global shutter or per-pixel memory. TSVs are still used for tasks such as power delivery and global I/O routing.

Stacked sensors have unlocked new capabilities in image sensor design. By separating the photodiode and circuitry layers, manufacturers can improve light sensitivity, reduce noise, and add advanced processing features directly beneath the pixel array. This architecture forms the foundation for ongoing innovation in sensor performance and functionality.

<!-- Stacked Sensor Structures Google doc 
A significant step towards multi-layer integration was taken by Mitsumasa Koyanagi of Tohoku University, who pioneered the technique of wafer-to-wafer bonding with TSV in 1989
-->

::: {.callout-note title="Stacked sensor applications" collapse="true"}
Stacked CMOS sensors build on the BSI CMOS architecture by integrating the pixel array with a separate logic layer. This close integration enables advanced features such as high dynamic range (HDR) imaging, faster frame rates, and on-chip processing—including artificial intelligence (AI) functions.

Some stacked sensors incorporate high-speed DRAM directly beneath the pixel array, allowing for rapid data readout and temporary storage. This architecture made possible innovations like the Sony a9 (2019), which could capture images at 20 frames per second (fps) with a continuous, blackout-free viewfinder experience.
:::

## Digital pixel sensor {#sec-digital-pixel}
The Digital Pixel Sensor (DPS) was a significant parallel development in image sensor technology.  The initial ideas were presented in the mid 1990s, at roughly the same time as the APS 3T and 4T circuits were implemented.  The APS circuitry included an analog-to-digital conversion (ADC) at the column or chip level. The DPS architecture extended the pixel circuit by including an ADC within the pixel circuit @fowler1995-dps-patent @Udoy2025-dps-review.

Digitizing the signal at the pixel had several advantages for high dynamic range imaging, global shuttering and readout speed @elgamal2005-cmos-imagesensors @kleinfelder2001-10000fps @Wandell2002-CommonPrinciples. An important feature of the original digital pixel sensors was that the pixel voltage was read non-destructively and repeatedly, say at 1 ms, 2 ms, 4 ms, and so forth. As the photon generated electrons increased in the well, the voltage was sampled.  The voltage for each pixel was reported when the pixel had passed one half well capacity, assuring it was much larger than the noise.  It would be reported at a time before the well capacity was reached, so that it was below saturation. The information from each pixel was the a voltage and and the time at which it reached that voltage. Thus, each pixel had its own exposure duration -or put another way, the sensor had a space-varying exposure duration. Pixels in dark regions of the scene had longer exposure times than pixels in the bright regions. We explain the value of this approach for high dynamic range imaging in @sec-exposure-control.  

The DPS concept was introduced at an early point in the development of CMOS imagers, and thus it faced significant hurdles in miniaturization. The idea of an ADC on the sensor foreshadowed an opportunity that has arisen as pixel architecture evolves. The stacked pixel structure offers a remedy by allowing the ADC and memory circuits to be placed in a separate layer underneath the photodiode. This enables higher pixel density and broader application of in-pixel conversion. It is quite possible that the next generation of image sensors will implement the DPS concept (@note-digital-pixel).

::: {.callout-note #note-digital-pixel collapse="true" title="Commercialization of the digital pixel sensor"}
El Gamal co-founded Pixim, Inc. in 1998, a fabless semiconductor company that commercialized chipsets for security cameras based on this DPS technology. A challenge for early DPS implementations was the reduced photodiode area in the planar array due to the additional ADC circuitry. This increased the cost of the chip and narrowed the field of potential applications and prevented widespread mainstream adoption for many years. Pixim was acquired by Sony Electronics in 2012.

In 2023 Sony introduced the [Sony a9 iii](https://en.wikipedia.org/wiki/Sony_%CE%B19_III). It has 24.6 Megapixels and a global shutter.  The camera can operate at high frame rates [(120 frames per sec)](https://www.sony.co.uk/interchangeable-lens-cameras/products/ilce-9m3?locale=en_GB). It appears to be using a digital pixel approach. 

It's highly probable that [Yusuke Oike](https://sites.google.com/site/yusukeoike) who worked in the El Gamal group from 2010-2012,  played a crucial role in the development and design of the groundbreaking global shutter sensor that is at the core of the a9 III's capabilities. He has been with Sony Corporation and Sony Semiconductor Solutions Corporation for many years, focusing on research and development of CMOS image sensors, pixel architecture, circuit design, and imaging device technologies. He's held positions like Senior General Manager of Research Division and is even listed as the newly appointed CTO of Sony Semiconductor Solutions Corporation in a 2025 news release.
:::

