# Pixels {#sec-pixels}
To understand how the photoelectric effect is harnessed in practical devices, we next review the behavior of electrons in semiconductors—the foundation of modern image sensor pixels.

## Electrons in semiconductors {#sec-sensor-electrons}
Atoms in a silicon semiconductor are arranged in a crystalline structure, and their electrons occupy specific energy levels grouped into two main bands: the **valence band** (lower energy) and the **conduction band** (higher energy). Under normal conditions, most electrons remain in the valence band. When a silicon atom absorbs a photon, an electron can be excited from the valence band to the conduction band (@fig-sensor-bandgap).

### The bandgap {sec-sensor-bandgap}
 The minimum energy required for this transition is called the **bandgap**, and it is measured in electron volts (eV). For silicon at room temperature ($21 \deg C$), the bandgap is 1.12 eV. Only photons with energy equal to or greater than this value can generate free electrons in silicon.

![Energy bands in silicon semiconductors. A photon can excite an electron in the Valence band.  If the photon has enough energy, the electron can shift to the conduction band, becoming a free electron that can be stored and counted. ](images/sensors/03-sensor-bandgap.png){#fig-sensor-bandgap width=50%}

Without special circuitry, the hole and electron will re-combine. To make an image sensor, we need a method to measure the number of electrons. In a CCD an electric field between the bands prevents recombination. In CMOS electrons are trapped in capacitors placed in the silicon. We explain how this is done in @sec-pixels.

### Wavelength effects {sec-sensor-wavelength}
There are several wavelength-dependent effects worth noting about silicon sensors. First, we can calculate the energy in Joules of a photon of wavelength $\lambda$ using this the photon energy formula:

We can convert from Joules (J) to electron volts (eV) with this formula

$$
1~\text{eV} = {1.602 \times 10^{-19}}~\text{J} 
$$ {#eq-joules-ev}

A wavelength of 1100 nm has just enough energy (1.12 eV) to transition an electron across the bandgap: Thus, silicon sensors are insensitive to electromagnetic radiation beyond 1100 nm and infrared sensors must use a different material. Second, photons with wavelengths less than 1100 nm have enough energy to excite electrons across the bandgap, and silicon will absorb radiation at these shorter wavelengths.  It is notable that the eye does not encode wavelengths below about 400 nm. 

Third, the chance that a photon excites an electron that crosses the bandgrap depends on the photon's energy level. Higher energy (shorter wavelength) photons are more likely to cause the electron to change bands. For this reason, different wavelengths are absorbed at different spatial depths in the material. Short-wavelength photons are much more likely to generate electrons near the surface compared to long-wavelength photons.  Conversely, long-wavelength photons are relatively more likely to generate electrons deeper in the material. This depth-dependence on wavelength has been used to create color image sensors (@sec-sensor-foveon).

## CMOS pixel circuits {#sec-cmos-circuits}
CMOS image sensors use photodiodes—and the photoelectric effect—to measure and store the optical light field at the sensor surface. The key technical breakthrough that enabled CMOS sensors was the invention of circuitry to store and then transmit the photon-generated electrons from the photodiode to the computer.

The photodiode and this circuitry are critical parts of the pixel; there are other essential components as well (@fig-pixel-overview). For example, effective image sensors place a small lens (microlens) above each pixel.  Color image sensors place a color filter between the microlens and the photodiode that passes certain wavelengths and blocks others. 

![Schematic of the early CMOS pixel architecture.  This image shows a microlens and color filter, which are typically part of the pixel. The microlens is centered over the pixel here, but for pixels at the edge of the sensor array it will be positioned a bit to the side. This color filter passes long wavelength (red-appearing) light, but different pixels will have different color filters. The first layer of silicon, with its photodiode and classic circuitry sharing space, are shown. Source: <a href="https://evidentscientific.com/en/microscope-resource/knowledge-hub/digital-imaging/cmosimagesensors" target="_blank">Evident Scientific.</a>](images/sensors/03-pixel-overview.png){#fig-pixel-overview width="60%"}

 Figure @fig-pixel-overview is a schematic that omits many details. For example, notice the gap between the photodiode and the color filter. In early CMOS imagers, this gap contained many metal lines that carried control and data signals. The metal lines were organized into layers and they are illustrated in  Figure @fig-pixel-layers. In the first generation of image sensors the distance from the microlens at the top to the photodiode at the bottom was fairly large compared to the size of the photodiode. In these early pixels, light from the main lens had to travel through what was effectively a tunnel to reach the photodiode.

![Color imagers use an array of color filters to create pixels with different wavelength selectivity. The color filters are typically arranged in a repeating pattern, and the smallest repeating unit of the color filter array is called a **super pixel**. The image illustrates a super pixel for a sensor with cyan, magenta, and yellow color filters. This image emphasizes the metal lines, omitted in @fig-pixel-overview. In the original CMOS imagers, light from the main imaging lens had to pass through these metal layers before reaching the photodiode.](images/sensors/03-pixel-overview2.png){#fig-pixel-layers width="50%}

<!-- fig-pixel-layers:  not sure where this image comes from -->

Pixel architecture has evolved significantly over the years, and there continue to be many novel designs. The most straightforward change is that technology has scaled, enabling pixel sizes to shrink and thus increasing spatial sampling resolution. In addition, the placement of the circuitry and metal lines has changed to below the photodiode; light no longer passes through a deep tunnel. There are other improvements to the circuitry, some experimental, that we will review later in @sec-sensor-innovations. It is helpful to first understand the concepts behind the classic circuitry, so that we can better appreciate these innovations.

The logical flow of the circuitry is as follows:

-   Before image capture, the sensor circuitry resets each pixel, clearing residual charge from previous exposures.
-   During image acquisition, the photodiode array collects electrons generated by incoming light; these electrons are stored within the photodiode (3T) or in a nearby capacitor (4T).
-   During readout, the circuitry transfers the stored charge to an analog-to-digital converter (ADC).
-   The digital image array records the amount of light captured by each pixel.

The role of the other essential pixel components, the color filter arrays and microlenses, is explained in Section @sec-sensors.  In @sec-sensors-characterization I describes how to calibrate and model sensor performance, from the scene light field to image capture.

### 3T circuit {#sec-3T-circuit}

![The original CMOS 3-transistor (3T) circuit design (@Fossum1997-early).](images/sensors/03-3T-circuit.png){#fig-sensor-3T-circuit width="60%"}

The original CMOS pixel design uses a three-transistor (3T) circuit to store and read out the charge collected by the photodiode (@fig-sensor-3T-circuit). Each pixel contains three key transistors: $M_{rst}$ (reset), $M_{sel}$ (select), and $M_{sf}$ (source follower). The $M_{rst}$ transistor resets the photodiode by connecting it to the supply voltage (Vdd), clearing any residual charge before image capture. The $M_{sel}$ transistor selects a specific row of pixels, connecting them to the readout circuitry. The $M_{sf}$ transistor acts as a buffer, transferring the stored charge to the analog-to-digital converter (ADC) for measurement. In most sensors, each column has its own ADC, but some designs use a single ADC shared among multiple columns—a technique known as multiplexing (mux).

Ideally, the photodiode’s response to light is linear, with the number of generated electrons following Poisson statistics. However, the surrounding circuitry introduces additional sources of noise and nonlinearity. For example, the storage capacitor has a limited capacity, so it can saturate at high light levels. The transistors used for readout can also introduce noise and small nonlinearities. Furthermore, variations in pixel properties across the sensor array can cause fixed-pattern noise. In modern CMOS sensors, these nonlinearities and noise sources are typically small—on the order of a few percent [@Wang2017-CMOSLinearity] -but they can still affect image quality. Careful characterization and calibration are necessary to minimize these effects and produce high-quality images.

```{=html}
<!-- 
 (http://encyclopedia2.thefreedictionary.com/Vdd also the drain voltage), Vrst is the reset voltage Vee is the negative supply voltage Ground is ground
http://en.wikipedia.org/wiki/Active_pixel_sensor , http://encyclobeamia.solarbotics.net/articles/vxx.html , 
http://en.wikipedia.org/wiki/Active_pixel_sensor , http://encyclobeamia.solarbotics.net/articles/vxx.html , 
Vdd is positive supply voltage  (http://encyclopedia2.thefreedictionary.com/Vdd also the drain voltage), Vrst is the reset voltage Vee is the negative supply voltage Ground is ground
-->
```

### 4T circuit {#sec-4T-circuit}

![The 4T pixel circuit introduces a pinned photodiode and an additional transistor. When activated, the new transistor transfers the accumulated charge from the photodiode to a storage capacitor (the floating diffusion node, $C_{fd}$). The rest of the circuit, including readout and row select, is similar to the 3T design.](images/sensors/03-4T-circuit.png){#fig-sensor-4T-circuit width="60%"}

Since about 2000, most CMOS image sensors have used a four-transistor (4T) pixel circuit [@Fossum2014-pinnedpd]. The 4T pixel adds two important features: a pinned photodiode (PPD) and a transfer gate transistor (@fig-sensor-4T-circuit). The pinned photodiode improves charge storage and reduces noise, while the transfer gate allows precise movement of the collected charge from the photodiode to a storage node called the floating diffusion, which has a capacitance $C_{fd}$. The other circuit elements—reset, row select, and source follower—remain the same as in the 3T design.

The transfer gate and floating diffusion give the circuit better control over when and how charge is transferred and measured, which helps reduce noise and improve image quality.

Today, the 4T pixel is the standard in almost all high-performance CMOS image sensors, including those found in smartphones, industrial cameras, cars, and scientific instruments.

<!--
There is also Theuwissen diagram, with five transistors that he calls a 4T circuit.  In the paper 'Linearity analysis ...' @Wang2017-CMOSLinearity.  He says the source follower is shared by multiple rows of pixels. It is Figure 2.  

The text: "Figure 2 shows the schematic of a typical voltage mode 4T pixel. The pixel circuit consists of a pinned photodiode, a charge
transfer switch (M1), a reset switch (M2), a source follower (M3) and a row select switch (M4). The current source transistor (M5) is shared by multiple rows of pixels. VDD is the power supply while VPIX is the output voltage of pixel. VLN provides an adjustable bias current. In the following analysis, the nonlinearity caused by the row select transistor (M4) is omitted to simplify the analysis."
-->

## Pixel circuit properties 
Simulations of the CMOS circuitry can -and do- become very complex during the design phase. Engineers use specialized software that incorporates the foundry's design rules that account for material properties and feature sizes. This design software helps circuit designers achieve a working system from the foundry.

After the chip is built, it is impractical to characterize each different line, device and junction. Designers may place certain test circuits within the chip. But more generally systems are evaluated with respect to a set of objective measurements using images as input and digital values of output. This process is called called **system characterization** or **system calibration**. The characterization measurements specify the limitations of system performance, including light sensitivity, system noise, dynamic range, and color accuracy.

The characterization measurements are chosen to be practical and insightful about potential system errors. They are chosen to characterize key limitations based on knowledge of the circuit properties.

### Sensitivity: Fill factor {#sec-pixel-fillfactor}
Because modern pixels are so small, each photodiode often collects only a modest number of photons—even under good lighting. For example, a $1~\mu\text{m}^2$ pixel in a dim scene (with a fast f/2 lens) might receive just 200 photons during an exposure. Photon arrivals follow Poisson statistics, so the standard deviation is $\sqrt{200} \approx 14$, or about 7% of the mean. This photon noise is a fundamental limit: it sets the minimum possible noise level and constrains the signal-to-noise ratio (SNR) in low-light conditions.

::: {#sensor-photons .callout-note collapse="true"}
## Number of photons at each pixel
The number of photons arriving at a pixel depends on many factors, including the light intensity, the optics, and the size of the pixel. ISETCam is a useful tool for estimating quantities such as the number of incident photons. The variance in the number of photons at a $1~\mu\text{m}^2$ pixel under moderate to dim lighting is considerable. Here is an image calculated using ISETCam, showing the Poisson variation in the number of photons across a small region.

![Poisson noise in a uniform region of a dim scene. Each pixel is \$1\~\\mu\\text{m}\^2\$ and the scene intensity is 50 nits.](images/sensors/03-photonimage.png){#fig-photonimage width="60%"}.

This [ISETCam script](../code/fise_opticsCountingPhotons.html) illustrates the calculation. Nearly every sensor simulation we run in ISETCam includes calculations like this.
:::

The photodiode occupies only a portion of the pixel area; some of the pixel area is taken up by the circuitry. The proportion of the pixel area that is sensitive to light—the photodiode area—further limits light sensitivity. This proportion is called the **fill factor**. A higher fill factor means more of the pixel is used to detect light, increasing sensitivity. Fill factor is usually expressed as a value between 0 and 1, or as a percentage between 0% and 100%. The first-generation of CMOS image sensors were built with semiconductor technology with feature size on the order of 350 nm, and thus the circuitry might occupy as much as half of the pixel area (fill factor of 0.5). Over years the feature size has shrunk considerably and the fill factor of modern pixels can be greater than 0.9. The fill factor combined with the light sensitivity of the photodiode might result in about 150 electrons generated from the 200 photons. This number, too, is Poisson distributed so the short noise will be about $\sqrt{150} \approx 14$, about 8% of the mean.

### Well capacity and dynamic range {#sec-pixel-wellcapacity}
The photodiode and floating diffusion node in each pixel act as charge storage devices, often referred to as **storage wells**. The maximum number of electrons that can be stored in these wells is called the **well capacity** or **full-well capacity**. When the number of photo-generated electrons exceeds this capacity, any additional electrons cannot be stored, and the pixel output no longer increases with light intensity. Well capacity is therefore a key factor in determining the pixel’s dynamic range and the point at which it stops responding to additional light.

The **dynamic range** of a pixel is defined as the ratio between the largest and smallest amounts of light that can be reliably measured. Specifically, we measure the minimum detectable light level ($L_{min}$) and the maximum light level ($L_{max}$) that fills the well just below saturation. The dynamic range is then calculated as:

$$
\text{DR} = 20~\log_{10}\left(\frac{L_{max}}{L_{min}}\right)
$$

The logarithm compresses the range into more manageable numbers, typically between 60 and 100. The factor of 20 comes from conventions in audio engineering, where dynamic range is measured in decibels (dB). Image scientists use the same convention, so dynamic range is usually reported in dB.

### Response curve {#sec-pixel-responsecurve}
The finite well capacity can also have an impact on how the response increases with intensity. @fig-sensor-linearity shows an estimate of the number of stored electrons as a function of number of incident photons for a real sensor. The number of stored electrons increases linearly over a large part of the range. Beyond a certain level, as we approach the storage limit, the curve starts to saturate. As the light intensity increases even further, the curve completely flattens at the well capacity, in this example about 70,000 electrons.

![A CMOS sensor response curve, showing the number of electrons captured as function of the number of incident photons. From Figure 7 in that paper.](images/sensors/03-responselinearity.png){#fig-sensor-linearity width="60%"}

This pattern can be understood in terms of the physics. At low and moderate light levels, nearly all photo-generated electrons are successfully stored in the well. The photoelectric effect is linear and thus there is a linear increase in stored charge with increasing light intensity. 

As the well nears its full capacity, several effects cause the response to become sublinear and eventually saturate. First, when the storage well is almost full, some newly generated electrons cannot be stored and may leak out or recombine, reducing the efficiency of charge collection. Second, some electrons generated by photons may diffuse into neighboring regions of the sensor substrate rather than being stored in the intended circuit element (e.g., floating diffusion noise). Both leakage and diffusion become more significant at higher light intensities, causing the response curve to flatten out as it approaches saturation.

The exact onset and severity of this nonlinearity depend on the specific circuit design and material properties of the sensor. In certain designs the nonlinearity is negligible, with only  some degree of nonlinearity as the well capacity is approached. Understanding the response curve is important for accurately interpreting sensor measurements, especially in applications that require high dynamic range or precise quantitative imaging.

## Pixel noise {#sec-pixel-noise}
 
If we acquire the same image repeatedly, even with no change in the scene, there will be fluctuations in a pixel's output. The difference in a pixel's output across time is called **temporal noise**. There are several contributors to this variation.  We have already reviewed one important contributor, the Poisson distribution of the electrons at the photodiode (Section @sec-photoelectric-statistics). 

Each of the different components of the pixel are potential sources of temporal noise. Here is a brief list.

### Reset noise {#sec-pixel-resetnoise}
In the 3T and 4T circuits, we must transfer the electrons from the photodiode (in 3T) or from the floating diffusion node (in 4T).  In both cases, there is the possibility that as we start to collect the electrons there will be a random number of electrons left over from the prior image capture.  This variable number of unwanted electrons is a failure to reset the storage to exactly the same state, thus it is called **reset noise**. 

This noise is present in both 3T and 4T circuits, but it has different properties because the storage location differs (@sec-4T-circuit). An important feature of the 4T circuit is that the floating diffusion noise can be read just prior to transferring the electrons from the photodiode, and then again after electrons have been transferred. The difference is read out as the pixel value.  This process is called **correlated double sampling** (CDS), and it significantly reduces reset noise.

In a 3T pixel, reading the voltage on the photodiode disturbs the accumulated charge. CDS relies on making two non-destructive measurements, which is not possible on the photodiode itself in the 3T circuit. 

Reset noise is also known as $kTC$ noise because its size depends on a formula related to the capacitance of the floating diffusion node. The most common representation of the noise is with respect to the pixel voltage.

$$
{\sigma_V}^2 = \frac{kT}{C}
$$ {#eq-ktc-volts}

The units of these parameters are:

* $k$ - Boltzmann constant - Joules per Kelvin
* $T$ - Temperature - Kelvin
* $C$ - Capacitance Farads = Coulombs per volt

The formula above shows that reset noise increases with temperature ($T$). As a result, sensors in colder environments (such as ski slopes) experience less reset noise than those in warmer settings (like beaches in summer). To address this, many cameras use temperature sensors and adjust their image processing to compensate for temperature-dependent noise, helping to maintain consistent image quality in different conditions.

::: {#sensor-ktc .callout-note collapse="true"}
### Units of reset noise

The reset noise formula in @eq-ktc-volts gives the noise variance in units of volts. However, it is often more useful to express reset noise in terms of the number of electrons. The equivalent formula for the variance in charge is

$$
{\sigma_Q}^2 = k T C
$$ {#eq-ktc-electrons}

Here, ${\sigma_Q}$ is in Coulombs. To convert this to electrons, divide by the elementary charge ($1.602 \times 10^{-19}$ Coulombs per electron):

$$
\sigma_e \approx \frac{\sqrt{k T C}}{1.602 \times 10^{-19}}
$$

Reset noise is typically modeled as a Gaussian distribution with zero mean and standard deviation $\sigma_e$ electrons.
:::

In the 4T pixel, the circuit first measures the voltage on the floating diffusion node before transferring charge from the photodiode (capturing a baseline), and then again after the charge is transferred. The difference between these two measurements represents the pixel signal. This subtraction cancels much of the reset noise, improving signal-to-noise ratio. 

The 4T design is now standard due to its superior noise performance. Further innovations, such as dual conversion gain and stacked pixel structures have also expanded the dynamic range of CMOS sensors. We will explore some of these developments in @sec-sensor-innovations.

### Dark current {#sec-pixel-darkcurrent}
Even in the absence of light, electrons are generated within the semiconductor material. These electrons make up the **dark current**. The size of the dark current depends on factors such as the temperature and the bandgap of the material. The number of accumulated dark electrons increases over time.

Dark electrons accumulate in the same storage as photon-generated electrons and are indistinguishable from them in the readout. But the dark electrons are not informative about the scene; in fact, they interfere with inferences one would make about the scene.

There is a relatively large impact of temperature on dark current $I_d$. The scaling of the current with temperature is specified by this standard formula:

$$
I_d(T) \propto T^3 e^{-E_g/kT}
$$ {#eq-sensor-darkcurrent}

 We can convert the dark current (in $C/s$) to a rate of electrons per second by dividing by the elementary charge ($1.602 \times 10^{-19}~C/e^-$). The symbol $C$ is Coulombs. The number of dark electrons is random and follows a Poisson distribution with a mean equal to the expected number of dark electrons for the exposure time. 

The dark current level depends on multiple features of the circuit and is hard to predict from first principles.  The dark current level is usually determined empirically and reported in the data sheets describing the sensor.

::: {#sensor-dark-current .callout-note collapse="true"}
### Units of dark current noise

<!-- https://www.onsemi.com/pdf/datasheet/noii4sm6600a-d.pdf -->
A data sheet for a (discontinued) ON Semiconductor sensor specifies a dark current of $78~e^{-}/\text{s}$ at $T = 21^\circ\text{C} = 294.15~\text{K}$. If we take a picture with a 0.03 second exposure, we expect an average of $2.34$ dark electrons per pixel. If the temperature rises by $30$ Kelvin, to $324.15~\text{K}$, the mean number of dark electrons increases to $3.13$. The number of dark electrons is Poisson distributed, so the variance equals the mean.
:::

### Temperature dependence {#sec-pixel-temperature}
The sections on reset noise and dark current were two examples of how noise depends on temperature. CMOS image sensors are sensitive to temperature in other ways as well, because many of their key physical processes depend on thermal energy. Here is a summary of the factors that change as temperature rises:

* **Dark current increases exponentially**, adding background signal and noise even in the absence of light.
* **Reset noise** (kTC noise) grows linearly with temperature, especially affecting sensors without correlated double sampling.
* **Analog circuit offsets and gain** can drift, altering pixel output levels and introducing fixed-pattern noise.
* **Leakage currents and charge transfer inefficiencies** rise, degrading low-light performance and introducing artifacts.

Together, these effects cause image quality to degrade—particularly in precision applications like scientific imaging or long exposures. To mitigate these issues, sensors often include:

* **Real-time temperature sensors**
* **Calibration tables** for dark current, gain, and noise
* **Active cooling systems**
* **Post-processing corrections** such as dark-frame subtraction and flat-field normalization

Temperature compensation by cooling helps maintain consistent signal-to-noise ratio, dynamic range, and color accuracy across operating conditions. If cooling is not possible, knowledge of how noise depends on temperature is important when processing and interpreting camera data.
