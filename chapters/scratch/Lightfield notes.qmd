Finally, although the wavelength of the incident and reflected light are most often the same, there are fluorescent materials that convert incident radiation at one wavelength into exitant radiation at different wavelengths. Fluorescent materials are widespread in biology, and such molecules are widely used in biomedical applications. They are also used by people who manufacture clothing detergent and want a washing to result in a brighter white.

<!--
https://chatgpt.com/c/67e2fe34-d268-8002-92d9-b6be5cf568e5

The term **"Lambertian reflectance"** originates from the work of the Swiss mathematician, physicist, and astronomer **Johann Heinrich Lambert (1728–1777)**. It is named after him because he was the first to formalize the mathematical description of **diffuse reflection** in his 1760 treatise *Photometria*, where he laid the foundations of modern photometry.

### **Scientific Background**
1. **Lambert's Cosine Law (1760)**  
   Lambert’s key insight was that an ideal **diffuse surface** (later called a Lambertian surface) reflects light **equally in all directions** regardless of the viewing angle. However, the perceived brightness (radiance) of the surface follows a cosine relationship relative to the surface normal and the incident light direction. Mathematically, this is expressed as:

   \[
   L = L_0 \cos\theta
   \]

   where:
   - \( L \) is the radiance observed in a particular direction,
   - \( L_0 \) is the radiance when viewed normal to the surface,
   - \( \theta \) is the angle between the surface normal and the viewing direction.

   This means that even though light is scattered uniformly, the intensity decreases with increasing angle from the normal because the projected area exposed to the observer diminishes.

2. **Diffuse vs. Specular Reflection**  
   - In contrast to **specular reflection** (e.g., mirror-like surfaces), where light is reflected in a single direction, **Lambertian surfaces** scatter incident light uniformly in all directions.  
   - Common examples include **matte surfaces, paper, and frosted glass**, which approximate Lambertian reflectance.

3. **Applications in Science and Engineering**  
   - Lambertian reflectance is widely used in **computer graphics**, **computer vision**, **remote sensing**, and **optics** to model realistic surface appearance.
   - Many real-world materials exhibit approximately Lambertian properties, making it a useful approximation in radiative transfer and reflectance modeling.

Thus, the name "Lambertian reflectance" honors **Johann Heinrich Lambert**, whose work in photometry provided the mathematical framework for understanding **diffuse reflection**.
-->


```{=html}
<!--
In computer vision and perceptual science, the division between *things* and *stuff* is a foundational classification for scene understanding:

**Things**  
- Refer to countable, discrete objects with defined shapes and boundaries (e.g., cars, chairs, animals).  
- Recognition relies on structural cues like shape, contours, and spatial arrangement[1][4].  

**Stuff**  
- Encompasses amorphous materials or textures without fixed boundaries (e.g., water, grass, sky, sand).  
- Identified primarily through local appearance features such as color, texture, and reflectance properties[1][8].  

**Key Differences**  
| Aspect          | Things                          | Stuff                          |  
|-----------------|---------------------------------|--------------------------------|  
| **Boundaries**  | Well-defined (countable)        | Indistinct (uncountable)       |  
| **Recognition** | Shape, structure                | Texture, color, reflectance[2][8] |  
| **Examples**    | Cars, buildings                 | Water, sky, foliage[1][4]      |  

This distinction simplifies computational tasks like segmentation, where *things* require instance-level detection, while *stuff* is segmented regionally based on texture or material properties[4][8]. Human vision similarly leverages these categories, with *stuff* often processed holistically for material properties (e.g., gloss, roughness)[2][9], and *things* analyzed for object identity and function[1].  

The interplay between the two is critical for panoptic segmentation, where unified models jointly label *things* (as instances) and *stuff* (as semantic regions)[4].

Citations:
[1] https://cacm.acm.org/research/scene-understanding-by-labeling-pixels/
[2] https://www.microsoft.com/en-us/research/video/on-seeing-stuff-the-perception-of-materials-and-surfaces/
[3] https://www.waterfootprint.org/resources/Report12.pdf
[4] https://openaccess.thecvf.com/content/CVPR2021/papers/Shen_Toward_Joint_Thing-and-Stuff_Mining_for_Weakly_Supervised_Panoptic_Segmentation_CVPR_2021_paper.pdf
[5] https://pmc.ncbi.nlm.nih.gov/articles/PMC7768321/
[6] https://catalogue.unccd.int/1659_Dasgupta_Review_-_Full_Report.pdf
[7] https://www.mindbodyevolution.info/the-science-of-seeing/
[8] https://www.annualreviews.org/doi/10.1146/annurev-vision-102016-061429
[9] https://www.nature.com/articles/s41562-023-01601-0

---
Answer from Perplexity: pplx.ai/share
-->
```