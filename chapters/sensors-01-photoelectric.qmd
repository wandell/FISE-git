---
date: last-modified
---

# Photons and Electrons {#sec-sensors-photons-electrons}
{{< include "includes/WIP-callout.qmd" >}}


## Photons and Electrons overview {#sec-sensors-photons-electrons-overview}

The transition from film to digital imaging, enabled by semiconductor technology, has profoundly changed how we record and process images (@boyle1970-ccd-first; see [Preface](../index.qmd#sec-preface)). Digital image sensors use arrays of tiny, light-sensitive elements called **photodiodes**.These are semiconductor devices that convert incoming light into electrical current. Photodiodes are the essential building block of all modern image sensors. 

The earliest digital image sensors, known as charge-coupled devices (CCD), were built using metal-oxide semiconductor technology (MOS). Measuring the electrons captured by the array of photodiodes in a CCD requires significant power, making them unsuitable for mobile devices and limiting their early adoption to scientific and other high-end applications.

A major breakthrough came with Eric Fossum's invention of the Complementary Metal-Oxide Semiconductor (CMOS) image sensor (@fossum1993-ccd-dinosaurs; @fossum1995-patent). By implementing an active pixel sensor (APS) circuit in CMOS, Fossum enabled image sensors to be manufactured using mainstream microelectronics processes. This dramatically reduced power consumption and allowed integration of light-sensing pixels with timing, control, and signal processing circuitry on the same chip (@fig-olympus-overview). These advances paved the way for the widespread adoption of electronic image sensors in mobile devices and many other applications.

![CMOS image sensors (CIS) incorporate integrated circuitry and multiple subsystems. The circuitry and subsystems, together, enable the capture, transfer and digital encoding of the image. Source: <a href="https://evidentscientific.com/en/microscope-resource/knowledge-hub/digital-imaging/cmosimagesensors" target="_blank">Evident Scientific.</a>.](images/sensors/13-photons/olympusCMOSSensorImage.png){#fig-olympus-overview style=".float-right" width="60%"}


## Photoelectric effect
The physical principle underlying the photodiode operation —converting light into electrons— is called the **photoelectric effect**. Understanding how materials convert light into electrical signals is a fascinating chapter in the history of physics and engineering, shaped by groundbreaking experiments and theories that changed our thinking about the world.

Heinrich Hertz was the first to report an unexpected phenomenon related to electromagnetic waves that would later be known as the photoelectric effect [@Hertz1887-photoelectric2; @Hertz1887-photoelectric1]. His primary goal was to measure the electromagnetic waves predicted by Maxwell's theory. For this, he used a two-part apparatus:

1.  **The Transmitter:** This consisted of a large induction coil connected to a spark gap (often with attached metal plates acting as a dipole antenna). The induction coil produced high-voltage pulses, causing powerful, oscillating sparks to jump across this gap. These sparks were the source of the electromagnetic radiation (radio waves) that Hertz aimed to study.

2.  **The Receiver:** This was a separate, much smaller loop of wire, also containing a small spark gap. The receiver was placed at a distance from the transmitter. When the electromagnetic waves from the transmitter propagated to the receiver, they induced an oscillating current in the receiver loop. If the induced current was strong enough, a tiny spark would jump across the receiver's gap, signaling the detection of the electromagnetic waves.

During these experiments Hertz made a crucial incidental observation: a spark in the *receiver* gap was significantly enhanced – jumping a longer distance – when the receiver gap was illuminated by the ultraviolet (UV) light emanating from the *transmitter's* spark. Conversely, placing a glass plate (which blocks UV) between the transmitter and receiver diminished the receiver spark.  A quartz plate (which transmits UV) did not impact the spark.

Hertz correctly concluded that UV light was somehow facilitating the spark in the receiver gap. This observation was the first report of the photoelectric effect – the phenomenon where electromagnetic radiation, specifically UV light in this case, causes the emission of charged particles (later identified as electrons) from a material surface, thereby influencing electrical signals.
<!--
He also noticed that glass, which blocks UV light, eliminated the effect. Quartz (which transmits UV light) did not.

https://g.co/gemini/share/4d34e0260c7f

-->

The central challenge of the photoelectric effect is to model how electromagnetic radiation—light—interacts with a material to generate an electrical signal.

## Discovery of the photon {#sec-photon-discovered}
It was not until 1897 that JJ Thomson conclusively demonstrated that electrons are discrete, countable particles. Thomson's experiments revealed that cathode rays are composed of negatively charged particles with a specific mass and charge, much smaller than that of an atom.

In 1902, armed with knowledge of electrons, Philipp Lenard carried out a series of important experiments. He observed that increasing the intensity of the incident light increased the number of emitted electrons but **did not** increase their kinetic energy. Instead, the energy of the emitted electrons depended on the **wavelength** of the light: shorter wavelengths (higher frequencies) produced more energetic electrons. Lenard also identified a **threshold wavelength**—if the light’s wavelength was too long (i.e., its frequency too low), no electrons were emitted, regardless of how intense the light was. These results posed a serious challenge to classical wave theory, which predicted that energy should increase smoothly with intensity, not depend on frequency.
<!--
https://chatgpt.com/c/WEB:606fcc5d-ec9a-46c9-a81a-86f918357a28
https://en.wikipedia.org/wiki/Philipp_Lenard
Lenard-rig svg:  By VectorVoyager - Own work, based on, CC0, https://commons.wikimedia.org/w/index.php?curid=124119939

The Experimental Setup (From ChatGPT link above)

Lenard built on the earlier work of Hertz and Hallwachs, who had discovered that ultraviolet light could cause electrons to be emitted from metal surfaces. But Lenard introduced major improvements in experimental control and measurement, using a specially designed photoelectric tube:

Two metal electrodes (a cathode and an anode) were enclosed in a highly evacuated glass tube.

The cathode (the “illuminated plate”) was made of a clean metal surface (often zinc or platinum).

Light entered the tube through a thin quartz window, which transmitted ultraviolet radiation.

A potential difference between the cathode and anode could be varied — Lenard used a small battery or adjustable voltage source.

The resulting photoelectric current was measured with a sensitive electrometer (a quadrant or Dolezalek type).

📏 What He Actually Measured

Lenard measured the current–voltage (I–V) characteristic of the photoelectric tube under various illumination conditions.

Current vs. voltage curve:
He plotted the photoelectric current 
𝐼
I as a function of the applied potential 
𝑉
V between the electrodes.

At positive potentials (anode positive relative to cathode), emitted electrons were collected efficiently, and the current reached a saturation value, 
𝐼
sat
I
sat
	​

.

At negative potentials (anode negative), the emitted electrons were repelled, and the current fell to zero at a particular voltage — the stopping potential, 
𝑉
𝑠
V
s
	​

.

Varying intensity:
Lenard changed the intensity of the incident light (by adjusting lamp brightness or interposing neutral-density filters) while keeping the wavelength constant.
→ Result: 
𝐼
sat
I
sat
	​

 increased (more electrons emitted), but the stopping potential 
𝑉
𝑠
V
s
	​

 — and hence the maximum kinetic energy of the electrons — stayed the same.

Varying wavelength:
He changed the color (wavelength) of the light, using different filters or gas-discharge lamps.
→ Result: 
𝑉
𝑠
V
s
	​

 changed systematically — shorter wavelengths (higher frequency) gave larger 
𝑉
𝑠
V
s
	​

, meaning more energetic electrons.

⚡ How the Electron Energy Was Deduced

The key measurable quantity was the stopping potential 
𝑉
𝑠
V
s
	​

:

𝑒
𝑉
𝑠
=
1
2
𝑚
𝑣
max
2
eV
s
	​

=
2
1
	​

mv
max
2
	​


where:

𝑒
e is the electron charge,

𝑣
max
v
max
	​

 is the maximum electron velocity.

By applying a retarding (negative) voltage to the anode and finding the point where the photoelectric current just dropped to zero, Lenard could determine the maximum kinetic energy of the emitted electrons.

That’s how he discovered:

Increasing intensity → more emitted electrons (higher 
𝐼
sat
I
sat
	​

), but no change in kinetic energy (
𝑉
𝑠
V
s
	​

 constant).

Increasing frequency (shorter wavelength) → emitted electrons had higher kinetic energy (larger 
𝑉
𝑠
V
s
	​

).

🧠 Why It Was Revolutionary

Classical wave theory predicted that higher light intensity should deliver more energy to each electron — i.e., both 
𝐼
sat
I
sat
	​

 and 
𝑉
𝑠
V
s
	​

 should increase together.
Lenard’s measurements clearly contradicted this: electron energy depended only on wavelength (frequency), not intensity.

This paradox directly led to Einstein’s photon hypothesis in 1905.

🧾 Key Reference

Lenard, P. (1902). Über die lichtelektrische Wirkung. Annalen der Physik, 8, 149–198.
(“On the Photoelectric Effect.”)
-->
At the same time, physicists were trying to explain the spectrum of light emitted by a blackbody—a perfect absorber and emitter of radiation. In 1900, **Max Planck** tackled this problem. He found a formula that matched the observed spectrum, but only if he assumed that energy exchange (joules) between matter and radiation occurred in discrete amounts.

$$
E = n h \nu
$$

Here, $n$ is an integer; $\nu$ is the frequency ($1/sec$) of the radiation; and $h$ is a constant — now called Planck’s constant ($h = 6.62607015 \times 10^{-34}$ joule-seconds). Planck proposed that the oscillators in the material’s walls could only transfer energies in discrete steps. This quantization explained why the measured absorption and emission of light matched the observed spectrum, even though the light field itself was continuous.

In 1905, Albert Einstein went further. He proposed that we should not think of light as a continuous wave that becomes quantized through an interaction with the material. Rather, he argued, light itself **quantized** into discrete packets of energy—later called **photons**. The energy of each photon is proportional to its frequency, and inversely proportional to its wavelength

$$
E = \frac{h c}{\lambda} 
$$ {#eq-plancks-formula}

where

* $h$ = Planck’s constant ($6.626 \times 10^{-34}$ J·s)
* $c$ = speed of light in vacuum ($3.00 \times 10^8$ m/s)
* $\lambda$ = wavelength (in meters)

This hypothesis went beyond Max Planck’s work, which assumed the quantization was part of the exchange between matter and radiation — not that light itself came in packets. Einstein suggested that **light in free space** behaves as localized energy quanta, capable of transferring energy to electrons one photon at a time. His theory explained both Planck's observations and all of Lenard’s experiments. His photoelectric theory laid the groundwork for the next generation's work in quantum physics.

## Wave-particle duality
For much of the 19th century, Newton’s idea that light was made of tiny particles (corpuscles) was set aside, as experiments overwhelmingly supported the wave theory of light—interference and diffraction patterns provided strong evidence for wave-like behavior. However, the photoelectric effect presented results that could not be explained by wave theory alone, forcing physicists to recognize that light sometimes behaves as a particle as well. This is the essence of the **wave-particle duality**.

Einstein did not ignore the wave phenomena. For much of his career, he worked towards a new set of principles that might be called a "statistical wave-particle" model, in which photons retain a wave-like "guiding field" or statistical distribution that determines their path. He imaginged that individual photons are particles, but their collective behavior is governed by an emergent, wave-like probability distributions that creates the familiar diffraction patterns.

<!-- Einstein and diffraction
https://g.co/gemini/share/48fba5e2af5a
-->

A different, revolutionary theory sprung forth from the next generation, including Neils Bohr. This theory, **quantum mechanics** describes electromagnetic radiation by a mathematical object called the **wavefunction**, which provides the probabilities of different outcomes. When experiments are designed to reveal wave-like effects (such as interference), light behaves as a wave. When experiments probe particle-like effects (such as the photoelectric effect), light behaves as a particle. 

![Niels Bohr was an important figure in developing and advocating for the statistical wave-particle model in quantum mechanics. As a central architect of quantum theory, and advocate for the probabilistic interpretation of quantum mechanics, he viewed the statistical nature of quantum events as a fundamental aspect of reality. Bohr’s debates with Einstein helped clarify and popularize these concepts.](images/people/bohr-einstein.png){#fig-bohr-einstein width="60%"}

The young theorists argued that both descriptions are necessary, and neither alone is sufficient to fully describe the nature of light. Einstein was reluctant to accept this framing calling it implausible; Bohr argued that Einstein was clinging to classical intuitions about reality that quantum mechanics had shown to be inadequate. From Bohr's perspective, the probabilistic nature of quantum mechanics wasn't a sign of incomplete knowledge - it was a fundamental feature of nature at the microscopic scale. Bohr believed that concepts like "determinism" and "objective reality independent of observation" were classical prejudices that had to be abandoned. He argued that quantum mechanics was complete precisely because it recognized the fundamental role of measurement and the impossibility of simultaneously knowing all properties of a quantum system.

This duality remains a central and intriguing feature of quantum physics. If you don't find all of this a bit puzzling, you may not be paying enough attention: [Physicists disagree wildly on what quantum mechanics says about reality](https://www.nature.com/articles/d41586-025-02342-y?utm_source=Live+Audience&utm_campaign=a8d315930b-nature-briefing-daily-20250730&utm_medium=email&utm_term=0_-33f35e09ea-49658492){target=_blank}.

::: {#einstein-lenard-bohr .callout-note collapse="false" title="Einstein, Bohr, and Lenard"}
Einstein’s proposal of the photon was initially met with skepticism, as the wave theory of light was deeply established in physics. Even Einstein himself was uneasy with the resulting **wave-particle duality** and especially with the probabilistic foundations of quantum mechanics; he believed it pointed to an incomplete understanding of nature. He famously rejected this probabilistic view, remarking that “God does not play dice.” Niels Bohr responded, “Einstein, don’t tell God what to do.” The tension between wave and particle descriptions remains a central—and still [unresolved—theme in physics](https://www.instagram.com/reel/DMqt68pTSZg/?igsh=NTc4MTIwNjQ2YQ==){target=_blank}.

In 1905—Einstein’s *annus mirabilis*—he published four groundbreaking papers that reshaped modern physics. In addition to the photoelectric effect, for which he was awarded the Nobel Prize, he introduced Special Relativity, showing that **simultaneity is relative** depending on the observer’s frame of reference. The excellent biography of Einstein by Walter Isaacson [-@Isaacson2008-einstein] provides an accessible account of these discoveries and the broader scientific context. 

Isaacson also details how Lenard, whose experimental work was important to Einstein's theory became an outspoken anti-Semite. Lenard, who was awarded a Nobel Prize prior to Einstein, actively opposed Einstein’s recognition in the scientific community. As Isaacson also chronicles, Bohr and Einstein maintained a deep mutual respect and professional friendship throughout their careers.

<!--
![Lenard made measurements that defined important properties of the photoelectric effect. Einstein offered a bold theoretical idea, what we now call the photon, that explained Lenard's and Planck's measurements. Einstein, who was Jewish, had a complex personal relationship with Planck, as they navigated Hitler's rise to power. Lenard became strongly anti-Semitic.](images/people/03-Lenard-Einstein.png){#fig-lenard-einstein fig-cap-location="bottom" fig-align="center" width="35%"}
-->
 <!--
Photoelectric effect and Lenard
https://www.perplexity.ai/search/when-einstein-worked-out-the-p-_doyK0P0SPCj2lF728iVpg 
-->
:::

## Quantifying the photoelectric effect
When we model light as a wavefunction, we can only predict the probability that a photon will generate an electron. This means that even with a perfectly stable light source, the number of electrons generated in a photodiode will fluctuate from one measurement to the next. These statistical fluctuations are a key feature of how light interacts with matter and are central to quantum optics.

The number of photon absorptions in a photodiode follows the Poisson probability distribution. This statistical property was formalized in the 1950s by two groups: one studying visual sensitivity in the retina (@Pirenne1951-poisson, @Barlow1956-retinalnoise), and another investigating the physics of electromagnetic radiation and image sensors (@Mandel1959-poissonabsorptions).[^sensors-01-photoelectric-1]

[^sensors-01-photoelectric-1]: The history of these discoveries is fascinating but too detailed to cover here. For more, see [these notes](resources/Poisson-history.html).

The **mean** number of electrons generated is proportional to the light intensity: doubling the light intensity doubles the mean number of electrons (@Preece2022-photontransfer). This linear relationship holds over a wide range, until the sensor material saturates. However, because photon arrivals and electron generation are random processes, the actual number of electrons generated in each measurement fluctuates around this mean. These fluctuations are described by Poisson statistics.

Not every photon generates an electron. The ratio of generated electrons to incident photons is called the photodiode's **quantum efficiency (QE)**. QE can be measured as the slope of the line relating the number of incident photons (x-axis) to the number of generated electrons (y-axis). A QE of 1 (or 100%) means every photon generates one electron; a QE of 0.5 means, on average, half of the photons generate an electron.

$$
\mathrm{QE}(\lambda) = \frac{\text{number of electrons generated}}{\text{number of incident photons at wavelength } \lambda}
$$

In practice, QE depends on wavelength due to the sensor material and device design. Manufacturers often provide QE curves showing efficiency as a function of wavelength, and we will examine examples later.

The photodiode is part of a circuit that stores and reads out the generated charge. While this circuitry can introduce nonlinearities and additional noise (@Bohndiek2008-sensorlinearity), the initial conversion from light to electrons is linear over a large range. Understanding this linear process is fundamental to CMOS image sensor operation; understanding the full system requires further study of the sensor's electronic components, which we discuss in later sections.

```{=html}
<!--
Bohndiek2008-sensorlinearity paper has figures.  Notice how nonlinear Figures 7 and 17 are.  But earlier figures (e.g., I probably have class data with response linearity and nonlinearity.  The Thieuwissen paper has an analysis of the impact of floating diffusion.
-->
```

## Statistics of the photoelectric effect {#sec-shot-noise}

The principles underlying the Poisson distribution of electrons are straightforward. Suppose we have a **steady source** of photons, all with the same wavelength. The probability of a photon arriving in any given small time interval, $\delta t$, is constant. This is the defining property of a Poisson process: events occur independently, with a constant average rate.

If the probability that a photon generates an electron is also constant (equal to the quantum efficiency, QE), then each photon arrival is an independent chance to generate an electron. The resulting number of electrons generated in a given interval is also Poisson distributed, with a mean scaled by the quantum efficiency. This statistical fluctuation in the number of electrons is known as the **shot noise** of the sensor.

::: {#photoelectric-poisson .callout-note collapse="false" title="Siméon Denis Poisson"}

I referred to Poisson earlier - he was the French academician who tried to disprove Fresnel's wave theory calculations. He predicted [a bright point that appears at the center of a circular object's shadow](https://en.wikipedia.org/wiki/Arago_spot) based on Fresnel's theory—thinking it impossible. But experiments showed that the spot was present!

![Siméon Denis Poisson.](images/people/poisson.png){#fig-image-poisson width="40%"}

I refer to Poisson here for an entirely different reason: the famous probability distribution with his name.  He developed the mathematics for the distribution in his 1837 work "Recherches sur la probabilité des jugements en matière criminelle et en matière civile (Researches on the Probability of Judgments in Criminal and Civil Matters)". The goal of this work was to describe the number of discrete events occurring in a fixed interval of time or space, given a known constant mean rate and independence between events. His applications included jurisprudence and statistics such as the number of soldiers killed by horse kicks in the Prussian army.

Although it was developed for a completely different purpose, the mathematical framework he established applies directly to the arrival of photons from a constant light source. In imaging, each electron generation is considered an 'event', all events are identical, and the probability of an event in any small time interval is the same.
:::

The Poisson formula describes the probability of observing $n$ electron events, given a mean $\lambda$:
$$
P(n; \lambda) = \frac{\lambda^n e^{-\lambda}}{n!}
$$

Here, $\lambda$ is both the expected (mean) number of events and the variance of the distribution. For large $\lambda$ (greater than about 15 or 20), the Poisson distribution closely resembles the Normal (Gaussian) distribution (@fig-sensor-poisson). The main differences are that the Poisson distribution only takes integer values and is skewed for small $\lambda$. As $\lambda$ increases, the distribution becomes more symmetric and approaches the Normal distribution.

![Poisson distribution examples. The Poisson formula (red lines) and simulations of Poisson counts (bars) are shown for three mean levels.](images/sensors/13-photons/03-Poisson.png){#fig-sensor-poisson width="70%"}

A key fact we will use later: for a Poisson distribution, the mean and variance are equal. This property distinguishes Poisson noise from other types of noise.

:::{#shotnoise-poisson .callout-note collapse="false"}
## ISETCam: shot noise
You can see a step-by-step statistical explanation of photon arrivals followed by electron generation in the [ISETCam script](../code/03Sensor/fise_photonsElectrons.html){target=_blank}. The script demonstrates why, in conventional CMOS image sensors, both the incident photons and the resulting electrons follow Poisson statistics. 

This relationship -Poisson distribution followed by another Poisson distribution- does not apply to other sensor types.  When a photon absorption generates multiple electrons (photomultipliers and SPADs), the statistical distribution of the output requires some more thought (@sec-sensor-spad).

:::
