---
date: last-modified
---

# Human visual encoding {#sec-human-encoding}

{{< include "includes/WIP-callout.qmd" >}}

```{=html}
<!--
# The chapters with David have a lot of the material for this chapter.
-->
```

The material about human vision in this chapter is under development, and the text is not ready to be reviewed.

Please refer to the chapters in [Foundations of Vision](https://foundationsofvision.stanford.edu){target="_blank"}.

-   [Wavelength encoding](https://wandell.github.io/FOV-1995/chapter-4-wavelength-encoding.html){target="_blank"}
-   [Pattern sensitivity](https://wandell.github.io/FOV-1995/chapter-7-pattern-sensitivity.html){target="_blank"}

## Human visual encoding overview {#sec-human-encoding-overview}

Some image systems are designed mainly to acquire information about the light field (cameras) in order to make inferences about the scene. Others are designed to capture and store information so that one might later reproduce key properties of the incident light field. The reproduction of the light field is typically intended to be viewed by a human. For this reason, the design of image reproduction systems is strongly influenced by the properties of the human vision.

We know a great deal about two aspects of the peripheral human visual system. We have many measurements of how the physiological optics of the eye transforms the incident light field into the retinal irradiance, and how this irradiance is encoded by the photoreceptors. This knowledge, derived over two centuries of vision science, can be used in many practical ways to define many features of human vision, and most clear this knowledge defines the limits of human spatial resolution and color.

There are certain obvious features of the human visual system that impact the design of image systems for reproduction. The clearest case for a match between the system and the human eye is the case of color reproduction. An image system intended for reproduction and later viewing should capture the same portion of the electromagnetic spectrum that we see. Missing the part of the spectrum we see would worsen the ability to create a good color reproduction; capturing parts of the spectrum we do not see would either be wasteful or potentially interfere with creating a high quality color reproduction.

The role of spatial resolution in image systems design is different. It would surely be a disappointment if the image system failed to capture the scene at least as precisely as the spatial resolution of human eye. We see the scene, and we hope the camera to do at least as well. But people are also pleased if the camera captures the scene at a finer resolution than they could see at the time. It is always possible to reduce the spatial resolution by blurring and downsampling. The increased spatial resolution will not interfere with the image rendering; it will just take more resources. Thus, understanding the wavelength encoding sets a maximum encoding rate in the visible range; capturing more would have little value. Understanding the spatial sampling sets a minimum encoding rate; capturing less information would be disappointing.

In this section, we review the anatomical and functional properties of the eye that limit how it encodes the lightfield. The properties of the eye do not account for what we see, but they do impose critical limits on what we cannot see. The way we encode wavelength and sample space within the eye represents an inescapable bottleneck on visual resolution and sensitivity. This makes measurements of the visual encoding an important part of the image systems engineering. In the next chapter, @sec-human-metrics-overview, I introduce various image quality metrics that are informed by these measurements. A more extensive description of the human visual encoding, including its biological substrate, is in @wandellFoundationsVision1995.

## The Encoding Systems {#sec-human-encoding-systems}

The eye includes two distinct functional systems [@fig-retina-color]. The first system is the physiological optics (cornea and lens). This system converts the incident light field into the optical light field, forming an image at the retina. The second system is the retina. This is a complex system that serves multiple functions. For the purpose of this chapter, the key function is that the arrays of light sensitive cells in the retina (principally cones and rods) convert the optical light field into a neural signal. This signals from the photoreceptor arrays is processed by multiple types of neuronal arrays within the retina. The output cells of the retina are the retinal ganlgion cells (RGCs). Their axons join together and form the optic nerve, which exits each eye. The axons in the optic nerve terminate in multiple locations in the brain @wandell2025-visual-processing.

The retina is plenty complicated - but one wonderful simplification is that there is no neural feedback from the brain to the retina. This means we can characterize the inputs (photoreceptors) and outputs (RGCs) without having to model signals from other parts of the nervous system. This is rarely true for other brain regions.

There is one important type of feedback: The brain controls body, head and eye movements. These movements guide which parts of the environmental light field we acquire. These movements are an extremely important part of how we see. But to understand what the retina itself does, we can fix the eye and head to eliminate these signals. In this way we can characterize just the pathway from the photoreceptors to the signals on the optic nerve.

## Physiological optics

The cornea and flexible lens, together, control the transformation of the incident light field to the optical light field. Even for a fixed head and eye position, the physiological optics are dynamic: both the size of the entrance pupil and the shape of the lens vary. The pupil size changes mainly in response to the general light level, opening to about 7 mm diameter and closing to about 2mm. This produces a factor of about 10 in how much light enters the eye, which is a rather small factor compared to the mean level of natural illuminations which span 4 or 5 log units. The pupil size is not controlled willfully; it changes depending on the ambient light conditions and as well as some non-visual events in the brain.

Maybe a tabset for these two images

::: panel-tabset
## Eyeball image

![Image of the physiological optics](images/human/02-encoding/eyeball-image.png){#fig-eyeball-image width="50%"}

## Eyeball sketch

![Schematic of the physiological optics](images/human/02-encoding/eyeball-sketch.png){#fig-eyeball-sketch width="50%"}
:::

<figcaption>The two panels illustrate the physiological optics and their relationship to other important components of the eye. See the text for details.
</figcaption>

By the varying shape of the lens, and thus its optical power, the eye adjusts which image plane is brought into focus on the retina. This process, called  **accommodation**, is based on neural signals sent to the ciliary muscles that are attached to the lens.  The signals cause the muscles to pull on the lens to make it thinner and thus focus on objects at a distance, or relax which causes the lens to become thicker lens and focus on nearby objects. As I recall -from a younger age- this is a wonderful system. Unfortunately, lens flexibility is lost with age - for everybody. The gradual loss of your eyes' ability to focus on nearby objects is called *presbyopia*. It's a natural, and somewhat annoying, part of aging. Presbyopia usually becomes noticeable in your early to mid-40s and continues to worsen until accommodation is lost. To compensate for this reduced flexibility, we use reading glasses. It will happen to you.

<!-- 
https://commons.wikimedia.org/wiki/File:3D_Medical_Animation_Eye_Structure.jpg 
https://www.scientificanimations.com, CC BY-SA 4.0 <https://creativecommons.org/licenses/by-sa/4.0>, via Wikimedia Commons
-->

## Retina
We now have many measurements of the quality of the physiological optics. In the sections below, I will summarize these measurements and explain how to simulate how the incident light field is transformed into the retinal image by ISETBio.

Relationship between the physiological optics and the retina.

::: panel-tabset
## Eye and retina 1
![Human physiological optics. Source:https://www.scientificanimations.com via Wikimedia.](images/human/02-encoding/2048px-3D_Medical_Animation_Eye_Structure.jpg){#fig-human-eyeball width="60%"}

## Eye and retina 2
![Overview of the eye.](images/human/02-encoding/retina-fovea-color.png){#fig-retina-color width="100%" fig-align="center" width="75%"}
:::
<p>
<figcaption> Relationship between the eye and retina. </figcaption>
</p>

The retina is a 150-350 micron thick layered structure of neural tissue that lines the back of the eye (5 cm x 5 cm). There are about 80 different cell types that can be identified through their genetic expression. Specific retinal cell types form stereotypical connections that make up specific, identifiable circuits. There are about 20 known circuits, and these are likely to account for nearly all of the circuits in the retina. The output signals from these circuits are carried on axons in the optic nerve that project to a variety of locations in the brain.

The vast majority of light-driven activity is initiated in the photoreceptors (rods and cones). The rods are the dominant source under very low light levels, and the cones are the dominant source under moderate to high light levels. The typical retinal circuit is driven by activity that starts in a local region of the photoreceptors. The same basic circuit will be present throughout the retina, tiling the photoreceptor mosaic, though the absolute size of the cells and their input regions generally vary as one measures across the retina. The size of the region increases as one measures from the highly specialized central fovea into the periphery.

::: panel-tabset

### Retinal layers
![Layers from EJ](images/human/02-encoding/retina-ej.png){#fig-retina-ej width="70%"}

### Five retinal cell types
![Cell types from Rodieck](images/human/02-encoding/retina-rodieck.png){#fig-retina-rodieck width="70%"}

### Peripheral retinal layers
![Layers from Massey - phase contrast microscopy image.](images/human/02-encoding/retina-massey.png){width="90%"}

### Fovea 
![Fovea in detail. GCL: Ganglion cell layer.  BPL:  Bipolar layer, also called inner nuclear layer. REC: Receptors.  PE:  Pigment epithelium.](images/human/02-encoding/retina-fovea.png){#fig-retina-fovea width="40%"}

:::
<p>
<figcaption> A variety of ways of describing the retina. </figcaption>
</p>

The short wavelength light is focused at the RGC layer when the middle wavelengths are in good focus at the inner segment.

## Pointspread functions

We need the retina and its sampling mosaic to make sense of the point spread function, IMHO.

Where should the adaptive optics measurement method be? Here or FOV? Adaptive optics measurements of wavefront aberrations, expressed as point spread functions. Thibos and Artal data.

![S-cone sampling iamge frim Wikler and Rakic, 1990. Maybe we add the one from Stan Schein in a tabset.](images/human/02-encoding/s-cone-sampling.png){#fig-scone-sampling width="60%"}

### Adaptive optics {#sec-adaptive-optics}

The ability to measure certain properties of the optics was revolutionized by adpative optics - a method that makes real time measurements of the wavefront aberrations of the physiological optics using a Shack-Hartmann wavefront sensor. Just the measurement of the aberrations alone is useful. In addition, it has proben possible to engineer systems that correct for these aberrations in real time, and thus visualize the apertures of the photoreceptors (1-5 $\mu \text{m}$ diameter) cells in the retina. Further, using video tracking of the eye movements it has proven possible to deliver stimuli to specific, targeted photoreceptors.

First order approximations of the human optics. Simulations with ISETBio of maybe the Westheimer or Ijspeert functions. Eye models?

### Field height dependence

Artal data. Point to ISETBio and maybe the other book.

### Ground-based telescopes {#sec-telescopes}

These same technologies can be used to look inward, within the eye, rather than outward toward the stars. Realizing this was a very important insight that continues to lead to important insights about the peripheral human visual system (@sec-human-seeing-overview).

::: {.callout-note title="Shack-Hartmann wavefront sensor"}
### Shack–Hartmann wavefront sensor {#sec-wavefront-sensor}

Johannes Franz Hartmann was a German astronomer working on the problem of detecting optical defects in telescope objectives. Reasoning from the ray theory of light, he measured the optical light field from a simple stimulus: an on-axis point at the focal length of the lens. In principle, the rays emerging from an ideal lens would all be parallel to the main axis. To look for deviations, he created a screen comprising an array of pinholes. If the rays were parallel, the light through the pinhole would be nicely centered behind the pinhole. If the rays were not quite parallel, the image behind the pinhole would be displaced by some distance and amount from the center. The deviations could be considered defects in the lens that could be corrected.

As I observed above, the image behind each pinhole measures the intensity of the rays incident at different angles. But in practice, a small pinhole produces a blurry spot due to diffraction. Thus Hartmann did not try to extract an image but he just estimated the central position of the blurry spot. Also, because the pinholes only let through a small amount of light, the technique was not very sensitive.

Because there is an array of these pinholes, the accumulation of pinhole images measures the rays at multiple angles at multiple positions. In principle, by performing this measurement for different wavelengths and polarizations, we sample the optical light field (@eq-lightfield-optical).

```{=html}
<!--
The Raskar version in which you put spots behind the lenslet array and adjust the spots to make the array uniform is kind of funny.  It is intended to measure the optical aberrations of the eye
https://en.wikipedia.org/wiki/Shack%E2%80%93Hartmann_wavefront_sensor
-->
```

```{=html}
<!--
# What about the color matching experiment? Maybe James Clerk Maxwell mentioned but referred to FOV 
-->
```

[Arizona History of the Shack work.](https://www.optics.arizona.edu/sites/default/files/2022-07/Historical-Development-Shack-Hartman-Wavefront-Sensor.pdf?utm_source=chatgpt.com)
:::

## Photoreceptor spatial sampling

Maybe point mainly to the other book. Image showing the fovea and inhomogeneous sampling of the photoreceptor mosaics. Maybe Curcio. Maybe the S-cone image sampling mosaic. Adaptive optics measurements of retinal sampling Rods, ipRGCs, too in here. Other book - ISETBio: Examples of simulation. David also has these new measurements.

## Photoreceptor wavelength encoding

The neural tissue of the retina lines the curved surface at the back of the eye. The retina is about 5 x 5 cm, and its properties vary considerably across space. The retinal image is converted into a neural signal by a special class of neurons, the photoreceptors. The retina contains many types of specialized cells that combine into stereotypical local circuits. Multiple copies of these local circuits, but usually with different parameters, are present throughout the retina. The circuit outputs leave the retina via the optic nerve. One of the ways we can identify these circuits is that their outputs are transmitted to multiple brain regions. The circuit properties are dynamic in the sense that the circuit response to a simple stimulus, say a small flashed spot, varies with changes in the mean and variance of the retinal image.

There are some similarities between the eye and a conventional camera. For example, the optics and light sensing components are integrated into a single package, and the components are adaptive with respect to light level and focus. But there are lots of differences, as well. One major difference is that the retinal encoding does not sample the image uniformly; it is very inhomogeneous compared to modern cameras. The central human retina is specialized for the cone photoreceptors, which have small (\~ 1.5 um) apertures and are tightly packed. The size of the photoreceptor apertures increases significantly from fovea to periphery, and the receptors for nighttime vision (rods) are inserted between the cones, further increasing their center-to-center spacing. This spatial sampling is quite unlike a typical image sensor whose pixels are all of the same size. Finally, the human visual system relies on eye movements to bring regions of interest into focus at the fovea. The keep integration of eye movements is very important for such a spatially inhomogeneous system.

I will describe measurements of the optics and encoding in several sections below. These are selected to support calculations of image quality and engineering design. For more about the biology and the scientific methods used to derive these measurements, consult [@wandellFoundationsVision2024]

## Wavelength encoding

```{=html}
<!--
https://chatgpt.com/s/t_68af5af69c688191b21118844094d0b5

By Henry Brougham in Edinburgh Review, an ardent admirer of Newton, criticizing Thomas Young’s Bakerian Lectures.

>  ...this paper contains nothing which deserves the name, either of experiment or of discovery, and as it is, in fact, destitute of every species of merit. We now dismiss, for the present, the feeble lucubrations of this author, in which we have searched without success for some traces of learning, acuteness, and ingenuity, that might compensate his evident deficiency in the powers of solid thinking, calm and patient investigation … 

Thomas Young wrote a reply (@young1804-reply)

 > A MAN who has a proper regard for the dignity of his own character, although his sensibility may sometimes be awakened by the unjust attacks of interested malevolence, will esteem it ingeneral more advisable to bear, in silence, the temporary effects of a short lived injury, than to suffer his own pursuits to be interrupted, in making an effort to repel the invective, and to punish the aggressor. 
-->
```

### Color-matching

### Chromatic aberrations

Wavelength dependence (chromatic aberration)