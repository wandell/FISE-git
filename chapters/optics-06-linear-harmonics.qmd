# Linear optics - harmonics {#sec-optics-linear-harmonics}
Until now, we have analyzed optical systems using point-based stimuli (impulses and point spread functions). As discussed in @sec-appendix-linear-systems, this is only one of many useful signal representations. In this section, we introduce an alternative: image harmonics. These representations are common in the image systems engineering literature for both theory and measurement because they provide a valuable approach to measuring how many systems behave.

There are two practical motivations. First, while the point spread function is fundamental, it is not always the most convenient tool for predicting how finely a system can resolve detail; spatial resolution is often easier to analyze using harmonics. Second, point stimuli are photon-inefficient and difficult to measure precisely. Extended harmonic targets deliver more light, produce higher signal-to-noise measurements, and allow precise control of spatial frequency, orientation, and phase.

The next sections define image harmonics and show how to use them to characterize system performance. Much of the mathematical foundations underpinning this representation appear in @sec-appendix-linear-systems. In brief: for linear, shift-invariant systems, harmonics form a spatial basis for arbitrary stimuli (@sec-ls-spatial-basis) and are nearly eigenfunctions of the system (@sec-ls-eigenfunctions). These properties make harmonics especially valuable for analysis, design.

## Image harmonics {#sec-optics-harmonics}
Harmonics are a convenient way to model and measure imaging systems. For real-valued signals—such as light intensity—the basic building blocks are the sine and cosine functions. You likely first met these as one-dimensional functions: they take a single input and produce a single output.

$$
I(x) = \sin(2\pi f_x x + \phi_x)
$${#eq-harmonics-1d}

Here, $f_x$ is the spatial frequency (in cycles per unit distance) and $\phi_x$ is the phase (a horizontal shift).

In imaging, we typically need a two-dimensional representation to describe variation across both $x$ and $y$. A useful form that keeps intensities non-negative is

$$
I(x,y) = M\big(1 + A \sin(2\pi f_x x + \phi_x + 2\pi f_y y + \phi_y)\big).
$${#eq-harmonics-2d}

- $M$ is the mean (DC) intensity.
- $A$ is the contrast or modulation depth. To ensure $I(x,y) \ge 0$, we require $|A| \le 1$. In practice, we often take $A \in [0,1]$ and absorb any sign into the phase.
- The pair $(f_x, f_y)$ is the 2D spatial frequency. When both are nonzero, the pattern has an orientation; the bars are perpendicular to the vector $(f_x, f_y)$, with angle $\theta = \mathrm{atan2}(f_y, f_x)$.

@fig-optics-harmonics-2d shows four image harmonics. When $f_y = 0$ the pattern is vertical (varies only along $x$); when $f_x = 0$ it is horizontal (varies only along $y$).[^spatial-frequency]

[^spatial-frequency]: Different fields use different terminology. Vision scientists often call these spatial frequency gratings. Engineers sometimes approximate sinusoidal patterns with evenly spaced bars or lines. We use the term harmonics.

![Harmonic images. These have different $f_x$ and $f_y$ values, but all have contrast $A=1$. The image is vertical if $f_y = 0$ and horizontal if $f_x = 0$.](images/optics/optics-harmonics.png){#fig-optics-harmonics-2d width="80%"}

For many linear-systems principles, it is enough to consider one-dimensional harmonics by fixing $f_y = 0$. The examples in @fig-optics-harmonics-1d vary only along $x$ and illustrate different spatial frequencies $f_x$ and contrasts $A$.

![A set of one-dimensional image harmonics. They vary in $x$ with $f_y = 0$. The contrast $A$ varies from top to bottom $(1, 0.5, 0.1)$. The $x$-range is $(0,1)$, and the spatial frequencies are $f_x = (1, 2, 4, 8)$.](images/optics/optics-harmonics-1d.png){#fig-optics-harmonics-1d width="60%"}

## Harmonics and lines {#eq-optics-harmonics-lines}
To many people it is counterintuitive that a thin line can be constructed by summing harmonics, each of which extends across the entire image. How does the combination of these broad functions result in a localized feature like a line? 

@fig-optics-reconstruction provides insight into how this happen. The figure shows how the image evolves as we sum increasing numbers of harmonics: starting with just a few, the result is broad and diffuse, but as more harmonics are added, the image becomes increasingly concentrated. The key is that the positive and negative regions of the harmonics cancel each other out everywhere except at the center, where all the cosine terms are positive ($\cos(0)$). With enough harmonics, the sum converges to a single, sharp line.

In this example, we reconstruct a line at the center (column 65) of a 129-column image by setting all harmonic weights to $w_f = 1$. Because the target image is even symmetric, only the cosine terms are needed. The red traces in the figure show the intensity profile: as more harmonics are added, the central peak narrows and side ripples diminish. By the 64th harmonic, the line is well-formed; adding more harmonics briefly reintroduces some ringing, which disappears when the full set is included. This phenomenon is a classic result in linear systems—ask your instructor for more details!

![Summing harmonics to become a line. The panels show the sum of the first 4, 16, 64, 72, and finally all 128 harmonics. To match a single line in the middle (col=65) of this 129 column image, the weights of each harmonic are all $w_f = 1$. Because the image is even symmetric, we need to sum only the $cos()$ terms. The red traces superimposed on the images are the intensity. The central region becomes thinner and the ripples on the side are eliminated by the 64th harmonic.  They return as we keep going to f = 72, but are eliminated again at f=128.  Ask your linear systems instructor about this; s/he will like you for it.](images/optics/optics-reconstruction.png){#fig-optics-reconstruction width="100%"}

::: {.callout-note collapse="false" title="Line reconstruction: the movie"}
This movie illustrates how the sum converges as harmonics are added one by one. Initially, the harmonics are distributed across the image. As more are included, their contributions cancel everywhere except at the central position ($0$), where all are positive ($\cos(0)$). 

Adding harmonics from $f=0$ to $f=64$ produces the line; adding $f=65$ to $f=128$ introduces some ringing, which is eliminated when the sum is complete. This is a special case for even symmetric functions, where only cosines are required.

![A line reconstructed by adding many harmonics at different frequencies. ](images/optics/lineReconstruct.mp4){#fig-optics-reconstruction-movie width="60%" fig-align="center"}

:::

## Harmonic characterization {#sec-optics-characterization}

There are many reasons why people would like to describe the quality of an optical system.  Commercial vendors that sell parts may need to specify the quality and the tolerance of the quality measure.  We may want to optimize the properties of a design, using the image quality measure as a metric for the optimization.  We may wish to simulate the system and learn which signals are preserved by the optics and which are lost to image quality.

If the point spread function can be quantified using a few parameters, such as the Airy Pattern, it can be helpful in system characterization.  But more often than not, the point spread function is an image that has not natural parameterization to quantify system performance.

The harmonic functions, on the other hand, have a natural set of parameters, and the different frequencies and orientations have a simple intuitive interpretation.  This has led to their widespread use in image quality characterization.

## Optical Transfer Function {#sec-optical-transfer-function}

## Modulation Transfer {#sec-modulation-transfer-function}

## Contrast sensitivity {#sec-contrast-sensitivity-function}

## Local shift-invariance 

Say stuff about how they apply to isoplanatic regions (shift-invariant) but not to the image as a whole. 

The point spread depends on parameters such as the pupil aperture, which often changes as people adjust for light level.  Hence, it is not always possible to have a unique point spread for a point.

It is tough, by the way, to have depth dependent point spreads and to interpolate between point spreads.

What else?

## More examples

Slides from class for inspiration here. Define the MTF and the OTF.
Start an image of the 2D FFT amplitude, perhaps.  

Get to image representations of the harmonic images, and then images more broadly.

Parts of the Fourier representation (center, edges, and so forth)

High frequency fall off.

Develop the ISO 12233 standard, relating edges and MTF50

Illustrate aliasing for the whole system?  Even if we aren't through the image sensor?

Separability for 2D might be good.  Not sure which formulae to add.


:::{.callout-note title='2D Fourier Transforms' collapse='false'}
Bracewell and Goodman for introducing the 2D Fourier transform for image applications.

[Link to Google resource.](resources/history-2D-fourier-transform.html){target=_blank}

:::






