<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>sensors-02-pixels</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">




<section id="sec-pixels" class="level1">
<h1>Pixels and sensors</h1>
<p>Having established the fundamental physics of light-to-electron conversion, we now examine how these principles are implemented in the design of image sensor pixels. To understand how the photoelectric effect is harnessed in practical devices, we next review the behavior of electrons in semiconductors—the foundation of modern image sensor pixels.</p>
<section id="sec-semiconductor-physics" class="level2">
<h2 class="anchored" data-anchor-id="sec-semiconductor-physics">Semiconductors and electrons</h2>
<p>Atoms in a silicon semiconductor are arranged in a crystalline structure, and their electrons occupy specific energy levels grouped into two main bands: the <strong>valence band</strong> (lower energy) and the <strong>conduction band</strong> (higher energy). Under normal conditions, most electrons remain in the valence band. When a silicon atom absorbs a photon, an electron can be excited from the valence band to the conduction band (<a href="#fig-sensor-bandgap" class="quarto-xref">Figure&nbsp;1</a>).</p>
<!--
A particularly clear explanation with figures and a video.
https://scientificimaging.com/knowledge-base/photoelectric-effect/ 
-->
<section id="sec-bandgap-photon-absorption" class="level3">
<h3 class="anchored" data-anchor-id="sec-bandgap-photon-absorption">Bandgap</h3>
<p>The minimum energy required for this transition is called the <strong>bandgap</strong>, and it is measured in electron volts (eV). For silicon at room temperature (<span class="math inline">\(21 \deg C\)</span>), the bandgap is 1.12 eV. Only photons with energy equal to or greater than this value can generate free electrons in silicon.</p>
<div id="fig-sensor-bandgap" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sensor-bandgap-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/sensors/03-sensor-bandgap.png" class="img-fluid figure-img" style="width:50.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sensor-bandgap-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Energy bands in silicon semiconductors. A photon can excite an electron in the Valence band. If the photon has enough energy, the electron can shift to the conduction band, becoming a free electron that can be stored and counted.
</figcaption>
</figure>
</div>
<p>Without special circuitry, the hole and electron will re-combine. To make an image sensor, we need a method to measure the number of electrons. In a CCD an electric field between the bands prevents recombination. In CMOS electrons are trapped in capacitors placed in the silicon. We explain how this is done in <a href="#sec-pixels" class="quarto-xref">Section&nbsp;1</a>.</p>
</section>
<section id="sec-wavelength-effects" class="level3">
<h3 class="anchored" data-anchor-id="sec-wavelength-effects">Wavelength effects in silicon sensors</h3>
<p>There are several wavelength-dependent effects worth noting about silicon sensors. First, we can calculate the energy in Joules of a photon of wavelength <span class="math inline">\(\lambda\)</span> using this the photon energy formula:</p>
<p>We can convert from Joules (J) to electron volts (eV) with this formula</p>
<p><span id="eq-joules-ev"><span class="math display">\[
1~\text{eV} = {1.602 \times 10^{-19}}~\text{J}
\tag{1}\]</span></span></p>
<p>A wavelength of 1100 nm has just enough energy (1.12 eV) to transition an electron across the bandgap: Thus, silicon sensors are insensitive to electromagnetic radiation beyond 1100 nm and infrared sensors must use a different material. Second, photons with wavelengths less than 1100 nm have enough energy to excite electrons across the bandgap, and silicon will absorb radiation at these shorter wavelengths. It is notable that the eye does not encode wavelengths below about 400 nm.</p>
<p>Third, the chance that a photon excites an electron that crosses the bandgrap depends on the photon’s energy level. Higher energy (shorter wavelength) photons are more likely to cause the electron to change bands. For this reason, different wavelengths are absorbed at different spatial depths in the material. Short-wavelength photons are much more likely to generate electrons near the surface compared to long-wavelength photons. Conversely, long-wavelength photons are relatively more likely to generate electrons deeper in the material. This depth-dependence on wavelength has been used to create color image sensors (<strong>?@sec-sensor-foveon</strong>).</p>
</section>
</section>
<section id="sec-cmos-pixel-fsi" class="level2">
<h2 class="anchored" data-anchor-id="sec-cmos-pixel-fsi">CMOS Pixel: Front-side illuminated</h2>
<p>CMOS image sensors use photodiodes—and the photoelectric effect—to measure and store the optical light field at the sensor surface. The key technical breakthrough that enabled CMOS sensors was the invention of circuitry to store and then transmit the photon-generated electrons from the photodiode to the computer.</p>
<p>The photodiode and this circuitry are critical parts of the pixel; there are other essential components as well (<a href="#fig-pixel-overview" class="quarto-xref">Figure&nbsp;2</a>). For example, effective image sensors place a small lens (microlens) above each pixel. Color image sensors place a color filter between the microlens and the photodiode that passes certain wavelengths and blocks others.</p>
<div id="fig-pixel-overview" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pixel-overview-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/sensors/03-pixel-overview.png" class="img-fluid figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pixel-overview-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Schematic of the early CMOS pixel architecture. This image shows a microlens and color filter, which are typically part of the pixel. The microlens is centered over the pixel here, but for pixels at the edge of the sensor array it will be positioned a bit to the side. This color filter passes long wavelength (red-appearing) light, but different pixels will have different color filters. The first layer of silicon, with its photodiode and classic circuitry sharing space, are shown. Source: <a href="https://evidentscientific.com/en/microscope-resource/knowledge-hub/digital-imaging/cmosimagesensors" target="_blank">Evident Scientific.</a>
</figcaption>
</figure>
</div>
<p>Figure <a href="#fig-pixel-overview" class="quarto-xref">Figure&nbsp;2</a> is a schematic that omits many details. For example, notice the gap between the photodiode and the color filter. In early CMOS imagers, this gap contained many metal lines that carried control and data signals. The metal lines were organized into layers and they are illustrated in Figure <a href="#fig-pixel-layers" class="quarto-xref">Figure&nbsp;3</a>. In the first generation of image sensors the distance from the microlens at the top to the photodiode at the bottom was fairly large compared to the size of the photodiode. In these early pixels, light from the main lens had to travel through what was effectively a tunnel to reach the photodiode.</p>
<div id="fig-pixel-layers" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pixel-layers-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/sensors/03-pixel-overview2.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pixel-layers-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Color imagers use an array of color filters to create pixels with different wavelength selectivity. The color filters are typically arranged in a repeating pattern, and the smallest repeating unit of the color filter array is called a <strong>super pixel</strong>. The image illustrates a super pixel for a sensor with cyan, magenta, and yellow color filters. This image emphasizes the metal lines, omitted in <a href="#fig-pixel-overview" class="quarto-xref">Figure&nbsp;2</a>. In the original CMOS imagers, light from the main imaging lens had to pass through these metal layers before reaching the photodiode.
</figcaption>
</figure>
</div>
<!-- fig-pixel-layers:  not sure where this image comes from -->
<p>Pixel architecture has evolved significantly over the years, and there continue to be many novel designs. The most straightforward change is that technology has scaled, enabling pixel sizes to shrink and thus increasing spatial sampling resolution. In addition, the placement of the circuitry and metal lines has changed to below the photodiode; light no longer passes through a deep tunnel. There are other improvements to the circuitry, some experimental, that we will review later in <strong>?@sec-sensor-innovations</strong>. It is helpful to first understand the concepts behind the classic circuitry, so that we can better appreciate these innovations.</p>
<p>The logical flow of the circuitry is as follows:</p>
<ul>
<li>Before image capture, the sensor circuitry resets each pixel, clearing residual charge from previous exposures.</li>
<li>During image acquisition, the photodiode array collects electrons generated by incoming light; these electrons are stored within the photodiode (3T) or in a nearby capacitor (4T).</li>
<li>During readout, the circuitry transfers the stored charge to an analog-to-digital converter (ADC).</li>
<li>The digital image array records the amount of light captured by each pixel.</li>
</ul>
<p>The role of the other essential pixel components, the color filter arrays and microlenses, is explained in Section <strong>?@sec-sensor-components</strong>. In <strong>?@sec-system-modeling</strong> I describes how to calibrate and model sensor performance, from the scene light field to image capture.</p>
</section>
<section id="sec-sensor-circuits" class="level2">
<h2 class="anchored" data-anchor-id="sec-sensor-circuits">CMOS circuits</h2>
<section id="sec-3T-pixel-design" class="level3">
<h3 class="anchored" data-anchor-id="sec-3T-pixel-design">Three-Transistor (3T) Pixel Design</h3>
<div id="fig-sensor-3T-circuit" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sensor-3T-circuit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/sensors/03-3T-circuit.png" class="img-fluid figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sensor-3T-circuit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: The original CMOS 3-transistor (3T) circuit design (<span class="citation" data-cites="Fossum1997-early">@Fossum1997-early</span>).
</figcaption>
</figure>
</div>
<p>The original CMOS pixel design uses a three-transistor (3T) circuit to store and read out the charge collected by the photodiode (<a href="#fig-sensor-3T-circuit" class="quarto-xref">Figure&nbsp;4</a>). Each pixel contains three key transistors: <span class="math inline">\(M_{rst}\)</span> (reset), <span class="math inline">\(M_{sel}\)</span> (select), and <span class="math inline">\(M_{sf}\)</span> (source follower). The <span class="math inline">\(M_{rst}\)</span> transistor resets the photodiode by connecting it to the supply voltage (Vdd), clearing any residual charge before image capture. The <span class="math inline">\(M_{sel}\)</span> transistor selects a specific row of pixels, connecting them to the readout circuitry. The <span class="math inline">\(M_{sf}\)</span> transistor acts as a buffer, transferring the stored charge to the analog-to-digital converter (ADC) for measurement. In most sensors, each column has its own ADC, but some designs use a single ADC shared among multiple columns—a technique known as multiplexing (mux).</p>
<p>Ideally, the photodiode’s response to light is linear, with the number of generated electrons following Poisson statistics. However, the surrounding circuitry introduces additional sources of noise and nonlinearity. For example, the storage capacitor has a limited capacity, so it can saturate at high light levels. The transistors used for readout can also introduce noise and small nonlinearities. Furthermore, variations in pixel properties across the sensor array can cause fixed-pattern noise. In modern CMOS sensors, these nonlinearities and noise sources are typically small—on the order of a few percent <span class="citation" data-cites="Wang2017-CMOSLinearity">[@Wang2017-CMOSLinearity]</span> -but they can still affect image quality. Careful characterization and calibration are necessary to minimize these effects and produce high-quality images.</p>
<!-- 
 (http://encyclopedia2.thefreedictionary.com/Vdd also the drain voltage), Vrst is the reset voltage Vee is the negative supply voltage Ground is ground
http://en.wikipedia.org/wiki/Active_pixel_sensor , http://encyclobeamia.solarbotics.net/articles/vxx.html , 
http://en.wikipedia.org/wiki/Active_pixel_sensor , http://encyclobeamia.solarbotics.net/articles/vxx.html , 
Vdd is positive supply voltage  (http://encyclopedia2.thefreedictionary.com/Vdd also the drain voltage), Vrst is the reset voltage Vee is the negative supply voltage Ground is ground
-->
</section>
<section id="sec-4T-pixel-design" class="level3">
<h3 class="anchored" data-anchor-id="sec-4T-pixel-design">Four-Transistor (4T) pixel design</h3>
<div id="fig-sensor-4T-circuit" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sensor-4T-circuit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/sensors/03-4T-circuit.png" class="img-fluid figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sensor-4T-circuit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: The 4T pixel circuit introduces a pinned photodiode and an additional transistor. When activated, the new transistor transfers the accumulated charge from the photodiode to a storage capacitor (the floating diffusion node, <span class="math inline">\(C_{fd}\)</span>). The rest of the circuit, including readout and row select, is similar to the 3T design.
</figcaption>
</figure>
</div>
<p>Since about 2000, most CMOS image sensors have used a four-transistor (4T) pixel circuit <span class="citation" data-cites="Fossum2014-pinnedpd">[@Fossum2014-pinnedpd]</span>. The 4T pixel adds two important features: a pinned photodiode (PPD) and a transfer gate transistor (<a href="#fig-sensor-4T-circuit" class="quarto-xref">Figure&nbsp;5</a>). The pinned photodiode improves charge storage and reduces noise, while the transfer gate allows precise movement of the collected charge from the photodiode to a storage node called the floating diffusion, which has a capacitance <span class="math inline">\(C_{fd}\)</span>. The other circuit elements—reset, row select, and source follower—remain the same as in the 3T design.</p>
<p>The transfer gate and floating diffusion give the circuit better control over when and how charge is transferred and measured, which helps reduce noise and improve image quality.</p>
<p>Today, the 4T pixel is the standard in almost all high-performance CMOS image sensors, including those found in smartphones, industrial cameras, cars, and scientific instruments.</p>
<!--
There is also Theuwissen diagram, with five transistors that he calls a 4T circuit.  In the paper 'Linearity analysis ...' @Wang2017-CMOSLinearity.  He says the source follower is shared by multiple rows of pixels. It is Figure 2.  

The text: "Figure 2 shows the schematic of a typical voltage mode 4T pixel. The pixel circuit consists of a pinned photodiode, a charge
transfer switch (M1), a reset switch (M2), a source follower (M3) and a row select switch (M4). The current source transistor (M5) is shared by multiple rows of pixels. VDD is the power supply while VPIX is the output voltage of pixel. VLN provides an adjustable bias current. In the following analysis, the nonlinearity caused by the row select transistor (M4) is omitted to simplify the analysis."
-->
</section>
</section>
<section id="sec-sensor-evolution" class="level2">
<h2 class="anchored" data-anchor-id="sec-sensor-evolution">Sensor evolution</h2>
<p>There has been a large engineering effort to reduce various sources of noise, including temporal and fixed pattern noise, as well as other limitations in the sensor design relating to color and dynamic range. The engineering effort was supported by the enormous demand for CMOS sensors. In 2020, approximately 6.5 billion sensors were shipped. In 2025, we expect about 13 billion sensors to be produced. That’s a lot of sensors and the demand motivates a lot of engineering. This section covers the evolution of the pixel and sensor design.</p>
<p>The sensor architecture I have described is known as <strong>front-side illumination (FSI)</strong>. The architecture has the obvious challenge of placing multiple metal layers above the semiconductor substrate (<a href="#fig-pixel-layers" class="quarto-xref">Figure&nbsp;3</a>). Even at the time of the original implementation, it was clear that requiring light to pass through the metal layers to reach the light-sensitive photodiode created many problems.</p>
<p>Additionally, both the photodiodes and their associated circuitry share the same substrate. Requiring the circuitry and photodiode to share space reduced the sensitivity the sensor. Finally, the original pixels were fairly large, <span class="math inline">\(6 \mu \text{m}\)</span> or more. As technology scaled and pixels became smaller -many CMOS imagers have pixels as small as <span class="math inline">\(0.8 \mu \text{m}\)</span> a reduction in area of <span class="math inline">\(50x\)</span>- additional problems arose. Different opportunities arose as well.</p>
<p>These challenges, and the huge success of CMOS sensors, were met by a massive, world-wide, engineering effort to improve the system. The evolution of image sensor pixel technology that came from this effort enabled dramatic improvements in image quality, sensitivity, and device functionality. The following sections describe the key milestones that were achieved through academic and industry partnerships. For an authoritative review, consult <span class="citation" data-cites="Oike2022-stackedevolution">@Oike2022-stackedevolution</span>.</p>
<!--# https://www.pcmag.com/how-to/whats-the-difference-between-cmos-bsi-cmos-and-stacked-cmos 
https://news.skhynix.com/evolution-of-pixel-technology-in-cmos-image-sensor
-->
<div class="callout callout-style-default callout-note callout-titled" title="Pixel Evolution timeline">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Pixel Evolution timeline
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<!-- See Research doc 'The Genesis and Evolution of Stacked Pixel Structures ....' -->
<p>Here is a timeline for the key milestones described in the sections below.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 29%">
<col style="width: 19%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>Technology</th>
<th>Commercialization</th>
<th>Key Benefit</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Front-Side Illumination (FSI)</strong></td>
<td>1990s–2000s</td>
<td>Simplicity, but limited by wiring obstruction</td>
</tr>
<tr class="even">
<td><strong>Back-Side Illumination (BSI)</strong></td>
<td>~2007–2010</td>
<td>Higher QE, better low-light, smaller pixels</td>
</tr>
<tr class="odd">
<td><strong>Deep Trench Isolation (DTI)</strong></td>
<td>~2010s</td>
<td>Reduced crosstalk, higher pixel density</td>
</tr>
<tr class="even">
<td><strong>Deep Photodiode</strong></td>
<td>~2015+</td>
<td>Higher sensitivity, dynamic range</td>
</tr>
<tr class="odd">
<td><strong>Stacked Sensor</strong></td>
<td>~2015+</td>
<td>Advanced features, compact design</td>
</tr>
</tbody>
</table>
<!--
- **Front-Side Illumination (FSI):** Early standard; light passes through wiring before reaching the photodiode.
- **Back-Side Illumination (BSI):** Introduced ~2010; light enters from the back, improving efficiency for small pixels.
- **Deep Trench Isolation (DTI):** Early 2010s; insulating trenches reduce crosstalk between pixels.
- **Deep Photodiode:** Mid-2010s; deeper photodiodes increase sensitivity and dynamic range.
- **Stacked Sensor:** Mid-2010s onward; sensor and circuitry are stacked for advanced features and compact design.


- **Front-Side Illumination (FSI) (1970s–2000s)**
    - Standard architecture for early CCD and CMOS sensors.
    - Light enters from the front, passing through metal wiring and transistors before reaching the photodiode.
    - Wiring blocks some light, reducing quantum efficiency, especially as pixel sizes shrink.

- **Back-Side Illumination (BSI) (Late 2000s, commercialization ~2007–2010)**
    - Sensor wafer is thinned and flipped so light enters from the back, directly reaching the photodiode.
    - Dramatically improves quantum efficiency, especially for small pixels.
    - First widely adopted in smartphone cameras (e.g., iPhone 4 in 2010).

- **Deep Trench Isolation (DTI) (Early 2010s)**
    - Introduced to reduce electrical and optical crosstalk between pixels.
    - Trenches filled with insulating material are etched between pixels, confining photo-generated electrons.
    - Enables higher pixel density and better color fidelity.

- **Deep Photodiode (Mid 2010s)**
    - Photodiodes are engineered to extend deeper into the silicon substrate.
    - Increases the volume for photon absorption, improving sensitivity and full-well capacity.
    - Particularly beneficial for small pixels and high dynamic range imaging.

- **Stacked Sensor (3D Stacking) (Mid–Late 2010s)**
    - Sensor and circuitry are fabricated on separate silicon layers and then stacked (bonded) together.
    - Allows for more complex circuitry (e.g., faster readout, on-chip memory, AI processing) without sacrificing pixel area.
    - Now common in high-end smartphone and professional sensors.
  
  -->
</div>
</div>
</div>
</section>
<section id="sec-sensor-bsi" class="level2">
<h2 class="anchored" data-anchor-id="sec-sensor-bsi">Back-side illuminated (BSI)</h2>
<p>After years of incremental improvements, engineers developed a new architecture to overcome the limitations of front-side illuminated (FSI) sensors. The resulting design, known as <strong>Back-Side illuminated (BSI)</strong> CMOS, rearranges the device layers to improve sensitivity. In BSI sensors, the photodiode and wiring layers are flipped; the light no longer needs to pass through the wires.</p>
<div id="fig-sensor-bsi2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sensor-bsi2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/sensors/03-sensor-bsi2.png" class="img-fluid figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sensor-bsi2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: The back-side illuminated pixel. (a) The front-side illuminated pixel has metal lines above the silicon substrate. (b) The back side illuminated sensor flips the metal lines and silicon substrate. Source: <a href="https://scientificimaging.com/knowledge-base/front-side-illuminated-and-back-side-illuminated-imagers" target="_blank">Scientific Imaging.</a>
</figcaption>
</figure>
</div>
<p>In addition to flipping the order of the silicon and wires, it was necessary to change the thickness of the substrate (<a href="#fig-sensor-bsi2" class="quarto-xref">Figure&nbsp;6</a>). In the FSI pixel, the photodiode is embedded in a relatively thick layer of silicon. Flipping this structure would require the light to pass through a substantial amount of silicon prior to reaching the photodiode. This is problematic because the silicon absorbs light, and particularly short-wavelength light (<strong>?@sec-sensor-wavelength</strong>). Thus, it was necessary to find a way to thin the silicon substrate and still have a working photodiode and circuitry.</p>
<div id="sensor-bsi" class="callout callout-style-default callout-note callout-titled" title="BSI technology development">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
BSI technology development
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The concept of back-side illuminated (BSI) CMOS image sensors was first proposed by Eric Fossum in 1994 to address the limitations caused by metal wiring obstructing incoming light and to improve sensitivity. While the benefits of rearranging the sensor layers were clear to many, developing a practical manufacturing process proved challenging. It took more than a decade for BSI technology to become commercially viable. OmniVision Technologies was among the first to introduce a commercial BSI sensor, releasing the OV8810 (1.4 μm pixels, 8 megapixels) in September 2008. Sony soon followed, launching the first widely adopted BSI sensors in 2009.</p>
<p><a href="https://en.wikipedia.org/wiki/Back-illuminated_sensor">Wikipedia article on BSI</a></p>
</div>
</div>
</div>
</section>
<section id="sec-sensor-deepdiode" class="level2">
<h2 class="anchored" data-anchor-id="sec-sensor-deepdiode">Deep photodiodes</h2>
<p><a href="#fig-sensor-skhynix" class="quarto-xref">Figure&nbsp;7</a> illustrates a further improvement in the BSI design. SK Hynix, Omnivision and other manufacturers recognized that simply flipping the sensor (BSI) improved light capture. But as pixel sizes continued to shrink there was relatively little ability to store electrons. Thus the well capacity was reduced and which limited the pixel’s dynamic range (<strong>?@sec-wellcapacity-dynamicrange</strong>). Moreover, the sensitivity to longer wavelengths, which penetrates deeper into silicon, became a challenge (<strong>?@sec-sensor-wavelength</strong>).</p>
<p>To overcome these limitations, many vendors invented methods to build photodiodes that extend significantly deeper into the silicon (deep photodiode technology). Increasing the volume of the photodiode has two benefits. First, the photodiode can gather more charge before saturating. Second, the photodiode has higher long-wavelength sensitivity.</p>
<div id="fig-sensor-skhynix" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sensor-skhynix-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/sensors/03-sensor-bsi.png" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sensor-skhynix-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Deep photodiodes in the BSI. (a) A super pixel of a front-side illuminated CMOS sensor, including the microlenses, color filters, metal lines, and silicon. (b) A back-side illuminated super pixel with deep photodiode technology. Source: <a href="https://news.skhynix.com/evolution-of-pixel-technology-in-cmos-image-sensor/" target="_blank">Skhynix.</a>
</figcaption>
</figure>
</div>
</section>
<section id="sec-sensor-stacked" class="level2">
<h2 class="anchored" data-anchor-id="sec-sensor-stacked">Stacked sensors</h2>
<p>In the original planar technology, photodiodes are adjacent to the transistor circuits within the silicon substrate. Thus, photodiodes must compete for space with the transistors. Reducing the photodiode area means less light efficiency; reducing the transistor size means noisier performance. Youse pays yer money and yer makes yer choice.</p>
<p>The <strong>stacked sensor</strong> architecture overcomes this limitation by physically separating the photodiode layer from the readout and processing circuitry. In a stacked sensor, the photodiodes occupy one silicon layer, while the supporting electronics are fabricated on a separate layer beneath. This structure maximizes the light-sensitive area (fill factor) for each pixel and enables the use of more advanced, lower-noise circuitry without sacrificing pixel size.</p>
<div id="fig-sensor-stacked" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sensor-stacked-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/sensors/03-pixel-stacked.png" class="img-fluid figure-img" style="width:50.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sensor-stacked-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: Pixels in the stacked sensor are based on die stacking technology. Several layers of silicon that are bonded and connected using Through-Silicon Via (TSV) technology. The technology began with 2-layers, but when have you ever known Silicon Valley to stop?
</figcaption>
</figure>
</div>
<p>The key enabling technology is called <strong>die stacking</strong>. The silicon layers are bonded together and electrically connected using two main methods. The original approach used <strong>Through-Silicon Vias (TSVs)</strong>—tiny holes etched through the silicon and filled with a conductive material (such as copper or tungsten) to create vertical electrical connections.</p>
<p>More recently, <strong>Cu-Cu (copper-to-copper)</strong> direct bonding has become common. In this method, patterned copper pads on each wafer are precisely aligned and bonded under heat and pressure, allowing for dense, fine-pitch interconnects between the pixel and logic layers. Cu-Cu bonding is ideal for high-resolution, high-speed sensors, enabling features like global shutter or per-pixel memory. TSVs are still used for tasks such as power delivery and global I/O routing.</p>
<p>Stacked sensors have unlocked new capabilities in image sensor design. By separating the photodiode and circuitry layers, manufacturers can improve light sensitivity, reduce noise, and add advanced processing features directly beneath the pixel array. This architecture forms the foundation for ongoing innovation in sensor performance and functionality.</p>
<!-- Stacked Sensor Structures Google doc 
A significant step towards multi-layer integration was taken by Mitsumasa Koyanagi of Tohoku University, who pioneered the technique of wafer-to-wafer bonding with TSV in 1989
-->
<div class="callout callout-style-default callout-note callout-titled" title="Stacked sensor applications">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Stacked sensor applications
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Stacked CMOS sensors build on the BSI CMOS architecture by integrating the pixel array with a separate logic layer. This close integration enables advanced features such as high dynamic range (HDR) imaging, faster frame rates, and on-chip processing—including artificial intelligence (AI) functions.</p>
<p>Some stacked sensors incorporate high-speed DRAM directly beneath the pixel array, allowing for rapid data readout and temporary storage. This architecture made possible innovations like the Sony a9 (2019), which could capture images at 20 frames per second (fps) with a continuous, blackout-free viewfinder experience.</p>
</div>
</div>
</div>
</section>
<section id="sec-digital-pixel" class="level2">
<h2 class="anchored" data-anchor-id="sec-digital-pixel">Digital pixel sensor</h2>
<p>The Digital Pixel Sensor (DPS) was a significant parallel development in image sensor technology. The initial ideas were presented in the mid 1990s, at roughly the same time as the APS 3T and 4T circuits were implemented. The APS circuitry included an analog-to-digital conversion (ADC) at the column or chip level. The DPS architecture extended the pixel circuit by including an ADC within the pixel circuit <span class="citation" data-cites="fowler1995-dps-patent">@fowler1995-dps-patent</span> <span class="citation" data-cites="Udoy2025-dps-review">@Udoy2025-dps-review</span>.</p>
<p>Digitizing the signal at the pixel had several advantages for high dynamic range imaging, global shuttering and readout speed <span class="citation" data-cites="elgamal2005-cmos-imagesensors">@elgamal2005-cmos-imagesensors</span> <span class="citation" data-cites="kleinfelder2001-10000fps">@kleinfelder2001-10000fps</span> <span class="citation" data-cites="Wandell2002-CommonPrinciples">@Wandell2002-CommonPrinciples</span>. An important feature of the original digital pixel sensors was that the pixel voltage was read non-destructively and repeatedly, say at 1 ms, 2 ms, 4 ms, and so forth. As the photon generated electrons increased in the well, the voltage was sampled. The voltage for each pixel was reported when the pixel had passed one half well capacity, assuring it was much larger than the noise. It would be reported at a time before the well capacity was reached, so that it was below saturation. The information from each pixel was the a voltage and and the time at which it reached that voltage. Thus, each pixel had its own exposure duration -or put another way, the sensor had a space-varying exposure duration. Pixels in dark regions of the scene had longer exposure times than pixels in the bright regions. We explain the value of this approach for high dynamic range imaging in <strong>?@sec-exposure-control</strong>.</p>
<p>The DPS concept was introduced at an early point in the development of CMOS imagers, and thus it faced significant hurdles in miniaturization. The idea of an ADC on the sensor foreshadowed an opportunity that has arisen as pixel architecture evolves. The stacked pixel structure offers a remedy by allowing the ADC and memory circuits to be placed in a separate layer underneath the photodiode. This enables higher pixel density and broader application of in-pixel conversion. It is quite possible that the next generation of image sensors will implement the DPS concept (<span class="citation" data-cites="note-digital-pixel">@note-digital-pixel</span>).</p>
<div id="note-digital-pixel" class="callout callout-style-default callout-note callout-titled" title="Commercialization of the digital pixel sensor">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Commercialization of the digital pixel sensor
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>El Gamal co-founded Pixim, Inc.&nbsp;in 1998, a fabless semiconductor company that commercialized chipsets for security cameras based on this DPS technology. A challenge for early DPS implementations was the reduced photodiode area in the planar array due to the additional ADC circuitry. This increased the cost of the chip and narrowed the field of potential applications and prevented widespread mainstream adoption for many years. Pixim was acquired by Sony Electronics in 2012.</p>
<p>In 2023 Sony introduced the <a href="https://en.wikipedia.org/wiki/Sony_%CE%B19_III">Sony a9 iii</a>. It has 24.6 Megapixels and a global shutter. The camera can operate at high frame rates <a href="https://www.sony.co.uk/interchangeable-lens-cameras/products/ilce-9m3?locale=en_GB">(120 frames per sec)</a>. It appears to be using a digital pixel approach.</p>
<p>It’s highly probable that <a href="https://sites.google.com/site/yusukeoike">Yusuke Oike</a> who worked in the El Gamal group from 2010-2012, played a crucial role in the development and design of the groundbreaking global shutter sensor that is at the core of the a9 III’s capabilities. He has been with Sony Corporation and Sony Semiconductor Solutions Corporation for many years, focusing on research and development of CMOS image sensors, pixel architecture, circuit design, and imaging device technologies. He’s held positions like Senior General Manager of Research Division and is even listed as the newly appointed CTO of Sony Semiconductor Solutions Corporation in a 2025 news release.</p>
</div>
</div>
</div>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>