# Optics and Linear Systems {#sec-optics-linear}
Geometric optics helps us understand how lenses work by focusing on things like the shape of the lens, what it’s made of, and how far it is from the sensor. This approach is great for designing lenses and figuring out their basic properties.

But there’s another way to look at optical systems: as signal processors. In this view, the system takes in light from the scene and transforms it into the light that arrives at the sensor. This signal processing perspective is powerful because it lets us think about what the system does, without worrying too much about the details of how it’s built. It’s also a practical way to simulate and analyze optical systems.

This approach is especially useful because many optical systems are close to being linear—they follow the **Principle of Superposition**. You can actually test this property in the lab. There are lots of tools for working with linear systems, and you’ll see them used again and again in science and engineering. In this section, we’ll introduce these ideas using a simple optical system as an example.

## Principle of Superposition
A system $L$ is linear if it satisfies the superposition rule:

$$
L(x + y) = L(x) + L(y).
$$ {#eq-superposition}

Here, $x$ and $y$ are two possible inputs, and $L(x)$ is the system's response to the input $x$. The input variable can be a scalar, vector, matrix, or tensor. For example, in an optical system, $x$ might represent the incident light field at the entrance aperture, and $L(x)$ could be the spectral irradiance measured at the sensor .

### Real-World Linearity 

No real system is perfectly linear over all possible signals. Extreme inputs with a massive amount of energy may behave unpredictably or even break! However, many systems are linear over an input range that covers all the likely use cases. 

### Local Linearity 
Some systems are only **locally linear**.  This means they behave linearly within a limited region, such as near a particular input $x_1$.  A system might be approximately linear in different ways depending on the input region. For example, the linear approximation near $x_1$ could be different from the one near $x_2$. This idea of local linearity is a fundamental concept in mathematics, physics, and engineering.[^taylor-series]

 [^taylor-series]: If you’ve seen the Taylor series expansion in calculus, you may recognize the principle of local linearity in mathematics!

### Principle of Homogeneity
A special case of superposition is **homogeneity**:

$$
L(\alpha x) = \alpha L(x)
$$ {#eq-homogeneity}

Homogeneity follows from superposition. For example, adding an input to itself:

$$\begin{aligned}
L(x + x) &= L(2x) \nonumber  \\ 
&= L(x) + L(x) \nonumber \\
&= 2L(x). \nonumber
\end{aligned}
$$

This generalizes to any integer $m$:

$$
L(m x) = m L(x).
$$

::: {.callout-note title="Extension to Rational Numbers" collapse="true"}

For real-valued systems that obey superposition, homogeneity holds for any rational number. If $\alpha = \frac{p}{q}$ is rational:

$$
L(\alpha x) = L\left(\frac{p}{q}x\right) = p L\left(\frac{x}{q}\right).
$$

Let $x' = x/q$, so

$$
L(x) = L(q x') = q L(x') \implies L(x') = \frac{1}{q} L(x).
$$

Therefore,

$$
L(\alpha x) = p L(x') = p \frac{1}{q} L(x) = \frac{p}{q} L(x) = \alpha L(x).
$$

To extend this to all real numbers, continuity and far more extensive proof is required. But how about we just work with the rationals and be engineers for now?
:::

It is important to note that a system can be homogeneous without being linear. For example, $f(x) = |x|$ and the vector length function

$$
f(\mathbf{x}) = \|\mathbf{x}\|_2 = \sqrt{x_1^2 + x_2^2 + \cdots + x_n^2}
$$

are homogeneous but not linear. The function $ReLU(x) = \max(0, x)$, widely used in neural networks, is also homogeneous for $\alpha > 0$ but not linear:

$$
1 = ReLU(2 + -1) \neq ReLU(2) + ReLU(-1) = 2 + 0.
$$


## Point spread functions

Linearity is a very imortant foundation for think about how optics turns an input scene into a sensor image. Imagine the scene as the intensity at a collection of points, each with its own intensity $I(\mathbf{p})$, where $\mathbf{p} = (p_x, p_y, p_z)$ is the location of each point in the scene. For each of these points, the optics spreads out the light in a certain way—this is called the point spread function, or $PSF_\mathbf{p}(x, y)$. Here, $(x, y)$ are positions in the image.

If we know all these point spread functions, and if the system is linear (so superposition holds), we can figure out what the image will look like for any scene. We just add up the contribution from every point in the scene. The formula looks like this:

$$
O(x, y) = \sum_{\mathbf{p}} I(\mathbf{p})\, PSF_\mathbf{p}(x, y)
$$ {#eq-pointspread-1}

Here, $O(x, y)$ is the brightness at each spot in the image. Each point in the scene sends out its own little blur, and the image is just the sum of all those blurs.

::: {.callout-note title="A limitation of PSFs" collapse="true"}
There’s a catch: in real scenes, objects can block each other (occlusion). That means the point spread from one spot might be blocked or changed by something else in the scene, so the formula above isn’t always perfect.

But if all the points are on a flat surface, or if everything is far away (so nothing blocks anything else), this approach works well. The main idea is that you can build up the whole image by adding up the point spreads from each part of the scene. It’s a great starting point for understanding how images form.
:::

## Shift-invariant linear systems
A practical limitation of characterizing a general linear systems is that it can have many different point spread functions - one for every point! But there are real systems that are much simpler. These systems have essentially the same point spread function across many points, with the only difference being that the point spread function is shifted. We say that the shape of the point spread function remains the same, apart from being shifted. We say such a system is **shift invariant**. 

The simplicity of shift invariance makes it a very useful tool, and some courses entitled 'linear systems' focus almost entirely on shift-invariant linear systems. Across the many branches of science and engineering, there are a large number of physical phenomena and engineering devices that are well-approximated as shift invariant linear systems. These systems are relatively straightforward to calibrate because we only need to estimate a single point spread function. Also, there are many computational tools that help us simulate shift invariant systems[^time-invariance].

[^time-invariance]: In imaging the point spread function is the key response.  In applications studying systems that are a function of time, the response to a short temporal response is the key response. These are called **impulse response** functions, and they are the equivalent of the point spread function in an imaging system. 

For some readers, a little mathematical notation may be useful. Suppose we write image translation as a function, $T()$. If we shift an image, $x$, by an amount, $\delta$, we write it as $T(x,\delta)$. A shift invariant system means that the output of a linear system, $L()$, to a translated image is the translated linear response

$$
L(T(x,\delta)) \approx T(L(x),\alpha \delta).
$$ {#eq-shiftinvariance-defined}

Notice that @eq-shiftinvariance-defined allows the size of the image shift, $\delta$, to differ from the size of the output shift, $\alpha \delta$. So if we displace an object by an amount $\delta \text{m}$, the image will be displaced by an amount $\alpha \delta \text{m}$ on the sensor. This scale factor depends on the image magnfication. 

The image formation component (optics) of most image systems are typically linear, but they are not globally shift-invariant. Rather, they are shift invariant over some part, but not all, of the full visual field. Usually there is a region near the optical axis that is shift invariant, and the system is also locally shift-invariant at off-axis points. This local shift invariance is similar to a system that is locally linear. In optics the shift invariant region is called an **isoplanatic** region. 

## Common PSFs in simulation

When simulating optical systems, there are a few point spread functions (PSFs) that come up again and again (@fig-psf-graph).

The **Airy pattern** is based on the physics of light waves; it’s the pattern for a perfect, diffraction-limited lens imaging a point at a distance. The Airy pattern depends on the wavelength of light, and it represents an inescapable physical limitation. The **Gaussian** and **Lorentzian** (sometimes called Cauchy) PSFs are simple formulas that are used to approximate how some real lenses blur light. The Gaussian is especially popular because it’s easy to work with in calculations. The Lorentzian has higher tails, which means it’s better for modeling systems where light gets scattered a lot—like if your lens is scratched or there’s a lot of internal reflection.

![Cross-section of three  point spread functions that are widely used in simulation. These point spreads were applied for a lens and sensor configuration with an $f/\#$ of 4 and a focal length of 3.9mm. The radius of the Airy disk for this configuration is 3.4 microns (see the zero-crossing of the Airy pattern).](images/optics/optics-siKernels-graph.png){#fig-psf-graph width="60%"}

The cross-section graphs of these PSFs are a useful way to see the details of their differences. We can go beyond the curves to visualize how these different PSF shapes are manifest in spectral radiance images (@fig-psf-image). That image compares the three custom PSFs and a fourth one, called the **Pillbox**.  I created the images using the ISETCam script <a href="../code/fise_opticsPSF.html" target="_blank">fise_opticsPSF.</a>  

![Images created by applying four different point spreads to an image of grid lines. The diffraction limited pattern and the Gaussian pattern have point spread functions that vary with wavelength.  The *pillbox* and *Lorentzian* PSFs were made wavelength-indepedent. Notice how the high tail of the Lorentzian PSF produces the appearance of a veiling glare. The pillbox has about the same with as the other functions, but introduces much more blur because it stays at a high level over the entire width.  I introduced a wavelength dependence to the Gaussian so that that long wavelengths (reddish) spread a little more than the short wavelengths.  That is also true of the Airy pattern, but to a smaller extent. See fise_siKernels.m ](images/optics/optics-siKernels-image.png){#fig-psf-image width="90%"}

Here is a description of the different PSFs and a brief descrition of their properties.

### The Airy pattern
The Airy pattern (see @sec-airy-pattern and @fig-blur-comparison) is what you get if you shine a point of light through a perfect, circular lens or a pinhole camera. It’s the sharpest spot you can get, limited only by the physics of light. The pattern is circularly symmetric, so you only need to know the distance from the center, $r$, to describe it.

The intensity at a distance $r$ from the center is:

$$
I(r) = I_0 \left[ \frac{2 J_1\left( \frac{\pi r}{\lambda N} \right)}{ \frac{\pi r}{\lambda N} } \right]^2
$$

Where:

- $I_0$ is the peak intensity at the center
- $J_1$ is the first-order Bessel function
- $N$ is the $f/\#$ of the lens
- $\lambda$ is the wavelength of light

The first dark ring (where the intensity drops to zero) is at a radius:

$$
d = 2.44 \, \lambda N
$$

This tells you the smallest spot you can create.

### Gaussian

The Gaussian is the classic “bell curve.” It’s simple and often a used as an approximation for optical systems.
$$
I(r) = I_0 \exp\left( -\frac{r^2}{2\sigma^2} \right)
$$

Here, $\sigma$ sets how spread out the blur is. Larger $\sigma$ means a blurrier image.

### Lorentzian (Cauchy)

The Lorentzian function pops up in physics and probability. It’s famous for its “fat tails”—meaning it doesn’t drop off as quickly as a Gaussian. This makes it handy for modeling blur when there’s a lot of stray light or scattering.

$$
I(r) = \frac{I_0}{1 + \left( \frac{r}{\gamma} \right)^2}
$$

Here, $\gamma$ controls how wide the central peak is. A bigger $\gamma$ means a wider, flatter blur with more light in the tails.

## Line spread functions
When I created @fig-psf-image, I decided to illustrate the effect of the PSFs using grid lines rather than points. It was my judgment that the differences would be easier to visualize.  It is fairly common to assess the impact of the point spread this way.  When we calculate the image of a line we are calculating the **line spread function (LSF)**.

If the PSF is circularly symmetric, there is only one LSF associated with it. Also, given an LSF it is possible to recover a unique circularly symmetric PSF.  It is straightforward to compute the LSF by adding up the PSF along one dimension. To find the PSF in the y-direction, we simply sum this way:

$$
LSF = \sum_x PSF(x,y)
$$

Here are the linespread functions of the Gaussian and Lorenzian.


I am not plotting the Pillbox because it is not circularly symmetric. In has the same LSF for the horizontal and vertical directions, but its LSF differs in other directions. 

## Chromatic aberration

This is already in, earlier.  Maybe an example here? Explain transverse and longitudinal chromatic aberration here? 



