# Optics and Linear Systems {#sec-optics-linear}
Geometric optics is valuable for understanding and characterizing many properties of optical systems. Its concepts are closely linked to the physical characteristics of lenses, making geometric optics especially useful in lens design.

Another perspective is to view an optical system from as a signal processing device:  it converts the radiation entering the aperture into radiation at the sensor surface. This approach treats the optics as mapping one spectral image (the incident light field) into another (the irradiance at the sensor). Thinking of the system in terms of signal processing is a powerful method for system simulation and analysis.

This perspective is particularly important because many optical systems are approximately linear. Specifically, they satisfy the **Principle of Superposition**, a property that can be tested experimentally. Linear systems are common in science and engineering, and we will encounter linear systems repeatedly throughout the book.

## Principle of Superposition
A system $L$ is linear if it satisfies the superposition rule:

$$
L(x + y) = L(x) + L(y).
$$ {#eq-superposition}

Here, $x$ and $y$ are two possible inputs, and $L(x)$ is the system's response to the input $x$. The input variable can be a scalar, vector, matrix, or tensor. For example, in an optical system, $x$ might represent the incident light field at the entrance aperture, and $L(x)$ could be the spectral irradiance measured at the sensor .

### Real-World Linearity 

No real system is perfectly linear over all possible signals. Extreme inputs with a massive amount of energy may behave unpredictably or even break! However, many systems are linear over an input range that covers all the likely use cases. 

### Local Linearity 
Some systems are only **locally linear**.  This means they behave linearly within a limited region, such as near a particular input $x_1$.  A system might be approximately linear in different ways depending on the input region. For example, the linear approximation near $x_1$ could be different from the one near $x_2$. This idea of local linearity is a fundamental concept in mathematics, physics, and engineering.[^taylor-series]

 [^taylor-series]: If you’ve seen the Taylor series expansion in calculus, you may recognize the principle of local linearity in mathematics!

### Principle of Homogeneity
A special case of superposition is **homogeneity**:

$$
L(\alpha x) = \alpha L(x)
$$ {#eq-homogeneity}

Homogeneity follows from superposition. For example, adding an input to itself:

$$\begin{aligned}
L(x + x) &= L(2x) \nonumber  \\ 
&= L(x) + L(x) \nonumber \\
&= 2L(x). \nonumber
\end{aligned}
$$

This generalizes to any integer $m$:

$$
L(m x) = m L(x).
$$

::: {.callout-note title="Extension to Rational Numbers" collapse="true"}

For real-valued systems that obey superposition, homogeneity holds for any rational number. If $\alpha = \frac{p}{q}$ is rational:

$$
L(\alpha x) = L\left(\frac{p}{q}x\right) = p L\left(\frac{x}{q}\right).
$$

Let $x' = x/q$, so

$$
L(x) = L(q x') = q L(x') \implies L(x') = \frac{1}{q} L(x).
$$

Therefore,

$$
L(\alpha x) = p L(x') = p \frac{1}{q} L(x) = \frac{p}{q} L(x) = \alpha L(x).
$$

To extend this to all real numbers, continuity and far more extensive proof is required. But how about we just work with the rationals and be engineers for now?
:::

It is important to note that a system can be homogeneous without being linear. For example, $f(x) = |x|$ and the vector length function

$$
f(\mathbf{x}) = \|\mathbf{x}\|_2 = \sqrt{x_1^2 + x_2^2 + \cdots + x_n^2}
$$

are homogeneous but not linear. The function $ReLU(x) = \max(0, x)$, widely used in neural networks, is also homogeneous for $\alpha > 0$ but not linear:

$$
1 = ReLU(2 + -1) \neq ReLU(2) + ReLU(-1) = 2 + 0.
$$


## Point spread functions

Linearity is a very imortant foundation for think about how optics turns an input scene into a sensor image. Imagine the scene as the intensity at a collection of points, each with its own intensity $I(\mathbf{p})$, where $\mathbf{p} = (p_x, p_y, p_z)$ is the location of each point in the scene. For each of these points, the optics spreads out the light in a certain way—this is called the point spread function, or $PSF_\mathbf{p}(x, y)$. Here, $(x, y)$ are positions in the image.

If we know all these point spread functions, and if the system is linear (so superposition holds), we can figure out what the image will look like for any scene. We just add up the contribution from every point in the scene. The formula looks like this:

$$
O(x, y) = \sum_{\mathbf{p}} I(\mathbf{p})\, PSF_\mathbf{p}(x, y)
$$ {#eq-pointspread-1}

Here, $O(x, y)$ is the brightness at each spot in the image. Each point in the scene sends out its own little blur, and the image is just the sum of all those blurs.

::: {.callout-note title="A limitation of PSFs" collapse="true"}
There’s a catch: in real scenes, objects can block each other (occlusion). That means the point spread from one spot might be blocked or changed by something else in the scene, so the formula above isn’t always perfect.

But if all the points are on a flat surface, or if everything is far away (so nothing blocks anything else), this approach works well. The main idea is that you can build up the whole image by adding up the point spreads from each part of the scene. It’s a great starting point for understanding how images form.
:::

## Shift-invariant linear systems
A practical limitation of characterizing a general linear systems is that it can have many different point spread functions - one for every point! But there are real systems that are much simpler. These systems have essentially the same point spread function across many points, with the only difference being that the point spread function is shifted. We say that the shape of the point spread function remains the same, apart from being shifted. We say such a system is **shift invariant**. 

The simplicity of shift invariance makes it a very useful tool, and some courses entitled 'linear systems' focus almost entirely on shift-invariant linear systems. Across the many branches of science and engineering, there are a large number of physical phenomena and engineering devices that are well-approximated as shift invariant linear systems. These systems are relatively straightforward to calibrate because we only need to estimate a single point spread function. Also, there are many computational tools that help us simulate shift invariant systems[^time-invariance].

[^time-invariance]: In imaging the point spread function is the key response.  In applications studying systems that are a function of time, the response to a short temporal response is the key response. These are called **impulse response** functions, and they are the equivalent of the point spread function in an imaging system. 

For some readers, a little mathematical notation may be useful. Suppose we write image translation as a function, $T()$. If we shift an image, $x$, by an amount, $\delta$, we write it as $T(x,\delta)$. A shift invariant system means that the output of a linear system, $L()$, to a translated image is the translated linear response

$$
L(T(x,\delta)) \approx T(L(x),\alpha \delta).
$$ {#eq-shiftinvariance-defined}

Notice that @eq-shiftinvariance-defined allows the size of the image shift, $\delta$, to differ from the size of the output shift, $\alpha \delta$. So if we displace an object by an amount $\delta \text{m}$, the image will be displaced by an amount $\alpha \delta \text{m}$ on the sensor. This scale factor depends on the image magnfication. 

The image formation component (optics) of most image systems are typically linear, but they are not globally shift-invariant. Rather, they are shift invariant over some part, but not all, of the full visual field. Usually there is a region near the optical axis that is shift invariant, and the system is also locally shift-invariant at off-axis points. This local shift invariance is similar to a system that is locally linear. In optics the shift invariant region is called an **isoplanatic** region. 

## Widely used PSFs

When simulating optical systems, there are a few point spread functions (PSFs) that come up again and again.

![Cross-section of three  point spread functions that are widely used in simulation. These point spreads were applied for a lens and sensor configuration with an $f/\#$ of 4 and a focal length of 3.9mm. The radius of the Airy disk for this configuration is 3.4 microns (see the zero-crossing of the Airy pattern).](images/optics/optics-siKernels-graph.png){#fig-psf-graph width="60%"}

The **Airy pattern** is the gold standard for sharpness in an ideal lens. It’s based on the physics of light waves, so it’s the “best case” for a perfect, diffraction-limited lens. The Airy pattern depends on the wavelength of light, so it’s a real physical limit, not just a mathematical idea.

The **Gaussian** and **Lorentzian** (sometimes called Cauchy) PSFs are practical shortcuts. They are simple formulas that do a good job of approximating how some real lenses blur light. The Gaussian is especially popular because it’s easy to work with in calculations. The Lorentzian has fatter tails, which means it’s better for modeling systems where light gets scattered a lot—like if your lens is scratched or there’s a lot of internal reflection.

![Images of applying the different point spreads to an image of grid lines.  The diffraction limited pattern and the Gaussian pattern have point spread functions that vary with wavelength.  The pillbox and Lorentzian were assigned wavelength-indepent point spreads.  Notice how the Lorentzian point spread, which has a high tail, produces something that appears like a glare in the image. The pillbox has about the same with as the other functions, but introduces much more blur because it stays at a high level over the entire width.  I introduced a wavelength dependence to the Gaussian so that that long wavelengths (reddish) spread a little more than the short wavelengths.  That is also true of the Airy pattern, but to a smaller extent. See fise_siKernels.m ](images/optics/optics-siKernels-image.png){#fig-psf-image width="90%"}

### The Airy pattern

The Airy pattern (see @sec-airy-pattern and @fig-blur-comparison) is what you get if you shine a point of light through a perfect, circular lens or a pinhole camera. It’s the sharpest spot you can get, limited only by the physics of light. The pattern is circularly symmetric, so you only need to know the distance from the center, $r$, to describe it.

The intensity at a distance $r$ from the center is:

$$
I(r) = I_0 \left[ \frac{2 J_1\left( \frac{\pi r}{\lambda N} \right)}{ \frac{\pi r}{\lambda N} } \right]^2
$$

Where:

- $I_0$ is the peak intensity at the center
- $J_1$ is the first-order Bessel function
- $N$ is the $f/\#$ of the lens
- $\lambda$ is the wavelength of light

The first dark ring (where the intensity drops to zero) is at a radius:

$$
d = 2.44 \, \lambda N
$$

This tells you the smallest spot you can create.

### Gaussian

The Gaussian is the classic “bell curve.” It’s simple and often a used as an approximation for optical systems.
$$
I(r) = I_0 \exp\left( -\frac{r^2}{2\sigma^2} \right)
$$

Here, $\sigma$ sets how spread out the blur is. Larger $\sigma$ means a blurrier image.

### Lorentzian (Cauchy)

The Lorentzian function pops up in physics and probability. It’s famous for its “fat tails”—meaning it doesn’t drop off as quickly as a Gaussian. This makes it handy for modeling blur when there’s a lot of stray light or scattering.

$$
I(r) = \frac{I_0}{1 + \left( \frac{r}{\gamma} \right)^2}
$$

Here, $\gamma$ controls how wide the central peak is. A bigger $\gamma$ means a wider, flatter blur with more light in the tails.

**Put in a picture illustrating them via ISETCam**

### Line spread functions

Sometimes, instead of a single point, you want to know how a thin line gets blurred. The **line spread function** (LSF) is what you measure when the input is a line.  As you might imagine, the If you know the PSF, you can compute the LSF by adding up the PSF along one direction. If your PSF is circularly symmetric, it’s easy to go from PSF to LSF. But going back from LSF to PSF is trickier—you have to assume the blur is the same in all directions.

## Chromatic aberration

This is already in, earlier.  Maybe an example here? Explain transverse and longitudinal chromatic aberration here? 



