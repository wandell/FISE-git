# Sensor encoding {#sec-sensors}

The [Preface](../index.qmd#sec-preface) provides a very brief discussion of the history of image sensor technologies, from film to charge-coupled devices (CCD), and to the invention of CMOS image sensors (CIS).

Film, the first major commercial technology for capturing and storing image irradiance, relied heavily on chemistry. Light-sensitive molecules were placed in multiple layers onto flexible rolls of plastic, and the roll was loaded into the darkened body of the camera. With the body closed, the film was mechanically advanced, placing a small flat section of film in the focal plane behind the lens. To take a picture, shutter was opened and the spectral irradiance from the lens formed an image on the film. The light changed the structure of molecules in the layers with molecules that were designed to encode the image. For color film, there were multiple layers that differed in their wavelength sensitivity. These molecular changes represented the local brightness of the captured image in different wavebands. Using chemical processes, the density of changes to the light-sensitive molecules could be measured and transferred to other media, such as prints. This process required a variety of chemical consumables, and the film could not be reused. Specialized labs were needed to create the film and to develop and printing the image. Some home hobbyists could develop the film, but mainly the processes were carried out by large commercial organizations.

![Film was the dominant image sensor and recording material for a hundred years. The chemistry of film for both encoding and developing into images was the core scientific discipline. Film could not be reused, producing a steady commercial profit for large companies, such as Kodak, Fuji, Polaroid and Agfa.](images/sensors/film.png){#fig-film width="60%"}

The transition to electronics emerged with the discovery that semiconductor technology could also be used to record irradiance @boyle1970-ccd-first. Light incident on the silicon releases electrons (photoelectric effect), and the spatial pattern of these electrons can be stored and measured. This breakthrough was first implemented in 1970 by Michael Tompsett, who developed the image sensor known as the charge-coupled device (CCD). The semiconductor based sensor revolutionized imaging, moving it from a field based on chemistry to electrical engineering. Importantly, the digital readout from these electrical devices could be immediately integrated with computers. Images could be stored, shared, edited and analyzed directly from the camera output.

The first successful implementation of this principle in an image sensor was the charge-coupled device (CCD), developed in 1970 by Michael Tompsett, building on earlier work by George Smith and Willard Boyle [@boyle1970-ccd-first]. This innovation marked a fundamental shift in imaging, transitioning from a field rooted in chemistry (like photographic film) to one based on electrical engineering. A key advantage was the digital nature of the readout, allowing immediate integration with computers for storage, sharing, editing, and analysis of images directly from the camera.

While both CCD and modern CMOS (Complementary Metal-Oxide-Semiconductor) imagers utilize the photoelectric effect to detect light, they are built using distinct semiconductor technologies. These differences typically necessitate separate fabrication facilities or specialized process flows within a general CMOS facility. A pivotal development was Eric Fossum's circuit design - the active pixel sensor (APS) - which demonstrated that image sensors could be successfully implemented using mainstream CMOS microelectronics processes (@Fossum2023-invention).

![CMOS image sensors incorporate circuitry, comprising multiple subsystems and enabling many different image processing features. Maybe get a figure from one of Eric's earlier papers.](images/sensors/olympusCMOSSensorImage.png){#fig-olympus-overview style=".float-right" width="489"}

Fossum's CMOS imager breakthrough was very significant. By leveraging standard CMOS technology, it became possible to integrate not only the light-sensing pixels but also the timing, control, and signal processing circuitry onto the same sensor chip. This integration enabled new functionalities directly at the sensor level (@fig-olympus-overview). Furthermore, this shift dramatically reduced power consumption, a crucial factor that opened the door to the widespread adoption of image sensors in mobile devices. Thus, beyond the technology itself, the breakthrough enabled the massive global adoption of electronic image sensors. Their widespread use continues to impact many aspects of societies across the globe.

Considering the technological advantages and widespread adoption of CMOS image sensors, we primarily focus on their properties and operation. We will dedicate separate sections to explore the characteristics of CCD imagers and single-photon avalanche detectors (SPADs).

<!-- Fossum says: power reduced (100×) as well as avoid many of the charge‐transfer degradation issues associated with exposure to radiation in space [11] (see Figure 23.6).  (From the Book chapter https://onlinelibrary.wiley.com/doi/chapter-epub/10.1002/9781394202478.ch23)-->

## The photoelectric effect
Image sensors encode the optical light field using an array of light-sensitive photodiodes. Each photodiode converts the light field energy at its aperture, usually on the order of 1 micron in diameter, into electrons. That certain materials convert light to electrons was first reported by Heinrich Hertz (@Hertz1887-photoelectric1, @Hertz1887-photoelectric2) and is called the **photoelectric** effect.  His measurements of the photoelectric effect in 1887 used a simple apparatus: a loop of wire with a small gap. When exposed to a strong electromagnetic wave at the right frequency, a current is induced in the loop, and if the current is sufficient, a spark jumps across the gap. 

The photoelectric effect was discovered when Hertz observed that illuminating the wire loop with ultraviolet (UV) light made the spark easier to produce—the spark length increased. He concluded that the UV light was generating additional current in the wire and that light produces electrical signals. 

<!--
He also noticed that glass, which blocks UV light, eliminated the effect. Quartz (which transmits UV light) did not.
-->

The problem of the photoelectric effect, therefore becomes this: how can we model how light generates an electrical signal.

## Quantifying the photoelectric effect
Over time, we learned that the conversion is inescapably noisy: even a perfectly stable light source will generate a variable number of electrons that follows the Poisson probability distribution. This is part of the statistical quantum theory.

The photoelectric effect is linear over a large range, in the sense that doubling the light intensity doubles the **mean** number of electrons generated. Thus, the initial capture of light is usually modeled as a linear function (@Preece2022-photontransfer). The linearity holds until the material saturates (there are no more free electrons). Within this linear range, the number of electrons is a noisy estimate of the incident light intensity.

The ratio between the number of incident photons and the number of generated electrons is called the photodiode's **quantum efficiency (QE)**. 
You can measure the QE from the slope of the line relating number of incident photons (x-axis) to the number of generated electrons (y-axis).  A QE of 1 (or 100%) means that every incident photon generates one electron; a lower QE indicates that some photons do not generate an electron. So, a QE of 0.5 means that half of the photons - again randomly - produce an electron.  

In practice, QE varies with wavelength due to the physical properties of the sensor material and the design of the device. Manufacturers often provide QE curves that show the efficiency as a function of wavelength for a given sensor, and we will see examples later when we analyze specific sensors.

$$
\mathrm{QE}(\lambda) = \frac{\text{number of electrons generated}}{\text{number of incident photons at wavelength } \lambda}
$$

The photoelectric effect is the initial stage of the CMOS circuit. But as we describe later, the photodiode is integrated within circuitry that stores and then reads out the voltage associated with the electrons. The circuitry can introduce some nonlinearities and noise that are part of the whole system response (@Bohndiek2008-sensorlinearity). But the initial transduction from light to electrons, the photoelectric effect, is linear with respect to the mean number of electrons over a large range. A basic understanding of CIS principles begins with this linear formulation. We discuss the limitations later, after we specify the pixel circuitry.

```{=html}
<!--
Bohndiek2008-sensorlinearity paper has figures.  Notice how nonlinear Figures 7 and 17 are.  But earlier figures (e.g., I probably have class data with response linearity and nonlinearity.  The Thieuwissen paper has an analysis of the impact of floating diffusion.
-->
```

## Statistics of the photoelectric effect
The number of electrons generated by a flash of a constant intensity light source is inescapably random.  The number of absorptions obeys the Poisson distribution. This insight was formalized in the 20th century in two groups of scientists: People working on visual sensitivity (@Pirenne1951-poisson, @Barlow1956-retinalnoise) and people working on light and image sensors (@Mandel1959-poissonabsorptions).

The principles that justify the Poisson distribution are simply stated.  Suppose we have a **steady source** of photons, all with the same wavelength. 

1. The likelihood of a photon being absorbed in any given time interval, $\delta t$ is the same
2. The probability that a photon generates an electron is constant (equal to the quantum efficiency, QE)

These principles meet the definition of the **Poisson** probability distribution, which was first developed Siméon Denis Poisson.  He described the idea in his 1837 work "Recherches sur la probabilité des jugements en matière criminelle et en matière civile (Researches on the Probability of Judgments in Criminal and Civil Matters)". Poisson's goal was to describe the number of discrete events occurring in a fixed interval of time or space if these events occur with a known constant mean rate and independently of the time since the last event, and his applications were in areas like jurisprudence and the number of soldiers killed by horse kicks in the Prussian army. But the mathematical framework he established directly applies to the arrival of photons from a constant light source. In image systems applications, an electron generation is considered an 'event', all the events are the same, and the chance of an event in any small interval of time is the same. 

The Poisson formula that describes the chance of $n$ electrons events has only a single parameter $\lambda$,
$$
P(n; \lambda) = \frac{\lambda^n e^{-\lambda}}{n!}
$$

The parameter $\lambda$ is the expected (mean) number of events, and the variance of the Poisson distribution is also $\lambda$.  The shape of the Poisson distribution is very much like the Normal distribution for mean greater than 15 or 20 (@fig-sensor-poisson). The main differences between the Poisson and Normal distribution are that the Poisson only takes integer values and asymmetric (skewed) for small values of $\lambda$.  The Poisson becomes more symmetric as $\lambda$ increases, and the chance of a negative value becomes very small. For large $\lambda$, the Poisson distribution approaches the Normal distribution, and for small $\lambda$ the Poisson is noticeably skewed to the right. 

![Poisson distribution examples.  The Poisson formula (red lines) and simulations of Poisson counts (bars) are shown for three mean levels.](images/sensors/03-Poisson.png){#fig-sensor-poisson width=70%}

A fact that we will use later is that the Poisson distribution for electrons is not just any Gaussian - the electron distribution must have a mean equal to the variance. 

::: {#sensory-physiologists-poisson .callout-note collapse="true"}

## Vision science and Poisson absorptions

Long exchange about Einstein, Lenard, Mandel, Wolf, Pirenne, Barlow.  Stored here in ChatGPT.
https://chatgpt.com/share/682ceccb-4c9c-8002-9e5d-d623a14348d9

The section about Poisson distribution, Barlow, and such is here.

Barlow (1956)
In his influential paper, “Retinal noise and absolute threshold” (J. Opt. Soc. Am.), Barlow proposed that the variability in visual threshold is due to random fluctuations in photon arrival, and he interpreted Hecht’s findings in terms of quantum shot noise. While Barlow did not explicitly name the Poisson distribution, he was clearly using Poisson-like reasoning — modeling the discrete and probabilistic nature of photon arrivals at low light levels.

Emil Mandel, along with Leonard Mandel (no relation) and Emil Wolf, provided the rigorous quantum-optical foundations for the Poisson statistics of coherent light sources — such as lasers or thermal light in certain regimes. Their work clarified when and why the number of photon detections should follow a Poisson distribution, and established this as a consequence of the quantum state of the light field.

Barlow was conceptually ahead of Mandel in applying Poisson reasoning to photon absorption in the retina. But Mandel and Wolf provided the formal, quantum-theoretic justification for using Poisson statistics in photon counting, which wasn't fully developed until the late 1950s and early 1960s.

Suggested text from ChatGPT:

"In 1956, Horace Barlow formalized this interpretation, reasoning that visual sensitivity is limited not by retinal inefficiency but by the quantum nature of light itself—a constraint imposed by the probabilistic arrival of photons. Although the formal derivation of Poisson statistics in quantum optics came later—most notably through the work of Mandel and Wolf in 1959—Barlow’s insight preceded and anticipated that mathematical foundation."

Equation 1 in this paper states that absorptions are Poisson.  He writes "The Poisson distribution is not particularly convenient to deal with, but fortunately, the values have been tabulated by Molina25 over a wide range."

Barlow, H. B. (1956).
"Retinal noise and absolute threshold."
Journal of the Optical Society of America, 46(8), 634–639.
https://doi.org/10.1364/JOSA.46.000634

But now, I see that Barlow cites Pirenne for the Poisson "This probability is quite independent of the number of similar events in the past or the neighboring retina, and therefore the number of events in a given retinal area and time follows the Poisson distribution as shown in detail by Pirenne. 7 (b)"  And in fact, Pirenne has a whole section of his 1951 paper, see Equation (2) for the derivation of the Poisson distribution for retinal absorptions.

Pirenne's insights were instrumental in shaping subsequent research on visual thresholds. His work provided a theoretical foundation that influenced Horace Barlow's 1956 paper, "Retinal Noise and Absolute Threshold", where Barlow cites Pirenne's 1951 study as a key authority on the statistical nature of photon absorption. Barlow extended Pirenne's ideas, suggesting that the limits of visual sensitivity are imposed by the quantum properties of light itself, rather than by imperfections in the visual system.

Thus, Pirenne's 1951 paper stands as a pivotal work that bridged the gap between quantum physics and biological vision, laying the groundwork for the application of Poisson statistics in understanding the fundamental limits of human visual perception.

In Foundations of Vision I should explain that Barlow concluded (a) one quantum excites a rod, but (b) several rods must be excited to generate a percept.  I think that was the conclusion in Cornsweet.  It is a good project for ISETCam and the class.

Correct — based on available records and close examination of Mandel and Wolf’s early publications (especially Mandel's 1958 and 1959 papers and their later comprehensive work, *Optical Coherence and Quantum Optics*), it appears that **they did not cite Pirenne**.

Mandel and Wolf were focused primarily on building a rigorous quantum theory of photon statistics, often referencing foundational work in quantum electrodynamics and statistical optics rather than physiological or psychophysical studies of vision. Their mathematical formalism emerged somewhat independently of the vision science tradition.

In contrast, **Pirenne (1951)** and **Barlow (1956)** came from a sensory physiology perspective, using Poisson statistics to explain photon absorption in the retina based on empirical and theoretical arguments grounded in visual detection thresholds.

So, although Pirenne clearly articulated the use of the Poisson distribution in photon absorption well before Mandel and Wolf’s formal treatments, his work appears to have been **overlooked or simply outside the citation scope** of their quantum optics papers. The convergence of ideas from physiology and quantum optics happened more fully in later decades as interdisciplinary fields like biophotonics matured.

I should have realized that Barbara Sakkitt and Horace must have worked on this question together - even in the 1970s when she published her work.

The stochastic nature of absorptions was recognized since the earliest days of quantum mechanics. Specifically showing that the number of electrons generated by light will be Poisson distributed is widely credited to work by Mandel and Wolf in papers starting around 1958. I am genuinely surprised that the Poisson statistics of photon counting was not rigorously demonstrated until about 1960. 

One reason for my surprise is that the stochastic nature was known and even used in human experiments by Hecht, Schaler and Pirenne that are quite famous in the vision science literature.  But they did not specifically name the distribution of absorptions. The idea was somehow 'in the air', as the remarkable vision scientist, Horace Barlow, included it in a famous 1956 paper that 

A friend pointed out to me that practical devices to measure electrons were only starting to arise in the 60s and 70s. Perhaps once counting electrons became widespread, a more precise theory of the randomness of photo-electrons had a practical purpose and the theory followed.
:::

## The birth of the photon
How do we explain the photoelectric effect?  Two key experimental insights that led to the modern theory.

Hertz' initial photoelectric demonstration was in 1887, and it was not until 1897 that JJ Thomson conclusively demonstrated that electrons are discrete, countable particles. Thomson's experiments revealed that cathode rays are composed of negatively charged particles with a specific mass and charge, much smaller than that of an atom. 

In 1902, armed with this knowledge, Philipp Lenard carried out a series of revealing experiments. He observed that increasing the intensity of the incident light increased the number of emitted electrons but **did not** increase their kinetic energy. Instead, the energy of the emitted electrons depended on the **wavelength** of the light: shorter wavelengths (higher frequencies) produced more energetic electrons. Lenard also identified a **threshold wavelength**—if the light’s wavelength was too long (i.e., its frequency too low), no electrons were emitted, regardless of how intense the light was. These results posed a serious challenge to classical wave theory, which predicted that energy should increase smoothly with intensity, not depend on frequency.

In parallel, physicists in the late 19th century sought to understand the spectrum of light emitted by a blackbody — a perfect emitter and absorber of radiation. In 1900, **Max Planck** tried to match the observed blackbody spectrum with a formula. After some trial and error, he arrived at an expression that fit the data perfectly—but only if he assumed that energy exchange between matter and radiation was quantized. 

$$
E = n h ν
$$ 

where 𝑛 is an integer, 𝜈 is the frequency (inverse to wavelength) of the light, and ℎ is the constant he introduced—now known as Planck’s constant.
Planck speculated that the energy of oscillators in the cavity walls of the material could only take on discrete values and thus the measured absorptions and emissions from a continuous light field needed to match these discrete levels.

In 1905, Albert Einstein offered a revolutionary explanation. He proposed that we should not think of light as a continuous wave that becomes quantized through an interaction with the material.  Rather, he argued, light itself **quantized** into discrete packets of energy—later called **photons**. The energy of each photon is proportional to its frequency,

$$
E = h \nu
$$

where $h$ is Planck’s constant and $\nu$ is the frequency of the light. This hypothesis went beyond Max Planck’s earlier work, which had only assumed quantized energy exchange between matter and radiation — not that light itself came in packets. Einstein suggested that **light in free space** behaves as localized energy quanta, capable of transferring energy to electrons one photon at a time. His theory explained both Planck's observations and all of Lenard’s experiments.  His photoelectric theory laid the groundwork for the next generation's work in quantum physics.

::: {#lenard-einstein-bohr .callout-note collapse="true"}
## The Duality of Light, the Duality of People

![Lenard made measurements that defined important properties of the photoelectric effect. Einstein offered a bold theoretical idea, what we now call the photon, that explained Lenard's and Planck's measurements. Einstein, who was Jewish, had a complex personal relationship with Planck, as they navigated Hitler's rise to power. Lenard became strongly anti-Semitic.](images/people/03-Lenard-Einstein.png){#lenard-einstein width="50%" fig-align="center"}

Einstein’s proposal was met with skepticism. The wave theory of light, supported by extensive experimental evidence like interference and diffraction, remained central to physics. Even Einstein himself was uneasy with the resulting **wave-particle duality**. He believed that this duality hinted at an incomplete understanding of nature, and he famously rejected the probabilistic foundations of quantum mechanics, remarking that “God does not play dice.” Niels Bohr, a champion of the new quantum theory and Einstein’s intellectual foil, is said to have replied, “Einstein, don’t tell God what to do.” The tension between wave and particle descriptions remains a central—and still puzzling—theme in physics.

In 1905 —Einstein’s *annus mirabilis*— he published four groundbreaking papers that reshaped modern physics. In addition to the photoelectric effect, for which he was awarded the Nobel prize, he published his work on Special Relativity which showed that **simultaneity is relative**, depending on the observer’s frame of reference. Walter Isaacson’s biography of Einstein (@Isaacson2008-einstein) provides an accessible account of all four discoveries and the broader scientific context. Isaacson also details how Lenard, who was also a Nobel Laureate, became an outspoken anti-Semite who actively opposed Einstein’s recognition.
<!--
Photoelectric effect and Lenard
https://www.perplexity.ai/search/when-einstein-worked-out-the-p-_doyK0P0SPCj2lF728iVpg 
-->
:::


## Wave particle duality
The duality of light, which acts as both a particle and a wave, is one of the most profound and puzzling concepts in physics. While Newton's corpuscular theory of light was set aside due to the overwhelming evidence for wave-like behavior—such as interference and diffraction—the discovery of the photoelectric effect forced physicists to accept that light also exhibits particle-like properties. Many people - and large language models - will tell you that this duality is not a contradiction, but rather a reflection of the *quantum nature* of light: depending on the experiment, light can behave as a wave or as a collection of discrete photons. 

If you don't think that is puzzling , you are not paying close enough attention. The wave-particle duality is not a simple matter of context or perspective—it is a fundamental feature of quantum systems that defies classical intuition. Even Einstein, who introduced the concept of photons, struggled with the philosophical implications of this duality for the rest of his life. In quantum mechanics, light is described by a mathematical object called the wavefunction, which encodes the probabilities of observing different outcomes. When we measure properties associated with waves (such as interference patterns), the wave nature is revealed; when we measure properties associated with particles (such as discrete energy transfer in the photoelectric effect), the particle nature emerges. The fact that both descriptions are necessary—and that neither alone is sufficient—remains one of the most remarkable and mysterious aspects of modern physics.

## Statistics of photons generating electrons
According to the photon theory of light, the number of photons arriving at a photodiode during a fixed interval follows a Poisson distribution. Each photon has a certain probability (the quantum efficiency) of generating an electron, and each photon can produce at most one electron. As a result, the number of electrons generated by the photodiode is also Poisson distributed, but with a mean reduced by the quantum efficiency. 

The inherent randomness in the number of electrons generated by incident photons is called **shot noise**. You can see a step-by-step statistical explanation of this process in the [ISETCam script](../code/fise_photonsElectrons.html). The script demonstrates why, in conventional CMOS image sensors, both the incident photons and the resulting electrons follow Poisson statistics. Note, however, that this relationship may not apply to other sensor types—such as those that can generate multiple electrons from a single photon.

::: {#particle-wave .callout-note collapse="true"}

## Einstein 1909 — Poisson-like, but More Than Poisson
Einstein’s 1909 analysis of energy fluctuations in blackbody radiation marks an important early step toward our modern understanding of photon statistics. He examined the variance of the energy contained within a small subvolume of a cavity in thermal equilibrium and found it consisted of two distinct terms: one linear and one quadratic in the mean photon number. The linear term corresponds to fluctuations from discrete, independently arriving photons—what we now recognize as **shot noise**, characteristic of **Poisson statistics**. The quadratic term, however, arises from interference effects and reflects **wave-like correlations**, which do not occur in a purely Poisson process.

$$
\langle (\Delta E)^2 \rangle = h \nu \langle n \rangle + \left( h \nu \langle n \rangle \right)^2
$$

Here, $\langle n \rangle$ is the average number of photons at frequency $\nu$, and $h$ is Planck’s constant.

Einstein interpreted this two-part structure as evidence that light cannot be fully described either as a particle or as a wave alone—it exhibits both aspects, depending on the observable under consideration. This insight was a remarkable early hint of **wave-particle duality**, predating the formal development of quantum electrodynamics by decades.

While this analysis includes a term that resembles Poissonian fluctuations, it goes beyond the framework of simple photon counting. By considering energy fluctuations in a volume—not just discrete detection events—Einstein’s approach was more general and conceptually distinct from the later, more focused treatments of photon counting and the photoelectric effect.

In summary, Einstein’s result contains a Poisson-like term but also highlights wave-induced correlations, making it richer than a purely Poisson model. The full realization that photon arrivals at a detector follow a Poisson distribution—under suitable conditions—emerged later with the advent of quantum optics and the formal theory of photon statistics.
:::
