# Sensors {#sec-sensors}

The [Preface](../index.qmd#sec-preface) provides a very brief discussion of the history of image sensor technologies, from film to charge-coupled devices (CCD), and to the invention of CMOS image sensors (CIS).

Film, the first major commercial technology for capturing and storing image irradiance, relied heavily on chemistry. Light-sensitive molecules were placed in multiple layers onto flexible rolls of plastic, and the roll was loaded into the darkened body of the camera. With the body closed, the film was mechanically advanced, placing a small flat section of film in the focal plane behind the lens. To take a picture, shutter was opened and the spectral irradiance from the lens formed an image on the film. The light changed the structure of molecules in the layers with molecules that were designed to encode the image. For color film, there were multiple layers that differed in their wavelength sensitivity. These molecular changes represented the local brightness of the captured image in different wavebands. Using chemical processes, the density of changes to the light-sensitive molecules could be measured and transferred to other media, such as prints. This process required a variety of chemical consumables, and the film could not be reused. Specialized labs were needed to create the film and to develop and printing the image. Some home hobbyists could develop the film, but mainly the processes were carried out by large commercial organizations.

![Film was the dominant image sensor and recording material for a hundred years. The chemistry of film for both encoding and developing into images was the core scientific discipline. Film could not be reused, producing a steady commercial profit for large companies, such as Kodak, Fuji, Polaroid and Agfa.](images/sensors/film.png){#fig-film width="60%"}

The transition to electronics emerged with the discovery that semiconductor technology could also be used to record irradiance @boyle1970-ccd-first. Light incident on the silicon releases electrons (photoelectric effect), and the spatial pattern of these electrons can be stored and measured. This breakthrough was first implemented in 1970 by Michael Tompsett, who developed the image sensor known as the charge-coupled device (CCD). The semiconductor based sensor revolutionized imaging, moving it from a field based on chemistry to electrical engineering. Importantly, the digital readout from these electrical devices could be immediately integrated with computers. Images could be stored, shared, edited and analyzed directly from the camera output.

The first successful implementation of this principle in an image sensor was the charge-coupled device (CCD), developed in 1970 by Michael Tompsett, building on earlier work by George Smith and Willard Boyle [@boyle1970-ccd-first]. This innovation marked a fundamental shift in imaging, transitioning from a field rooted in chemistry (like photographic film) to one based on electrical engineering. A key advantage was the digital nature of the readout, allowing immediate integration with computers for storage, sharing, editing, and analysis of images directly from the camera.

While both CCD and modern CMOS (Complementary Metal-Oxide-Semiconductor) imagers utilize the photoelectric effect to detect light, they are built using distinct semiconductor technologies. These differences typically necessitate separate fabrication facilities or specialized process flows within a general CMOS facility. A pivotal development was Eric Fossum's approach, which demonstrated that image sensors could be successfully fabricated using standard, mainstream CMOS microelectronics processes (@Fossum2023-invention).

![CMOS image sensors incorporate circuitry, comprising multiple subsystems and enabling many different image processing features.](images/sensors/olympusCMOSSensorImage.png){#fig-olympus-overview style=".float-right" width="489"}

Fossum's breakthrough was very significant as a technology. By leveraging standard CMOS technology, it became possible to integrate not only the light-sensing pixels but also the timing, control, and signal processing circuitry onto the same sensor chip. This integration enabled new functionalities directly at the sensor level (@fig-olympus-overview). Furthermore, this shift dramatically reduced power consumption, a crucial factor that opened the door to the widespread adoption of image sensors in mobile devices. Thus, the technology enabled the massive global adoption of electronic image sensors which continues to impact societies across the globe.

Considering the technological advantages and widespread adoption of CMOS image sensors, we primarily focus on their properties and operation. We will dedicate separate sections to explore the characteristics of CCD imagers and single-photon avalanche detectors (SPADs).

<!-- Fossum says: power reduced (100√ó) as well as avoid many of the charge‚Äêtransfer degradation issues associated with exposure to radiation in space [11] (see Figure 23.6).  (From the Book chapter https://onlinelibrary.wiley.com/doi/chapter-epub/10.1002/9781394202478.ch23)-->

## Transduction: The photoelectric effect

The incident light field is encoded by the light-sensitive photodiodes in the image sensor array. Each photodiode converts the energy in the irradiance at its aperture into some number of electrons. The conversion of photons to electrons is called **transduction**. The conversion of light to electrons first reported by Heinrich Hertz (@Hertz1887-photoelectric1, @Hertz1887-photoelectric2) is called the **photoelectric** effect.

The effect is linear over a large range, in the sense that doubling the light intensity will double the number of electrons generated, and the initial capture of light is usually modeled as a linear function (@Preece2022-photontransfer). The linearity holds until the material saturates (there are no more free electrons). Thus, within the linear range, the number of electrons is a direct and reliable measure of the incident light intensity.  

The ratio between the number of incident photons and the number of generated electrons is called the **quantum efficiency (QE)** of the device. The QE is a wavelength-dependent function. The QE is the slope of the line relating number of incident photons (x-axis) to the number of generated electrons (y-axis),

$$
\mathrm{QE}(\lambda) = \frac{\text{number of electrons generated}}{\text{number of incident photons at wavelength } \lambda}
$$

A QE of 1 (or 100%) means that every incident photon generates an electron, while a lower QE indicates that some photons do not result in electron generation. In practice, QE varies with wavelength due to the physical properties of the sensor material and the design of the device. Manufacturers often provide QE curves that show the efficiency as a function of wavelength for a given sensor.

As we describe later, when the photodiode is embedded within sensor circuitry, some nonlinearities arise in the full transduction. But the initial transduction from light to electrons, the photoelectric effect, is linear over a large range. A basic understanding of CIS principles begins with this linear formulation.  The nonlinearities are important to understand, but can be set aside until we specify more about the pixel circuitry.

::: {#photoelectric .callout-note collapse="true" appearance="simple"}
## The Photoelectric Effect and the Birth of the Photon**
Physicists in the late 19th century sought to understand the spectrum of light emitted by a blackbody ‚Äî a perfect emitter and absorber of radiation. In 1900, Max Planck tried to match the observed blackbody spectrum with a formula. After some trial and error, he arrived at an expression that fit the data perfectly‚Äîbut only if he assumed that energy exchange between matter and radiation was quantized. He thought that, perhaps, the energy of oscillators in the cavity walls of the radiator could only take on discrete values and thus the measured absorptions and emissions needed to match these discrete levels.

$$
E = n h ŒΩ
$$
where 
ùëõ is an integer, 
ùúà is the frequency (inverse to wavelength) of the light, and 
‚Ñé is the constant he introduced‚Äînow known as Planck‚Äôs constant.

In 1902, Philipp Lenard published a series of experiments that revealed surprising features of the **photoelectric effect**‚Äîthe emission of electrons from a metal surface exposed to light. He observed that increasing the intensity of the incident light increased the number of emitted electrons but **did not** increase their kinetic energy. Instead, the energy of the emitted electrons depended on the **wavelength** of the light: shorter wavelengths (higher frequencies) produced more energetic electrons. Lenard also identified a **threshold wavelength**‚Äîif the light‚Äôs wavelength was too long (i.e., its frequency too low), no electrons were emitted, regardless of how intense the light was. These results posed a serious challenge to classical wave theory, which predicted that energy should increase smoothly with intensity, not depend on frequency.

In 1905, Albert Einstein offered a revolutionary explanation. He proposed that light is not just a continuous wave but is **quantized** into discrete packets of energy‚Äîlater called **photons**. The energy of each photon is proportional to its frequency, given by:

$$
E = h \nu
$$

where $h$ is Planck‚Äôs constant and $\nu$ is the frequency of the light. This bold hypothesis went beyond Max Planck‚Äôs earlier work, which had only assumed quantized energy exchange between matter and radiation‚Äînot that light itself came in packets. Einstein suggested that **light in free space** behaves as localized energy quanta, capable of transferring energy to electrons one photon at a time. His theory explained all of Lenard‚Äôs observations and laid the groundwork for quantum physics.

![Lenard/Einstein image created by ChatGPT, 2025](images/sensors/03-Lenard-Einstein.png){#lenard-einstein width="60%" fig-align="center"}

Einstein‚Äôs proposal was met with skepticism. The wave theory of light, supported by extensive experimental evidence like interference and diffraction, remained central to physics. Even Einstein himself was uneasy with the resulting **wave-particle duality**. He believed that this duality hinted at an incomplete understanding of nature, and he famously rejected the probabilistic foundations of quantum mechanics, remarking that ‚ÄúGod does not play dice.‚Äù Niels Bohr, a champion of the new quantum theory and Einstein‚Äôs intellectual foil, is said to have replied, ‚ÄúEinstein, don‚Äôt tell God what to do.‚Äù The tension between wave and particle descriptions remains a central‚Äîand still puzzling‚Äîtheme in physics.

That same year‚ÄîEinstein‚Äôs *annus mirabilis*‚Äîhe published four groundbreaking papers that reshaped modern physics. Alongside the photoelectric effect, his work on Special Relativity showed that **simultaneity is relative**, depending on the observer‚Äôs frame of reference. Walter Isaacson‚Äôs biography of Einstein provides an accessible account of these discoveries and the broader scientific context. Isaacson also details how Lenard, despite receiving the Nobel Prize for research that Einstein had admired and extended, became an outspoken anti-Semite who actively opposed Einstein‚Äôs recognition by the Nobel committee.

<!--
Photoelectric effect and Lenard
https://www.perplexity.ai/search/when-einstein-worked-out-the-p-_doyK0P0SPCj2lF728iVpg 
-->
:::

## Storing the electrons: Well capacity
To create an image, we must record the number of electrons generated at each of the photodiodes during the exposure duration. To make this measurement CMOS image sensors must store the electrons. They do so in a capacitor, one capacitor for each photodiode. These capacitors have a finite capacity, which is called the **well capacity** or **full-well capacity**. The well capacity is simply the maximum number of electrons that can be stored in the capacitor before it saturates. When the number of photo-generated electrons exceeds the well capacity, the photodiode is saturated and its output no longer increases with increasing light intensity. This limits the dynamic range of the sensor and is an important consideration in sensor design and performance evaluation.

## Reading the array of electrons

The electrons stored in the photodiode capacitors are readout by the CMOS circuitry. 

### Electronic circuitry

In the real device, nonlinearities do arise. Some of these are caused by the circuitry that manages the acquisition, storage, and readout of the data. These basic circuit functions introduce nonlinearities of a few percent @Wang2017-CMOSLinearity, and hardware design choices can influence the degree of nonlinearity. For example, the capacitance of the storage element and the characteristics of the amplifiers used in the readout process can both contribute to deviations from linearity.

Another source of nonlinearity arises from the photodiode itself. At high irradiance levels, the photodiode can saturate, meaning that it cannot generate additional electrons regardless of the incoming light. This saturation effect imposes a hard limit on the dynamic range of the sensor.

Temperature variations can also affect the linearity of the sensor. Changes in temperature can alter the behavior of the photodiode and the associated circuitry, leading to temperature-dependent nonlinearities. Modern sensors often include compensation mechanisms to mitigate these effects, but residual nonlinearities may still remain.

Understanding and modeling these nonlinearities is critical for applications that require high precision, such as scientific imaging or computational photography. Calibration techniques, such as measuring the sensor response under controlled conditions, can help correct for these nonlinearities in post-processing.

Over time, as technology has scaled, more complex electrical circuitry has been placed on the sensor. Modern image sensors frequently include circuitry that performs local processing to increase the dynamic range of the sensor (well recycling), or to reduce the intrinsic noise (correlated double sampling). Some of this processing is *adaptive*, that is the circuit actions depend on the property of the input image. Consequently, the sensor output can depend upon both the control parameters set by the user and the image content.

### Optical components

There are also other non-electrical components of most image sensors. These are small lens (lenslet) arrays and small color filter arrays that influence which part of the irradiance is capable of producing electrons from each photodiode.

The sensor records the light that exits from the back of the imaging optics. The photodiodes are small and they are located at the bottom of small holes in the sensor. It is necessary to guide the irradiance from the camera lens with an array of microlenses that account for the position of the pixel with respect to the center of the lens. The original role of the microlens array was to compensate for the challenges inherent in this geometry. Overtime, the microlens array has been designed to achieve additional objectives including determining how to focus the image and estimate the scene light field.

To make a color image, the sensor must measure some information about the relative intensity of different wavelengths. To make this measurement small color filters are placed in front of each pixel, so that different pixels are sensitive to different wavelengths. These spectral encoding differences enable the system to reconstruct a color image that is convincing to the human visual system.

## Sensor physics

### Photoelectric effect is linear

Conduction band and valence band.

Linearity of the absorption, up to and including saturation. When you run out of space to store the free electrons.

### Poisson statistics

### Wavelength information

Examples of the wavelength dependency of the silicon, as a function of depth.

Color filter arrays.

### Microlens arrays

Light field possibility.

## ARVS mobile photography article

I was reading @delbracio-mobile-2021 and it surprised me how it glossed over various claims. For example, a small sensor was deemed to be worse than a big sensor without reference to pixel size. The notion that one had to apply gain and thus multiply the noise without considering the light level or pixel size. No mention of well capacity.

Probably some of this stuff is true, some is false, but the whole article doesn't explain the way we should do here. Also, this kind of vague stuff:

3.2.7. Tone mapping. A tone map is a 1D LUT that is applied per color channel to adjust the tonal values of the image. Figure 10 shows an example. Tone mapping serves two purposes. First, combined with color manipulation, it adjusts the image‚Äôs aesthetic appeal, often by increasing the contrast. Second, the final output image is usually only 8 to 10 bits per channel (i.e., 256 or 1,024 tonal values), while the raw-RGB sensor represents a pixel‚Äôs digital value using 10‚Äì14 bits (i.e., 1,024 up to 16,384 tonal values). As a result, it is necessary to compress the tonal values from the wider tonal range to a tighter range via tone mapping. This adjustment is reminiscent of the human eye‚Äôs adaptation to scene brightness (Land 1974). Figure 11 shows a typical 1D LUT used for tone mapping.

### Curved sensors

Finally, it is possible to reduce spherical aberration by changing the imaging surface. An image system that couples a spherical lens with an appropriately curved sensor can have very small spherical aberration. This idea is partly inspired by the fact that the retina in many animals lines the interior of the eyeball, forming a curved sensor surface. While this is not widely used, there have been several projects to create sensors on bendable materials, including some very expensive research projects. The idea is to build these sensors for cameras with very large field of view and very high resolution across the field of view.

```{=html}
<!-- 
Curved sensor projects
[HRL](https://www.hrl.com/news/2024/03/11/hrl-advances-to-camera-build-phase-of-curved-sensor-technology?t)
[Sony](https://www.dpreview.com/articles/2279255612/sony-s-curved-sensors-may-allow-for-simpler-lenses-and-better-images?comment=5515068691)
[John Rogers](https://thefutureofthings.com/6161-a-spherical-camera-sensor/)
-->
```