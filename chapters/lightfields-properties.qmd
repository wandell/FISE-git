# Light field properties {#sec-lightfield-properties}

Many image systems are engineered to quantify scene properties, such as an object's size, its distance to the sensor, or its material. These measurement-oriented systems have many applications, including industrial quality control, autonomous navigation, and medical diagnostics. Even in consumer photography, where the primary goal is often to capture and render a visually compelling representation of a scene, estimating underlying physical characteristics — like the ambient illumination — can significantly enhance performance. As highlighted in the Preface with the quote from Helmholtz [@fig-helmoltz], inferring scene properties is arguably the fundamental task of visual perception.

A common challenge in many image system applications is that the measurement process is often underdetermined. This means that multiple distinct scene properties could potentially lead to the same observed measurement. In such situations, a powerful principle, often attributed to Thomas Bayes, becomes invaluable. Bayes' Rule suggests that when faced with multiple plausible solutions, we should select the one that is most probable. While the general principle is conceptually elegant, the practical challenge lies in accurately determining the likelihood of each potential solution.

While we may not always possess explicit knowledge about the probabilities of different solutions, we can often leverage prior information or learn from experience. One effective strategy for making more informed inferences is to understand the inherent regularities present in the world. In this section, we will explore some fundamental regularities concerning the spectral (wavelength-dependent) and spatial characteristics of natural images.

Identifying and leveraging regularities within scene radiance is a central theme we will revisit throughout this book In specific, well-defined applications, such as medical imaging or controlled industrial inspection environments, these regularities can significantly constrain the set of plausible interpretations, providing a valuable and precise guide for analysis.

## Representing regularities

When considering the scene regularities, we face two interconnected challenges. First, how do we come to understand these underlying patterns? In some instances, our knowledge stems from building physical models of the signals, where regularities emerge as a natural consequence of our understanding. The solar radiation analysis below provides a good example of this. In other cases, we learn about regularities through empirical observation of numerous examples, employing computational tools to quantify these patterns. Our discussions on surface reflectances and the characteristic spatial structure of natural images will illustrate this approach.

Two classical mathematical frameworks are fundamental for specifying regularities across various scientific and engineering disciplines, and both will feature prominently in this book. Bayes' Rule offers a powerful yet straightforward way to reason about probabilities. More generally, probability distributions provide a versatile language for representing the likelihood of observations. While these distributions can sometimes be directly used in simulations, their incorporation can often present significant computational hurdles.

A second widely employed mathematical tool is dimensionality reduction, particularly techniques based on linear models. Dimensionality reduction offers a highly practical computational approach for simplifying complex datasets. However, it often provides only an approximate representation of the underlying statistical regularities.

More recently, neural network training has emerged as a potent method for inferring statistical regularities from large datasets, simultaneously yielding a computationally tractable model. This data-driven approach is proving increasingly valuable across numerous applications.

In this section, we will delve into the two classical techniques. They remain indispensable tools, often providing crucial insights into the physical principles governing the system. We will then apply them to key descriptions of scene spectral radiance. As we progress through subsequent sections of the book, we will explore how neural network training is increasingly being integrated into image systems engineering.

```{=html}
<!--
Eero's statistical work surely gets in here somewhere.  Not obviously in this section, but perhaps in the spatial regularity part.
-->
```

### Bayes Rules

If we wish to know the probability of a specific condition, $C$, being true given measured data, $d$, we can use the laws of probability to write

$$
P(C | d) = \frac{P(d \cap C)}{P(d)} = \frac{P(d \cap C) P(C)}{P(d) P(C)}= \frac{P(d | C) P(C)}{P(d)}
$$

::: {#Bays-terms .callout-note style="color: gray" appearance="simple"}
### The terms in Bayes' Rule

-   $P(C | d)$ - **posterior probability** of C given d.
-   $P(d|C)$ - **likelihood** of d given C.
-   $P(C)$ - **prior probability** of C.
-   $P(d)$ - **marginal likelihood** of d.
:::

Implementing Bayes' Rule through this formula requires knowing the likelihood of the data given the condition, $P(d|C)$. This knowledge can come from a model of the process that enables us to simulate the data. Being able to work out this probability is a fundamental task for the scientist or engineer. A more difficult challenge in using Bayes' Rule is that we often have little idea about the prior, $P(C)$. The cases in which we do have this world knowledge can be very valuable.

There are many cases, often in the applied sciences and engineering, when enough is known to apply Bayes' Rule. A good example is a medical image diagnostic, say the chance of having a dark spot on of contrast $d$ on the lung to detect a cancerous lesion. We may have measurements from many people, and in retrospect they either will have the disease or not. We can use these data to estimate the likelihood of $P(d \mid C)$ and $P(d \mid \neg C)$. We may further have historical data of the percentage of people in the population who have the disease, $P(C)$. The marginal probability is

$$
P(d) = P(d \mid C) P(C) + P(d \mid \neg C) P(\neg C)
$$

From these terms, we can compute the probability of having the posterior probability, \$P(C \mid d)\$. There are many cases in which we do not have the data to estimate all of the needed likelihoods. In those cases, we can use other tools.

### Linear models

Linear models have many uses in science and engineering. One application is representing certain regularities in the data. Specifically, suppose we have a collection of measurement vectors, each with $D$ values. We can express each of the vectors as the weighted sum of a set of $M$ vectors, which we call the *basis* vectors. If $M$ is much smaller than $D$, the basis set captures a regularity that is present in the original data set.

As an example, suppose the data are a set of radiance spectra, $d(\lambda)$, we measured over the visible range from 380 nm to 780 nm in 1 nm steps; hence, each vector, $d$, has 401 values. After making a few thousand (or million) measurements, we discover that we can closely approximate any of the measurements, $d_j$, as the weighted sum of $M$ basis functions, $B_i$. We write the approximation this way: 
$$
d_j(\lambda) \approx \sum_{i=1}^{M} B_i(\lambda) w_{ij}
$$ {#eq-linear1}

There are a many tools to find basis vectors, $\mathbf{B_i}$, and these tools rely on different objectives. Sometimes, the basis vectors are selected from physical or computational principles. This is the case for the Fourier basis functions, which are the harmonic functions, and are a natural part of magnetic resonance imaging, sound, and heat transfer. It is also common to select $M$ basis functions that explain the most variance in the data, which is the choice we make when using Principal Components Analysis (PCA) that relies on the important matrix decomposition the singular value decomposition (SVD).

In both of these cases, Fourier and PCA, the basis vectors are orthogonal, which means that the inner product[^lightfields-properties-1] of any pair of basis vectors is zero. The mathematical expression for the inner product and orthogonality is

[^lightfields-properties-1]: Widely used synonyms for the term inner product are *dot product* and *scalar product*.

$$
0 = \sum_{\lambda} B_i(\lambda) B_j(\lambda)
$$

It is quite common to use matrix notation to express the linear model. With this notation, the linear model places the basis vectors into the columns of a matrix, $\mathbf{B}$. Each data vector, $\mathbf{d_j}$, is the product of a weight vector, $\mathbf{w_j}$ times the matrix of basis vectors.

$$
\mathbf{d_j} = \mathbf{B} \mathbf{w_j}
$$ {#eq-linearmatrix}

This regularity is often called dimension-reduction. It is a helpful regularity, but quite different from knowing the full probability distribution. The regularity allows us to take the high dimensional data, say with 401-vectors, and represent them with many fewer numbers, say the 10 or 20 weights. In that sense, the reduction informs us about the likelihood of certain data sets. But there remains a much more that can be learned about the data because not all of the different weights are likely, or in some cases physically realizable. Thus, the dimensionality gets you part of the way there, but studying the properties of thew weights can provide even more information (@DiCarlo2003-spectralestimationtheory)

Linear models appear in many places in this book, with many different applications. If you would like to read more about them, have a look at the chatty discussion in Appendix @sec-linearmodels. From there you can find your way to more advanced and authoritative literature. The Appendix also contains pointers to important nonlinear methods that - in some circumstances - can be very helpful (@Maaten2008-dimensionalityreduction).

In the following sections I describe regularities that have been discovered about various spectral measures. We begin with regularities concerning the spectral properties in the visible band. We then turn to regularities concerning the spatial properties of natural images, again in the visible band.

## Spectral regularities

```{=html}
<!-- [https://chatgpt.com/share/679ebc49-5a7c-8002-a282-598e7f1bbb30 ]
Solar spectrum. Fraunhofer lines.  Stability of the solar spectrum 

https://chatgpt.com/c/679ebbfc-b510-8002-b372-489cd173d5d0

CIE Data downloaded to ISETCam from here.  Placed in data/lights/daylight and data/lights/solar  These appear to be distinguished, for now, based on Earth's surface or Sun's surface.

https://cie.co.at/datatable/components-relative-spectral-distribution-daylight
-->
```

### Solar Radiation

The Sun serves as Earth's primary radiation source. The radiation emitted from the Sun's surface in the visible spectrum remains relatively consistent over time. Due to its significance for imaging and human vision, skylight became an early subject of scientific investigation.

In 1814, Joseph Fraunhofer observed and carefully documented numerous dark lines in the solar spectrum, now known as **Fraunhofer lines** It was subsequently discovered that these lines are present because light generated in the Sun's hot, dense interior (photosphere) travels through its cooler outer layers (chromosphere and corona). During this passage, atoms and ions absorb specific wavelengths of light that correspond precisely to their electron transitions. The reliability of these solar absorption lines allows scientists to use them as reference points to calibrate laboratory instruments for spectral measurements. For example, James Clerk Maxwell used these lines to calibrate the first experimental rig for human color-matching (@maxwell1860-IVTheoryCompound,@maxwell1993-ReprintTheoryCompoundColours).

![The black curve displays the relative solar spectrum, which is stable over time. The red lines indicate are drawn at wavelengths where the spectrum exhibits narrow intensity reductions, known as Fraunhofer lines, that originate from ions and atoms in the Sun's outer layers. The blue dotted lines mark intensity reductions caused by atmospheric water vapor.](images/lightfields/01-solar-lines.png){#fig-solarspectrum width="60%" fig-align="center" width="70%"}

::: {#fraunhofer .callout-note collapse="true" appearance="simple"}
## [Joseph Fraunhofer (1787-1826)](https://en.wikipedia.org/wiki/Joseph_von_Fraunhofer)

Fraunhofer was a brilliant applied researcher who worked on the physical properties of materials, particularly glass. He also made important contributions to the theory of image formation and optics.

The Fraunhofer lines, which he discovered while building a device to measure the spectral transmissivity of glass, are reported in a paper reviewing the properties of glass. These relative intensity of these lines are determined by the material composition of the stars, and these measurements are stilled used in astronomy to determine the composition of celestial bodies. Fraunhofer died at the age of 39, poisoned by heavy metal vapors used in his material research. Europe's largest body for the advancement of applied research, the Fraunhofer Society, is named to honor him.

![](images/people/Fraunhofer.png){fig-align="center" width="391"}

This is a good historical review of the [Fraunhofer lines](https://joachimweise.github.io/post/2020-10-20-fraunhofer-lines/) and subsequent work.
:::

### Atmospheric Effects

Some of the spectral absorption lines observed at Earth's surface originate from a different source: water vapor (H₂O) and other molecules in Earth's atmosphere. Unlike the stable solar absorption lines, these atmospheric absorptions fluctuate relatively quickly with changing weather conditions. There is also atmospheric variation in the skylight spectral power with the time of day, as the radiation's path length through the atmosphere varies. Longer path lengths — such as during sunrise or sunset — modify the spectral composition of radiation reaching Earth's surface.

A number of different groups have reported measurements of solar radiation at the Earth's surface at varying times of day and weather conditions [@Judd1964-daylight]. Figure #fig-daylights shows examples of typical variation in the shape of the spectral power distribution in Granada, Spain [@Hernandez2001-daylight] and Stanford, California [@DiCarlo2000-daylight].

![The spectral power distribution of daylights sampled at the Earth's surface in Granada, Spain (left) and Stanford, California (right) at different times of day and weather conditions. The spectra have been scaled to be equal at 450 nm. The curves have different slopes, and their shapes vary with weather and time-of-day. Identifiable Fraunhofer lines and water vapor lines are present in the measurements.](images/lightfields/01-solar-samples.png){#fig-daylights width="90%" fig-align="center" width="90%"}

These combined factors—atmospheric composition and path length variations—create a complex, but restricted, distribution of sky radiances at the Earth's surface. To summarize the regularities, scientists and engineers have created linear models that can represent the natural variations. The original model was introduced by Judd, Macadam and Wyszecki [@Judd1964-daylight]. They analyzed skylight at the Earth's surface using about 650 different measurements. They found that the spectral power distributions, normalized to remove absolute radiance level, could be modeled as the weighted sum of three terms: a mean and two basis functions (Figure @fig-ciedaylight). Such a model is useful because it enables us to represent any particular sample using just three numbers, the two weights and an overall scalar to match the level. The model is also useful for applications in which a scene is acquired under full sky illumination, and we would like to estimate the spectral power distribution of the skylight. We only need to estimate the weights, and from those we can make a good estimate of the full spectrum.

![The solid curve shows the mean daylight spectrum measured by Judd, Macadam and Wyszecki @Judd1964-daylight. Their measurements evolved into a CIE standard linear model for daylights. The solid circles are placed at the wavelengths of Fraunhofer and atmospheric water vapor lines. The dotted and dashed curves are basis functions. Together with the mean, they form a linear model of daylight spectral power distributions.](images/lightfields/01-solar-basis.png){#fig-ciedaylight width="50%" fig-align="center"}

```{=html}
<!--
Moonlight regularities. Used for space travel.https://chatgpt.com/share/67410dd0-ff54-8002-9b73-65294a849fe6 “The Potential of Moonlight Remote Sensing: A Systematic Assessment” explores the potential of using moonlight in remote sensing for nighttime Earth observation. The study uses data from VIIRS, the ISS, and UAV systems to evaluate moonlight’s capabilities.“Simulation Study of the Lunar Spectral Irradiances and the Earth-Based Moon Observation Geometry” focuses on modeling the spectral irradiance of moonlight using the Hapke model and its applications in satellite radiometric calibration.
-->
```

::: {#california-sun .callout-note collapse="true" appearance="simple"}
## Do it yourself daylight spectra

When he was a student at Stanford, Jeff DiCarlo decided to measure the daylight outside his office for himself [@DiCarlo2000-daylight]. Checking things for himself is in his nature. Jeff pointed a spectral radiometer at a calibrated surface outside his window and configured a computer to make about ten thousand measurements, one a minute, during the course of a few rainy weeks in California (January 28, 2000 to February 16, 2000). I included a sample of 1000 of these measurements in ISETCam, and I have the whole group along with time stamps if you want them.

![Average daylight measurements on the Stanford campus (solid) and CIE daylight basis fit to the measured average (dotted). The solid circles are placed at the locations of Fraunhofer and water vapor wavelengths.](images/lightfields/01-solar-stanford.png){#fig-stanfordsun fig-cap-location="bottom" width="50%" fig-align="center"}

@fig-stanfordsun shows the mean spectral power distribution of these measurements. You can see that the instrument recorded several 'notches' the spectrum at wavelengths corresponding to the Fraunhofer and water vapor lines. We also asked whether these data could be fit by the CIE linear model - and the least-squares fit is shown by the dotted line. I suspect there is an instrument calibration error of a few nanometers at wavelengths above 700nm, and were I to write a research paper based on these data, I would probably introduce the correction because I believe the physics on the locations of these wavelengths is secure. This analysis might provide you with a sense of the relative accuracy of measuring and approximating spectra with conventional lab equipment.
:::

```{=html}
<!-- Later in the book use these daylights to show the appearance (chromaticity) of the different daylights.  The Granada paper does this nicely. "Color and spectral analysis of daylight in southern Europe"
-->
```

The regularities in the sky radiance are even more constrained than the linear model. First, the weights are correlated: When you know one of the weights, you can make a reasonable guess about the other weight. Also, some weights are impossible because they would produce negative values in the spectrum @DiCarlo2000-daylight. Thus, the linear model gets us part of the way to establishing the regularities, but there is even more to be measured.

::: {#daylight-weights .callout-note collapse="true" appearance="simple"}
### Correlated weights

![The daylight basis weights from the Granada and Stanford data sets. The weights are clustered within a plane, although there are occasional outliers. The orthogonality of the basis functions does not imply independence or orthogonality of the measured basis weights.](images/lightfields/01-dayweights3d.gif){#fig-dayweights fig-align="center" width="372"}
:::

## Surface spectral reflectance regularities

Vision is a remarkable invention that enables us to interpret the environment by recording the how the ambient illumination interacts with it. Four things can happen when electromagnetic radiation interacts with a surface. The radiation arriving at a surface might be (a) reflected, (b) absorbed, (c) pass through, or (d) interact with the material and generate new radiation (fluorescence). We will consider each of these interactions in different sections of this book. Here, we describe some regularities in the surface spectral reflectance of many common materials.

A full description of surface spectral reflectance can include many parameters. The radiation incident on a small, planar patch of material can come from many different angles (two dimensions). The radiation may be reflected in many different directions (two more dimensions). The reflected light might be measured with respect to both wavelength and polarization (two more dimensions). We will set aside fluorescence for the present, returning to it later.

Despite this potential complexity, there are many relatively simple cases that are simpler to describe and still useful. Perhaps the simplest and most important are materials that reflected light equally in all directions, no matter what direction the light is coming from. Such materials have *diffuse reflectance*, also called *Lambertian reflectance*.

You can judge to what extent a material is diffuse if you have a laser pointer. Direct the laser pointer onto a planar surface patch and then move about, checking what the reflected light looks like as you view it from positions. In many cases, the reflected light appears pretty much as the same intensity as you move around. Also, you can change the position of the laser, but still illuminate the same point. For diffuse materials the reflected light will be about the same. These materials reflect diffusely because their surfaces have rough microscopic facets, oriented in many directions. When the incident light arrives, it may be reflected by facets oriented randomly, in many different directions.

::: {#Lambert .callout-note collapse="true" appearance="simple"}
## Johan Lambert

The term Lambertian surface honors the Swiss physicist, [Johann Lambert](https://en.wikipedia.org/wiki/Johann_Heinrich_Lambert) (1728-1777) who formalized the mathematical description. Many surfaces are modeled as Lambertian in computer graphics, or assumed to be Lambertian in computer vision estimates of the environment. Examples of materials with Lambertian reflectance include chalk, plaster, uncoated paper, matte paints made from pigments.

Adding additional details about the surface reflectance can improve the system performance, but it is common to start with a Lambertian approximation.

![Johann Lambert. His ideas were spread widely.](images/people/Lambert.png){#fig-lambert fig-align="center" width="180"}

Lambert worked on many topics concerning geometric optics, and he contributed an important book **Photometria** that incorporated many important geometric ideas, explaining the relative brightness of objects as seen from different points of view. Using his excellent mathematics, and the assumption that light travels in straight lines, he showed that illumination is - proportional to the strength of the source, - inversely proportional to - the square of the distance of the illuminated surface, and - the [sine of the angle](https://en.wikipedia.org/wiki/Lambert%27s_cosine_law "Lambert's cosine law") of inclination of the light's direction to that of the surface. He also wrote a classic work on [perspective](https://en.wikipedia.org/wiki/Perspective_(visual) "Perspective (visual)") and contributed to [geometrical optics](https://en.wikipedia.org/wiki/Geometrical_optics "Geometrical optics").

I was interested to learn that Lambert was the first to show that $\pi$ is an irrational number, a claim that the great Euler sought to prove as well.
:::

In the case of diffuse reflectance, we can summarize the material relatively simply. We only need to measure the fraction of incident light, for each wavelength, is reflected. This is called the spectral reflectance function

The spectral reflectance is measured wavelength-by-wavelength, that is they are functions of wavelength. In the visible band, these functions are typically quite smooth, and it is possible to summarize them using only a few parameters. It is not realistic to obtain a sample of all surfaces, but we might consider the surfaces in a particular context (e.g, walking in a forest, the interior of a commercial office building, driving on a highway, or medical images of the body). The likely surfaces in some context can be sampled and usefully summarized with a linear model [@Cohen1964-linearmodel, @Maloney1986-linearmodels-surface, @Hardeberg2002-reflectancedimensionalty]. The best basis functions will depend, somewhat, on the where you are measuring (forrest, desert, city, countryside). The number of basis functions needed will depend on the task requirements (e.g., 5% or 1% accuracy). But the general usefulness of a linear model approximation as a computational tool is not in doubt.

![Sample spectral reflectance functions of clothing. The fraction of reflected light, wavelength-by-wavelength, is between 0 and 1. Such smooth spectral reflectance curves are typical of many surfaces. Surfaces that are not diffuse can be modeled as having a diffuse component with a smooth spectral reflectance.](images/lightfields/01-reflectance-clothes.png){#fig-reflectance-cloths fig-align="center" width="60%"}

::: {#Things-stuff .callout-note collapse="true" appearance="simple"}
## Things and Stuff

We introduce the notion of surface reflectance as a general property, allowing that it depends on context. In an insightful paper, Ted Adelson further divided the environment into *things and stuff*" @Adelson2001-stuff. Things are the countable, discrete objects with defined shapes and boundaries (e.g., cars, chairs, animals). Stuff is amorphous materials or textures without fixed boundaries (e.g., water, grass, sky, sand). Dividing the environment into these categories also helps us set computational goals.

![Illustration of *things* and *stuff*. I am not sure how to count the grass - the collection of stuff or the blades of grass. Is the ocean stuff but a wave a thing? The sky, and beach (globally) seem like stuff. The rabbit and birds are definitely things, but their coats might be considered stuff. It is worth thinking about the difference. It is also worth noticing how Imagen3 produced some pretty odd animal faces.](images/paste-1.png){#fig-things-stuff fig-align="center" width="70%"}

For example, segmentation applies to *things* which are analyzed for object identity and function; in contrast, *stuff* is regionally based on texture and material properties. A great deal of computer vision focuses on tasks such as naming or counting cars on the road, people in the room, airplanes in the sky. We wish to be able to recognize such things whatever their material might be. A vision system must also be able to make judgments about a sandy beach, a cloudy sky, a grassy hillside. We wish to be able to recognize such stuff whatever its shape might be.

A related challenge in human vision is quantifying the appearance of things and stuff, and for this challenge surface reflectance, texture, and context (e.g., ambient illumination, or what is nearby) all play a role @Schmid2023-materialperception. Whether there is a corresponding concept - something we might call appearance for a computer vision system - is less studied. Perhaps the meaning of appearance for a computer vision system might be explained by its estimate of material reflectance and texture.
:::

Over the decades a number of imaging groups have measured the reflectance of many types of materials, including paints, minerals, plants, clothing, and various biological specimen such as skin and teeth. Government agencies, particularly those involved in remote sensing, have measured the reflectance of many materials that might be seen in satellite images. As a general rule, the spectral reflectances of the samples reported in the literature are approximated to an accuracy of a few percent by linear models using between 3 and 8 basis functions.

A Finnish group, for example, measured 1269 different color swatches from the Munsell Book of Colors. This widely used book spans a wide range of color samples that provides designers with a method for identifying, selecting, matching, and communicating about color. @Parkkinen1989-munsellspectra made their spectral measurements at 1 nm resolution. A linear model using three basis functions fits all the measurements with an error of about 2.5%. A linear model with 5 basis functions fits the data to within about 1%. This accuracy is usually often enough for image systems applications.

![Three basis functions for the Munsell Book of Color, from measurements by @Parkkinen1989-munsellspectra. The first linear model function is very close to the mean and is all positive. The 2nd and 3rd functions capture the principal variation in the samples. The three basis functions, calculated using the singular value decomposition, are orthogonal to one another. Notice the location of the zero-crossing of the 2nd function, which is near 560 nm, and the two zero-crossings of the 3rd function which are near 490 nm and 600 nm.](images/lightfields/01-munsell-basis.png){#fig-munsell-basis fig-align="center" width="60%" fig-cap-location="bottom"}

The linear models calculated for several types of common materials are often similar. This is illustrated in a set of measurements Vrhel and colleagues @Vrhel1994-MeasurementAnalysisObject. Their data, also included ISETCam, measures a few dozen surfaces from several different groups (paint, natural objects, man-made objects, food). It is likely the authors chose the samples to have similar means by insisting on a broad representation of color appearances. Had they chosen only the leaves in a forest, or only the rocks in the desert, the means might have differed. The properties of the 2nd and 3rd basis functions are also similar, and this is not likely to be a selection artifact. The measured spectral reflectance functions are relatively smooth with respect to wavelength, and thus the basis functions are smooth. The zero-crossings of these basis functions fall at locations that divide the visible spectrum into three, similar sections - bluish, greenish, and reddish.

![Three basis functions comparing the basis functions derived from the @Parkkinen1989-munsellspectra Munsell Color Book data (upper left) and three different categories measured by @Vrhel1994-MeasurementAnalysisObject. The basis functions are not identical, but they have many similarities, including the zero-crossings of the 2nd and 3rd basis functions.](images/lightfields/01-reflectance-bases.png){#fig-reflectance-bases fig-align="center" width="80%"}

```{=html}
<!-- 
Scripts to look over: s_surfaceMunsell, s_reflectanceBasis, s_sceneReflectanceChartBasisFunctions 
Maybe illustrate the visual size of the errors, as I have in other scripts.\
-->
```

### Dichromatic reflectance model (DRM)

The DRM is a useful surface reflectance model that is somewhat more complex than diffuse (Lambertian) materials, but simpler than a full reflectance model. This model is still used in computer vision applications where the user would like to identify specular reflections. The DRM is a good approximation for light reflected from **inhomogeneous dielectric materials** (non-conductors like plastics, paints, wood, skin, etc.). The model approximates reflectance as a weighted sum of a diffuse term and a mirror-like (specular) term.

The idea is that light is reflected by two mechanisms. Some of the light is reflected at the material's **interface** with air. This light does not interact with the material, and thus its spectral power is the approximately the same as the light source. Another portion of the light enters into the material (**subsurface**). In the subsurface it interacts with the material's pigments, both by scattering and absorption. The light that emerges from the subsurface determines the object's spectral reflectance. The subsurface portion is approximately Lambertian, as the subsurface pigments are randomly positioned and oriented.

The DRM represents the reflected light, $C(\lambda)$ as the sum of the interface and subsurface terms

$$
C(\lambda) = m_s(g) I(\lambda) E(\lambda) + m_d(g) D(\lambda) E(\lambda)
$$

$E(\lambda)$ is the spectral power distribution of the light; $I(\lambda)$ is the interface spectral reflectance (often assumed to be constant across wavelengths); $D(\lambda)$ is the diffuse subsurface reflectance, which is characteristic of the material. The terms $m_s(g)$ and $m_d(g)$ are geometric scaling factors whose values depend on the illumination and viewing angle. These angles are ordinarily represented with respect to the surface normal at the surface point $g$. The reflected light, $C(\lambda)$ is sometimes called the *color signal* because this spectral radiance is an important contributor to the visual system's color judgment. (Though, I have friends who object to this term).

The DRM finds application in topics such as illuminant estimation, specular highlight identification and removal, and material classification. It is, however, limited in various ways because it does not accurately model complex surfaces, and it does not apply well to scenes with multiple light sources or very large light sources, say spanning the sky.

### Bidrectional reflectance distribution function (BRDF) {#sec-properties-brdf}

The *bidirectional reflectance distribution function* (BRDF) characterizes how light rays of wavelength $\lambda$, incident at the surface at an angle $\omega_i$, are reflected. The reflected, outgoing light is specified at all angles $\omega_o$. Both $\omega_i$ and $\omega_o$ are measured with respect to the surface normal. We can write the intensity as a function of input and output angles as the function $B(\omega_i,\omega_o,\lambda)$. The reciprocity of principle of light[^lightfields-properties-2] implies that the BRDFs, with only a few exceptions, obey this symmetry relationship.

[^lightfields-properties-2]: The principle states that if a ray of light travels from point A to point B through any optical system, then the same ray can travel in the reverse direction from B to A, following the same path. This remains true regardless of the complexity of the optical system, including reflections, refractions, and scattering. The concept is often attributed to Hermann von Helmholtz, who formally stated it in his 1856 book Handbook of Physiological Optics @helmholtz1896.

$$
B(\omega_i,\omega_o,\lambda) = B(\omega_o,\omega_i,\lambda)
$$

Each of the direction vectors is specified by two parameters, typically defined with respect to the surface normal. One parameter is the angle around the surface normal (azimuth) $\phi_i, \phi_o$, and the second is the elevation $\theta_i, \theta_o$ of the ray. We conventionally describe the BRDF with three parameters, but there are really five parameters in there. That makes the BRDF tedious to measure.

The instrument used to measure the BRDF is called a gonioreflectometer. The instrument has a light source that can be positioned to illuminate the surface at many different angles (typically on a sphere), and a detector that can be positioned at many locations on the sphere. There are many different designs for *gonioreflectometers*. The one in @fig-gonioreflectometer shows a surface is placed on a stage that can be rotated. The light and camera are on two arms whose positions can be controlled, sweeping out the possible angles for the incident and reflected light.  To measure the reflectance of a surface requires calibrating the amount of incident light, say by making a measurement with a perfect, white diffusing surface, and then making a second measurement with the material of interest.  The ratio of the responses is the reflectance.  The same calibration measurement can be re-used for multiple materials of interest.

![Example of a **gonioreflectometer**. The light source and detector are on robot arms that can be positioned around the surface. If we fix the wavelength and angle of the incident light ray, we can specify the relative intensity of the reflected light as an image that spans the two angle parameters](images/lightfields/01-gonioreflectometer.png){#fig-gonioreflectometer width="75%"}

The input and output angles are 2-vectors and the BRDF is a scalar function. We can make all of the angles explicit this way.

$$
B(\omega_i, \omega_o, \lambda) = f_r(\phi_i,\theta_i,\phi_o,\theta_o,\lambda)
$$

<!-- 
I created a Gemini document about data driven papers 
[Document on data-driven BRDFs](https://docs.google.com/document/d/1Wy2ygwu30i5quFX21kgz34OGSszNwUXn39Dfh4ypavA/edit?usp=sharing)

[Source for spherical azimuth and elevation and zenity.](https://www.researchgate.net/publication/334197029_A_new_model_for_reduction_of_Azimuth_asymmetry_biases_of_tropospheric_delay/figures?lo=1)

 -->

#### Data driven

There are papers measuring BRDFs and modeling them using linear models and nonlinear models. Data driven approach.

@Matusik2003-datadrivenBRDF @Tongbuasirilai2020-BRDF

There is a MERL database.

@MERL-BRDF-Brand

#### Theoretical

Cook Torrance mainly here.

```{=html}
<!--

Sand, reasonable figures. 

Tongbuasirilai2020-BRDF
 - https://opg.optica.org/ao/fulltext.cfm?uri=ao-54-31-F243&id=330097

Excellent source for material models and some simple formula of reflection.  Introduces dielectrics (Plastic, Stone, Wood, Leather, Glass, Water, Air), and conductors (metals)
https://leeyngdo.github.io/blog/computer-graphics/2024-03-22-reflection-model/?utm_source=chatgpt.com

Fermat's principle: light will follow the path that requires the least amount of time, as compared to other nearby paths, to pass between two points.

This web page has no graphs illustrating the microfacet theory from Ken Torrance and his students.  It has a nice introduction to the formulae for Cook Torrance, from Sparrow Torrance, as well.

-->
```

## Spatial regularities

Something about fractals. Textures. Other stuff.

### Fractals and Power Spectra scale invariance

The scale invariance of natural images arises from their power-law spectral decay, which mathematically ensures statistical consistency across spatial scales. The connection between 1/f\^α spectra and scale invariance can be expressed through:

The power spectrum of natural images follows:

$$
P(\omega) \propto \frac{1}{|\omega|^{2-\eta}} \quad \text{where } \eta \approx 0.8\text{--}1.5
$$

Here, $\omega$ represents spatial frequency magnitude, and $\eta$ controls the decay rate[^lightfields-properties-3][^lightfields-properties-4][^lightfields-properties-5].

[^lightfields-properties-3]: https://people.csail.mit.edu/danielzoran/zoranweiss09.pdf

[^lightfields-properties-4]: https://web.mit.edu/torralba/www/ne3302.pdf

[^lightfields-properties-5]: https://www.sciencedirect.com/science/article/pii/0042698996000028

For scale invariance under spatial scaling $x \to \lambda x$, the power spectrum must satisfy:

$$
P(\lambda\omega) = \lambda^{-\kappa}P(\omega)
$$

Substituting the power-law form:

$$
P(\lambda\omega) = \frac{1}{|\lambda\omega|^{2-\eta}} = \lambda^{-(2-\eta)}P(\omega)
$$

This matches the scale-invariance condition with $\kappa = 2-\eta$, demonstrating that statistical properties remain consistent across scales[^lightfields-properties-6][^lightfields-properties-7][^lightfields-properties-8].

[^lightfields-properties-6]: https://people.csail.mit.edu/danielzoran/zoranweiss09.pdf

[^lightfields-properties-7]: https://web.mit.edu/torralba/www/ne3302.pdf

[^lightfields-properties-8]: http://vigir.missouri.edu/\~gdesouza/Research/Conference_CDs/IEEE_ICCV_2009/contents/pdf/iccv2009_285.pdf

**Fractal connection** The power-law exponent relates to fractal dimension $D$ through:

$$
D = 3 - \frac{\eta}{2}
$$

where $D$ quantifies space-filling characteristics. Natural images typically exhibit $D \approx 2.2\text{--}2.6$, consistent with their 1/f\^α spectra[^lightfields-properties-9][^lightfields-properties-10].

[^lightfields-properties-9]: https://people.csail.mit.edu/danielzoran/zoranweiss09.pdf

[^lightfields-properties-10]: https://www.nature.com/articles/srep46672

This mathematical framework shows that 1/f\^α spectra inherently encode fractal, scale-invariant structure - the same statistical regularities appear whether analyzing fine details or coarse features of natural scenes[^lightfields-properties-11][^lightfields-properties-12][^lightfields-properties-13].

[^lightfields-properties-11]: https://people.csail.mit.edu/danielzoran/zoranweiss09.pdf

[^lightfields-properties-12]: https://web.mit.edu/torralba/www/ne3302.pdf

[^lightfields-properties-13]: http://vigir.missouri.edu/\~gdesouza/Research/Conference_CDs/IEEE_ICCV_2009/contents/pdf/iccv2009_285.pdf

::: {style="text-align: center"}
⁂
:::

### Natural scenes and the 1/f spatial frequency falloff

Who first pointed this out? Relationship to the scale invariant idea of fractals, and the fractal nature of many things.

Ruderman and Bialek paper on natural image statistics.

Field, Olshausen

@lee2001-deadleaves

### Natural scenes: The dead leaves model

Jon's Matlab script as a basis for discussing this. [Software from Jon showing 1/f issues.](https://github.com/isetbio/isetbio/blob/main/scripts/oneoverf/s_oneOverF1D.m)

Also, the deadleaves function in ISETCam.