# Light field properties {#sec-lightfield-properties}

Many image systems are designed to measure properties of a scene, such as the size of an object, its distance from the camera, or the properties of the material it is made of. Image systems that measure these quantitites are fundamental to applications like industrial inspection, autonomous vehicles, and medical imaging. Even in consumer photography applications, whose primary mission is to capture and display the visual impression of the scene, estimating scene physical characteristics - such as the ambient lighting - can be useful.  And in the Preface, I quoted Helmholtz who wrote that inferring scene properties is the core function of human vision [@fig-helmoltz].

In many image systems applications the measurement is under constrained, by which I mean that multiple object properties might produce the same measurement. There is a useful principle that applies to these situations, often attributed to Thomas Bayes. If many potential solutions exist to a problem, Bayes' Rule says select the most likely one. The general principle seems great; the hard part is knowing which answer is the most likely one.

We do not always have information about the likelihood of different solutions, but sometimes we do. One way to make better guesses is to learn about regularities in the world. In this section we describe some regularities about the spectral (wavelength) and spatial properties of natural images. In certain specific applications, such as medical imaging or industrial inspection conditions, the regularities may narrow the set of possible answers greatly and provide a very valuable and precise guide.

## Representing regularities
How to specify and use regularities in the scene radiance is a recurring theme in this book.  The issue comes up when we  design image acquisition systems, and when we design image representations (e.g., compression). It will come up as well when we want to make inferences about the scene itself, such as estimating the spectral reflectance of a surface, or the health status of biological tissue.

There are two related issues that we must manage when we consider scene radiance regularities. The first is how we learn about the regularities.  Sometimes we learn about them because we can build physical models of the signals; the regularities often follow naturally. This is the case when I describe solar radiation. In other cases, we learn about regularities because we have experience with a many examples, and we use computational tools to quantify the regularities. This is the case when I describe surface reflectances and the typical spatial structure of natural images.

<!--
Eero's statistical work surely gets in here somewhere.  Not obviously in this section, but perhaps in the spatial regularity part.
-->

There are two classical tools that are widely used to specify regularities throughout science and engineering, and both will come up in this book. I just mentioned Bayes' Rule, which is famous and simple.  Probability distributions are a very general way to represent the likelihood of making an observation.  In certain cases it is possible to perform simulations using these probability distributions, but often incorporating these distributions in a simulation is hard or impossible to achieve.

A second mathematical tool, dimensionality reduction, is also widely used. Dimensionality reduction, particularly based on linear models, is an extremely practical computational approach.  Usually, however, this approach is a coarse way to capture the statistical regularities of the data. 

More recently, training neural networks has proven to be a very useful way to infer statistical regularities from a large data set, and it also provides a computable model. This approach is proving to be quite practical in many contexts.  

In this section, I describe the two classical techniques.  They remain quite useful and often provide good insight into the physics of the system. I then apply them to important descriptions of the scene spectral radiance. As we move through different sections of the book, I will introduce examples of how neural network training is increasingly finding application in image systems engineering.

### Bayes Rules
 If we know the probability of a specific condition, $C$, being true given measured data,Â $d$, we use the laws of probability to write

$$
P(C | d) = \frac{P(d \cap C)}{P(d)} = \frac{P(d \cap C) P(C)}{P(d) P(C)}= \frac{P(d | C) P(C)}{P(d)}
$$

::: {#Bays-terms .callout-note style="color: gray" appearance="simple"}
### The terms in Bayes' Rule

-   $P(C | d)$ - **posterior probability** of C given d.
-   $P(d|C)$ - **likelihood** of d given C.
-   $P(C)$ - **prior probability** of C.
-   $P(d)$ - **marginal likelihood** of d.
:::

The application of Bayes' Rule requires that we know the prior and the likelihood of the data, $d$, given the condition, $C$. In many applications, this knowledge comes from a simulation of the process.  If we know the condition, then we can simulate the data. Being able to work out this probability is a fundamental task for the scientist or engineer.

A more difficult challenge in using Bayes' Rule is that we often have little idea about the prior, $P(C)$. The cases in which we do have this world knowledge can be very valuable. How we represent this knowledge can also be important. That is where linear models can be quite useful.

*PUT IN AN EXAMPLE.  THE FREEMAN/BRAINARD is the one I know.  Maybe there is a better one?*

### Linear models
Linear models have many uses in science and engineering.  In this section, we will use them to represent certain types of regularity in the data. Specifically, suppose we have a large collection of vector measurements, with each vector represented by $D$ samples.  We can use express each of the measurements as the weighted sum of a small, set of $M$ vectors, which we call the *basis* vectors. If the $N$ is much smaller than $D$, the basis set captures an essential regularity about the original data set.

To connect the linear model idea to the case at hand, suppose we the data are a set of radiance spectra, $d(\lambda)$. We might have measured these spectra over the visible range from 380 nm to 780 nm in 1 nm steps (hence, \$d\$ is 401 dimensions). After making a few thousand (or million) measurements, we might discover that we can closely approximate any of the measurement, $d_j$, as the weighted sum of $N$ basis functions. We write the approximation this way:
$$
d_j(\lambda) \approx \sum_{i=1}^{N} B_i(\lambda) w_{ij}
$$ {#eq-linear1}

There are a many tools to find basis vectors, $B_i$, and these tools rely on different objectives. Sometimes, the choice of basis vectors arise from physical principles. This is the case for the Fourier basis functions, which are the harmonic functions, and are a natural part of magnetic resonance imaging, sound, and heat transfer. It is also common to select $N$ basis functions that explain the most variance in the data, which is the choice we make when using Principal Components Analysis (PCA) that relies on the important matrix decomposition the singular value decomposition (SVD). In both of these cases, Fourier and PCA, the basis vectors are orthogonal, which means that the inner product[^lightfields-properties-1] of any pair of basis vectors is zero. The mathematical expression for the inner product and orthogonality is

[^lightfields-properties-1]: Widely used synonyms for the term inner product are *dot product* and *scalar product*.
 $$
    0 = \sum_{\lambda} B_i(\lambda) B_j(\lambda)
$$

It is quite common to use matrix notation to express the linear model. With this notation, the linear model places the basis vectors into the columns of a matrix, $\mathbf{B}$.  Each data vector, $\mathbf{d_j}$, is the product of a weight vector, $\mathbf{w_j}$ times the matrix of basis vectors.

$$
\mathbf{d_j} = \mathbf{B} \mathbf{w_j}
$$ {#eq-linearmatrix}

Linear models appear in many places in this book, with many different applications. When you are ready to read more about them as a general mathematical tool, go to Appendix @sec-linearmodels, which has a chatty discussion.  From there you can find your way to more advanced and authoritative literature. The Appendix also contains pointers to important nonlinear methods that - in some circumstances - can be very helpful @DiCarlo2003-spectralestimationtheory also ISOMAP.

In the next two sections we describe some of the regularities that have emerged from studying and measuring radiance.  We begin with regularities concerning the spectral properties in the visible band.  We then turn to regularities concerning the spatial properties of natural images, again in the visible band.

## Spectral regularities

```{=html}
<!-- [https://chatgpt.com/share/679ebc49-5a7c-8002-a282-598e7f1bbb30 ]
Solar spectrum. Fraunhofer lines.  Stability of the solar spectrum 

https://chatgpt.com/c/679ebbfc-b510-8002-b372-489cd173d5d0

CIE Data downloaded to ISETCam from here.  Placed in data/lights/daylight and data/lights/solar  These appear to be distinguished, for now, based on Earth's surface or Sun's surface.

https://cie.co.at/datatable/components-relative-spectral-distribution-daylight
-->
```

### Solar Radiation

The Sun serves as Earth's primary radiation source. The radiation emitted from the Sun's surface in the visible spectrum remains relatively consistent over time. Due to its significance for imaging and human vision, skylight became an early subject of scientific investigation.

In 1814, Joseph Fraunhofer observed and carefully documented numerous dark lines in the solar spectrum, now known as **Fraunhofer lines** It was subsequently discovered that these lines are present because light generated in the Sun's hot, dense interior (photosphere) travels through its cooler outer layers (chromosphere and corona). During this passage, atoms and ions absorb specific wavelengths of light that correspond precisely to their electron transitions. The reliability of these solar absorption lines allows scientists to use them as reference points to calibrate laboratory instruments for spectral measurements. For example, James Clerk Maxwell used these lines to calibrate the first experimental rig for human color-matching.

![The black curve displays the relative solar spectrum, which is stable over time. The red lines indicate are drawn at wavelengths where the spectrum exhibits narrow intensity reductions, known as Fraunhofer lines, that originate from ions and atoms in the Sun's outer layers. The blue dotted lines mark intensity reductions caused by atmospheric water vapor.](images/lightfields/01-solar-lines.png){#fig-solarspectrum width="60%" fig-align="center" width="70%"}

::: {#fraunhofer .callout-note collapse="true" appearance="simple"}
## [Joseph Fraunhofer (1787-1826)](https://en.wikipedia.org/wiki/Joseph_von_Fraunhofer)

Fraunhofer was a brilliant applied researcher who worked on the physical properties of materials, particularly glass. He also made important contributions to the theory of image formation and optics.

The Fraunhofer lines, which he discovered while building a device to measure the spectral transmissivity of glass, are reported in a paper reviewing the properties of glass. These relative intensity of these lines are determined by the material composition of the stars, and these measurements are stilled used in astronomy to determine the composition of celestial bodies. Fraunhofer died at the age of 39, poisoned by heavy metal vapors used in his material research. Europe's largest body for the advancement of applied research, the Fraunhofer Society, is named to honor him.

![](images/people/Fraunhofer.png){fig-align="center" width="391"}

This is a good historical review of the [Fraunhofer lines](https://joachimweise.github.io/post/2020-10-20-fraunhofer-lines/) and subsequent work.
:::

### Atmospheric Effects

Some of the spectral absorption lines observed at Earth's surface originate from a different source: water vapor (HâO) and other molecules in Earth's atmosphere. Unlike the stable solar absorption lines, these atmospheric absorptions fluctuate relatively quickly with changing weather conditions. There is also atmospheric variation in the skylight spectral power with the time of day, as the radiation's path length through the atmosphere varies. Longer path lengths â such as during sunrise or sunset â modify the spectral composition of radiation reaching Earth's surface.

A number of different groups have reported measurements of solar radiation at the Earth's surface at varying times of day and weather conditions [@Judd1964-daylight]. Figure #fig-daylights shows examples of typical variation in the shape of the spectral power distribution in Granada, Spain [@Hernandez2001-daylight] and Stanford, California [@DiCarlo2000-daylight].

![The spectral power distribution of daylights sampled at the Earth's surface in Granada, Spain (left) and Stanford, California (right) at different times of day and weather conditions. The spectra have been scaled to be equal at 450 nm. The curves have different slopes, and their shapes vary with weather and time-of-day. Identifiable Fraunhofer lines and water vapor lines are present in the measurements.](images/lightfields/01-solar-samples.png){#fig-daylights width="90%" fig-align="center" width="90%"}

These combined factorsâatmospheric composition and path length variationsâcreate a complex and dynamic radiation environment at Earth's surface. To account for these complexities, scientists and engineers have created linear models that can represent the natural variations of skylight incident upon Earth's surface. The original model was introduced by Judd, Macadam and Wyszecki [@Judd1964-daylight]. They analyzed skylight at the Earth's surface from about 650 different conditions. They found that the spectral power distributions, normalized to remove absolute radiance, could be modeled as the weighted sum of three terms: a mean and two basis functions (Figure @fig-ciedaylight). Such a model is useful because it enables us to represent any particular sample using just three numbers, the two weights and an overall scalar to match the level. The model is also useful for applications in which a scene is acquired under full sky illumination, and we would like to estimate the spectral power distribution of the skylight. We only need to estimate the weights, and from those we can make a good estimate of the full spectrum.

![The solid curve shows the mean daylight spectrum measured by Judd, Macadam and Wyszecki @Judd1964-daylight. Their measurements evolved into a CIE standard linear model for daylights. The solid circles are placed at the wavelengths of Fraunhofer and atmospheric water vapor lines. The dotted and dashed curves are basis functions. Together with the mean, they form a linear model of daylight spectral power distributions.](images/lightfields/01-solar-basis.png){#fig-ciedaylight width="50%" fig-align="center"}

```{=html}
<!--
Moonlight regularities. Used for space travel.https://chatgpt.com/share/67410dd0-ff54-8002-9b73-65294a849fe6 âThe Potential of Moonlight Remote Sensing: A Systematic Assessmentâ explores the potential of using moonlight in remote sensing for nighttime Earth observation. The study uses data from VIIRS, the ISS, and UAV systems to evaluate moonlightâs capabilities.âSimulation Study of the Lunar Spectral Irradiances and the Earth-Based Moon Observation Geometryâ focuses on modeling the spectral irradiance of moonlight using the Hapke model and its applications in satellite radiometric calibration.
-->
```

::: {#california-sun .callout-note collapse="true" appearance="simple"}
## Do it yourself daylight spectra

When he was a student at Stanford, Jeff DiCarlo decided to measure the daylight outside his office for himself [@DiCarlo2000-daylight]. Checking things for himself is in his nature. Jeff pointed a spectral radiometer at a calibrated surface outside his window and configured a computer to make about ten thousand measurements, one a minute, during the course of a few rainy weeks in California (January 28, 2000 to February 16, 2000). I included a sample of 1000 of these measurements in ISETCam, and I have the whole group along with time stamps if you want them.

![Average daylight measurements on the Stanford campus (solid) and CIE daylight basis fit to the measured average (dotted). The solid circles are placed at the locations of Fraunhofer and water vapor wavelengths.](images/lightfields/01-solar-stanford.png){#fig-stanfordsun fig-cap-location="bottom" width="50%" fig-align="center"}

@fig-stanfordsun shows the mean spectral power distribution of these measurements. You can see that the instrument recorded several 'notches' the spectrum at wavelengths corresponding to the Fraunhofer and water vapor lines. We also asked whether these data could be fit by the CIE linear model - and the least-squares fit is shown by the dotted line. I suspect there is an instrument calibration error of a few nanometers at wavelengths above 700nm, and were I to write a research paper based on these data, I would probably introduce the correction because I believe the physics on the locations of these wavelengths is secure. This analysis might provide you with a sense of the relative accuracy of measuring and approximating spectra with conventional lab equipment.
:::

```{=html}
<!-- Later in the book use these daylights to show the appearance (chromaticity) of the different daylights.  The Granada paper does this nicely. "Color and spectral analysis of daylight in southern Europe"
-->
```

In which we say something about the weights can be correlated, even though the basis functions are orthogonal. Non-negativity constraint, for example. See what I put in the DiCarlo article illuminating bases. Also, there are plots of the tight relationship that will show up in the human vision part for the Granada data.

::: {#daylight-weights .callout-note collapse="true" appearance="simple"}
### Correlated weights

![The daylight basis weights from the Granada and Stanford data sets. The weights are clustered within a plane, although there are occasional outliers. The orthogonality of the basis functions does not imply independence or orthogonality of the measured basis weights.](images/lightfields/01-dayweights3d.gif){#fig-dayweights fig-align="center" width="372"}
:::

## Surface spectral reflectance regularities

Vision is a remarkable invention that enables us to interpret the environment by recording the how the ambient illumination interacts with it. Four things can happen when electromagnetic radiation interacts with a surface. The radiation arriving at a surface might be (a) reflected, (b) absorbed, (c) pass through, or (d) interact with the material and generate new radiation (fluorescence). We will consider each of these interactions in different sections of this book. Here, we describe some regularities in the surface spectral reflectance of many common materials.

A full description of surface spectral reflectance can include many parameters. The radiation incident on a small, planar patch of material can come from many different angles (two dimensions). The radiation may be reflected in many different directions (two more dimensions). The reflected light might be measured with respect to both wavelength and polarization (two more dimensions). We will set aside fluorescence for the present, returning to it later.

Despite this potential complexity, there are many relatively simple cases that are simpler to describe and still useful. Perhaps the simplest and most important are materials that reflected light equally in all directions, no matter what direction the light is coming from. Such materials have *diffuse reflectance*, also called *Lambertian reflectance*.

You can judge to what extent a material is diffuse if you have a laser pointer. Direct the laser pointer onto a planar surface patch and then move about, checking what the reflected light looks like as you view it from positions. In many cases, the reflected light appears pretty much as the same intensity as you move around. Also, you can change the position of the laser, but still illuminate the same point. For diffuse materials the reflected light will be about the same. These materials reflect diffusely because their surfaces have rough microscopic facets, oriented in many directions. When the incident light arrives, it may be reflected by facets oriented randomly, in many different directions.

::: {#Lambert .callout-note collapse="true" appearance="simple"}
## Johan Lambert

The term Lambertian surface honors the Swiss physicist, [Johann Lambert](https://en.wikipedia.org/wiki/Johann_Heinrich_Lambert) (1728-1777) who formalized the mathematical description. Many surfaces are modeled as Lambertian in computer graphics, or assumed to be Lambertian in computer vision estimates of the environment. Examples of materials with Lambertian reflectance include chalk, plaster, uncoated paper, matte paints made from pigments.

Adding additional details about the surface reflectance can improve the system performance, but it is common to start with a Lambertian approximation.

![Johann Lambert. His ideas were spread widely.](images/people/Lambert.png){#fig-lambert fig-align="center" width="180"}

Lambert worked on many topics concerning geometric optics, and he contributed an important book **Photometria** that incorporated many important geometric ideas, explaining the relative brightness of objects as seen from different points of view. Using his excellent mathematics, and the assumption that light travels in straight lines, he showed that illumination is - proportional to the strength of the source, - inversely proportional to - the square of the distance of the illuminated surface, and - the [sine of the angle](https://en.wikipedia.org/wiki/Lambert%27s_cosine_law "Lambert's cosine law") of inclination of the light's direction to that of the surface. He also wrote a classic work on [perspective](https://en.wikipedia.org/wiki/Perspective_(visual) "Perspective (visual)") and contributed to [geometrical optics](https://en.wikipedia.org/wiki/Geometrical_optics "Geometrical optics").

I was interested to learn that Lambert was the first to show that $\pi$ is an irrational number, a claim that the great Euler sought to prove as well.
:::

In the case of diffuse reflectance, we can summarize the material relatively simply. We only need to measure the fraction of incident light, for each wavelength, is reflected. This is called the spectral reflectance function

The spectral reflectance is measured wavelength-by-wavelength, that is they are functions of wavelength. In the visible band, these functions are typically quite smooth, and it is possible to summarize them using only a few parameters. It is not realistic to obtain a sample of all surfaces, but we might consider the surfaces in a particular context (e.g, walking in a forest, the interior of a commercial office building, driving on a highway, or medical images of the body). The likely surfaces in some context can be sampled and usefully summarized with a linear model [@Cohen1964-linearmodel, @Maloney1986-linearmodels-surface, @Hardeberg2002-reflectancedimensionalty]. The best basis functions will depend, somewhat, on the where you are measuring (forrest, desert, city, countryside). The number of basis functions needed will depend on the task requirements (e.g., 5% or 1% accuracy). But the general usefulness of a linear model approximation as a computational tool is not in doubt.

![Sample spectral reflectance functions of clothing. The fraction of reflected light, wavelength-by-wavelength, is between 0 and 1. Such smooth spectral reflectance curves are typical of many surfaces. Surfaces that are not diffuse can be modeled as having a diffuse component with a smooth spectral reflectance.](images/lightfields/01-reflectance-clothes.png){#fig-reflectance-cloths fig-align="center" width="60%"}

::: {#Things-stuff .callout-note collapse="true" appearance="simple"}
## Things and Stuff

We introduce the notion of surface reflectance as a general property, allowing that it depends on context. In an insightful paper, Ted Adelson further divided the environment into *things and stuff*" @Adelson2001-stuff. Things are the countable, discrete objects with defined shapes and boundaries (e.g., cars, chairs, animals). Stuff is amorphous materials or textures without fixed boundaries (e.g., water, grass, sky, sand). Dividing the environment into these categories also helps us set computational goals.

![Illustration of *things* and *stuff*. I am not sure how to count the grass - the collection of stuff or the blades of grass. Is the ocean stuff but a wave a thing? The sky, and beach (globally) seem like stuff. The rabbit and birds are definitely things, but their coats might be considered stuff. It is worth thinking about the difference. It is also worth noticing how Imagen3 produced some pretty odd animal faces.](images/paste-1.png){#fig-things-stuff fig-align="center" width="70%"}

For example, segmentation applies to *things* which are analyzed for object identity and function; in contrast, *stuff* is regionally based on texture and material properties. A great deal of computer vision focuses on tasks such as naming or counting cars on the road, people in the room, airplanes in the sky. We wish to be able to recognize such things whatever their material might be. A vision system must also be able to make judgments about a sandy beach, a cloudy sky, a grassy hillside. We wish to be able to recognize such stuff whatever its shape might be.

A related challenge in human vision is quantifying the appearance of things and stuff, and for this challenge surface reflectance, texture, and context (e.g., ambient illumination, or what is nearby) all play a role @Schmid2023-materialperception. Whether there is a corresponding concept - something we might call appearance for a computer vision system - is less studied. Perhaps the meaning of appearance for a computer vision system might be explained by its estimate of material reflectance and texture.
:::

Over the decades a number of imaging groups have measured the reflectance of many types of materials, including paints, minerals, plants, clothing, and various biological specimen such as skin and teeth. Government agencies, particularly those involved in remote sensing, have measured the reflectance of many materials that might be seen in satellite images. As a general rule, the spectral reflectances of the samples reported in the literature are approximated to an accuracy of a few percent by linear models using between 3 and 8 basis functions.

A Finnish group, for example, measured 1269 different color swatches from the Munsell Book of Colors. This widely used book spans a wide range of color samples that provides designers with a method for identifying, selecting, matching, and communicating about color. @Parkkinen1989-munsellspectra made their spectral measurements at 1 nm resolution. A linear model using three basis functions fits all the measurements with an error of about 2.5%. A linear model with 5 basis functions fits the data to within about 1%. This accuracy is usually often enough for image systems applications.

![Three basis functions for the Munsell Book of Color, from measurements by @Parkkinen1989-munsellspectra. The first linear model function is very close to the mean and is all positive. The 2nd and 3rd functions capture the principal variation in the samples. The three basis functions, calculated using the singular value decomposition, are orthogonal to one another. Notice the location of the zero-crossing of the 2nd function, which is near 560 nm, and the two zero-crossings of the 3rd function which are near 490 nm and 600 nm.](images/lightfields/01-munsell-basis.png){#fig-munsell-basis fig-align="center" width="60%" fig-cap-location="bottom"}

The linear models calculated for several types of common materials are often similar. This is illustrated in a set of measurements Vrhel and colleagues @Vrhel1994-MeasurementAnalysisObject. Their data, also included ISETCam, measures a few dozen surfaces from several different groups (paint, natural objects, man-made objects, food). It is likely the authors chose the samples to have similar means by insisting on a broad representation of color appearances. Had they chosen only the leaves in a forest, or only the rocks in the desert, the means might have differed. The properties of the 2nd and 3rd basis functions are also similar, and this is not likely to be a selection artifact. The measured spectral reflectance functions are relatively smooth with respect to wavelength, and thus the basis functions are smooth. The zero-crossings of these basis functions fall at locations that divide the visible spectrum into three, similar sections - bluish, greenish, and reddish.

![Three basis functions comparing the basis functions derived from the @Parkkinen1989-munsellspectra Munsell Color Book data (upper left) and three different categories measured by @Vrhel1994-MeasurementAnalysisObject. The basis functions are not identical, but they have many similarities, including the zero-crossings of the 2nd and 3rd basis functions.](images/lightfields/01-reflectance-bases.png){#fig-reflectance-bases fig-align="center" width="80%"}

```{=html}
<!-- 
Scripts to look over: s_surfaceMunsell, s_reflectanceBasis, s_sceneReflectanceChartBasisFunctions 
Maybe illustrate the visual size of the errors, as I have in other scripts.\
-->
```

### Dichromatic reflectance model (DRM)

The DRM is a useful surface reflectance model that is somewhat more complex than diffuse (Lambertian) materials, but simpler than a full reflectance model. This model is still used in computer vision applications where the user would like to identify specular reflections. The DRM is a good approximation for light reflected from **inhomogeneous dielectric materials** (non-conductors like plastics, paints, wood, skin, etc.). The model approximates reflectance as a weighted sum of a diffuse term and a mirror-like (specular) term.

The idea is that light is reflected by two mechanisms. Some of the light is reflected at the material's **interface** with air. This light does not interact with the material, and thus its spectral power is the approximately the same as the light source. Another portion of the light enters into the material (**subsurface**). In the subsurface it interacts with the material's pigments, both by scattering and absorption. The light that emerges from the subsurface determines the object's spectral reflectance. The subsurface portion is approximately Lambertian, as the subsurface pigments are randomly positioned and oriented.

The DRM represents the reflected light, $C(\lambda)$ as the sum of the interface and subsurface terms

$$
C(\lambda) = m_s(g) I(\lambda) E(\lambda) + m_d(g) D(\lambda) E(\lambda)
$$

$E(\lambda)$ is the spectral power distribution of the light; $I(\lambda)$ is the interface spectral reflectance (often assumed to be constant across wavelengths); $D(\lambda)$ is the diffuse subsurface reflectance, which is characteristic of the material. The terms $m_s(g)$ and $m_d(g)$ are geometric scaling factors whose values depend on the illumination and viewing angle. These angles are ordinarily represented with respect to the surface normal at the surface point $g$. The reflected light, $C(\lambda)$ is sometimes called the *color signal* because this spectral radiance is an important contributor to the visual system's color judgment. (Though, I have friends who object to this term).

The DRM finds application in topics such as illuminant estimation, specular highlight identification and removal, and material classification. It is, however, limited in various ways because it does not accurately model complex surfaces, and it does not apply well to scenes with multiple light sources or very large light sources, say spanning the sky.

### Bidrectional reflectance distribution function (BRDF) {#sec-properties-brdf}

The complete description of surface reflectance is called the *bidirectional reflectance distribution function* (BRDF). The measurement characterizes how light rays of wavelength $\lambda$, arriving at a point on the surface at an angle $\omega_i$, are reflected. The reflected light intensity is specified at all angles $\omega_o$. Both $\omega_i$ and $\omega_o$ are conventionally measured with respect to the surface normal. We can write the BRDF as $B(\omega_i,\omega_o,\lambda)$. The reciprocity of principle of light, often attributed to Helmholtz in his Handbook of Optics, implies that the BRDFs for most materials, with only a few exceptions, obey this symmetry relationship.

$$
B(\omega_i,\omega_o,\lambda) = B(\omega_o,\omega_i,\lambda)
$$

Each of the direction vectors is specified by two parameters, typically defined with respect to the surface normal. One parameter is the angle around the surface normal (azimuth) $\phi_i, \phi_o$, and the second is the elevation $\theta_i, \theta_o$ of the ray. We conventionally describe the BRDF with three parameters, but there are really five parameters in there. That makes the BRDF tedious to measure.

The instrument used to measure the BRDF is called a gonioreflectometer. The instrument has a light source that can be positioned to illuminate the surface at many different angles (typically on a sphere), and a detector that can be positioned at many locations on the sphere. There are many different designs for *gonioreflectometers*. The one in @fig-gonioreflectometer shows a surface is placed on a stage that can be rotated. The light is on a second curved stage that can be rotated. The detector position is fixed.

::: {#fig-gonioreflectometer fig-cap="Illustration of a gonioreflectometer. The light source and detector are on robot arms that can be positioned around the surface. If we fix the wavelength and angle of the incident light ray, we can specify the relative intensity of the reflected ray as an image that spans the two angle parameters."}

::: {.column-page}
::: {layout-ncol=2}
![](images/lightfields/01-gonioreflectometer.png){#fig-gonio-1 width=45%}

![](images/lightfields/01-gonioreflectometer.png){#fig-gonio-2 width=45%}
:::
:::

Illustration of a gonioreflectometer. The light source and detector are on robot arms that can be positioned around the surface. If we fix the wavelength and angle of the incident light ray, we can specify the relative intensity of the reflected ray as an image that spans the two angle parameters.
:::

The input and output angles are 2-vectors that define . Thus the BRDF is a scalar function. Given a light ray of unit intensity, incident at $\omega_i$, the output intensity at different angles $\omega_o$ will be given by

$$
f_r(\phi_i,\theta_i,\phi_o,\theta_o,\lambda) = f_r(\omega_i, \omega_o, \lambda)
$$

```{=html}
<!-- 
I created a Gemini document about data driven papers 
[Document on data-driven BRDFs](https://docs.google.com/document/d/1Wy2ygwu30i5quFX21kgz34OGSszNwUXn39Dfh4ypavA/edit?usp=sharing)

[Source for spherical azimuth and elevation and zenity.](https://www.researchgate.net/publication/334197029_A_new_model_for_reduction_of_Azimuth_asymmetry_biases_of_tropospheric_delay/figures?lo=1)

 -->
```

#### Data driven

There are papers measuring BRDFs and modeling them using linear models and nonlinear models. Data driven approach.

@Matusik2003-datadrivenBRDF @Tongbuasirilai2020-BRDF

There is a MERL database.

@MERL-BRDF-Brand

#### Theoretical

Cook Torrance mainly here.

```{=html}
<!--

Sand, reasonable figures. 

Tongbuasirilai2020-BRDF
 - https://opg.optica.org/ao/fulltext.cfm?uri=ao-54-31-F243&id=330097

Excellent source for material models and some simple formula of reflection.  Introduces dielectrics (Plastic, Stone, Wood, Leather, Glass, Water, Air), and conductors (metals)
https://leeyngdo.github.io/blog/computer-graphics/2024-03-22-reflection-model/?utm_source=chatgpt.com

Fermat's principle: light will follow the path that requires the least amount of time, as compared to other nearby paths, to pass between two points.

This web page has no graphs illustrating the microfacet theory from Ken Torrance and his students.  It has a nice introduction to the formulae for Cook Torrance, from Sparrow Torrance, as well.

-->
```

## Spatial regularities

Something about fractals. Textures. Other stuff.

### Natural scenes and the 1/f spatial frequency falloff

Who first pointed this out? Relationship to the scale invariant idea of fractals, and the fractal nature of many things.

Ruderman and Bialek paper on natural image statistics.

Field, Olshausen

David M - famous mathematician guy

### Natural scenes: The dead leaves model

Jon's Matlab script as a basis for discussing this.

Also, the deadleaves function in ISETCam.