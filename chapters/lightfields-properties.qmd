# Light field properties {#sec-lightfield-properties}

Many image systems are designed to measure the scene. These measurements might be used to identify an object and its properties, such as its size, distance, and material. These systems are fundamental to applications like industrial inspection, autonomous vehicles, and medical imaging. Even in consumer photography applications, whose primary mission is to reproduce the light field, estimating scene physical characteristics - such as the ambient lighting - can be useful.

In many image systems applications the problem is under constrained, by which I mean that many different scenes or objects might have produced the measured image. There is a useful principle that applies to these situations, often attributed to Thomas Bayes, is quite simple. If many potential solutions exist to a problem, Bayes' Rule tells us to select the most likely one. The general principle seems pretty obvious, no? The hard part, of course, is how to know which answer is the most likely one.

We do not always have information about the relative likelihood of different solutions, but sometimes we do. One way to make better guesses is to learn about regularities in the world. In this section we describe some regularities about the spectral (wavelength) and spatial properties of natural images. In certain specific applications, such as medical imaging or industrial inspection conditions, the regularities may narrow the set of possible answers greatly.

## Representing regularities

### Bayes Rules

How to specify regularities in the scene, and how to take advantage of them, will be a recurring theme as we describe various image systems. There are two classical mathematical tools that are widely used to specify regularities throughout science and engineering, and both will come up in this book. I just mentioned Bayes' Rule, which is extremely famous and simple. If we know the probability of a specific condition, $C$, being true given a measurement, $d$, we use the laws of probability to write

$$
P(C | d) = \frac{P(d \cap C)}{P(d)} = \frac{P(d \cap C) P(C)}{P(d) P(C)}= \frac{P(d | C) P(C)}{P(d)}
$$

::: {#Bays-terms .callout-note style="color: gray" appearance="simple"}
## The terms in Bayes' Rule

-   $P(C | d)$ - **posterior probability** of C given d.
-   $P(d|C)$ - likelihood of d given C.
-   $P(C)$ - **prior probability** of C.
-   $P(d)$ - marginal probability of d.
:::

The challenge in using Bayes' Rule is also obvious. In many cases we have very little idea about the prior, and for many applications we do not know how to even compute the likelihood of $d$ given $C$.

### Linear models

Linear models are a second way to capture the regularity in the data. These are the standard methodology of linear algebra, in which we express the data - which is usually a vector of measurements - as the weighted sum of a known, fixed, set of possible vectors. To connect the idea to the case at hand, suppose we consider the data to be a spectrum, $d(\lambda)$, where we measure over the visible range from 380 nm to 780 nm in 1 nm steps (hence, \$d\$ is 401 dimensions). After making a few thousand (or million) measurements we might discover that we can closely approximate the $j^{th}$ measurement as the weighted sum of $N$ basis functions.

$$d_j(\lambda) = \sum_{i=1,N} B_i(\lambda) w_{ij}$$ {#eq-linearmodel}

This representation also captures a regularity in the measurements, particularly when the value of $N$ is small (say, 5, 10, 20) compared to the dimension of $d$, in this case 401. We are representing each observation by just $N$ numbers, as long as we have the side condition of knowing the basis functions.

By now, some of you will notice that we can use matrix notation to express the linear model. The linear model simply expresses the data, $\mathbf{d_j}$ vector as the product of a weight vector, $\mathbf{w_j}$ and the basis vectors placed in the columns of the matrix $\mathbf{B}$.

$$\mathbf{d_j} = \mathbf{B} \mathbf{w_j}$$

## Spectral regularities

```{=html}
<!-- [https://chatgpt.com/share/679ebc49-5a7c-8002-a282-598e7f1bbb30 ]
Solar spectrum. Fraunhofer lines.  Stability of the solar spectrum 

https://chatgpt.com/c/679ebbfc-b510-8002-b372-489cd173d5d0

CIE Data downloaded to ISETCam from here.  Placed in data/lights/daylight and data/lights/solar  These appear to be distinguished, for now, based on Earth's surface or Sun's surface.

https://cie.co.at/datatable/components-relative-spectral-distribution-daylight
-->
```

### Solar Radiation and Fraunhofer Lines

The Sun serves as Earth's primary radiation source. The radiation emitted from the Sun's surface in the visible spectrum remains relatively consistent over time. Due to the significance of skylight, it became an early subject of scientific investigation, with physicists identifying several distinctive characteristics of sunlight reaching Earth's surface.

In 1814, Joseph Fraunhofer observed numerous dark lines in the solar spectrum, now known as Fraunhofer lines. These lines form when light generated in the Sun's hot, dense interior (photosphere) travels through its cooler outer layers (chromosphere and corona). During this passage, atoms and ions absorb specific wavelengths of light that correspond precisely to their electronic transitions. The reliability of these solar absorption lines allowed scientists to use them as reference points to calibrate laboratory instruments for spectral measurements. For example, James Clerk Maxwell used these lines to calibrate the first experimental rig for human color-matching.

![The black curve displays the relative solar spectrum, which maintains remarkable stability over time. The red lines indicate specific wavelengths where the spectrum exhibits narrow intensity reductions, known as Fraunhofer lines. These absorption features originate from ions and atoms in the Sun's outer layers. The blue dotted lines mark atmospheric absorption bands caused by water vapor, with particularly prominent features between 700-850 nm.](images/lightfields/01-solar-lines.png){#fig-solarspectrum fig-align="center" width="70%"}

### Atmospheric Effects on Solar Radiation
Several spectral absorption lines observed at Earth's surface originate not from the Sun itself but from water vapor (H₂O) and other molecules in Earth's atmosphere. Unlike the stable solar absorption lines, these atmospheric absorptions can fluctuate relatively quickly with changing weather conditions.

As solar radiation traverses Earth's atmosphere, it undergoes substantial alteration by atmospheric particles and moisture. Additionally, the radiation's path length through the atmosphere varies with the time of day. Longer path lengths—such as during sunrise or sunset—further modify the spectral composition of radiation reaching Earth's surface.

A number of different groups have reported measurements of solar radiation at the Earth's surface at varying times of day and weather conditions. Figure #fig-daylights Examples of the typical variation in the shape of the spectral power distribution in Granada, Spain (reference) and Stanford, California.

![The spectral power distribution of daylights sampled at the Earth's surface in Granada, Spain (left) and Stanford, California (right) at different times of day and weather conditions. The spectra have been scaled to be equal at 450nm. Notice that they have different slopes and shapes as weather and times of day change. Also notice that the Fraunhofer lines and white vapor lines are prominent in most of the measurements.](images/lightfields/01-solar-samples.png){#fig-daylights fig-align="center" width="90%"}

These combined factors—atmospheric composition and path length variations—create a complex and dynamic radiation environment at Earth's surface. To account for these complexities, scientists and engineers have created linear models that can represent the natural variations of skylight incident upon Earth's surface.  The original model was introduced by Judd, Macadam and Wyszecki (REF).  They analyzed skylight at the Earth's surface from about 650 different conditions.  They found that the spectral power distributions, normalized to remove absolute radiance, could be modeled as the weighted sum of three terms: a mean and two basis functions (Figure @fig-ciedaylight).  Such a model is useful because it enables us to represent any particular sample using just three numbers, the two weights and an overall scalar to match the level. The model is also useful for applications in which a scene is acquired under full sky illumination, and we would like to estimate the spectral power distribution of the skylight.  We only need to estimate the weights, and from those we can make a good estimate of the full spectrum.

![The solid curve shows the mean daylight spectrum measured by Judd, Macadam and Wyszecki which became a CIE standard linear model for daylights. The solid circles are placed at the wavelengths of the Fraunhofer and atmospheric water vapor lines. The additional two curves are basis functions that, together with the mean, form a linear model of daylight spectral power distributions. Most daylights can be modeled as proportional to the mean plus weighted sums of these two bases.](images/lightfields/01-solar-basis.png){#fig-ciedaylight fig-align="center" width="60%"}

<!--
Moonlight regularities. Used for space travel.https://chatgpt.com/share/67410dd0-ff54-8002-9b73-65294a849fe6 “The Potential of Moonlight Remote Sensing: A Systematic Assessment” explores the potential of using moonlight in remote sensing for nighttime Earth observation. The study uses data from VIIRS, the ISS, and UAV systems to evaluate moonlight’s capabilities.“Simulation Study of the Lunar Spectral Irradiances and the Earth-Based Moon Observation Geometry” focuses on modeling the spectral irradiance of moonlight using the Hapke model and its applications in satellite radiometric calibration.
-->

::: {#california-sun .callout-note collapse="true" appearance="simple"}
## Do it yourself daylight spectra

When he was a student at Stanford, Jeff DiCarlo decided to measure the daylight outside his office for himself. Checking things for himself is in his nature. Jeff pointed a spectral radiometer at a calibrated surface outside his window and configured a computer to make about ten thousand measurements during the course of a year. These measurements were made at times that pass for different weather conditions in California. I have included a random sample of 1000 of these measurements in ISETCam, and this figure shows the mean spectral power distribution of these measurements.

The data he measured were well approximated by the weighted sum of the CIE daylight bases. You can see that the instrument recorded several 'notches' the spectrum at wavelengths corresponding to the Fraunhofer and water vapor lines - though, I suspect there is an instrument calibration error of a few nanometers at wavelengths above 700nm. Were I to write a research paper based on these data, I would probably introduce the correction because I believe the physics on the locations of these wavelengths is secure.

![Average daylight measurements on the Stanford campus (solid) and CIE daylight basis fit to the measured average (dotted). The solid circles are placed at the locations of Fraunhofer and water vapor wavelengths.](images/lightfields/01-solar-stanford.png){#fig-stanfordsun fig-align="center" width="60%"}
:::

### Surface reflectance regularities

Perhaps most of the value of vision arises from the ability to interpret the interaction between the ambient electromagnetic radiation and environmental materials. The environment itself is usefully divided into "things and stuff"[^lightfields-properties-1]. It is not an exaggeration to say that human vision is the remarkable invention of being able to decode the properties of things and stuff in the environment from the way illumination interacts with these objects and materials.

[^lightfields-properties-1]: Things are the countable, discrete objects with defined shapes and boundaries (e.g., cars, chairs, animals). Stuff is amorphous materials or textures without fixed boundaries (e.g., water, grass, sky, sand).

::: callout-note
## Things and Stuff

The division between *things* and *stuff* is an important distinction for projects involving scene understanding:

Thingsrefers to countable, discrete objects with defined shapes and boundaries (e.g., cars, chairs, animals). Stuff refers to amorphous materials or textures without fixed boundaries (e.g., water, grass, sky, sand).

This distinction simplifies computational tasks like segmentation, where *things* require instance-level detection, while *stuff* is segmented regionally based on texture or material properties. Human vision similarly leverages these categories, with *stuff* often processed holistically for material properties (e.g., gloss, roughness), and *things* analyzed for object identity and function.

Citations:

\[2\] [Ted Adelson on things and stuff](https://www.microsoft.com/en-us/research/video/on-seeing-stuff-the-perception-of-materials-and-surfaces/).

\[8\] [Roland Fleming on things and stuff](https://www.annualreviews.org/doi/10.1146/annurev-vision-102016-061429)

\[9\] [Measurements for categorizing materials](https://www.nature.com/articles/s41562-023-01601-0)
:::

```{=html}
<!--
In computer vision and perceptual science, the division between *things* and *stuff* is a foundational classification for scene understanding:

**Things**  
- Refer to countable, discrete objects with defined shapes and boundaries (e.g., cars, chairs, animals).  
- Recognition relies on structural cues like shape, contours, and spatial arrangement[1][4].  

**Stuff**  
- Encompasses amorphous materials or textures without fixed boundaries (e.g., water, grass, sky, sand).  
- Identified primarily through local appearance features such as color, texture, and reflectance properties[1][8].  

**Key Differences**  
| Aspect          | Things                          | Stuff                          |  
|-----------------|---------------------------------|--------------------------------|  
| **Boundaries**  | Well-defined (countable)        | Indistinct (uncountable)       |  
| **Recognition** | Shape, structure                | Texture, color, reflectance[2][8] |  
| **Examples**    | Cars, buildings                 | Water, sky, foliage[1][4]      |  

This distinction simplifies computational tasks like segmentation, where *things* require instance-level detection, while *stuff* is segmented regionally based on texture or material properties[4][8]. Human vision similarly leverages these categories, with *stuff* often processed holistically for material properties (e.g., gloss, roughness)[2][9], and *things* analyzed for object identity and function[1].  

The interplay between the two is critical for panoptic segmentation, where unified models jointly label *things* (as instances) and *stuff* (as semantic regions)[4].

Citations:
[1] https://cacm.acm.org/research/scene-understanding-by-labeling-pixels/
[2] https://www.microsoft.com/en-us/research/video/on-seeing-stuff-the-perception-of-materials-and-surfaces/
[3] https://www.waterfootprint.org/resources/Report12.pdf
[4] https://openaccess.thecvf.com/content/CVPR2021/papers/Shen_Toward_Joint_Thing-and-Stuff_Mining_for_Weakly_Supervised_Panoptic_Segmentation_CVPR_2021_paper.pdf
[5] https://pmc.ncbi.nlm.nih.gov/articles/PMC7768321/
[6] https://catalogue.unccd.int/1659_Dasgupta_Review_-_Full_Report.pdf
[7] https://www.mindbodyevolution.info/the-science-of-seeing/
[8] https://www.annualreviews.org/doi/10.1146/annurev-vision-102016-061429
[9] https://www.nature.com/articles/s41562-023-01601-0

---
Answer from Perplexity: pplx.ai/share
-->
```

A full description of this interaction can be complex and difficult to measure. Consider that the radiation incident on a small, planar patch of material can come from many different incident angles (two dimensions). This radiation may be reflected in many different directions (two more dimensions). The division into reflected light, absorbed light, and transmitted light may depend on the wavelength and polarization of the incident light (two more dimensions). Finally, although the wavelength of the incident and reflected light are most often the same, there are fluorescent materials that convert incident radiation at one wavelength into exitant radiation at different wavelengths. Fluorescent materials are widespread in biology, and such molecules are widely used in biomedical applications. They are also used by people who manufacture clothing detergent and want a washing to result in a brighter white.

Despite the general complexity, which we will discuss in more detail later, there are certain surface reflectance properties that are common. These are useful to know, and we can represent them as a priori information or in the form of a linear model. First, many materials reflect light incident from any direction equally in all directions. That is why illuminating a planar surface patch with a narrow laser beam, incident from one direction, is visible even as we move about and view the surface from positions. Such materials have *diffuse reflectance*, also called *Lambertian reflectance* to honor the Swiss physicist, Johan Lambert (1728-1888) who formalized the mathematical description. In addition to characterizing diffuse reflectance, Lambert observed that the apparent brightness changes as we move around. He explained this by noting that the projected area seen by the observer depends on the angle between the normal of the planar patch and the observer. Many surfaces are modeled as Lambertian in computer graphics, or assumed to be Lambertian in computer vision. Adding additional surface properties can improve the system performance, but it is common to start with a Lambertian approximation. <!--
https://chatgpt.com/c/67e2fe34-d268-8002-92d9-b6be5cf568e5

The term **"Lambertian reflectance"** originates from the work of the Swiss mathematician, physicist, and astronomer **Johann Heinrich Lambert (1728–1777)**. It is named after him because he was the first to formalize the mathematical description of **diffuse reflection** in his 1760 treatise *Photometria*, where he laid the foundations of modern photometry.

### **Scientific Background**
1. **Lambert's Cosine Law (1760)**  
   Lambert’s key insight was that an ideal **diffuse surface** (later called a Lambertian surface) reflects light **equally in all directions** regardless of the viewing angle. However, the perceived brightness (radiance) of the surface follows a cosine relationship relative to the surface normal and the incident light direction. Mathematically, this is expressed as:

   \[
   L = L_0 \cos\theta
   \]

   where:
   - \( L \) is the radiance observed in a particular direction,
   - \( L_0 \) is the radiance when viewed normal to the surface,
   - \( \theta \) is the angle between the surface normal and the viewing direction.

   This means that even though light is scattered uniformly, the intensity decreases with increasing angle from the normal because the projected area exposed to the observer diminishes.

2. **Diffuse vs. Specular Reflection**  
   - In contrast to **specular reflection** (e.g., mirror-like surfaces), where light is reflected in a single direction, **Lambertian surfaces** scatter incident light uniformly in all directions.  
   - Common examples include **matte surfaces, paper, and frosted glass**, which approximate Lambertian reflectance.

3. **Applications in Science and Engineering**  
   - Lambertian reflectance is widely used in **computer graphics**, **computer vision**, **remote sensing**, and **optics** to model realistic surface appearance.
   - Many real-world materials exhibit approximately Lambertian properties, making it a useful approximation in radiative transfer and reflectance modeling.

Thus, the name "Lambertian reflectance" honors **Johann Heinrich Lambert**, whose work in photometry provided the mathematical framework for understanding **diffuse reflection**.
-->

The spectral characteristics of the surfaces in our environment, over the visible range, are fairly regular as well. There are many different surfaces, so that unlike the solar radiation and we cannot assess the whole world. But for many applications, say walking in a forest, or working inside of a commercial office building, or driving on the road, the collection of likely surfaces can be sampled. It is generally agreed that the surface spectral reflectance functions - the fraction of incident light that is reflected by the surface - is a relatively smooth function of wavelength. Joseph Cohen () and E.L. Krinov collected a few hundred surface reflectance samples over the visible range. a number of investigators \## Spatial regularities in natural scenes

### 1/f spatial frequency fall

Who first pointed this out? Relationship to the scale invariant idea of fractals, and the fractal nature of many things.

Ruderman and Bialek paper on natural image statistics.

Field, Olshausen

David M - famous mathematician guy

### Dead leaves model

Jon's Matlab script as a basis for discussing this.

Also, the deadleaves function in ISETCam.