# Linear Space-Invariant (LSI) Systems {#sec-appendix-spaceinvariance}
Linear models are a cornerstone of science and engineering, and they can be described using different mathematical representations. Two representations are particularly important for image systems.

The first is the **point basis**, which represents an image as a collection of individual points. This is the most intuitive way to think about an image, and we often use it without a second thought. The second is the **harmonic basis**, which represents an image as a sum of sine and cosine waves of different spatial frequencies.

While the point basis is natural, the harmonic basis has a remarkable property: it simplifies the analysis of a special but very common class of systems known as **linear space-invariant (LSI) systems**. This section explains why harmonics are so valuable for understanding these systems.

::: {.callout-note title="Point vs. Impulse"}
The analysis here focuses on images, which are spatial signals. For spatial signals, the fundamental input is a **point**. The system's response to a point input is its **point spread function (PSF)**.

In fields that analyze signals over time (like audio or electronics), the equivalent of a point is an **impulse**—a signal that is non-zero for only a very brief instant. The system's response is then called the **impulse response**.

In both cases, the underlying mathematical concept is the **Dirac delta function**, named for the physicist Paul Dirac who formalized its use.
:::

## Shift-invariance  {#sec-ls-shiftinvariance}
A practical challenge in characterizing general linear systems is that each input point could have a different point spread function (PSF). That’s a lot to measure! Fortunately, many real systems are much simpler: their PSF is essentially the same, except that it’s shifted to match the shift in input location. In other words, the shape of the PSF doesn’t change—it just translates. We call such systems **space-invariant**.

space-invariant linear systems are common across science and engineering. Many physical devices and phenomena can be well-approximated as space-invariant, at least over a useful range. This property makes them much easier to calibrate: we only need to measure a single PSF -the response to a single point!- rather than measure every possible point. You can see why for a space-invariant system the PSF is the key measurement that characterizes the system. 

Because space invariance simplifies both analysis and computation, many courses on “linear systems” focus almost entirely on the space-invariant case.

::: {.callout-note title="space invariance notation" collapse="true"}
Let’s make the idea of space invariance a bit more precise. Suppose we use $T()$ to represent a shift (translation) operation. If we shift an image $x$ by an amount $\delta$, we write $T(x, \delta)$. A system $L()$ is space-invariant if shifting the input and then applying the system gives the same result as applying the system first and then shifting the output (possibly by a scaled amount):

$$
L(T(x, \delta)) \approx T(L(x), \alpha \delta).
$$ {#eq-shiftinvariance-defined}

Here, $\alpha$ is a scale factor that accounts for magnification or other effects. For example, if you move an object by $\delta$ meters, its image might move by $\alpha \delta$ meters on the sensor.
:::

## LSI system matrix
An LSI system's matrix representation, $\mathbf{L}$, has a special, highly structured form. If the input is a single point, represented by a vector with a 1 in one entry and 0s elsewhere, the output is the system's point spread function (PSF). This output corresponds to one column of the matrix $\mathbf{L}$.

Because the system is space-invariant, the PSF's shape is the same for every input point; it is only shifted. Consequently, the columns of the matrix $\mathbf{L}$ are all shifted versions of a single PSF. If we assume the signal wraps around at the edges (a common practice called periodic boundary conditions), the resulting matrix has a specific structure known as a circulant matrix, as shown in @fig-ls-circulant-matrix.

![A circulant matrix, representing a Linear space-invariant (LSI) system with periodic boundary conditions. Each column is a circularly shifted version of the first column, which is the system's point spread function (PSF).](images/linearsystems/circulant-matrix.png){#fig-ls-circulant-matrix width=70%}

For computer simulations, there are many details to keep track of, especially issues relating to the edge and wrapping. For example, we often treat the point spread functions as 'circular', so when a value shifts below the bottom of the matrix, we do not throw it away but we copy it into the missing entry at the top of the column. In @fig-ls-circulant-matrix, the edges of the point spread are all zero, and that made the diagram simpler. 

Have a good look at @fig-ls-circulant-matrix. Feel how simple an LSI system matrix is compared to a general linear system (@sec-ls-matrix-tableau). Linear systems in general are great to work with.  If you are working with an LSI, you are particularly fortunate.  The next section gives another reason why.

## Convolution {#sec-ls-convolution}
The matrix multiplication for a space-invariant system, which we visualized as a sum of scaled and shifted point spread functions, is a fundamental operation known as **convolution**.

Formally, the convolution of an input signal $I[n]$ with a point spread function $h[n]$ is written using the asterisk symbol ($*$) and is defined by the summation:
$$
O[n] = (I * h)[n] = \sum_{k} I[k]\,h[n-k]
$$ {#eq-convolution-definition}
Here, the output at position $n$ is a weighted sum of the input values, where the weights are determined by a "flipped and shifted" version of the PSF. This formula is the mathematical expression of the process shown in the LSI matrix tableau.

Convolution is the essential calculation for determining the output of any LSI system in the spatial domain. The next sections explain why we often prefer to analyze these systems in the frequency domain, where this computationally intensive summation is replaced by simple multiplication.

## Harmonics:  Eigenvectors for shift-invariance {#sec-ls-harmonics-LSI}
We have reached the observation that is the basis of many calculations in the main chapters: the relationship between harmonics and space invariant linear systems. These ideas are introduced in the main text in @sec-optics-linear-harmonics, where we develop the Discrete Fourier Series (@sec-optics-fourier-series). The ideas continue to be used in many subsequent chapter.

Haromonic functions are *almost* eigenfunctions of space-invariant linear systems. By almost, I mean that when a harmonic is the input, the output is a scaled harmonic with a possible shift in position (phase). It is not exactly an eigenfunction because two parameters, the amplitude and phase, can change.

The mathematics of this near-miss is managed by expressing the signal using complex instead of real numbers. For complex numbers, which inherently have two parameters, we can say without any guilt that the complex exponentials are eigenvectors of a space-invariant linear system. Moreover, the complex exponentials are a complete, orthogonal set of eigenfunctions; any input stimulus can be described as the weighted sum of complex exponentials. We relate the complex exponentials back to the real harmonics, using Euler's formula (@eq-Eulers-formula). [^harmonics-FOV].

[^harmonics-FOV]: I explain the relationship between harmonics and space-invariant linear systems in detail, staying away from complex exponentials, in the Appendix to [Foundations of Vision](https://wandell.github.io/FOV-1995/appendix.html){target=_blank}.

Taken together this means space-invariant linear systems and harmonics can take advantage of the full power and simplicity of the eigenvector calculations (@eq-linear-eigencalculations). When the input to an optical system is a harmonic function at spatial frequency $f$ and unit contrast, the output image will be a harmonic function at the same frequency. The amplitude and of the output will differ from the input, but we only need to measure these two parameters. This input-output simplicity makes the harmonics very useful tools for system characterization.

::: {.callout-note collapse="true" title='History - linear algebra and matrices'}
As an assistant professor, I had the privilege of attending faculty meetings with senior colleagues I deeply admired—Joe Goodman, Ron Bracewell, Tom Cover, and others. We often discussed how to best teach key mathematical ideas to engineering students. The senior faculty generally favored starting with continuous functions and integrals, arguing these were the true mathematical foundation. The younger faculty, who spent much of their time programming, leaned toward teaching with matrices and vectors. I vividly remember Ron Bracewell remarking that continuous functions are the "reality," and we should only later show students the "vulgar" discrete approximations that our computational tools require.

These were lively debates, rooted in a deep respect for the mathematical foundations of our field. We all agreed that discrete representations are essential for computation. Today, mathematics framed in terms of vectors and matrices is at the heart of modern neural networks and computational science. That is the approach I use throughout this book. Still, I remember Bracewell’s advice and sometimes feel a bit of guilt for not starting with the continuous case.

It helps to know that representing problems with vectors and matrices is not just a modern computational convenience. The importance of linear algebra and matrix methods was recognized nearly 200 years ago. If you’re interested, you can read more about the [history of these powerful ideas](resources/history-linear-algebra.html){target=_blank}.
:::

## Discrete Fourier Series (DFS) {#sec-optics-fourier-series}
The Discrete Fourier Series (DFS) represents sampled data as sums of harmonics. It’s the frequency-domain counterpart to the point (impulse) basis (@sec-ls-spatial-basis). More background appears in @sec-appendix-linear-systems.

For a sampled 1D signal we write $I[n]$ with $n = 0,\ldots,N-1$ (there are $N$ samples). The DFS uses integer spatial frequencies $f = 0,\ldots,N-1$ and assumes the signal repeats every $N$ samples (periodic extension). Here are three equivalent forms.

1. Sine–cosine form

$$
I[n] = \frac{a_0}{2} + \sum_{f=1}^{N-1} \, s_f \,\sin\!\left(\frac{2 \pi f n}{N}\right) + c_f \,\cos\!\left(\frac{2 \pi f n}{N}\right)
$$ {#eq-fourier-series}


2. Amplitude–phase form

$$
I[n] = \frac{a_0}{2} + \sum_{f=1}^{N-1} a_f \,\sin\!\left(\frac{2 \pi f n}{N} + \phi_f\right),
\qquad a_f \ge 0
$$ {#eq-fourier-series-phase}

3. Complex exponential form

$$
I[n] = \sum_{f=0}^{N-1} w_f \, e^{\,i \, 2 \pi f n / N},
$$ {#eq-fourier-series-complex}

where the weights $w_f$ are complex and $i = \sqrt{-1}$. 

## Discrete Fourier transform (DFT) {#sec-dft-defined}
We need a way to transform the sampled image intensities, $I[n]$, into the Fourier weights, $w_f$. The most common method is the **Discrete Fourier Transform (DFT)**:
$$
w_f = \frac{1}{N} \sum_{n=0}^{N-1} I[n] \, e^{-i 2 \pi f n / N}
$$ {#eq-discrete-fourier-transform}

At first glance, this formula might look intimidating, but it's actually quite simple. On the right, we have the sampled image values multiplied by a complex exponential. You might imagine this as a matrix multiplication in which we place $I[n]$ into a vector and we place the complex exponentials into the columns of a matrix.  This is the idea we explained in general terms in @sec-ls-spatial-basis. 

For some, it is preferable to get rid of the complex notation.  Thanks to **Euler's formula**, we know that any complex exponential can be written in terms of sine and cosine:

$$
e^{i\theta} = \cos(\theta) + i\sin(\theta).
$$ {#eq-Eulers-formula}

This means the DFT uses the complex exponential as a method to combine the familiar sine and cosine terms into a compact expression. Once you get used to it, this complex formulation will seem neater than keeping track of separate sine and cosine coefficients.

It's important to note that while $I[n]$ is real-valued, the DFT coefficients $w_f$ are generally complex. For a real signal, there is redundancy: the coefficients satisfy $w_{f} = \overline{w_{N-f}}$, where $\overline{w_f}$ is the complex conjugate of $w_f$.[^dft-coefficients]

[^dft-coefficients]:  The complex conjugate of $a + i b$ is $a - i b$.

If you prefer to work with real numbers, you can compute the sine and cosine coefficients, $s_f$ and $c_f$, directly from the image intensities:

$$
\begin{align}
a_0 &= \frac{2}{N} \sum_{n=0}^{N-1} I[n] \\
s_f &= \frac{2}{N} \sum_{n=0}^{N-1} I[n] \sin\left(\frac{2\pi f n}{N}\right) \\
c_f &= \frac{2}{N} \sum_{n=0}^{N-1} I[n] \cos\left(\frac{2\pi f n}{N}\right)
\end{align}
$$

Here, $a_0$ gives twice the mean value of $I[n]$, which is why the Discrete Fourier Series (@eq-fourier-series) starts with $\frac{a_0}{2}$.

## Relating the DFS representations

There are several equivalent ways to write the Discrete Fourier Series, and it's helpful to know how the parameters relate:

- $\frac{a_0}{2}$ and $w_0$ both represent the mean value of the signal $I[n]$.

- The amplitude and phase parameters relate to the sine and cosine coefficients as follows:

$$
\begin{align}
a_f & = \sqrt{{s_f}^2 + {c_f}^2} \\
\phi_f & = \arctan\left(\frac{c_f}{s_f}\right)
\end{align}
$$

- The complex weights $w_f$ are related to the sine and cosine coefficients by

$$
w_f  = \frac{1}{2} (c_f - i s_f ) \qquad \text{for } f > 0
$$

- And the complex weights $w_f$ are related to the amplitude and phase by

$$
w_f = \frac{a_f}{2} e^{i (\phi_f - \pi/2)} \qquad \text{for } f > 0
$$

So, whether you prefer to think in terms of sines and cosines, amplitudes and phases, or complex exponentials, you can always translate between these forms.

::: {.callout-note title="About Jean Baptiste Joseph Fourier" collapse="false" #fourier-history}
See <a href="https://mathshistory.st-andrews.ac.uk/Biographies/Fourier/" target="_blank">a brief biography of Fourier</a>, who had a fascinating life. He served as a scientific adviser on Bonaparte’s Egyptian expedition and later as Prefect of Isère in Grenoble, where he pursued his theory of heat conduction and the use of trigonometric series.

In 1807 Fourier presented his heat memoir to the Paris Institute. The committee (Lagrange, Laplace, Monge, Lacroix) balked at representing arbitrary functions by trigonometric series and questioned the rigor; Biot and Poisson added objections. Fourier won the Institute’s 1811 prize, but publication lagged until 1822—evidence of the Academy’s initial resistance to what we now call Fourier series.

:::{.columns}
::: {.column width="50%"}
- Son of a tailor; Orphaned at 9
- Educated by Benedictines
- Supported the Revolution, becoming an adviser to Bonaparte in Egypt
- A statue of Fourier, erected in Grenoble, was later melted for armaments
:::
::: {.column width="50%"}
![Jean Baptiste Joseph Fourier](images/optics/optics-fourier.png){#fig-optics-fourier width="60%"}
:::
:::

:::

### Counting parameters
A length‑$N$ real signal $I[n]$ has $N$ real values. At first glance, each DFS parameterization seems to use more than $N$ parameters. For example, the sine–cosine form has both $s_f$ and $c_f$ for each $f$, and the complex exponential form uses $N$ complex weights $w_f$ (that’s $2N$ real numbers). How can that be?

The resolution is that for real signals the Fourier coefficients obey symmetries that reduce the number of independent values back to $N$ real degrees of freedom.

The cleanest way to see this is in the complex form. For real $I[n]$,
$$
w_{f} \;=\; \overline{w}_{\,N-f}, \qquad f=1,\ldots,N-1,
$$
so the coefficients come in complex‑conjugate pairs. Using Euler’s formula (@eq-Eulers-formula), these relate to the sine–cosine and amplitude–phase parameters as
$$
w_f = \tfrac{1}{2}\,(c_f - i\,s_f), \qquad
w_{N-f} = \tfrac{1}{2}\,(c_f + i\,s_f),
$$ {#eq-nyquist-1}
and
$$
w_f = \tfrac{a_f}{2}\,e^{\,i(\phi_f - \pi/2)}, \qquad
w_{N-f} = \tfrac{a_f}{2}\,e^{\,-i(\phi_f - \pi/2)}.
$$ {#eq-nyquist-2}

Two special bins are purely real for real signals:
- DC ($f=0$): only the cosine term contributes; this is the mean.
- When $N$ is even, the Nyquist bin ($f=N/2$): $\sin(\pi n)=0$ for all integers $n$, so only cosine remains. This bin is also purely real.

These symmetries remove the apparent over‑parameterization.

### Sampling rate and Nyquist frequency
If image samples are spaced every $\Delta x$, we say the sampling rate is $f_s = 1/\Delta x$ (samples per unit). The Nyquist frequency is half the sampling rate,
$$
f_N \;=\; \frac{f_s}{2} \;=\; \frac{1}{2\,\Delta x},
$$
and is measured in cycles per unit (e.g., cycles/degree or cycles/mm for spatial signals). In DFT index units (dimensionless bin indices), $f_N$ aligns with the bin at index $N/2$ when $N$ is even; when $N$ is odd there is no bin exactly at $f_N$.

As we saw in the previous section, because of conjugate symmetry, only about “half” of the spectrum is unique:

- Even $N$: There are two special, real bins—DC ($f=0$) and Nyquist ($f=N/2$). The remaining bins form conjugate pairs $(f,\,N-f)$ for $f=1,\ldots,N/2-1$. The count of distinct frequency bins is
    $$
    1 \text{ (DC)} \;+\; (N/2-1) \text{ pairs} \;+\; 1 \text{ (Nyquist)} \;=\; N/2 + 1.
    $$
    Each conjugate pair contributes two real degrees of freedom (real and imaginary parts), and the two special bins contribute one each, totaling $N$ real degrees of freedom.

- Odd $N$: There is no Nyquist bin. We have DC ($f=0$) plus conjugate pairs $(f,\,N-f)$ for $f=1,\ldots,(N-1)/2$. The count of distinct bins is
    $$
    1 \text{ (DC)} \;+\; (N-1)/2 \text{ pairs} \;=\; (N+1)/2,
    $$
    again yielding $N$ real degrees of freedom.

It is common to compute and report only this non‑redundant “half‑spectrum” (plus DC, and the Nyquist bin when $N$ is even). In continuous‑domain sampling language, frequencies above the Nyquist frequency $f_N$ “fold” or alias back; in DFT index terms, information in bins above $N/2$ is redundant with bins below $N/2$ for real‑valued signals. Some authors therefore call the Nyquist frequency the folding frequency.

## The Convolution Theorem {#sec-ls-convolution-theorem}
The fact that harmonics are eigenvectors of linear, space-invariant (LSI) systems leads to one of the most powerful results in signal processing: the **Convolution Theorem**.

In the spatial domain, calculating the output of an LSI system requires performing a convolution—summing the input signal with shifted and scaled versions of the point spread function (PSF). This operation can be computationally intensive.

The Convolution Theorem provides an elegant and highly efficient alternative. It states that **convolution in the spatial domain is equivalent to simple multiplication in the frequency domain.**

Let's formalize this. If the output image $O[n]$ is the convolution of the input image $I[n]$ and the system's point spread function $h[n]$:
$$
O[n] = (I * h)[n],
$$ {#eq-convolution-theorem-1}

then in the Fourier Transform domain the relationship is 
$$
\mathscr{F}\{O\} = \mathscr{F}\{I\} \cdot \mathscr{F}\{h\}
$$

where $\mathscr{F}$ is the Discrete Fourier Transform.

Using the complex weights notation, where $W_f$ are the DFT coefficients of the input and $H_f$ are the DFT coefficients of the PSF, the output coefficients $Y_f$ are simply:
$$
Y_f = W_f \cdot H_f
$$ {#eq-convolution-theorem-2}

This is a profound simplification. A complex convolution is replaced by straightforward, element-wise multiplication. The term $H_f$, the DFT of the point spread function, is so important that it has its own name: the **Optical Transfer Function (OTF)**. The OTF describes how the system scales the amplitude and shifts the phase of each input harmonic.

This theorem is the primary reason why Fourier analysis is indispensable for image systems engineering. It allows us to:

1.  **Analyze** a system's performance by examining its OTF.
2.  **Calculate** the output of an LSI system efficiently: transform the input to the frequency domain, multiply by the OTF, and transform back.
3.  **Design** systems by specifying a desired OTF.

You will see all of these ideas used in various ways in the main chapters of this book.  Fourier series play a role in modeling optics and characterizing optics, modeling sensors, and analyzing pereceived image quality. The harmonic basis is used as a tool in most aspects of science and engineering.


