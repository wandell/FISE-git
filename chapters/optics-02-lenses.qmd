# Lens principles {#sec-optics-lenses}

## Lens overview

<!-- I downloaded these slides. Some good concepts for this book. Overlaps a lot with my class, but additions. Link to PDF http://graphics.cs.cmu.edu/courses/15-463/2020_fall/lectures/lecture3.pdf 

Link to Born and Wolf anniversary edition
https://www-cambridge-org.stanford.idm.oclc.org/core/books/principles-of-optics/9D54D6FF0317074912CB285C3FF7341C

I used Jenkins and White, which I have in PDF.

This is a nice, but flawed, online book with a lot of good stuff.
https://pressbooks.online.ucf.edu/osuniversityphysics3/chapter/thin-lenses/

The Feynman lectures have a lot of great stuff about optics.  And color!
https://www.feynmanlectures.caltech.edu/
-->

A small pinhole renders a perfectly good image. Why do we need to add any optics? The critical reason is that pinhole apertures, by definition, only measure a small amount of electromagnetic radiation.  We often find ourselves in conditions where the amount of radiation passing through the aperture is a limiting factor.  We need more signal!

Let me tell you what I mean by **small**. Suppose we are in a dim room illuminated with a broad wavelength spectrum (10 $cd/m^2$). Consider a camera with a 1 mm pinhole, a sensor with 1 micron pixels, a focal length of 4mm, and an exposure duration of 50 milliseconds. In that case only about 35 photons arrive at each pixel. If we want to see in color, we must further divide up the wavelength range and the situation worsens. Because of the inescapable photon noise, which I describe later, this signal only permits a reliable intensity discrimination when two pixels differ by more than 20% (i.e., one edge is 20% brighter than the other, see [fise_opticsCountingPhotons](../code/fise_opticsCountingPhotons.html)).

A lens (i.e., optics) is a way to enlarge the entrance aperture to acquire more photons, without paying a penalty in spatial blur. This capability must have great value because the solution has been used by many different animal species.

::: {.callout-note collapse="true" title="Optics in animals"}

The most primitive animal eyes are called 'pigment cup'. These are made of a small receptor surface, about 0.1mm in diameter, and the eye is simply open. The size of the opening is a bit large to be considered a pinhole.

The eyes of some animals, in particular mollusks, have more plausible pinhole architectures. The receptor surface itself is 10 mm in diameter, and the opening is 0.4 - 2.8mm. These eyes evolved about 500 million years ago and have little changed.

A few animals species have a visual system based on pinholes. But the vast majority have evolved visual systems with a lens.

<!--# https://www.perplexity.ai/search/there-are-many-animals-that-ha-QJcp0_O_Ru6a0nVWEFkkmw  Perplexity search about tradeoffs between pinhole optics and lenses, particularly concerning biological systems. -->

<!-- Details https://www.britannica.com/science/photoreception/Single-chambered-eyes Authored by Michael Land.  Detailed on optics and different species.  -->
:::

## Rays and wavefronts
In the section on thin lenses, we describe lenses and treat light as rays (geometric optics). The reader may find this odd, given the clear demonstration of light as waves [@sec-lightfields-diffraction], and the obvious failure of the ray model. But to model refraction, and many other optics characteristics, geometric optics is both intuitive and accurate. We will return to calculations using waves later, when we consider more advanced calculations (@sec-optics-wavefront).

It is useful to draw the connection between geometric and wave representations. In **wave optics** a collection of rays headed in the same direction, and of the same wavelength and phase, are represented as a plane wave (@fig-waves-rays). We can visualize the wave by drawing a  line through the peaks of their (in-phase, common amplitude) waves. We call this line the **wavefront**.  The wavefront diagrams do not show the amplitude of the wave.  Various ray-wavefront combinations are illustrated in the different panels of @fig-waves-rays.

<!-- **Notes about wavefront representations** 
(https://www.rp-photonics.com/wavefronts.html).  
-->

![Relationship between the ray (arrows) and wavefront (dashed) representations. (a) A collimated set of rays is a planar wavefront. (b) Rays from a point source form a spherical wavefront. (The image shows a slice through the sphere). (c) Smoothly varying wavefronts with many different shapes are common. Often these shapes are due to imperfections (aberrations) in the optics.  We reason about them as a collection of rays; the direction of each ray is drawn perpendicular to the tangent of the wavefront. (d) Only a narrow section of the rays from a distant point are incident at the entrance aperture of the imaging device. Hence, the incident light field from the point is close to a planar wavefront.](images/optics/rays-wave.png){#fig-waves-rays fig-cap-location="bottom" fig-align="center" width="75%"}

It is common for optics tutorials to represent wavefront directions as extended in space, but it is uncommon to represent the amplitude of the wavefront across space. In reality the amplitude of the wavefront will vary across space. For example, a typical laser emits a set of parallel (collimated) rays.  The amplitude of these rays has a Gaussian profile that limits the wavefront in space.  That's why the spot a laser pointer produces is limited in space. In recent years, image systems engineers have had access to **spatial light modulators (SLMs)**.  These devices modulate either the local amplitude or phase of the wavefront at different positions in space, controlling the light field. 

The software calculations that accompany this book use both wavefront and ray methods[^optics-intro-1].

[^optics-intro-1]: The ISETCam optics calculations uses the wave model. The ISET3d graphics calculations use the ray model.

## Material interactions
When a light rays arrive at the boundary between regions with different optical properties - whether those regions contain matter (like glass or water), air or vacuum - several things can happen (@fig-reflected-scattered).  The rays might be reflected back, as in a mirror.  Or, the rays might enter into the material, undergo a series of internal reflections, and emerge back in one of many different directions.  Also, the rays might enter the material and be absorbed, giving up their energy to the atoms in the material. (Maybe reference the dichromatic reflection model here?)

![When rays arrive at the boundary of two regions, The rays (a) might be reflected, (b) might enter the medium, be internally reflected multiple times, and emerge in one of many different directions (scattering). (Not shown) The ray may might enter the medium and be absorbed.](images/optics/snell-reflected-scattered.png){#fig-reflected-scattered fig-align="center" width="100%"}

The most important case for controlling the rays, and thus for optical design, is this other possibility: the ray enters the new material and continues on its way. We call this change in direction **refraction**. 

![A ray crossing the boundary between two materials may continue but change direction. The change occurs because the speed of electromagnetic waves differs between the regions. The *refraction index* ($n$ and $n'$) is the material parameter that describes the speed.](images/optics/snell-refraction.png){#fig-snell-law fig-align="center" fig-cap-location="bottom" width="50%"}

### Snell's Law
Snell's Law quantifies how light changes direction when passing between regions with different optical properties. The mathematical formulation of Snell's Law is defined using the variables shown in @fig-snell-law. A light ray (Incident ray) is shown crossing a planar boundary between one medium (say, air) in a second medium (say, glass). The angle between the incident ray and a line perpendicular to the surface (the surface normal) is $\phi$. The refracted ray changes direction and thus has a different angle, $\phi '$, with respect to the surface normal. Snell's Law captures the relationship between these two ray angles 

$$ n \sin(\phi) = n' \sin(\phi') $$ {#eq-snell}

The variables $n$ and $n'$ are the *refraction index* of the materials, which is related to the speed of the electromagnetic radiation in the material.

$$ n = \frac{c}{v} $$ {#eq-indexofrefraction}

where $c$ is the speed of light in a vacuum, $v$ is the speed in the material. By definition, $n=1$ in a vacuum. It is very close to $1.0$ in air. In some materials, such as glass, the velocity of light is much slower, so that the index might be $1.3$ or even $1.5$. 

In many common materials the velocity is reduced because the light, which is electromagnetic radiation, induces an electrical signal in the electrons of the material. The specific properties of this signal depend on the material. The induced electrical signal sums with the electrical field of the light. The combined signal is predicted to travel slower and in a different direction, with the exact change in speed and direction dependent on the electrical properties of the material[^optics-intro-2]. 

[^optics-intro-2]:  This excellent pair of (nicely snarky) Fermilab videos from Don Lincoln explain the change in [velocity](https://www.youtube.com/watch?v=CUjt36SD3h8) and [direction.](https://www.youtube.com/watch?v=NLmpNM0sgYk).  We can thank James Clerk Maxwell for providing the basis of this understanding.

Interestingly, there are less common materials (e.g.,liquid crystals) that have more complex interactions with the electrical field of light. These materials create interesting and useful effects (birefringence) that we will describe in @sec-displays.

### Wavelength dependence {#sec-chromatic-aberration}
<!-- [Abbe Number](https://chatgpt.com/share/676a2564-e72c-8002-923a-d865491f30ed) -->
For many materials that index of refraction is wavelength dependent. Following Snell's Law, the each wavelengths changes direction by different amount. Consequently, when we set the lens to be in best focus for one wavelength, it will not be in best focus for other wavelengths.  The inability to produce a good focus for all wavelengths simultaenously is called **chromatic aberration**. 

For most materials the change of the refraction index over the visible wavelength is roughly linear, just varying in extent.  It is common to summarize the dispersion using the *Abbe number*, $V_d$, which is computed by comparing the refraction index at three different wavelengths.

$$ V_d = \frac{n_{489.3} - 1}{n_{486.1} - n_{653.3}}$$ {#eq-abbe}

Conventionally, an Abbe number greater than 50 implies very little dispersion and a number less than 30 implies significant dispersion that will impact image quality.  The human eye has very significant chromatic aberration, with an Abbe number of about 15. 

When possible, it is my preference in computation to provide numerical values describing the index of refraction as function of wavelength and to measure the impact by computing the line spread or point spread function. We will perform analyses of chromatic aberration several times in this book, including in @sec-human.

::: {.callout-note collapse="true" title="Historical note:  Refraction"}

Refraction was studied by the Greek astronomer Ptolemy in about the year 150. He documented that rays passing from air into, say water or glass, change direction. Ibn Sahl in Persia, around 930, quantified the change in direction as light crossed the boundary between certain materials, and he used this knowledge to design lenses.  

<!-- Feynman copies in Ptolemy's tables. A longer discussion of the history is here by Smith:
Ptolemy's Search for a Law of Refraction: A Case-Study in the Classical Methodology of "Saving the Appearances" and its Limitations
Descartes' crazy idea of infinite and then faster.  A bit of Buzz Lightyear.
There are those who say Ptolemy held back the field by not noticing the Snell's law relationship.  Apparently his work was ignored for centuries and only partially returned.  See the Smith article
-->

The formula for refraction was discovered independently by two scientists in the 17th century. Willebrord Snellius (also known as Snell) first found the mathematical relationship in 1621, but his work wasn't published during his lifetime. He was an astronomer, and in 1617 he determined the size of the earth based on measurements of its curvature between the cities of Alkmaar and Bergen-op-Zoom.

Renée Descartes independently discovered the relationship as well, and he published it in 1637. Although Descartes published first, the formula became known as Snell's Law to acknowledge Snell's earlier discovery. In modern times, the French still frequently refer to Snell’s Law as “la loi de Descartes” or “la loi de Snell-Descartes”. 

The physicist Christiaan Huygens showed that Snell's Law could be explained by treating light as a wave that travels at different speeds in different materials. More about the history of the discovery and quantification, including the references, [makes for interesting reading](https://en.wikipedia.org/wiki/Snell%27s_law#History).  

<!-- I think the wikipedia article attributes the discovery to yet another person, too.  And Feynman has a derivation based on Fermat's principle of shortest time. This is a phenomenological principle, not a mechanism. -->
:::

## Lenses control ray directions

::: margin
![The rays from a nearby point that pass through a relatively large pinhole diverge.  They form an image of the point that is larger than the pinhole.](images/optics/pinhole-rays.png){#fig-pinhole-rays fig-align="center" width="75%"}
:::
Consider the rays from a point as they pass through a pinhole. The image they form will not be sharp because the ray directions are diverging. Narrowing the pinhole sharpend the image formation by selecting a small subset of the rays. 

We can sharpen the image by redirecting some the rays to converge at the image plane. One way to think about optical devices is that they are built to control the direction of the environmental light field rays at the image system entrance aperture. When forming a sharp image is the goal, we evaluate the optics by asking whether they adjust the ray directions so that the image of a point in the scene is also a point. A useful fact to remember is that diffraction means the best (sharpest) we can do for a circular aperture is defined by the diameter of the Airy disk.

::: margin
![Lenses transform the direction of the rays in the incident light field. In this example, the diverging rays from a nearby point are transformed to converge on the image plane. Diffraction imposes a limit on precision of the convergence. ](images/optics/pinhole-lens.png){#fig-pinhole-rays fig-align="center" width="75%"}
:::

## Making a lens
To make a lens that focuses a light field into an image plane requires changing the direction of many rays. The simple case of rays from one point is easy to see.  

![Refraction transforms the rays diverging from an object point (left) to converging to an image point (right).](images/optics/thinlens-prism.png){#fig-thinlens-prism fig-align="center" width="50%"}

Imagine that the rays are diverging from an object point at the left. Were the rays passing through a pinhole, they would continue to diverge. Refraction by the prism changes the direction of the rays at the  from upward to downward (upper prism). Similarly, refraction changes the ray directions from downward to upward (lower prism). These two prisms converge the rays from the object point onto a point on a properly positioned image plane.

<!--# This link describes the issues in making a diffraction limited thin lens.  I think the problem is a flat sensor surface will not work with a spherical lens surface. You need a aspheric. It has other interesting facts. To get close to diffraction limited spherical you need to use a multi element optics to correct for different aberrations.  It is possible to change the materials (GRIN) or shape (aspherics) as well.
https://chatgpt.com/share/6748e5dc-267c-8002-844c-a83ee0cdb144 

Diffraction limited thin lens issues.  Roughly same as the one above.
https://chatgpt.com/c/6748e492-0310-8002-8da3-89a1517a6062 
-->

![A lens can be approximated as many small prisms.  The outline shows a spherical surface on the entrance and exit side.](images/optics/thinlens-assembled.png){#fig-thinlens-prism fig-align="center" width="80%"}

Expanding on this idea, we can stack a collection of prisms to gather more of the rays.  Or, more likely, we might polish a glass surface to have two spherical surfaces, forming a continuous form of the stacked prisms. The entrance aperture of this lens can capture much more light than a pinhole, and it guides the rays to converge. Spherical lenses are quite popular because they are easy to manufacture. Using Snell's Law it is relatively straightforward to calculate the refraction for a ray arriving at any location on the sphere [^optics-intro-3].

[^optics-intro-3]:  Put the formula for refraction of a ray at a spherical lens in here.

The diagram makes it clear that there are many lens design choices. For example, we might change the shape of the lens surfaces. Also, we might use different materials in the prisms. What considerations would guide such choices?
 
## Lens limitations
Spherical surfaces are relatively easy to manufacture and thus cost-effective. But spherical lenses have a noticeable defect. In the 11th century book, "Kitāb al-Manāẓir" (Book of Optics), Alhazen observed that spherical lenses often produce a blurry image. The reason is that at increasing distance from the optical axis, the refraction is too large. Thus, the rays passing through the edge of the sphere do not converge precisely with those passing near the optical axis. The amount of this *spherical aberration* depends on a number of factors, such as the size of the lens aperture, the curvature, and the distance to the objects.

### Multi-element optics
To reduce the spherical aberration and improve image sharpness, designers have created multi-element optics. Often, these are additional spherical lenses added to the light path whose purpose is to reduce spherical and other aberrations of the whole system. THere are several famous multi-element optics designs, such as the *double Gauss* and *Petzval* systems. Very high quality multi-element optics can be fairly large. 

Spherical lenses remain in use in devices that must be low cost, do not require  high spatial resolution, or that can use relatively large optics.  In recent years, other optics have been implemented.

### Aspherics
To reduce spherical aberration, we can use a different shape. This was difficult to manufacture, but advances in computer-controlled polishing and the ability to create molded plastics and glass permit designers to implement aspherical lenses.  The use of aspherics accelerated in multiple industries after 2010.  
<!--
From Perplexity search

https://www.perplexity.ai/search/when-did-it-become-cost-effect-bqLBLfjZTRuYA.uF6hOvnA#2

https://www.laserfocusworld.com/directory/finished-optics-coatings-components/lenses/blog/14310615/avantier-inc-replacing-spherical-lens-with-aspheric-lens?t

https://www.shanghai-optics.com/about-us/resources/technical-articles/optical-revolution-aspheric-lenses-advances-and-solutions/?t

https://avantierinc.com/resources/technical-article/aspheric-lens-breakthroughs/?t
 -->

Designers frequently use a particular class of aspherical lenses.  These are described by a lens surface position, $z$, that is specified as a function of radial distance $\rho$, from the optical axis.

\begin{equation*}
z(\rho) = \frac{\rho^2 / R}{1 + \sqrt{1 - (1 + k)(\rho/R)^2}} + A_4 \rho^4 + A_6 \rho^6 + A_8 \rho^8 + \cdots
\end{equation*}

$z(\rho)$ is called the sag of the lens surface.  $R$ is the radius of curvature of the lens at the vertex of the lens.  $k$ is called the  conic constant; it determines the deviation from the spherical shape. The parameters $A_i$ are called higher-order aspheric coefficients; they are adjusted to reduce different types of lens imperfections. Lens design software is used to search through the parameters, given the materials and other design constraints, to create circularly symmetric aspherical lenses with minimal spherical aberration.

<!--
[See this page and fise_aspherics.m for the shape formula of aspherical lenses.](https://chatgpt.com/c/6748e492-0310-8002-8da3-89a1517a6062).[Also this page.](https://g.co/gemini/share/a89a3d193d26)
-->

### Grin lenses
In 1854 James Clerk Maxwell, noticing that biological eyes are often spherical, suggested that the spherical aberration could be reduced by using lenses with a refraction index that varies from the optical axis to the edge (Maxwell reference here). It was later discovered that some biological eyes have this property (Reference here). Manufactured lenses with a variable gradient refraction index (*GRIN* lenses) are valuable in optical design, although they are more difficult to manufacture than aspherics. 

<!-- https://www.britannica.com/science/photoreception/Single-chambered-eyes Michael Land's Brittanic article has the story. Lenses made of homogeneous material (e.g., glass or dry protein) suffer from a defect known as spherical aberration, in which peripheral rays are focused too strongly, resulting in a poor image. 
-->

### Metalenses
The purpose of the lens is to redirect the rays in the incident light field. Suppose we can specify how we would like to redirect the rays.  We would call this specification a *ray transfer function* (Goossens reference).  It would be ideal if we could manufacture a very thin surface whose optical properties implement that function. 

Advances in material science, optics, and computation are making progress on this goal. They are creating *metalenses*, which are very thin surfaces made up of small elements - smaller even than the wavelengths of light - with different refractive indices. The placement of these elements can produce many different ray transfer functions. Using advanced computational methods and hardware (e.g., GPU clusters), groups are learning how to define these surfaces and build metalenses that implement specific functions.  (References.)
