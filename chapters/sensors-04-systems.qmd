# Sensor Subsystems {#sec-sensor-systems}

Images are captured under a wide variety of conditions, and there is no single set of parameters—such as focus, exposure time, or sensor gain—that guarantees high-quality results in every situation. To address this, sensors are equipped with control systems that automatically adjust acquisition parameters. These systems help prevent pixel saturation, reduce image noise, and set optical properties like focal plane and depth of field. In computer vision applications, such as robotics or autonomous driving, these control systems are essential for obtaining reliable quantitative data. In consumer photography, they assist users in quickly achieving settings that produce visually appealing images.

The sensor control systems are typically implemented by the commercial vendors. The general principles of these control methods can be described, but the specific implementations can be quite complex, with many parameters, and are trade secrets. Moreover, the control systems themselves need to be adjusted in various ways that depend on the details of the optics and sensor in the camera. This section describes the principles of the control systems. It is my view that there are opportunities to develop better open standards for designing and evaluating these control systems through simulation.

## Focus Control Systems {#sec-focus-control}

In some imaging applications, the lens position can be fixed. For example, consider an autonomous driving system where the camera is designed to detect vehicles or pedestrians at distances greater than 3 meters. With a focal length of 30 mm, the ideal sensor-to-lens distance for objects beyond 3 meters varies only slightly—from $\frac{300}{299} \times 25$ mm to 25 mm—about $83 \mu \text{m}$. Over this range, image sharpness remains nearly constant, with the effect depending mainly on the aperture size (see @sec-optics-dof). In such cases, the lens can be set to focus at infinity, and no adjustment is needed.

However, in many other imaging systems, the camera must adjust the lens position to bring objects at different distances into focus. This is especially important in consumer photography, for close-up portraits or enabling the photography to put some objects into focus while blurring the background.

### Lens Positioning in SLR and DSLR Cameras {#sec-lens-positioning-slr}

In **single-lens reflex (SLR)** and digital SLR (DSLR) cameras focusing was achieved mechanically. The imaging lens -typically comprising multiple lenses and called either multi-element optics or a lens assembly- was mounted on a screw thread. The position of some of the lenses could be adjusted by rotating the assembly along the thread. These lenses were rotationally symmetric, so the rotation was irrelevant to the image quality. The thread pitch of the screw determined how finely the position can be adjusted.

SLR camera bodies and lenses were relatively large. I lugged quite a few of them around on trips. For my wife. Moving the entire assembly back and forth was challenging. This was called the **unit focusing** design. To lighten the load, lens designers created systems that could change the focus by moving only a small, relatively light, subgroup of lenses within the assembly. If the lenses were in the middle of the assembly, this was called **internal focusing**, and if the lenses were at the read this was called **rear focusing**. This had several advantages.

-   **Faster and Quieter Autofocus:** Moving a smaller, lighter group of elements requires a much smaller, less powerful motor. This allows for significantly faster, quieter, and more energy-efficient autofocus performance, which is critical for tracking moving subjects.
-   **Constant Lens Length:** Because all the movement happens inside the sealed barrel, the overall physical length of the lens does not change during focusing. This improves the balance and handling of the camera and lens combination.
-   **Non-Rotating Front Element:** The front of the lens does not move or rotate. This is a huge advantage when using filters like **polarizers** or **graduated neutral density filters**, which must be kept in a specific orientation to work correctly.
-   **Improved Optical Performance:** By freeing designers from the constraint of moving the entire optical block, internal focusing allows for more complex and better-corrected optical formulas. It can lead to sharper images and better close-focusing capabilities.

Moving only a small, internal group of lens elements is a sophisticated design that is fundamental to the performance of modern autofocus lenses.

::: {#sensor-slr .callout-note collapse="true" title="Single-lens reflex"}
SLR cameras were the coolest ones when I was middle-aged. The SLR design replaced the **rangefinder** approach. A rangefinder camera has two separate optical paths:

-   The "Taking" or "Imaging" Lens: This is the main lens that focuses the image onto the film or sensor. You do not look through this lens.
-   The Viewfinder Window: This is a separate, small window on the camera body that you look through to compose your shot.

There were often slight differences between the view through the lens and the view through the viewfinder, and this added to the challenge of using these cameras well.

![Overview of an SLR and its subsystems](images/sensors/03-sensor-slr.png){#fig-sensor-slr width="60%"}

The SLR breakthrough was that it used a single optical path through the lens assembly for view finding, autofocus, and capturing the image (hence, "single lens"). The light after the lens assembly would pass through a beam splitter, which directs about 70% of the light to the viewfinder and the rest to the autofocus subsystem. The beam splitter usually takes the form of a semi-transparent "reflex" mirror. When it was time to capture the image, the mirror would be moved out of the light path allowing all the light to form an image on the film (SLR) or digital sensor (DSLR) (@fig-sensor-slr).

The SLR design also lets the user separately control the size of the aperture, and this has an impact on the depth of field (@sec-optics-dof). An advantage of the SLR design is that you can see the depth of field narrow or increase in the viewfinder of the SLR design.
:::

### Lens Positioning in Mobile Devices {#sec-lens-positioning-mobile}

Smartphones and other compact devices required a different approach due to space constraints. These cameras also use multi-element lenses, but they are much smaller. Focus is commonly controlled by a voice coil motor (VCM), which is reliable, inexpensive, and widely used in the industry (@fig-sensor-vcm).

![Cell phone optics. The optics themselves are very compact but still multi-element. The position of the optics is usually controlled by a voice-coil motor.](images/sensors/03-sensor-vcm.png){#fig-sensor-vcm width="60%"}

There is also a growing market for lens positioning using Micro-Electro-Mechanical Systems (MEMS) devices. MEMS actuators offer advantages such as low power consumption, fast response, and miniaturization, making them attractive for mobile imaging systems.

![A video showing lens assembly position controlled by a MEMS device.](videos/sensors/MEMS-autofocus.mov){#fig-sensor-mems width="70%"}

Conceptually, autofocus methods share similarities with techniques for estimating object distance. Both rely on understanding the optical light field (@sec-optical-lightfield). While the implementation of autofocus algorithms has evolved significantly in recent years, the underlying physical principles remain the same. We begin with the classic autofocus mechanism and then describe more recent implementations that are integrated directly into CMOS sensors.

<!-- liquid lens callout here? -->

### Phase Detection Autofocus (PDAF) {#sec-phasedetection-autofocus}

@fig-focusprinciples-classic shows a system for phase detection autofocus (PDAF). It illustrates the rays from a small object, near the lens, on the principal optical axis (horizontal black line). Because the point is reasonably close, the incident light field rays are diverging. The rays incident at the top of the aperture are shown as red and those at the bottom as green. If the point is brought to a good focuse, these rays will converge to a small spot on the sensor. The autofocus system shown here, which uses a mirror to send the rays to a pair of detectors, is designed to measure whether the rays from the top and bottom of the lens will meet at the sensor.

![Phase detection autofocus, as invented in the 1970s.](images/sensors/03-focusprinciples-2.png){#fig-focusprinciples-classic width="60%"}

In this design, a mirror behind the main lens directs the rays from the top and bottom of the lens to two separate microlenses. Each microlens focuses the rays onto a linear sensor array. The microlens and sensor positions are arranged so that when the image is in focus, both the light arriving at both linear arrays will be centered. If that is the case, we have good focus. The system might then move the mirror out of the way, allowing the sensor to capture the image.

If the image is out of focus, the rays from the top and bottom of the lens are displaced from the centers of their respective arrays. The relative position of the rays in the two arrays provides information about the defocus. The distance between these positions—called the phase difference—indicates how the lens needs to be adjusted to achieve focus. The autofocus subsystem uses this information to move the lens and bring the image into focus. This method, known as **phase detection autofocus (PDAF)**, became a key feature in single-lens reflex (SLR) cameras.

However, this approach has limitations. The mirror, microlenses, and sensor arrays form a relatively large and complex mechanical system, making it unsuitable for compact devices like smartphones. Also, this system only allows either focusing or image capture at one time, not both simultaneously. Later technologies addressed these issues.

::: {#sensor-phasedetect-autofocus .callout-note collapse="false" title="Phase detection autofocus: Commercialization"}
Phase detection autofocus (PDAF) for SLR cameras was pioneered by Honeywell, which patented the core concepts in the 1970s (U.S. Patents 3,875,401 and 4,185,191). Their design used a beam-splitting rangefinder rather than a flip mirror.

The first commercial SLR with PDAF was the Minolta Maxxum 7000 (α-7000 in Japan), released in 1985. It integrated in-body PDAF and motorized lens control, using phase differences detected by two 1D sensors to drive lens adjustments.

Leica had previously patented a different autofocus method based on contrast detection, which they licensed to Minolta. However, Minolta found it slow and unreliable, so they used a phase detection approach instead—without a proper license from Honeywell. Honeywell sued and won a [substantial award](https://www.nytimes.com/1992/02/12/business/honeywell-minolta-dispute-teaches-conflicting-lessons.html).
:::

### Dual Pixel Autofocus (DPAF) {#sec-dualpixel-autofocus}

The phase detection autofocus subsystem is difficult to fit within the cell phone form factor. Consequently, vendors tried a number of other methods. These were mainly based on estimating the contrast of the image through the lens and sensor, an approach called **contrast detection autofocus (CDAF)**. An in focus image should have a sharp edge, and CDAF methods would hunt for a lens position to increase edge contrast. There are no principled measurements that guide how to move the lens based on contrast, and the method struggles with noisy and low light images. Thus, the method was relatively slow. Other techniques were also investigated using time-of-flight technology (@sec-sensors-tof) to measure distance to objects in the field of view, and some of these continue to be used.

But in 2014 a method emerged based on the same principles as phase detection; the method could be directly integrated into the CMOS sensor! This technique, **dual pixel autofocus (DPAF)**, is now widely used by most vendors. The CMOS integration places two photodiodes underneath all or at least some microlenses (@fig-sensor-dpaf). When acquiring an image, the voltages from the two photodiodes are summed and readout. During the autofocus operation, however, the voltages from the two photodiodes are compared.

![Dual pixel autofocus sensor design.](images/sensors/03-sensor-dpaf.png){#fig-sensor-dpaf width="80%"}

::: {#sensor-dpaf .callout-note collapse="true" title="Dual pixel autofocus: Canon"}
<!-- CHATGPT:  FISE-Sensor/Phase Detection Autofocus -->

Canon **introduced DPAF** with the **EOS 70D in 2013**, building on earlier hybrid PDAF/live-view systems like the EOS 650D/T4i-D. This technology is now widespread, with vendors from Sony to Omnivision both shipping sensors with **two photodiodes under one microlens**. This enables on-sensor phase detection and fast, smooth autofocus—especially for video (\[usa.canon.com\]\[2\]).
:::

The logic of the design is based on understanding the optical light field. Consider some pixels near the center of the sensor array. The rays from the left and right sides of the lens arrive at the pixel at a two different angles. If we trace these rays through the microlens, we can see that they would end up on different sides of the silicon substrate. By placing two photodiodes in the substrate, say on the left and right side of the pixel, we estimate the optical light field from the left and right side of the lens.

If the image is in good focus at the sensor, the signals encoded by the two photodiodes will be the same, apart from noise. Thus, if we plot the intensity of the response of the left and right photodiodes, they will be the same. But if the focus is off, the rays from the left and right side of the lens will not be encoded by the same pixel. Instead, the rays from the left and right side of the optical light field will be spread out (defocus) and shifted different pixels. Because of their angles the rays from this point will tend to be on the photodiodes on the left on one side and they will be blurred and shifted to the photodiodes on the right on the other side. The same will be true for other nearby points. Thus, the response of the photodiodes on the left and right side will be shifted copies of one another. This shift has the same information as the shift in the PDAF method; it provides information about the size and direction of the defocus.

![Shift from defocus. Needs work. Also, could do it with the Chess Set image and true light field simulation. t_cameraLightField.](images/sensors/03-sensors-lf-shift.png){#fig-sensor-lf-shift width="80%"}

Using multiple photodiodes under a single microlens recovers some information about the optical light field -the left and right subpixels measure rays from different sides of the imaging lens. This raises the idea that we might be able to extend this idea to estimate more about the optical light field. Also, if we had such light field sensor, what could we do with it (@sec-sensors-lightfield)?

## Exposure Control {#sec-exposure-control}

Each scene creates an irradiance range at the sensor, and we acquire a high quality image of the scene when the darkest parts of the scene produce enough electrons to be above the noise floor, and the brightest elements of the scene generate electrons that do not exceed the well capacity. We think of this as adjusting the acquisition parameters so that the dynamic range of the scene fits in the dynamic range of the sensor (@sec-wellcapacity-dynamicrange).

Matching the scene and sensor is a key task for the exposure control system. There are three main components of image system that can be controlled to match the scene and sensor. These are the size of the lens aperture, the exposure duration, and the gain of the circuit that maps electrons into readout voltages.

set the acquisition parameters so that the scene radiance levels fit within the pixel's dynamic range If the camera integrates photons over a long period of time, the electron well at can fill up. If the well is filled, the voltage from the pixel is said to be **saturated**. Conversely, if the camera integrates over a short period of time, given the scene intensity, there will only be a few electrons in the well. These number might be small compared to the number of electrons generated by intrinsic camera noise. Also, the Poisson noise means that the signal-to-noise for a small number of electrons will be poor. Hence, it is important to select a good exposure duration to avoid noise and saturation.

### Exposure Value {#sec--exposure-value}

### Exposure Bracketing {#sec-exposure-bracketing}

### Burst Photography {#sec-burst-photography}

Google but others. Multiple capture.