---
date: last-modified
---

# Image processing: stereo {#sec-imgprocessing-stereo}

{{< include "includes/WIP-callout.qmd" >}}


## Image processing: stereo overview {#sec-imgprocessing-stereo-overview}

There is more to depth than stereo

## Hardware

### Camera arrays

### Lidar

## Stereo depth information

The light captured at each pixel in a conventional sensor arises from a small region (point) in three-dimensional space. The sensor data only have an $(x,y)$ position, and we require some additional inferencing to estimate the three dimensional position of the object point $(X,Y,Z)$.  There are many ways that we can make inferences about the distance to the object, and stereo cameras are one important technology.

Consider the ray tracing in The object could be anywhere along a line.  When we record a star, the object is many light-years away.  When we look at a flower, the light can be close at hand.  

![Using a stereo pair to locate a point in three-space](){#fig-stereo-camera-rays}

If we have a second camera measuring the scene, knowing the corresponding location in the second camera disambiguates where the object is in three space.  It is at the intersection of the two lines.

In human vision we call the relationship between corresponding points the disparity. There are neurons that are sensitive to specific disparities.  Not sure if that goes here or elsewhere, say in the human section.

## Epipolar geometry
The ideas of epipolar geometry are enormously helpful when we work with stereo cameras. The principles, derived using ray tracing geometry, help us search efficiently for potential stereo matches between two images. As we saw earlier, knowing which the correspondence between image points allows us to infer the position of the object in three-space.

The development of epipolar geometry was a gradual process that emerged from photogrammetry and computer vision research in the mid-20th century. The basic principles actually date back to early aerial surveying and mapping.

The fundamental concept builds on projective geometry principles established in the 19th century by mathematicians like Jean-Victor Poncelet. However, the specific application to stereo imaging and camera geometry took shape in the 1950s and 1960s with the rise of photogrammetry for mapping and surveying.

The key breakthrough was understanding that when two cameras view the same 3D point, the image points, the camera centers, and the 3D point all lie in the same plane - what we now call the epipolar plane. This geometric constraint significantly simplifies the stereo matching problem by reducing the search for corresponding points to a line rather than the entire image.

The formalization of epipolar geometry as we know it today, with the essential matrix and fundamental matrix, was developed in the 1980s. A major contribution came from Hans Longuet-Higgins in 1981 with his paper "A computer algorithm for reconstructing a scene from two projections," which introduced the essential matrix that captures the geometric relationship between corresponding points in calibrated cameras.

The fundamental matrix, which handles uncalibrated cameras, was developed later in the 1990s, with significant contributions from researchers like Richard Hartley and Andrew Zisserman.

Harlyn Baker (not Harlan) did indeed work in computer vision and robotics, particularly at SRI International in the 1980s along with Robert Bolles and David Marimont. They made contributions to the practical application and implementation of stereo vision systems during an important period when computer vision was becoming more computationally feasible.

The SRI team, including Baker, Bolles, and Marimont, worked on various aspects of computer vision and robotics systems, including stereo vision. Their work was part of a broader effort at SRI International to develop more robust computer vision systems for robotics and automated inspection applications.

There was a significant paper by Robert Bolles, Harlyn Baker, and David Marimont published in 1987 in the International Journal of Computer Vision. The Epipolar-Plane Image (EPI) analysis they described was an innovative approach to the structure-from-motion problem.

Their key contribution was developing the concept of analyzing the epipolar-plane image - essentially looking at a slice of a sequence of images taken from a moving camera. When you stack these images and look at them from the side (taking a slice), you get patterns of lines whose slopes relate to the depths of points in the scene. This was a clever way to exploit the constraints of camera motion to simplify the 3D reconstruction problem.

What made their approach particularly notable was:

1. It provided a way to analyze motion sequences without having to solve the correspondence problem for every frame
2. The linear patterns in the EPIs made it easier to detect and track features
3. It worked well for analyzing continuous motion sequences, not just discrete viewpoints

Their work at SRI International came during an important transition period in computer vision, when researchers were developing more practical approaches to 3D reconstruction that could actually be implemented with the computing power available at the time.

This paper influenced later work in structure-from-motion and multi-view stereo, as it provided both theoretical insights and practical methods for extracting 3D information from image sequences. It's still referenced today, particularly when dealing with camera arrays or linear camera motions.



