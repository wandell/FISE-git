# Control systems {#sec-sensor-control}

Images are captured under a wide variety of conditions, and there is no single set of parameters—such as focus, exposure time, or sensor gain—that guarantees high-quality results in every situation. To address this, sensors are equipped with control systems that automatically adjust acquisition parameters. These systems help prevent pixel saturation, reduce image noise, and set optical properties like focal plane and depth of field. In computer vision applications, such as robotics or autonomous driving, these control systems are essential for obtaining reliable quantitative data. In consumer photography, they assist users in quickly achieving settings that produce visually appealing images.

The sensor control systems are typically implemented by the commercial vendors. The general principles of these control methods can be described, but the specific implementations can be quite complex, with many parameters, and are trade secrets. Moreover, the control systems themselves need to be adjusted in various ways that depend on the details of the optics and sensor in the camera. This section describes the principles of the control systems. It is my view that there are opportunities to develop better open standards for designing and evaluating these control systems through simulation.

## Focus Control {#sec-focus-control}

In some imaging applications, the lens position can be fixed. For example, consider an autonomous driving system where the camera is designed to detect vehicles or pedestrians at distances greater than 3 meters. With a focal length of 30 mm, the ideal sensor-to-lens distance for objects beyond 3 meters varies only slightly—from $\frac{300}{299} \times 25$ mm to 25 mm—about $83 \mu \text{m}$. Over this range, image sharpness remains nearly constant, with the effect depending mainly on the aperture size (see @sec-optics-dof). In such cases, the lens can be set to focus at infinity, and no adjustment is needed.

However, in many other imaging systems, the camera must adjust the lens position to bring objects at different distances into focus. This is especially important in consumer photography, for close-up portraits or enabling the photography to put some objects into focus while blurring the background.

### Lens Positioning in SLR and DSLR Cameras {#sec-lens-positioning-slr}

In **single-lens reflex (SLR)** and digital SLR (DSLR) focus is usually achieved mechanically. The imaging lens -typically comprising multiple lenses and called either multi-element optics or a lens assembly- is mounted on a screw thread. The lens position is adjusted by rotating the assembly along the thread. The lenses are typically rotationally symmetric, so the rotation is irrelevant to the image quality. The thread pitch of the screw determines how finely the position can be adjusted.

SLR camera bodies and lenses are relatively large. I lugged quite a few of them around on trips. For my wife. Moving the entire assembly back and forth, called the **unit focusing**, is challenging. To lighten the load, lens designers created systems that could change the focus by moving only a small, relatively light, subgroup of lenses within the assembly. If the lenses were in the middle of the assembly, this was called **internal focusing**, and if the lenses were at the read this was called **rear focusing**. This had several advantages.

-   **Faster and Quieter Autofocus:** Moving a smaller, lighter group of elements requires a much smaller, less powerful motor. This allows for significantly faster, quieter, and more energy-efficient autofocus performance, which is critical for tracking moving subjects.
-   **Constant Lens Length:** Because all the movement happens inside the sealed barrel, the overall physical length of the lens does not change during focusing. This improves the balance and handling of the camera and lens combination.
-   **Non-Rotating Front Element:** The front of the lens does not move or rotate. This is a huge advantage when using filters like **polarizers** or **graduated neutral density filters**, which must be kept in a specific orientation to work correctly.
-   **Improved Optical Performance:** By freeing designers from the constraint of moving the entire optical block, internal focusing allows for more complex and better-corrected optical formulas. It can lead to sharper images and better close-focusing capabilities.

Moving only a small, internal group of lens elements is a sophisticated design that is fundamental to the performance of modern autofocus lenses.

::: {#sensor-slr .callout-note collapse="true" title="Single-lens reflex"}
SLR cameras were the coolest ones when I was middle-aged. The SLR design replaced the **rangefinder** approach. A rangefinder camera has two separate optical paths:

-   The "Taking" or "Imaging" Lens: This is the main lens that focuses the image onto the film or sensor. You do not look through this lens.
-   The Viewfinder Window: This is a separate, small window on the camera body that you look through to compose your shot.

There were often slight differences between the view through the lens and the view through the viewfinder, and this added to the challenge of using these cameras well.

![Overview of an SLR and its subsystems](images/sensors/03-sensor-slr.png){#fig-sensor-slr width="60%"}

The SLR breakthrough was that it used a single optical path through the lens assembly for view finding, autofocus, and capturing the image (hence, "single lens"). The light after the lens assembly would pass through a beam splitter, which directs about 70% of the light to the viewfinder and the rest to the autofocus subsystem. The beam splitter usually takes the form of a semi-transparent "reflex" mirror. When it was time to capture the image, the mirror would be moved out of the light path allowing all the light to form an image on the film (SLR) or digital sensor (DSLR) (@fig-sensor-slr).

The SLR design also lets the user separately control the size of the aperture, and this has an impact on the depth of field (@sec-optics-dof). An advantage of the SLR design is that you can see the depth of field narrow or increase in the viewfinder of the SLR design.
:::

### Lens Positioning in Mobile Devices {#sec-lens-positioning-mobile}

Smartphones and other compact devices required a different approach due to space constraints. These cameras also use multi-element lenses, but they are much smaller. Focus is commonly controlled by a voice coil motor (VCM), which is reliable, inexpensive, and widely used in the industry (@fig-sensor-vcm).

![Cell phone optics. The optics themselves are very compact but still multi-element. The position of the optics is usually controlled by a voice-coil motor.](images/sensors/03-sensor-vcm.png){#fig-sensor-vcm width="80%"}

There is also a growing market for lens positioning using Micro-Electro-Mechanical Systems (MEMS) devices. MEMS actuators offer advantages such as low power consumption, fast response, and miniaturization, making them attractive for mobile imaging systems.

![A video showing lens assembly position controlled by a MEMS device.](videos/sensors/MEMS-autofocus.mov){#fig-sensor-mems width="60%"}

Conceptually, autofocus methods share similarities with techniques for estimating object distance. Both rely on understanding the optical light field (@sec-optical-lightfield). While the implementation of autofocus algorithms has evolved significantly in recent years, the underlying physical principles remain the same. We begin with the classic autofocus mechanism and then describe more recent implementations that are integrated directly into CMOS sensors.

<!-- liquid lens callout here? -->

### Phase Detection Autofocus (PDAF) {#sec-phasedetection-autofocus}

@fig-focusprinciples-classic illustrates the principle of phase detection autofocus (PDAF). In this system, rays from a point object near the lens travel through different parts of the aperture—red rays from the top and green rays from the bottom—before reaching the sensor. When the lens is correctly focused, these rays converge to a single point on the sensor, producing a sharp image.

![Phase detection autofocus.](images/sensors/03-focusprinciples-2.png){#fig-focusprinciples-classic width="60%"}

The classic PDAF design, invented in the 1970s, uses a partially reflective mirror behind the main lens to direct light from the top and bottom of the lens to two separate microlenses. Each microlens focuses the incoming rays onto a linear sensor array. The system is arranged so that, when the image is in focus, the light from both the top and bottom of the lens is centered on their respective arrays. If the image is out of focus, the rays are displaced from the centers of the arrays. The relative displacement—called the phase difference—indicates both the amount and direction of defocus.

The autofocus subsystem uses this phase difference to determine how to move the lens to achieve focus. This method became a key feature in single-lens reflex (SLR) cameras, enabling fast and accurate autofocus.

However, this approach has drawbacks. The mirror, microlenses, and sensor arrays require a relatively large and complex mechanical assembly, making it impractical for compact devices like smartphones. Additionally, the system can only perform focusing or image capture at one time, not both simultaneously. Later technologies addressed these limitations.

::: {#sensor-phasedetect-autofocus .callout-note collapse="false" title="Phase detection autofocus: Commercialization"}
Phase detection autofocus (PDAF) for SLR cameras was pioneered by Honeywell, which patented the key concepts in the 1970s (U.S. Patents 3,875,401 and 4,185,191). Their design used a beam-splitting rangefinder rather than the later flip-mirror approach.

The first commercial SLR to feature PDAF was the Minolta Maxxum 7000 (α-7000 in Japan), released in 1985. It integrated in-body PDAF and motorized lens control, using phase differences measured by two linear sensor arrays to drive lens adjustments.

Leica had previously patented a different autofocus method based on contrast detection, which they licensed to Minolta. However, Minolta found this approach too slow and unreliable, so they developed a phase detection system instead—without securing a license from Honeywell. Honeywell sued and ultimately won a [substantial award](https://www.nytimes.com/1992/02/12/business/honeywell-minolta-dispute-teaches-conflicting-lessons.html).
:::

### Dual Pixel Autofocus (DPAF) {#sec-dualpixel-autofocus}

Traditional phase detection autofocus (PDAF) systems are difficult to implement in the compact form factor of smartphones. As a result, manufacturers initially adopted methods called **contrast detection autofocus (CDAF)**. These methods adjust the lens position to maximize image contrast, under the assumption that a well-focused image will have the sharpest edges. The CDAF methods lack a direct, physical measurement to guide lens movement, making it relatively slow and unreliable, especially in low-light or noisy conditions. Other approaches, such as time-of-flight (ToF) sensors (@sec-sensors-tof), have also been explored and are still used in some devices.

A major breakthrough came in 2014 with the introduction of **dual pixel autofocus (DPAF)**, which applies the principles of phase detection directly within the CMOS sensor. In DPAF, each pixel (or a subset of pixels) contains two photodiodes beneath a single microlens (@fig-sensor-dpaf). During normal image capture, the signals from both photodiodes are combined. For autofocus, however, the signals are read separately and compared.

![Dual pixel autofocus sensor design.](images/sensors/03-sensor-dpaf.png){#fig-sensor-dpaf width="80%"}

::: {#sensor-dpaf .callout-note collapse="true" title="Dual pixel autofocus: Canon"}
Canon introduced DPAF with the EOS 70D in 2013, building on earlier hybrid PDAF/live-view systems like the EOS 650D/T4i-D. Today, many vendors—including Sony and Omnivision—ship sensors with two photodiodes under each microlens, enabling fast, accurate on-sensor phase detection autofocus, especially for video.
:::

The underlying principle is based on the optical light field. For pixels near the center of the sensor, light rays from the left and right sides of the lens arrive at slightly different angles. The microlens directs these rays onto separate photodiodes within the pixel—one on the left, one on the right. When the image is in focus, both photodiodes receive similar signals. If the image is out of focus, the rays from the left and right sides of the lens are displaced, causing the signals from the two photodiodes to differ and shift relative to each other. The amount and direction of this shift indicate the degree and direction of defocus, just as in traditional PDAF.

![Shift from defocus. Needs work. Also, could do it with the Chess Set image and true light field simulation. t_cameraLightField.](images/sensors/03-sensors-lf-shift.png){#fig-sensor-lf-shift width="80%"}

By using two photodiodes under each microlens, DPAF sensors capture some information about the optical light field—the left and right subpixels sample rays from different parts of the lens. This concept could be extended further: with more subpixels, it may be possible to recover even richer light field information, opening up new possibilities for computational imaging and post-capture refocusing (@sec-sensors-lightfield).

## Exposure Control {#sec-exposure-control}
To capture a high-quality image, the exposure must be set so that dark regions generate enough electrons to rise above the sensor’s noise floor, while bright regions do not exceed the pixel’s full well capacity (@sec-wellcapacity-dynamicrange). If the exposure time is too short, pixels in dark areas collect very few electrons—sometimes fewer than the noise level. Even with a low-noise sensor, the inherent Poisson noise means that a small number of electrons results in a low signal-to-noise ratio. For example, if a pixel collects an average of 4 electrons, the standard deviation is 2, so the SNR is only 2.

Conversely, if the exposure time is too long, pixels in bright areas may generate more electrons than the pixel can store, causing the floating diffusion node to **saturate**. When this happens, a wide range of scene intensities are all recorded at the same maximum value.

Proper exposure control adjusts acquisition parameters so that the scene’s dynamic range fits within the sensor’s dynamic range, avoiding both excessive noise and saturation. In a conventional system, two main parameters control this fit: the lens aperture size and the exposure duration. Increasing the aperture allows more light to reach the sensor, raising the signal level. Extending the exposure duration gives the sensor more time to collect photons and convert them to electrons. The classic specification for exposure combines these two parameters, as described below.

### Exposure value {#sec-exposure-value}
The [**exposure value (EV)**](https://en.wikipedia.org/wiki/Exposure_value) is a single number that summarizes the combined effect of aperture size and exposure time on image brightness. For a lens with f-number $F$ and exposure time $T$ (in seconds), the exposure value is defined as

$$
\text{EV} = \log_{2}\left(\frac{F^2}{T}\right) = 2 \log_{2}(F) - \log_{2}(T)
$$ {#eq-exposure-value}

A lower EV means either a longer exposure time or a wider aperture (smaller $F$), both of which allow more light to reach the sensor. As scene brightness increases, a higher EV is needed to avoid overexposure. For example, if $F = 1$ and $T = 1$ second, then $EV = 0$. If the image is too bright and pixels are saturated, you can either halve the exposure time to $T = 1/2$ second (raising $EV$ to 1), or keep $T = 1$ second and increase the f-number to $F = \sqrt{2}$ (also $EV = 1$). Both changes reduce the light reaching the sensor by half[^exposure-fnumber].

[^exposure-fnumber]: The f-number is the ratio of the focal length to the aperture diameter (@sec-fnumber). For a fixed focal length, increasing the f-number means reducing the aperture diameter, so less light reaches the sensor.

In film cameras, where each exposure was costly, photographers often used a separate light meter to determine the correct aperture and exposure time. Modern electronic cameras estimate scene brightness automatically, usually by taking continuous measurements just before the main exposure. On DSLRs, this process is typically triggered by pressing the shutter button halfway. In smartphones, exposure estimation usually begins as soon as the camera app is opened.

### Exposure Bracketing {#sec-exposure-bracketing}
A common engineering strategy for finding optimal settings is to perform a parameter sweep—systematically varying a parameter to observe its effect. In imaging, this is known as **exposure bracketing**.

Exposure bracketing means capturing several images of the same scene at different exposure durations. The technique dates back to at least 1851, when [Gustave Le Gray](https://en.wikipedia.org/wiki/Gustave_Le_Gray) photographed seascapes by taking separate exposures for the sky and the sea, since film could not capture both correctly in a single shot.

Modern digital cameras often provide an exposure bracketing feature. Typically, the user selects the best image from the set, but it is also possible to combine the bracketed images to create a composite with improved dynamic range.

### Burst Photography {#sec-exposure-burst}
To prevent pixel saturation, a team at Google proposed capturing a sequence of images using short exposure durations—a technique known as **burst photography** @hasinoff2016-burst-photography. Their method involves aligning the individual images and summing them to reduce shot noise (@sec-shot-noise).

This approach is particularly effective when sensor read noise is low and shot noise is the dominant noise source. In this scenario, each pixel measurement can be modeled as a Poisson random variable with mean $\lambda$. Summing $N$ such measurements yields another Poisson variable with mean $N\lambda$, improving the signal-to-noise ratio.

The main advantage of burst photography is that it allows the use of short exposures to avoid saturation in bright regions, while still achieving high image quality by combining multiple frames. However, this method requires accurate alignment of the images. If there is motion in the scene or camera shake—especially if objects move in front of one another (occlusion)—alignment becomes challenging or may fail. Burst photography works best with static scenes and a stable camera, but can still provide benefits in more dynamic situations, even if not every frame can be perfectly aligned.

### Space-varying exposure duration (#sec-exposure-dps)



@#sec-digital-pixel

