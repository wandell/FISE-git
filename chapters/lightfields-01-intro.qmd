# Light fields {#sec-seeing}

## About seeing
Electromagnetic radiation fills the world around us, providing a rich and reliable stream of information. Mobile organisms — whether a hawk, a rabbit, or a human — have implemented ways to record and interpret this information. They use the radiation to make decisions and generally interact with their surroundings. 

We call the sensing and interpretation of the radiation **visual perception** or more simply, **seeing**. Visual perception is fundamental for guiding movement, enabling animals to locate resources, avoid threats, and navigate their environments. The use of radiation for mobility and decision-making by mobile organisms is in contrast to immobile life forms, such as trees and plants. For these organisms the ambient radiation serves primarily as a source of energy, rather than information.

In robotics and most computer applications, visual sensing serves a similar purpose: to enable actions and movement within dynamic and complex environments. Robots with imaging systems identify and manipulate parts. Cars with image systems navigate through their surroundings. Medical imaging provides diagnostic information to guide interventions. For both artificial and biological systems image systems provide information that enables goal-oriented actions.

::: {.callout-note collapse="false" title="Who needs a brain?"}

The deep connection between mobility, visual sensing, and the brain is illustrated by the remarkable life path of the [sea squirt](https://movementum.co.uk/journal/sea-squirts). It begins life as tadpole-like larvae, equipped with a brain and a tail for swimming. Once the tadpole finds a suitable spot to settle, it attaches to a hard surface like a rock or coral and becomes immobile.

As soon as the sea squirt fixes itself to the rock, with no more navigation or swimming, its brain is no longer needed. In a truly bizarre twist, the sea squirt digest its own brain. This adaptation reallocates energy resources from the now unnecessary brain to more vital functions.

Here are some [nice images of sea squirts!](https://goodheartextremescience.wordpress.com/2010/01/27/meet-the-creature-that-eats-its-own-brain/)\
<!-- Irv Weissmann has lots of neurodegeneration papers on this -->
:::

Consumer photography - a large and important image system application - is an important exception. Consumer photographs are created to store memories and evoke emotions, not to guide navigation or immediate decisions. As I discuss in later chapters, the technologies for evaluating consumer photography differ from standard engineering metrics because of their heavy reliance on the properties of human perception.

In summary, some image systems measure electromagnetic radiation because the signal is useful for movement or interacting with objects. Other image systems measure radiation to record a moment, enabling us to reproduce the signals at a later time. And finally, some image systems measure electromagnetic radiation at wavelengths we do not see, enabling us to explore domains that are beyond our senses [^lightfields-intro-1]. This section of the book is devoted to understanding the signal. 

[^lightfields-intro-1]: There are instruments that use different wavebands of the electromagnetic spectrum. [A brief introduction is here.](resources/lightfields-wavebands.html){target="_blank"}. More examples of instruments will be presented in later chapters.

## Light fields {#sec-lightfields}
One of the great achievements of science is the realization that seemingly different phenomena are deeply connected. By the early 1800s, scientists had discovered links between electricity and magnetism: @Øersted1820-electricity showed that electric currents create magnetic fields, and @Faraday1832-electricity demonstrated that changing magnetic fields induce electric currents.

James Clerk Maxwell brought these findings together in a mathematical formulation now known as **Maxwell’s equations**. He showed that oscillating electric and magnetic fields can be modeled as waves that propagate through space. He found that the predicted speed of these waves matched the measured speed of light! This led Maxwell to conclude that light, too, is an electromagnetic wave. His work unified electricity, magnetism, and optics into a single theoretical framework: electromagnetic radiation across a range of wavelengths.

Different electromagnetic wavelengths are not equally informative for vision. The human eye detects electromagnetic radiation in, roughly, the wavelength range from about 380 nm to 770 nm. *Light* is defined as *electromagnetic radiation that can be detected by the human eye*. Radiation in the visible wavelength range is strongly absorbed and reflected by objects of interest, making it a very useful band for measuring the objects in the environment. High energy radiation (e.g., X-ray 0.01-10 nm; Gamma-ray < 0.01 nm) passes through most objects undisturbed. Low frequency radiation (> 1500 nm) provides lower spatial resolution and is strongly influenced by thermal factors.

## The environmental light field {#sec-lightfields-types}
Electromagnetic radiation fills the environment, traveling in many directions and often interacting with materials. We call the environmental radiation field in the visible wavelength band the *environmental light field*. 

Leonardo Da Vinci's notebook [@davinciNotebooksLeonardoVinci1970] describes why he concluded that light fills the environment. He placed a small hole in a wall of a windowless room that was adjacent to a brightly illuminated piazza (@fig-ayscough). This produced an (inverted) image of the piazza on the wall within the room. Leonardo observed that one can place the pinhole anywhere in the wall and an image of the same objects is produced. He concluded that the light field is "all everywhere and all in each part"[^lightfields-intro-2].

[^lightfields-intro-2]: **PROVE HOW ALL OBJECTS, PLACED IN ONE POSITION, ARE ALL EVERYWHERE AND ALL IN EACH PART** <br> "I say that if the front of a building—or any open piazza or field—which is illuminated by the sun has a dwelling opposite to it, and if, in the front which does not face the sun, you make a small round hole, all the illuminated objects will project their images through that hole and be visible inside the dwelling on the opposite wall which may be made white; and there, in fact, they will be upside down, and if you make similar openings in several places in the same wall you will have the same result from each. Hence the images of the illuminated objects are all everywhere on this wall and all in each minutest part of it. The reason, as we clearly know, is that this hole must admit some light to the said dwelling, and the light admitted by it is derived from one or many luminous bodies. If these bodies are of various colours and shapes the rays forming the images are of various colours and shapes, and so will the representations be on the wall. (Leonardo's 1509 Notebook, curated by John Paul Richter)"

![Ayscough's book @ayscoughShortAccountEye1755 (“A short account of the eye and nature of vision. Chiefly ... to illustrate the use and advantage of spectacles”) opens with this illustration of Da Vinci’s pinhole camera. I superimposed the colored lines to indicate the courtyard light rays, emitted in all directions. The solid lines are selected by the pinhole and form the image in the dark room. The full collection of rays, solid and dotted, represents the *environment light field*. [Source: Wikipedia Camera Obscura](https://en.wikipedia.org/wiki/Camera_obscura#/media/File:1755_james_ayscough.jpg).](images/lightfields/Ayscough-1755-small-wikipedia.png){#fig-ayscough width="95%" fig-alt="James Ayscough's drawing of Da Vinci's concept." fig-align="center" width="80%"}

For the moment, it is convenient to consider the light field as comprising a large set of rays. We can describe the light field in the environment, $L_E$, by the intensity of each ray, expressing the intensity as an explicit function of its various parameters. At each point in the volume of space, $(x,y,z)$, there are rays traveling in many directions. We specify these directions by two direction angles $(\alpha,\beta)$, the azimuth and elevation. Each ray has wavelength $\lambda$ and polarization $\rho$. The ray intensities are described by the function:

$$
L_E(x,y,z,\alpha,\beta,\lambda,\rho)
$$

People in many fields are familiar with the concept of the light field, but they use different terminology. Physicists often describe the environmental light field as the spectral radiance of the environment (see this [lovely description from Feynman](https://youtu.be/FjHJ7FmV0M4?si=wi2twZVYjF9KJGRY)). The general *light field* terminology was introduced into optical engineering by @gershunLightField1939 as part of his work in understanding how to design lighting environments, such as the lighting in school rooms and public places[^lightfields-intro-3]. Ted Adelson and Jim Bergen [-@adelson-plenoptic-1991] used the term *plenoptic function* to describe the same idea[^adelson-plenoptic]. The great value of the light field concept in computer graphics was explained by Marc Levoy and Pat Hanrahan [-@Levoy-1996-light-field]. I will use the light field terminology because I find it more familiar and engaging than spectral radiance, electromagnetic radiation, or plenoptic.

[^adelson-plenoptic]: @adelson-plenoptic-1991 introduced the plenoptic function for a pinhole camera and a sensor, making it more closely related to the incident light field. (@sec-incident-lightfield). They explained it using a pinhole camera model, so that the two parameters describing the aperture position were not needed. With these restrictions, the plenoptic function is equivalent to the spectral irradiance at the image sensor [@Wandell2021-Principles-Ashby]. The authors clearly had the idea of the environmental and incident light fields in mind.

[^lightfields-intro-3]: I enjoyed the first sentence in the 1939 translation of Gershun's  [-@gershunLightField1939] paper.  The translators, Moon and Timoshenko, express frustration with the pace of advances in photometry. "Theoretical photometry constitutes a case of 'arrested development', and has remained basically unchanged since 1760 while the rest of physics has swept triumphantly ahead." Gershun himself expresses the same sentiment: "The problems of theoretical photometry were pushed aside from the main path of the development of physics."

## Incident light field {#sec-incident-lightfield}
An imaging system, say the eye or a pinhole, records only the portion of the environmental light field that arrives at its entrance pupil (@fig-incident-lightfield). It is worth distinguishing that part of the environmental light field with a name: *incident light field*.

![The incident light field is the subset of environmental light field rays that arrive at the image systems’ entrance pupil.](images/lightfields/01-incident-lightfield.png){#fig-incident-lightfield width="80%" fig-align="center"}

The rays of the incident light field can be described with one fewer parameter than the environmental light field because the entrance pupil is just a two-dimensional surface. Thus, the three dimensional position is replaced by the two-dimensional position in the entrance pupil, $(u,v)$. We need, however, additional parameters to define the image system: The aperture has a position in space $\mathbf{p}$ and and a viewing direction $\mathbf{n}$. We include these as a side-condition in the parameters that describe the ray intensities of the incident light field

$$
L_I(u,v,\alpha,\beta,\lambda,\rho; ~\mathbf{p},\mathbf{n}) .
$$

To acquire full information about the environmental light field, we must follow Leonardo and measure the incident light field at multiple locations and directions. Many animals have two eyes and thus acquire two simultaneous measurements of the light field. This has the benefit of acquiring more information about the light field, as well as providing one remaining eye in the case of disease or damage. Some animals point the two eyes in very different directions and thus sample the environmental light field very broadly (e.g., rabbits, and animals that are prey). Other animals overlap the visual field of the two eyes to acquire information useful for estimating distance (stereo vision, animals that are predators). Even more information about the environmental light field is measured, over time, as the head moves heads and the eyes rotate (motion parallax).

```{=html}
<!-- Feynman comment about this?  I searched but couldn't find it. I even searched for anyone commenting on it.  Oh well.
I find it remarkable, as many others must, that measuring just the local intensities in a small part of a scene we can estimate the properties of many objects in the whole scene volume.  We can make inferences about the object shapes, their distance, their surface reflectance and texture. -->
```

Engineers have built camera arrays that capture more than two incident light fields. Measuring from multiple positions and directions, enables us to create displays that reproduce the experience of being in the environment as a person changes position and direction of gaze. (If the environment is unchanging, a single camera that moves around can be used.) These systems rely on the principle Leonardo identified: the light field is everywhere, and we can measure it by recording images at many different positions using cameras at many positions and pointing in many different directions.

A large, rectangular camera array to measure the light field was built by my colleagues at the Stanford Graphics Lab [@Levoy-1996-light-field, @fig-camera-array]. The cameras measure multiple views; all the cameras point in the same direction. Arrays with different camera configuration, at the bottom of the figure, capture multiple directions from a particular position. These camera arrays acquire enough information so that we can render images that match what a person would see if they look around. Camera arrays that acquire enough information for both 360 deg and slightly different positions, sometimes called stacked omnistereo, have also been built [@thatte-2017-6dof]. From these data, along with an interpolation strategy, we can calculate images from a range of viewpoints.

![The Stanford Multi-Camera Array (top) comprised 128 cameras that measured the environmental light field from many positions. With this configuration one can recreate the experience of moving side to side as well as back and forth. Other array configurations (bottom) have cameras pointing outward from a point. These capture the light field from many directions and can recreate the experience of looking around the room from the point.](images/lightfields/01-camera-array.png){#fig-camera-array .margin-caption width="70%"}

## Optical light field {#sec-optical-lightfield}
The image system optics, whether a pinhole, thin lens, or multi-element lens, transforms the incident light field into a new light field within the camera. This *optical light field* exists between the exit pupil of the optics and the sensor. The optical light field originates in the environment, is reduced to the incident light field, and then transformed by the optics. The camera sensor, or the retina, measures the optical light field[^in-camera-lightfield].

[^in-camera-lightfield]:  @ng2005-lightfield used the phrase 'in-camera light field.'  I like that expression, too. 

![The rays emerging from the exit pupil of the optics are captured by the sensor (or film). These rays form a light field within the camera, the optical light field $L_O$. The only rays of this light field that matter for the image system are those that are captured by sensor.](images/lightfields/01-optical-lightfield.png){#fig-optical-lightfield .margin-caption width="80%"}

The environmental and incident light fields use parameters of position and angle for each ray. For the optical light field, however, we only measure rays that arrive at the sensor (or film). It is helpful, therefore, to parameterize the optical light field using two spatial parameters: where the ray exits the lens ($(u,v)$) and the position it arrives at the sensor, $(w,z)$. These four spatial parameters substitute for the positions and angles we use in the other light fields. It is possible, of course, to convert the sensor position parameters into angular parameters at the exit pupil.

$$L_O(u,v,w,z,\lambda,\rho)$$

The data recorded by the sensor is used to create a reproduction of the image, or as information for interpreting the scene. The sensing and perception systems that derive information from the light field are designed to account for which information in the environmental light field makes it all the way through to the optical light field.

The most common sensors in cameras record only partial information about the optical light field. The rays arriving at a point on the sensor surface are summed together, no matter where the ray exited the optics. This type of sensor does not not preserve information about where the rays arise in the exit pupil[^sum-optical-angle]. 

This implementation at the sensor is a decision, not a requirement. Various authors pointed out that measuring more information about the optical light field would enable us to know more about the environmental light field. For example, @adelson-wang-1992 built a camera that measures the ray intensity at multiple angles at each location on the sensor, and they used this information to estimate depth. Ren Ng's company, Lytro, developed a commercial sensor to measure the optical light field [@ng2005-lightfield]; they also developed algorithms to enable users to refocus images or change the depth of field after capture. In  @sec-focus-control I describe how modern sensors use light field information for camera autofocus, and in @sec-sensor-lightfield I describe how light field sensors capture a more complete version of the optical light field. 

[^sum-optical-angle]:  Or, equivalently, they sum across the angles of the rays arriving at the sensor.

## Interpreting the light field

Encoding the light field data is a first step in seeing. A next step is interpreting the measurements. The invention of algorithms that interpret the light field is a central to building computers that see. Discovering how the brain interprets the encoded signal is an important part of vision science.

Building an internal model of the scene from the encoded light field has long been believed to be what people do. Nearly two centuries ago by the great vision scientist, Helmholtz, wrote:

> The general rule determining the ideas of vision that are formed whenever an impression is made on the eye, is that *such objects are always imagined as being present in the field of vision as would have to be there in order to produce the same impression on the nervous mechanism* [Italics in the original; Southall, Physiological Optics, Vol. III, p. 2, 1865]

Understanding the light field and the instruments that measure it will help us achieve that goal.
