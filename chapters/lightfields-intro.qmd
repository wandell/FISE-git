# Light field definitions {#sec-lightfields}

## About seeing
Why do organisms sense and interpret their surroundings? From a biological perspective, sensory perception is fundamental for guiding movement, enabling animals to locate resources, avoid threats, and navigate their environments. Ambient electromagnetic radiation provides a rich and reliable stream of information, which mobile organisms — whether a hawk, a rabbit, or a human — analyze to make decisions and interact with their surroundings. For immobile life forms, such as trees and plants, this radiation serves primarily as a source of energy, rather than information.

In robotics, visual sensing serves a similar fundamental purpose: to enable movement and interaction within dynamic and complex environments. By equipping robots and autonomous vehicles with imaging systems, we allow their computer programs to interpret visual data so they can identify, reach, and navigate through their surroundings, closely paralleling the biological role of sensory systems in guiding adaptive, goal-oriented actions.

Not all electromagnetic radiation is equally informative. High energy radiation (e.g., X-ray and Gamma ray bands) passes through most objects undisturbed. Low frequency radiation provides limited spatial resolution. The human eye detects electromagnetic radiation in, roughly, the wavelength range from about 380 nm to 770 nm. We call this waveband "light", which is defined as "electromagnetic radiation that can be detected by the human eye." Electromagnetic radiation in this wavelength range is strongly absorbed and reflected by objects of interest, making it a very useful band for informing us about the environment.

Some image systems are designed to measure light because we want them to reproduce the signals that we might see. Others measure light because the signal is useful for their own function. And finally, some image systems measure electromagnetic radiation at wavelengths we do not see, exploring domains within the environment that are beyond our senses[^lightfields-1-1].

[^lightfields-1-1]: See [more about wavebands here.](../supplemental/lightfields-wavebands.html){target="_blank"}.

::: {.callout-note collapse="true" title="Who needs a brain?"}
### Movement requires a brain {.unnumbered}

The deep connection between mobility, sensing and the brain is illustrated by the remarkable life path of the [sea squirt](https://movementum.co.uk/journal/sea-squirts). It begins life as tadpole-like larvae, equipped with a brain and a tail for swimming. Once the tadpole finds a suitable spot to settle, it attaches to a hard surface like a rock or coral and becomes immobile.

As soon as the sea squirt fixes itself to the rock, with no more navigation or swimming, its brain is no longer needed. In a truly bizarre twist, the sea squirt digest its own brain. This adaptation reallocates energy resources from the now unnecessary brain to more vital functions.

Here are some [nice images of sea squirts!](https://goodheartextremescience.wordpress.com/2010/01/27/meet-the-creature-that-eats-its-own-brain/)\
<!-- Irv Weissmann has lots of neurodegeneration papers on this -->
:::

## The environmental light field {#sec-lightfields-types}

Electromagnetic radiation fills the environment, traveling in many directions and often interacting with materials. We call the ambient radiation in the visible band the *light field*. Leonardo Da Vinci's notebook [@davinciNotebooksLeonardoVinci1970] describes how we know that this field is everywhere. In a section of his notebook he describes placing a small hole in a wall of a windowless room. The room is adjacent to a brightly illuminated piazza (@fig-ayscough). The camera produces an image of the piazza (inverted) on the wall within the room. Leonardo further noted that an image is formed wherever the pinhole is placed. From this observation he concluded that the light field is "all everywhere and all in each part"[^lightfields-1-2].

[^lightfields-1-2]: **PROVE HOW ALL OBJECTS, PLACED IN ONE POSITION, ARE ALL EVERYWHERE AND ALL IN EACH PART** <br> "I say that if the front of a building—or any open piazza or field—which is illuminated by the sun has a dwelling opposite to it, and if, in the front which does not face the sun, you make a small round hole, all the illuminated objects will project their images through that hole and be visible inside the dwelling on the opposite wall which may be made white; and there, in fact, they will be upside down, and if you make similar openings in several places in the same wall you will have the same result from each. Hence the images of the illuminated objects are all everywhere on this wall and all in each minutest part of it. The reason, as we clearly know, is that this hole must admit some light to the said dwelling, and the light admitted by it is derived from one or many luminous bodies. If these bodies are of various colours and shapes the rays forming the images are of various colours and shapes, and so will the representations be on the wall. (Leonardo's 1509 Notebook, curated by John Paul Richter)"

![Ayscough's book @ayscoughShortAccountEye1755 (“A short account of the eye and nature of vision. Chiefly ... to illustrate the use and advantage of spectacles”) opens with this illustration of Da Vinci’s pinhole camera. I superimposed the colored lines to indicate the courtyard light rays, emitted in all directions. The solid lines are selected by the pinhole and form the image in the dark room. The full collection of rays, solid and dotted, represents the *environment light field*. [Source: Wikipedia Camera Obscura](https://en.wikipedia.org/wiki/Camera_obscura#/media/File:1755_james_ayscough.jpg).](../images/lightfields/Ayscough-1755-small-wikipedia.png){#fig-ayscough width="95%" fig-alt="Alt Text" fig-align="center" width="80%"}

For the moment, it is simple and convenient to consider the radiation as comprising a large set of light rays. It is useful to write the environmental light field as an explicit function so that we can keep track of its various parameters. At each point in the volume of space, $(x,y,z)$, there are rays traveling in many directions. We specify these directions by two angles $(\alpha,\beta)$, the azimuth and elevation. Each ray has wavelength $\lambda$ and polarization $\rho$. Thus, the intensity of the environmental light field function, $L_E$, depends on seven parameters.

<!--# Feynman saying the same thing as Da Vinci about light fields in modern terms. https://youtu.be/FjHJ7FmV0M4?si=wi2twZVYjF9KJGRY -->

$$L_E(x,y,z,\alpha,\beta,\lambda,\rho)$$

People in many fields are familiar with the concept of the light field, but they use different terminology. Physicists often describe the environmental light field as the spectral radiance of the enviroment. The term *light field* was introduced into optical engineering by @gershunLightField1939 as part of his work in understanding how to design lighting environments, such as the lighting in school rooms and public places[^lightfields-1-3]. The use of the term and the importance of the light field in practice was greatly amplified in the field of computer graphics by Marc Levoy and Pat Hanrahan @Levoy-1996-light-field. I will continue to use the light field terminology to follow the signal from the environment, to the entrance pupil of the camera, and then to the sensor. I find the term "light" to be more familiar and engaging than spectral radiance or electromagnetic radiation. Perhaps this is because I am a psychologist at heart.

[^lightfields-1-3]: I enjoyed the first sentence in the 1939 translation of Gershun's paper, from the translators, Moon and Timoshenko. They express frustration with the pace of advances in photometry. "Theoretical photometry constitutes a case of 'arrested development', and has remained basically unchanged since 1760 while the rest of physics has swept triumphantly ahead." Gershun himself writes "The problems of theoretical photometry were pushed aside from the main path of the development of physics."

## Incident light field

An imaging system, say the eye or a pinhole, records only the portion of the environmental light field that arrives at its entrance pupil (@fig-incident-lightfield). It is worth distinguishing that part of the light field with a name: *incident light field*. For this definition, it is convenient to conceive of the light as comprising rays traveling in space.

::: figure-wrap-right
![The incident light field is the subset of environmental light field rays that arrive at the image systems’ entrance pupil.](../images/lightfields/01-incident-lightfield.png){#fig-incident-lightfield width="80%" fig-align="center"}
:::

The rays of the incident light field can be described with one fewer parameter than the environmental light field because the entrance pupil is just a two-dimensional surface. Thus, the three dimensional position is replaced by the two-dimensional position in the entrance pupil, $(u,v)$. There are, however, additional parameters that define the sensing system, not the light field. The aperture has a position in space $\mathbf{p}$ and and a viewing direction $\mathbf{n}$. We

$$L_I(u,v,\alpha,\beta,\lambda,\rho; ~\mathbf{p},\mathbf{n})$$

To acquire more information about the environmental light field, we measure at multiple locations and directions. The human visual system makes two simultaneous measurements of the light field, one with each eye, and this is stereo vision. We acquire more information about the light field over time by moving our heads, walking around, and rotating our eyes.

Special purpose image systems have been built to extend how much of the environmental light field we measure. This information enables us to reproduce a more complete experience of the environment in virtual reality environments, as a person changes position and direction of gaze. These image system usually use camera arrays. They rely on the principle Leonardo identified: the light field is everywhere, and we can measure it by recording images at many different positions using cameras pointing in many different directions.

A large, rectangular camera array to measure the light field was built by Levoy and colleagues at the Stanford Graphics Lab [@Levoy-1996-light-field, @fig-camera-array]. The cameras in the rectangular array acquire rays from a large number of positions in the scene. As illustrated in the figure, the cameras measure multiple views, with all the cameras pointing in the same direction. Arrays with different camera configuration, at the bottom of the figure, capture multiple directions from a particular position. These camera arrays acquire enough information so that we can render images that match what a person would see if they look around. Camera arrays that acquire enough information for both 360 deg and slightly different positions, sometimes called stacked omnistereo, have also been built [@thatte-2017-6dof]. From these data, along with an interpolation strategy, we can calculate images from a range of viewpoints.

![The Stanford Multi-Camera Array (top) comprised 128 cameras that measured the environmental light field from many positions. With this configuration one can recreate the experience of moving side to side as well as back and forth. Other array configurations (bottom) have cameras pointing outward from a point. These capture the light field from many directions and can recreate the experience of looking around the room from the point.](../images/lightfields/01-camera-array.png){#fig-camera-array .margin-caption width="70%"}

::: {.callout-note collapse="true" title="Interpreting the light field"}
### Inverse rendering {.unnumbered}

Measuring the light field data is a first step in seeing. A next step is interpreting the measurements to find the things and stuff, to identify material properties, and to understand the lighting. The discovery of algorithms that achieve these goals are part of the broad project of building computers that see.

Some general guidance in how to make computers see is have their software construct an internal model of the things, stuff, materials, and lights in the scene. The software can calculate the predicted light field for the model, comparing it with the measured light field. Deviations provide information to update the model in a virtuous cycle until the two agree. In computer graphic methods this approach is called *inverse rendering*, and it is at the frontier of what is currently possible [@nimier-david2019].

Representing things, stuff, materials and lights is valuable in many graphics applications because it enables users to edit the scene in terms of objects they understand. The approach is essential in robotics and driving applications where decision making about actors in the environment is essential to guiding action.

Building an object and lighting model from the encoded light field has long been believed to be what people do. Nearly two centuries ago by the great vision scientist, Helmholtz, wrote:

> The general rule determining the ideas of vision that are formed whenever an impression is made on the eye, is that *such objects are always imagined as being present in the field of vision as would have to be there in order to produce the same impression on the nervous mechanism* \[Italics in the original; Southall, Physiological Optics, Vol. III, p. 2, 1865\]

This book is written with the belief that achieving this goal will benefit significantly from understanding the light field and the instruments that measure it. I am aware, however, that this belief is not universally held.
:::

## Optical light field

The image system optics, whether a pinhole, thin lens, or multi-element lens, transforms the incident light field into a new light field within the camera. This *optical light field* is between the exit pupil of the optics and the sensor. It originates in the environment, is reduced to the incident light field, and then transformed by the optics. The camera sensor, or the retina, measures the optical light field.

![The rays emerging from the exit pupil of the optics are captured by the sensor (or film). These rays form a light field within the camera, the optical light field $L_O$. The rays of this light field that matter for the image system are those that are captured by sensor.](/images/lightfields/01-optical-lightfield.png){#fig-optical-lightfield .margin-caption width="80%"}

The environment and incident light fields use parameters of position and angle for each ray. For the optical light field, however, we only measure rays that arrive at the sensor (or film). It is helpful, therefore, to describe how the radiation exits the optics using the sensor position, $(w,z)$, rather than angles. It is possible, of course, to convert these sensor position parameters to angular parameters.

$$L_O(u,v,w,z,\lambda,\rho)$$

The data recorded by the sensor is used to create a reproduction of the image, or as the basis for interpreting the image contents. The sensing and perception systems that derive information from the light field are designed to account for which information in the environmental light field makes it all the way through to the optical light field [^lightfields-1-4].

[^lightfields-1-4]: The *plenoptic* function, has been widely used to describe light fields [@adelson-plenoptic-1991]. The plenoptic function is defined for a pinhole camera and a sensor, removing two parameters $(u,v)$. In practice, the plenoptic function is equivalent to another familiar term, the spectral irradiance at the image sensor [@Wandell2021-Principles-Ashby].

The most common sensors in cameras record partial information about the optical light field. The rays arriving at a point on the sensor surface are summed together, no matter where the ray exited the optics. Measuring more information about the optical light field enables us to know and do more. For example, @adelson-wang-1992 demonstrated how to use this information to estimate depth. Ren Ng's innovative company, Lytro, developed a light field sensor and algorithms to enable users to refocus images or change the depth of field after capture. In @sec-sensors we describe how sensors capture a more complete version of the optical light field, and we will describe how many modern sensors use light field information for camera autofocus.

