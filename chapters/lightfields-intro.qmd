# Light fields {#sec-lightfields}

## About seeing

Why do organisms sense and interpret their surroundings? From a biological perspective, sensory perception is fundamental for guiding movement, enabling animals to locate resources, avoid threats, and navigate their environments. Ambient electromagnetic radiation provides a rich and reliable stream of information, which mobile organisms — whether a hawk, a rabbit, or a human — analyze to make decisions and interact with their surroundings. For immobile life forms, such as trees and plants, this radiation serves primarily as a source of energy, rather than information.

In robotics and most computer applications, visual sensing serves a similar purpose: to enable actions and movement within dynamic and complex environments. Robots with imaging systems and identify and manipulate parts. Cars with image systems navigate through their surroundings. Medical imaging provides diagnostic information to guide interventions. For both artificial and biological systems image systems provide information that enables goal-oriented actions.

Viewed from this perspective, consumer photography - a very large and important application - is the odd-one out. Consumer photographs are created to store memories and inspire emotions. This massive field, really the first of the imaging fields, serves these uniquely human functions. As we discuss in later chapters, the technologies for evaluating consumer photography differ from standard engineering metrics because of their heavy reliance on the properties of human perception.

Not all electromagnetic radiation is equally informative. High energy radiation (e.g., X-ray and Gamma ray bands) passes through most objects undisturbed. Low frequency radiation provides limited spatial resolution. The human eye detects electromagnetic radiation in, roughly, the wavelength range from about 380 nm to 770 nm. We call this waveband *light*, which is defined as *electromagnetic radiation that can be detected by the human eye*. Electromagnetic radiation in the visible wavelength range is strongly absorbed and reflected by objects of interest, making it a very useful band for measuring the environment.

In summary, some systems measure electromagnetic radiation because the signal is useful for functions such as movement or interacting with objects. Other image systems measure light because to record a moment, enabling us to reproduce the signals at a later time. And finally, some image systems measure electromagnetic radiation at wavelengths we do not see, enabling us to explore domains that are [beyond our senses](resources/lightfields-wavebands.html)

Delete me. [^lightfields-intro-1].

[^lightfields-intro-1]: [About different wavebands.](resources/lightfields-wavebands.html){target="_blank"}.

::: {.callout-note collapse="true" title="Who needs a brain?"}
### Movement requires a brain {.unnumbered}

The deep connection between mobility, sensing and the brain is illustrated by the remarkable life path of the [sea squirt](https://movementum.co.uk/journal/sea-squirts). It begins life as tadpole-like larvae, equipped with a brain and a tail for swimming. Once the tadpole finds a suitable spot to settle, it attaches to a hard surface like a rock or coral and becomes immobile.

As soon as the sea squirt fixes itself to the rock, with no more navigation or swimming, its brain is no longer needed. In a truly bizarre twist, the sea squirt digest its own brain. This adaptation reallocates energy resources from the now unnecessary brain to more vital functions.

Here are some [nice images of sea squirts!](https://goodheartextremescience.wordpress.com/2010/01/27/meet-the-creature-that-eats-its-own-brain/)\
<!-- Irv Weissmann has lots of neurodegeneration papers on this -->
:::

## The environmental light field {#sec-lightfields-types}

Electromagnetic radiation fills the environment, traveling in many directions and often interacting with materials. We call the radiation field in the visible wavelength band the *light field*; the definition of light is, in fact, electromagnetic radiation that we can see.

Leonardo Da Vinci's notebook [@davinciNotebooksLeonardoVinci1970] describes how he concluded that light fills the environment. He noted that placing a small hole in a wall of a windowless room that is adjacent to a brightly illuminated piazza (@fig-ayscough) can produce an (inverted) image of the piazza on the wall within the room. Leonardo further noted that one can place the pinhole anywhere in the wall. an image of the same objects is produced. He concluded that the light field is "all everywhere and all in each part"[^lightfields-intro-2].

[^lightfields-intro-2]: **PROVE HOW ALL OBJECTS, PLACED IN ONE POSITION, ARE ALL EVERYWHERE AND ALL IN EACH PART** <br> "I say that if the front of a building—or any open piazza or field—which is illuminated by the sun has a dwelling opposite to it, and if, in the front which does not face the sun, you make a small round hole, all the illuminated objects will project their images through that hole and be visible inside the dwelling on the opposite wall which may be made white; and there, in fact, they will be upside down, and if you make similar openings in several places in the same wall you will have the same result from each. Hence the images of the illuminated objects are all everywhere on this wall and all in each minutest part of it. The reason, as we clearly know, is that this hole must admit some light to the said dwelling, and the light admitted by it is derived from one or many luminous bodies. If these bodies are of various colours and shapes the rays forming the images are of various colours and shapes, and so will the representations be on the wall. (Leonardo's 1509 Notebook, curated by John Paul Richter)"

![Ayscough's book @ayscoughShortAccountEye1755 (“A short account of the eye and nature of vision. Chiefly ... to illustrate the use and advantage of spectacles”) opens with this illustration of Da Vinci’s pinhole camera. I superimposed the colored lines to indicate the courtyard light rays, emitted in all directions. The solid lines are selected by the pinhole and form the image in the dark room. The full collection of rays, solid and dotted, represents the *environment light field*. [Source: Wikipedia Camera Obscura](https://en.wikipedia.org/wiki/Camera_obscura#/media/File:1755_james_ayscough.jpg).](../images/lightfields/Ayscough-1755-small-wikipedia.png){#fig-ayscough width="95%" fig-alt="Alt Text" fig-align="center" width="80%"}

For the moment, it is convenient to consider the light field radiation as comprising a large set of rays. We can describe the light field in the environment, $L_E$, by the intensity of each ray, expressing the intensity as an explicit function of its various parameters. At each point in the volume of space, $(x,y,z)$, there are rays traveling in many directions. We specify these directions by two angles $(\alpha,\beta)$, the azimuth and elevation. Each ray has wavelength $\lambda$ and polarization $\rho$. The environmental light field rays are thus described by the function:

<!--# Feynman saying the same thing as Da Vinci about light fields in modern terms. https://youtu.be/FjHJ7FmV0M4?si=wi2twZVYjF9KJGRY -->

$$L_E(x,y,z,\alpha,\beta,\lambda,\rho)$$

People in many fields are familiar with the concept of the light field, but they use different terminology. Physicists often describe the environmental light field as the spectral radiance of the environment (see this [lovely description from Feyman](https://youtu.be/FjHJ7FmV0M4?si=wi2twZVYjF9KJGRY)). The *light field* terminology was introduced into optical engineering by @gershunLightField1939 as part of his work in understanding how to design lighting environments, such as the lighting in school rooms and public places[^lightfields-intro-3]. The use of the term and the importance of the light field in practice was greatly amplified in the field of computer graphics by Marc Levoy and Pat Hanrahan @Levoy-1996-light-field. I will use the light field terminology to follow the signal from the environment, to the entrance pupil of the camera, and then to the sensor. I find the term "light" to be more familiar and engaging than spectral radiance or electromagnetic radiation. Perhaps this is because I am a psychologist at heart.

[^lightfields-intro-3]: I enjoyed the first sentence in the 1939 translation of Gershun's paper, from the translators, Moon and Timoshenko. They express frustration with the pace of advances in photometry. "Theoretical photometry constitutes a case of 'arrested development', and has remained basically unchanged since 1760 while the rest of physics has swept triumphantly ahead." Gershun himself writes "The problems of theoretical photometry were pushed aside from the main path of the development of physics."

## Incident light field

An imaging system, say the eye or a pinhole, records only the portion of the environmental light field that arrives at its entrance pupil (@fig-incident-lightfield). It is worth distinguishing that part of the environmental light field with a name: *incident light field*.

::: figure-wrap-right
![The incident light field is the subset of environmental light field rays that arrive at the image systems’ entrance pupil.](../images/lightfields/01-incident-lightfield.png){#fig-incident-lightfield width="80%" fig-align="center"}
:::

The rays of the incident light field can be described with one fewer parameter than the environmental light field because the entrance pupil is just a two-dimensional surface. Thus, the three dimensional position is replaced by the two-dimensional position in the entrance pupil, $(u,v)$.

We need, however, additional parameters to define the aperture of the image system: The aperture has a position in space $\mathbf{p}$ and and a viewing direction $\mathbf{n}$. We include these as a side-condition in the parameters of the incident light field

$$L_I(u,v,\alpha,\beta,\lambda,\rho; ~\mathbf{p},\mathbf{n})$$

To acquire more information about the environmental light field, we must measure at multiple locations and directions. Many species acquire two simultaneous measurements of the light field, one with each eye. This has the benefit of acquiring more information about the light field, as well as providing one remaining eye in the case of disease or damage. Some animals point the two eyes in very different directions and thus sample the environmental light field very broadly (e.g., rabbits, and many animals that are prey). Other animals overlap the visual field of the two eyes to acquire information useful for estimating distance (stereo vision). Even more information can be estimated, over time, as the head moves heads and the eyes rotate (motion parallax).

```{=html}
<!-- Feynman comment about this?  I searched but couldn't find it. I even searched for anyone commenting on it.  Oh well.
I find it remarkable, as many others must, that measuring just the local intensities in a small part of a scene we can estimate the properties of many objects in the whole scene volume.  We can make inferences about the object shapes, their distance, their surface reflectance and texture. -->
```

Engineers have built special image systems to measure more of the environmental light field using camera arrays and thus capturing more than two incident light fields. Measuring the environmental light field from multiple positions and directions enables us to create virtual reality displays that reproduce the experience of being in the environment as a person changes position and direction of gaze (If the environment is unchanging, a single camera that moves around can be used.) These systems rely on the principle Leonardo identified: the light field is everywhere, and we can measure it by recording images at many different positions using cameras at many positions and pointing in many different directions.

A large, rectangular camera array to measure the light field was built by Levoy and colleagues at the Stanford Graphics Lab [@Levoy-1996-light-field, @fig-camera-array]. The cameras in the rectangular array acquire rays from a large number of positions in the scene. As illustrated in the figure, the cameras measure multiple views, with all the cameras pointing in the same direction. Arrays with different camera configuration, at the bottom of the figure, capture multiple directions from a particular position. These camera arrays acquire enough information so that we can render images that match what a person would see if they look around. Camera arrays that acquire enough information for both 360 deg and slightly different positions, sometimes called stacked omnistereo, have also been built [@thatte-2017-6dof]. From these data, along with an interpolation strategy, we can calculate images from a range of viewpoints.

![The Stanford Multi-Camera Array (top) comprised 128 cameras that measured the environmental light field from many positions. With this configuration one can recreate the experience of moving side to side as well as back and forth. Other array configurations (bottom) have cameras pointing outward from a point. These capture the light field from many directions and can recreate the experience of looking around the room from the point.](../images/lightfields/01-camera-array.png){#fig-camera-array .margin-caption width="70%"}

::: {.callout-note collapse="true" title="Interpreting the light field"}
### Inverse rendering {.unnumbered}

Measuring the light field data is a first step in seeing. A next step is interpreting the measurements to find the things and stuff, to identify material properties, and to understand the lighting. The discovery of algorithms that achieve these goals are part of the broad project of building computers that see.

Some general guidance in how to make computers see is have their software construct an internal model of the things, stuff, materials, and lights in the scene. The software can calculate the predicted light field for the model, comparing it with the measured light field. Deviations provide information to update the model in a virtuous cycle until the two agree. In computer graphic methods this approach is called *inverse rendering*, and it is at the frontier of what is currently possible [@nimier-david2019].

Representing things, stuff, materials and lights is valuable in many graphics applications because it enables users to edit the scene in terms of objects they understand. The approach is essential in robotics and driving applications where decision making about actors in the environment is essential to guiding action.

Building an object and lighting model from the encoded light field has long been believed to be what people do. Nearly two centuries ago by the great vision scientist, Helmholtz, wrote:

> The general rule determining the ideas of vision that are formed whenever an impression is made on the eye, is that *such objects are always imagined as being present in the field of vision as would have to be there in order to produce the same impression on the nervous mechanism* \[Italics in the original; Southall, Physiological Optics, Vol. III, p. 2, 1865\]

This book is written with the belief that achieving this goal will benefit significantly from understanding the light field and the instruments that measure it.
:::

## Optical light field

The image system optics, whether a pinhole, thin lens, or multi-element lens, transforms the incident light field into a new light field within the camera. This *optical light field* is between the exit pupil of the optics and the sensor. It originates in the environment, is reduced to the incident light field, and then transformed by the optics. The camera sensor, or the retina, measures the optical light field.

![The rays emerging from the exit pupil of the optics are captured by the sensor (or film). These rays form a light field within the camera, the optical light field $L_O$. The rays of this light field that matter for the image system are those that are captured by sensor.](/images/lightfields/01-optical-lightfield.png){#fig-optical-lightfield .margin-caption width="80%"}

The environment and incident light fields use parameters of position and angle for each ray. For the optical light field, however, we only measure rays that arrive at the sensor (or film). It is helpful, therefore, to describe how the radiation exits the optics using the sensor position, $(w,z)$, rather than angles. It is possible, of course, to convert these sensor position parameters to angular parameters.

$$L_O(u,v,w,z,\lambda,\rho)$$

The data recorded by the sensor is used to create a reproduction of the image, or as the basis for interpreting the image contents. The sensing and perception systems that derive information from the light field are designed to account for which information in the environmental light field makes it all the way through to the optical light field [^lightfields-intro-4].

[^lightfields-intro-4]: The *plenoptic* function, has been widely used to describe light fields [@adelson-plenoptic-1991]. The plenoptic function is defined for a pinhole camera and a sensor, removing two parameters $(u,v)$. In practice, the plenoptic function is equivalent to another familiar term, the spectral irradiance at the image sensor [@Wandell2021-Principles-Ashby].

The most common sensors in cameras record partial information about the optical light field. The rays arriving at a point on the sensor surface are summed together, no matter where the ray exited the optics. Measuring more information about the optical light field enables us to know and do more. For example, @adelson-wang-1992 demonstrated how to use this information to estimate depth. Ren Ng's innovative company, Lytro, developed a light field sensor and algorithms to enable users to refocus images or change the depth of field after capture. In @sec-sensors we describe how sensors capture a more complete version of the optical light field, and we will describe how many modern sensors use light field information for camera autofocus.