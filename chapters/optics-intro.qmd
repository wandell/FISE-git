# Principles {#sec-optics}

<!-- I downloaded these slides. Some good concepts for this book. Overlaps a lot with my class, but additions. Link to PDF http://graphics.cs.cmu.edu/courses/15-463/2020_fall/lectures/lecture3.pdf -->

A small pinhole renders a perfectly good image. Why do we need to add any optics? The critical reason is that pinhole cameras only measure a small amount of electromagnetic radiation.

Let me tell you what I mean by **small**. Suppose we are in a dim room illuminated with a broad wavelength spectrum (10 $cd/m^2$). Consider a camera with a 1 mm pinhole, a sensor with 1 micron pixels, a focal length of 4mm, and an exposure duration of 50 milliseconds. In that case only about 35 photons arrive at each pixel. If we want to see in color, we must further divide up the wavelength range and the situation worsens. Because of the inescapable photon noise, which I describe later, this signal only permits a reliable intensity discrimination when two pixels differ by more than 20% (i.e., one edge is 20% brighter than the other, see [fise_opticsCountingPhotons](../code/fise_opticsCountingPhotons.html)).

A lens is a way to enlarge the entrance aperture and acquire more photons. The lens can uses a larger aperture, and thus acquires more photons, without paying a penalty in blur. This capability must be the reason why lenses have evolved in so many different species.

::: {.callout-note collapse="true" title="Optics in animals"}
### Animals with pinhole optics {.unnumbered}

The most primitive are called 'pigment cup' eyes. These animals have a small receptor surface is only about 0.1mm in diameter, and it is simply open. The opening is a bit large to be considered a pinhole.

The eyes of some animals, in particular mollusks, have more plausible pinhole architectures. The receptor surface itself is 10 mm in diameter, and the opening is 0.4 - 2.8mm. These eyes evolved about 500 million years ago and have little changed.

A few animals species have a visual system based on pinholes. But the vast majority have evolved visual systems with a lens.

<!--# https://www.perplexity.ai/search/there-are-many-animals-that-ha-QJcp0_O_Ru6a0nVWEFkkmw  Perplexity search about tradeoffs between pinhole optics and lenses, particularly concerning biological systems. -->

<!-- Details https://www.britannica.com/science/photoreception/Single-chambered-eyes Authored by Michael Land.  Detailed on optics and different species.  -->
:::

## Rays and wavefronts
In the next section, we describe lenses and treat light as rays. The reader may find this odd, given the clear demonstration of light as waves [@sec-lightfields-diffraction]. But for the principles of refraction, and many other optics calculations,the ray model is both intuitive and highly accurate. 

It is useful to reason back and forth between the ray and wavefront representations. We can think of a ray as a traveling transverse wave. A collection of nearby rays traveling in the same direction, and of the same wavelength, and phase, form a plane wave.  We can visualize this as drawing a dashed line through the peaks of their (in-phase, common amplitude) waves. This is the wavefront.

<!-- **Notes about wavefront representations** 
(https://www.rp-photonics.com/wavefronts.html).  
-->

![Relationship between the ray (arrows) and wavefront (dashed) representations. (a) A collimated set of rays is a planar wavefront. (b) Rays from a point source form a spherical wavefront. (The image shows a slice through the sphere). (c) Smoothly varying wavefronts with many different shapes are common. Often these shapes are due to imperfections (aberrations) in the optics.  We reason about them as a collection of rays; the direction of each ray is drawn perpendicular to the tangent of the wavefront. (d) Only a narrow section of the rays from a distant point are incident at the entrance aperture of the imaging device. Hence, the incident light field from the point is close to a planar wavefront.](../images/optics/rays-wave.png){#fig-waves-rays fig-cap-location="bottom" fig-align="center" width="75%"}

It is uncommon for tutorials to represent the amplitude of the wavefront across space; but in reality the amplitude may vary. For example, a typical laser emits a set of parallel (collimated) rays with a Gaussian amplitude profile. 

In recent years, image systems engineers have had access to **spatial light modulators (SLMs)**.  These are devices that modulate the local amplitude and phase of the wavefront.  

The software calculations that accompany this book use both wavefront and ray methods[^optics-intro-1].

[^optics-intro-1]: The ISETCam optics calculations uses the wave model. The ISET3d graphics calculations use the ray model.

## Lenses: Controlling ray direction

::: margin
![The rays from a nearby point that pass through a relatively large pinhole diverge.  They form an image of the point that is larger than the pinhole.](../images/optics/pinhole-rays.png){#fig-pinhole-rays fig-align="center" width="75%"}
:::
Consider the rays from a point as they pass through a pinhole. The image they form will not be sharp because the ray directions are diverging. Narrowing the pinhole sharpend the image formation by selecting a small subset of the rays. 

We can sharpen the image by redirecting some the rays to converge at the image plane. One way to think about optical devices is that they are built to control the direction of the environmental light field rays at the image system entrance aperture. When forming a sharp image is the goal, we evaluate the optics by asking whether they adjust the ray directions so that the image of a point in the scene is also a point. A useful fact to remember is that diffraction means the best (sharpest) we can do for a circular aperture is defined by the diameter of the Airy disk.

::: margin
![Lenses transform the direction of the rays in the incident light field. In this example, the diverging rays from a nearby point are transformed to converge on the image plane. Diffraction imposes a limit on precision of the convergence. ](../images/optics/pinhole-lens.png){#fig-pinhole-rays fig-align="center" width="75%"}
:::


## Material interactions
When a light rays arrive at the boundary between regions with different optical properties - whether those regions contain matter (like glass or water), air or vacuum - several things can happen (@fig-reflected-scattered).  The rays might be reflected back, as in a mirror.  Or, the rays might enter into the material, undergo a series of internal reflections, and emerge back in one of many different directions.  Also, the rays might enter the material and be absorbed, giving up their energy to the atoms in the material. (Maybe reference the dichromatic reflection model here?)

![When rays arrive at the boundary of two regions, The rays (a) might be reflected, (b) might enter the medium, be internally reflected multiple times, and emerge in one of many different directions (scattering). (Not shown) The ray may might enter the medium and be absorbed.](../images/optics/snell-reflected-scattered.png){#fig-reflected-scattered fig-align="center" width="100%"}

The most important case for controlling the rays, and thus for optical design, is this other possibility: the ray enters the new material and continues on its way. We call this change in direction **refraction**. 

![A ray crossing the boundary between two materials may continue but change direction. The change occurs because the speed of electromagnetic waves differs between the regions. The *index of refraction* ($n$ and $n'$) is the material parameter that describes the speed.](../images/optics/snell-refraction.png){#fig-snell-law fig-align="center" fig-cap-location="bottom" width="50%"}

### Snell's Law
Snell's Law quantifies how light changes direction when passing between regions with different optical properties. The mathematical formulation of Snell's Law is defined using the variables shown in @fig-snell-law. A light ray (Incident ray) is shown crossing a planar boundary between one medium (say, air) in a second medium (say, glass). The angle between the incident ray and a line perpendicular to the surface (the surface normal) is $\phi$. The refracted ray changes direction and thus has a different angle, $\phi '$, with respect to the surface normal. Snell's Law captures the relationship between these two ray angles 

$$ n \sin(\phi) = n' \sin(\phi') $$ {#eq-snell}

The variables $n$ and $n'$ are the *index of refraction* of the materials, which is related to the speed of the electromagnetic radiation in the material.

$$ n = \frac{c}{v} $$ {#eq-indexofrefraction}

where $c$ is the speed of light in a vacuum, $v$ is the speed in the material. By definition, $n=1$ in a vacuum. It is very close to $1.0$ in air. In some materials, such as glass, the velocity of light is much slower, so that the index might be $1.3$ or even $1.5$. 

In many common materials the velocity is reduced because the light, which is electromagnetic radiation, induces an opposing electromagnetic signal in the material. This induced electromagnetic signal, which is predicted by Maxwell's equations, combines with the original signal in a way that [slows it down](https://chatgpt.com/share/676a2564-e72c-8002-923a-d865491f30ed).  In addition, materials such as liquid crystals have more complex interactions that produce interesting and useful effects (birefringence) that we will describe in @sec-displays.

### Wavelength dependence
<!-- [Abbe Number](https://chatgpt.com/share/676a2564-e72c-8002-923a-d865491f30ed) -->
For many materials that index of refraction is wavelength dependent. Following Snell's Law, the each wavelengths changes direction by different amount. This can cause unwanted artifacts in imaging systems, called chromatic aberration. 

For most materials the change of the index of refraction over the visible wavelength is roughly linear, just varying in extent.  It is common to summarize the dispersion using the *Abbe number*, $V_d$, which is computed by comparing the index of refraction at three different wavelengths.

$$ V_d = \frac{n_{489.3} - 1}{n_{486.1} - n_{653.3}}$$ {#eq-abbe}

Conventionally, an Abbe number greater than 50 implies very little dispersion and a number less than 30 implies significant dispersion that will impact image quality.  The human eye has very significant chromatic aberration, with an Abbe number of about 15. 

When possible, it is my preference in computation to provide numerical values describing the index of refraction as function of wavelength and to measure the impact by computing the line spread or point spread function. We will perform analyses of chromatic aberration several times in this book, including in @sec-human.

::: {.callout-note collapse="true" title="Historical note:  Refraction"}

Refraction was studied by the Greek astronomer Ptolemy in about the year 150. He documented that rays passing from air into, say water or glass, change direction. Ibn Sahl in Persia, around 930, quantified the change in direction as light crossed the boundary between certain materials, and he used this knowledge to design lenses.

The formula for refraction was discovered independently by two scientists in the 17th century. Willebrord Snellius (also known as Snell) first found the mathematical relationship in 1621, but his work wasn't published during his lifetime. Ren√©e Descartes independently made the same discovery and published it in 1637. Although Descartes published first, the formula became known as Snell's Law to acknowledge Snellius's earlier discovery. Later, physicist Christiaan Huygens showed that Snell's Law could be explained by treating light as a wave that travels at different speeds in different materials. The number of mis-steps in establishing the law and the basis for the law makes for [interesting reading](https://en.wikipedia.org/wiki/Snell%27s_law).  

```{=html}
<!-- Wikipedia:  Snell's Law.  "The law was eventually named after Snell, although it was first discovered by the Persian scientist Ibn Sahl, at Baghdad court in 984.[6][7][8] In the manuscript On Burning Mirrors and Lenses, Sahl used the law to derive lens shapes that focus light with no geometric aberration.[9]" 
Also https://en.wikipedia.org/wiki/Willebrord_Snellius
-->
```
:::

## Making a lens
To make a lens that focuses a light field into an image requires bending a lot of rays.  How this might work for a simple case, is easy to see.  Imagine that the rays are diverging from a point located at the left.  Were they passing through a pinhole, they would continue to diverge.

![Refraction transforms the rays diverging from an object point (left) to converging to an image point (right).](../images/optics/thinlens-prism.png){#fig-thinlens-prism fig-align="center" width="50%"}

Refraction by the prism changes the direction of the rays at the  from upward to downward (upper prism). Similarly, refraction changes the ray directions from downward to upward (lower prism). These two prisms act to converge the rays from the object point onto a point on a properly positioned image plane.

<!--# This link describes the issues in making a diffraction limited thin lens.  I think the problem is a flat sensor surface will not work with a spherical lens surface. You need a aspheric. It has other interesting facts. To get close to diffraction limited you need to use a multicomponent lens to correct for different aberrations. 
https://chatgpt.com/share/6748e5dc-267c-8002-844c-a83ee0cdb144 
-->

![The  diverging ray directions from an object point are redirected by refraction to converge on an image plane at the appropriate distance.](../images/optics/thinlens-assembled.png){#fig-thinlens-prism fig-align="center" width="50%"}

An image could be formed by stacking a collection of prisms, or more likely by polishing a glass surface to have two smooth, spherical surfaces. The entrance aperture of such a spherical lens can be much larger than a pinhole and thus capture much more light. 

Using Snell's Law, it is possible to calculate the convergence of the rays and thus the distance to the plane where an image would be formed. In 1854 James Clerk Maxwell noted that a spherical lens made with one material would change the ray directions at the edge too much, blurring the image. He suggested that biological eyes might have an index of refraction that varies from the center to the edge, reducing the refraction. This is true of many biological eyes. Lenses engineered to have a variable gradient index of refraction are valuable in optical design and called *GRIN* lens. 
<!-- https://www.britannica.com/science/photoreception/Single-chambered-eyes Michael Land's Brittanic article has the story. Lenses made of homogeneous material (e.g., glass or dry protein) suffer from a defect known as spherical aberration, in which peripheral rays are focused too strongly, resulting in a poor image. 
-->

It is also possible to build a lens from one material with an aspherical shape.  [See this page for the shape formula and other issues.](https://chatgpt.com/c/6748e492-0310-8002-8da3-89a1517a6062).

It wonderful if such a lens could be diffraction limited, forming an Airy point spread function of an object point. Creating lenses from different types of glass 