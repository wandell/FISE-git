# Shift invariant systems {#sec-appendix-shiftinvariance}

Linear models are central to science, engineering, and mathematics. Among the many possible linear models, two orthogonal basis sets are especially important. 

First, we have repeatedly used the point representation of the image. The point (or “impulse”) representation is so natural that we often take it for granted.  The second is the **harmonic basis** —that is, sine and cosine waves at different spatial frequencies. In this section, I explain why harmonics are so valuable for analyzing certain types of linear systems. 

::: {.callout-note title="Dynamic systems" collapse="false"}
In systems that respond over time (like audio or electronics), the equivalent measurement is the **impulse response**—the system’s output when given a very brief input. The impulse response plays the same role as the PSF, just in the time domain. In dynamic systems, the point basis is called the **impulse** and the point spread is the **impulse response**. The mathematical formulation of the impulse or point is often called the **Dirac delta function**, to honor the theoretical physicist Paul Dirac. 
:::

## Shift-invariant linear (SIL) systems  {#sec-ls-shiftinvariance}
A practical challenge in characterizing general linear systems is that each input point could have a different point spread function (PSF). That’s a lot to measure! Fortunately, many real systems are much simpler: their PSF is essentially the same, except that it’s shifted to match the shift in input location. In other words, the shape of the PSF doesn’t change—it just translates. We call such systems **shift-invariant**.

Shift-invariant linear systems are common across science and engineering. Many physical devices and phenomena can be well-approximated as shift-invariant, at least over a useful range. This property makes them much easier to calibrate: we only need to measure a single PSF -the response to a single point!- rather than measure every possible point. You can see why for a shift-invariant system the PSF is the key measurement that characterizes the system. 

Because shift invariance simplifies both analysis and computation, many courses on “linear systems” focus almost entirely on the shift-invariant case.

::: {.callout-note title="Shift invariance notation" collapse="true"}
Let’s make the idea of shift invariance a bit more precise. Suppose we use $T()$ to represent a shift (translation) operation. If we shift an image $x$ by an amount $\delta$, we write $T(x, \delta)$. A system $L()$ is shift-invariant if shifting the input and then applying the system gives the same result as applying the system first and then shifting the output (possibly by a scaled amount):

$$
L(T(x, \delta)) \approx T(L(x), \alpha \delta).
$$ {#eq-shiftinvariance-defined}

Here, $\alpha$ is a scale factor that accounts for magnification or other effects. For example, if you move an object by $\delta$ meters, its image might move by $\alpha \delta$ meters on the sensor.
:::

## SIL system matrix
The matrix representation of an SIL, $\mathbf{L}$ is simple and can be visualized.  If the input stimulus is a point, then it is a vector that has a $1$ in a single entry and $0$ elsewhere. The point spread function is the column of the matrix, $\mathbf{L}$.  Since the point spread functions are all the same, except for a shift, the columns are all the same except for a shift. 

If we arrange the sample spacing of the input and output measurements carefully, we can shift the input point by one row and the corresponding column, representing a point spread measurement, will also shift down one row. The entries of the matrix, shown in the tableau, will look like this @fig-ls-circulant-matrix.

![Circulant matrix.](images/linearsystems/circulant-matrix.png){#fig-ls-circulant-matrix width=70%}

For computer simulations, there are many details to keep track of, especially issues relating to the edge and wrapping. For example, we often treat the point spread functions as 'circular', so when a value shifts below the bottom of the matrix, we do not throw it away but we copy it into the missing entry at the top of the column. In @fig-ls-circulant-matrix, the edges of the point spread are all zero, and that made the diagram simpler. 

Have a good look at @fig-ls-circulant-matrix. Feel how simple an SIL system matrix is compared to a general linear system (@sec-ls-matrix-tableau). Linear systems in general are great to work with.  If you are working with an SIL, you are particularly fortunate.  The next section gives another reason why.

## Harmonics:  Eigenvectors for shift-invariance {#sec-ls-harmonics-sil}
We have reached the observation that is the basis of many calculations in the main chapters in the book: the relationship between harmonics and shift invariant linear systems. These ideas are introduced in the main text in @sec-optics-linear-harmonics, where we develop the Discrete Fourier Series (@sec-optics-fourier-series). The ideas continue to be used in many subsequent chapter.

Haromonic functions are *almost* eigenfunctions of shift-invariant linear systems. By almost, I mean that when a harmonic is the input, the output is a scaled harmonic with a possible shift in position (phase). It is not exactly an eigenfunction because two parameters, the amplitude and phase, can change.

The mathematics of this near-miss is managed by expressing the signal using complex instead of real numbers. For complex numbers, which inherently have two parameters, we can say without any guilt that the complex exponentials are eigenvectors of a shift-invariant linear system. Moreover, the complex exponentials are a complete, orthogonal set of eigenfunctions; any input stimulus can be described as the weighted sum of complex exponentials. We relate the complex exponentials back to the real harmonics, using Euler's formula (@eq-Eulers-formula). [^harmonics-FOV].

[^harmonics-FOV]: I explain the relationship between harmonics and shift-invariant linear systems in detail, staying away from complex exponentials, in the Appendix to [Foundations of Vision](https://wandell.github.io/FOV-1995/appendix.html){target=_blank}.

Taken together this means shift-invariant linear systems and harmonics can take advantage of the full power and simplicity of the eigenvector calculations (@eq-linear-eigencalculations). When the input to an optical system is a harmonic function at spatial frequency $f$ and unit contrast, the output image will be a harmonic function at the same frequency. The amplitude and of the output will differ from the input, but we only need to measure these two parameters. This input-output simplicity makes the harmonics very useful tools for system characterization.

::: {.callout-note collapse="true" title='History - linear algebra and matrices'}
When I was an assistant professor, I had the privilege of joining faculty meetings with senior colleagues I deeply admired—Joe Goodman, Ron Bracewell, Tom Cover, and others. In these meetings, we often discussed how best to teach key mathematical ideas to engineering students. The more senior faculty generally favored starting with integrals and continuous functions, emphasizing that these are the true foundation of the mathematics. The younger faculty, who spent much of their time programming, leaned toward teaching with matrices and vectors. I remember Ron Bracewell remarking that continuous functions are the reality, and we should introduce those first—then, only later, show students the "vulgar" discrete approximations that we use because our computational tools require them. Good times.

We all agreed that discrete representations are extremely useful—and today, they are essential. Mathematics framed in terms of vectors and matrices is at the heart of modern neural networks and much of computational science and engineering. That’s the approach I use throughout this book. Still, I remember Bracewell’s advice, and sometimes feel a bit of guilt for not starting with the continuous case.

It helps to know that representing problems with vectors and matrices is not a recent development. The importance of linear algebra and matrix methods was recognized nearly 200 years ago. If you’re interested in the history of these ideas, you can read more here: [History of matrices and linear algebra](resources/history-linear-algebra.html){target=_blank}
:::

## Discrete Fourier Series (DFS) {#sec-optics-fourier-series}
The Discrete Fourier Series (DFS) represents sampled data as sums of harmonics. It’s the frequency-domain counterpart to the point (impulse) basis (@sec-ls-spatial-basis). More background appears in @sec-appendix-linear-systems.

For a sampled 1D signal we write $I[n]$ with $n = 0,\ldots,N-1$ (there are $N$ samples). The DFS uses integer spatial frequencies $f = 0,\ldots,N-1$ and assumes the signal repeats every $N$ samples (periodic extension). Here are three equivalent forms.

1. Sine–cosine form

$$
I[n] = \frac{a_0}{2} + \sum_{f=1}^{N-1} \, s_f \,\sin\!\left(\frac{2 \pi f n}{N}\right) + c_f \,\cos\!\left(\frac{2 \pi f n}{N}\right)
$$ {#eq-fourier-series}


2. Amplitude–phase form

$$
I[n] = \frac{a_0}{2} + \sum_{f=1}^{N-1} a_f \,\sin\!\left(\frac{2 \pi f n}{N} + \phi_f\right),
\qquad a_f \ge 0
$$ {#eq-fourier-series-phase}

3. Complex exponential form

$$
I[n] = \sum_{f=0}^{N-1} w_f \, e^{\,i \, 2 \pi f n / N},
$$ {#eq-fourier-series-complex}

where the weights $w_f$ are complex and $i = \sqrt{-1}$. 

### Calculating the weights: Discrete Fourier transform (DFT)
We need a way to transform the sampled image intensities, $I[n]$, into the Fourier weights, $w_f$. The most common method is the **Discrete Fourier Transform (DFT)**:
$$
w_f = \frac{1}{N} \sum_{n=0}^{N-1} I[n] \, e^{-i 2 \pi f n / N}
$$ {#eq-discrete-fourier-transform}

At first glance, this formula might look intimidating, but it's actually quite elegant. On the right, we have the sampled image values multiplied by a complex exponential. Thanks to **Euler's formula**, we know that any complex exponential can be written in terms of sine and cosine:

$$
e^{i\theta} = \cos(\theta) + i\sin(\theta).
$$ {#eq-Eulers-formula}

This means the DFT is really just combining the familiar sine and cosine terms into a single, compact expression using complex numbers. Many people find this complex formulation neater than keeping track of separate sine and cosine coefficients.

It's important to note that while $I[n]$ is real-valued, the DFT coefficients $w_f$ are generally complex. For a real signal, there is redundancy: the coefficients satisfy $w_{f} = \overline{w_{N-f}}$, where $\overline{w_f}$ is the complex conjugate of $w_f$.[^dft-coefficients]

[^dft-coefficients]:  The complex conjugate of $a + i b$ is $a - i b$.

If you prefer to work with real numbers, you can compute the sine and cosine coefficients, $s_f$ and $c_f$, directly from the image intensities:

$$
\begin{align}
a_0 &= \frac{2}{N} \sum_{n=0}^{N-1} I[n] \\
s_f &= \frac{2}{N} \sum_{n=0}^{N-1} I[n] \sin\left(\frac{2\pi f n}{N}\right) \\
c_f &= \frac{2}{N} \sum_{n=0}^{N-1} I[n] \cos\left(\frac{2\pi f n}{N}\right)
\end{align}
$$

Here, $a_0$ gives twice the mean value of $I[n]$, which is why the Discrete Fourier Series (@eq-fourier-series) starts with $\frac{a_0}{2}$.

## Relating the DFS representations

There are several equivalent ways to write the Discrete Fourier Series, and it's helpful to know how the parameters relate:

- $\frac{a_0}{2}$ and $w_0$ both represent the mean value of the signal $I[n]$.

- The amplitude and phase parameters relate to the sine and cosine coefficients as follows:

$$
\begin{align}
a_f & = \sqrt{{s_f}^2 + {c_f}^2} \\
\phi_f & = \arctan\left(\frac{c_f}{s_f}\right)
\end{align}
$$

- The complex weights $w_f$ are related to the sine and cosine coefficients by

$$
w_f  = \frac{1}{2} (c_f - i s_f ) \qquad \text{for } f > 0
$$

- And the complex weights $w_f$ are related to the amplitude and phase by

$$
w_f = \frac{a_f}{2} e^{i (\phi_f - \pi/2)} \qquad \text{for } f > 0
$$

So, whether you prefer to think in terms of sines and cosines, amplitudes and phases, or complex exponentials, you can always translate between these forms.

::: {.callout-note title="About Jean Baptiste Joseph Fourier" collapse="false" #fourier-history}
See <a href="https://mathshistory.st-andrews.ac.uk/Biographies/Fourier/" target="_blank">a brief biography of Fourier</a>, who had a fascinating life. He served as a scientific adviser on Bonaparte’s Egyptian expedition and later as Prefect of Isère in Grenoble, where he pursued his theory of heat conduction and the use of trigonometric series.

In 1807 Fourier presented his heat memoir to the Paris Institute. The committee (Lagrange, Laplace, Monge, Lacroix) balked at representing arbitrary functions by trigonometric series and questioned the rigor; Biot and Poisson added objections. Fourier won the Institute’s 1811 prize, but publication lagged until 1822—evidence of the Academy’s initial resistance to what we now call Fourier series.

:::{.columns}
::: {.column width="50%"}
- Son of a tailor; Orphaned at 9
- Educated by Benedictines
- Supported the Revolution, becoming an adviser to Bonaparte in Egypt
- A statue of Fourier, erected in Grenoble, was later melted for armaments
:::
::: {.column width="50%"}
![Jean Baptiste Joseph Fourier](images/optics/optics-fourier.png){#fig-optics-fourier width="60%"}
:::
:::

:::

### Counting parameters
A length‑$N$ real signal $I[n]$ has $N$ real values. At first glance, each DFS parameterization seems to use more than $N$ parameters. For example, the sine–cosine form has both $s_f$ and $c_f$ for each $f$, and the complex exponential form uses $N$ complex weights $w_f$ (that’s $2N$ real numbers). How can that be?

The resolution is that for real signals the Fourier coefficients obey symmetries that reduce the number of independent values back to $N$ real degrees of freedom.

The cleanest way to see this is in the complex form. For real $I[n]$,
$$
w_{f} \;=\; \overline{w}_{\,N-f}, \qquad f=1,\ldots,N-1,
$$
so the coefficients come in complex‑conjugate pairs. Using Euler’s formula (@eq-Eulers-formula), these relate to the sine–cosine and amplitude–phase parameters as
$$
w_f = \tfrac{1}{2}\,(c_f - i\,s_f), \qquad
w_{N-f} = \tfrac{1}{2}\,(c_f + i\,s_f),
$$ {#eq-nyquist-1}
and
$$
w_f = \tfrac{a_f}{2}\,e^{\,i(\phi_f - \pi/2)}, \qquad
w_{N-f} = \tfrac{a_f}{2}\,e^{\,-i(\phi_f - \pi/2)}.
$$ {#eq-nyquist-2}

Two special bins are purely real for real signals:
- DC ($f=0$): only the cosine term contributes; this is the mean.
- When $N$ is even, the Nyquist bin ($f=N/2$): $\sin(\pi n)=0$ for all integers $n$, so only cosine remains. This bin is also purely real.

These symmetries remove the apparent over‑parameterization.

### Sampling rate and Nyquist frequency
If image samples are spaced every $\Delta x$, we say the sampling rate is $f_s = 1/\Delta x$ (samples per unit). The Nyquist frequency is half the sampling rate,
$$
f_N \;=\; \frac{f_s}{2} \;=\; \frac{1}{2\,\Delta x},
$$
and is measured in cycles per unit (e.g., cycles/degree or cycles/mm for spatial signals). In DFT index units (dimensionless bin indices), $f_N$ aligns with the bin at index $N/2$ when $N$ is even; when $N$ is odd there is no bin exactly at $f_N$.

As we saw in the previous section, because of conjugate symmetry, only about “half” of the spectrum is unique:

- Even $N$: There are two special, real bins—DC ($f=0$) and Nyquist ($f=N/2$). The remaining bins form conjugate pairs $(f,\,N-f)$ for $f=1,\ldots,N/2-1$. The count of distinct frequency bins is
    $$
    1 \text{ (DC)} \;+\; (N/2-1) \text{ pairs} \;+\; 1 \text{ (Nyquist)} \;=\; N/2 + 1.
    $$
    Each conjugate pair contributes two real degrees of freedom (real and imaginary parts), and the two special bins contribute one each, totaling $N$ real degrees of freedom.

- Odd $N$: There is no Nyquist bin. We have DC ($f=0$) plus conjugate pairs $(f,\,N-f)$ for $f=1,\ldots,(N-1)/2$. The count of distinct bins is
    $$
    1 \text{ (DC)} \;+\; (N-1)/2 \text{ pairs} \;=\; (N+1)/2,
    $$
    again yielding $N$ real degrees of freedom.

It is common to compute and report only this non‑redundant “half‑spectrum” (plus DC, and the Nyquist bin when $N$ is even). In continuous‑domain sampling language, frequencies above the Nyquist frequency $f_N$ “fold” or alias back; in DFT index terms, information in bins above $N/2$ is redundant with bins below $N/2$ for real‑valued signals. Some authors therefore call the Nyquist frequency the folding frequency.


