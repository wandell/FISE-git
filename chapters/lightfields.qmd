# Light fields

Both creatures of flesh and bone and machines of metal and code achieve many goals by sensing and perceiving their environment. Animals move in search of food and to avoid predators by relying on their ability to sense and interpret. Machines identify and grasp objects and navigate their environments by sensing and interpreting. For both the electromagnetic radiation in the environment is a main source of information. Immobile life forms, such as trees and plants, mainly use the radiation as a source of energy; but mobile life forms - a hawk, a rabbit, or a person - encode and interpret the radiation to glean information about the environment.

::: {.callout-note collapse="true" title="Who needs a brain?"}
### Movement requires a brain {.unnumbered}

The deep connection between mobility, sensing and the brain is illustrated by the remarkable life path of the [sea squirt](https://movementum.co.uk/journal/sea-squirts). It begins life as tadpole-like larvae, equipped with a brain and a tail for swimming. Once the tadpole finds a suitable spot to settle, it attaches to a hard surface like a rock or coral and becomes immobile.

As soon as the sea squirt fixes itself to the rock, with no more navigation or swimming, its brain is no longer needed. In a truly bizarre twist, the sea squirt digest its own brain. This adaptation reallocates energy resources from the now unnecessary brain to more vital functions.

Here are some [nice images of sea squirts!](https://goodheartextremescience.wordpress.com/2010/01/27/meet-the-creature-that-eats-its-own-brain/)\
<!-- Irv Weissmann has lots of neurodegeneration papers on this -->
:::

Not all electromagnetic radiation is equally informative. High energy radiation (e.g., X-ray and Gamma ray) passes through the objects. Low frequency radiation provides poor spatial resolution. Absorption and scattering of electromagnetic radiation in the wavelength range of a few hundred nanometers is useful for informing us about the environment.

The human eye encodes electromagnetic radiation in the wavelength range from about 400 nm to 770 nm; the word light means that portion of the electromagnetic radiation that humans see. Many image systems are designed to measure the radiation in this visible band, and some are designed to measure electromagnetic radiation at wavelengths we do not see, thus extending our sensing. We refer to the electromagnetic radiation that surrounds us and that we see as the light field. See [more about wavebands here.](../supplemental/lightfields-wavebands.html){target="_blank"}

The idea that the light field is everywhere is nicely described in Leonardo Da Vinci's 1509 notebook [@davinciNotebooksLeonardoVinci1970]. He described a pinhole camera (camera obscura) made by placing a small hole in a wall of a windowless room that is adjacent to a brightly illuminated piazza (@fig-ayscough). He observed an image of the piazza (inverted) on the wall within the room. Leonardo further noted that an image is formed wherever the pinhole is placed, and from this observation he concluded that the rays must be present at all of these positions[^lightfields-1].

[^lightfields-1]: **PROVE HOW ALL OBJECTS, PLACED IN ONE POSITION, ARE ALL EVERYWHERE AND ALL IN EACH PART** <br> "I say that if the front of a building—or any open piazza or field—which is illuminated by the sun has a dwelling opposite to it, and if, in the front which does not face the sun, you make a small round hole, all the illuminated objects will project their images through that hole and be visible inside the dwelling on the opposite wall which may be made white; and there, in fact, they will be upside down, and if you make similar openings in several places in the same wall you will have the same result from each. Hence the images of the illuminated objects are all everywhere on this wall and all in each minutest part of it. The reason, as we clearly know, is that this hole must admit some light to the said dwelling, and the light admitted by it is derived from one or many luminous bodies. If these bodies are of various colours and shapes the rays forming the images are of various colours and shapes, and so will the representations be on the wall. (Leonardo's 1509 Notebook, curated by John Paul Richter)"

![Ayscough's book @ayscoughShortAccountEye1755 (“A short account of the eye and nature of vision. Chiefly ... to illustrate the use and advantage of spectacles”) opens with this illustration of Da Vinci’s pinhole camera. I superimposed the colored lines to indicate the courtyard light rays, emitted in all directions. The solid lines are selected by the pinhole and form the image in the dark room. The full collection of rays, solid and dotted, represents the *environment light field*. [Source: Wikipedia Camera Obscura](https://en.wikipedia.org/wiki/Camera_obscura#/media/File:1755_james_ayscough.jpg).](../images/lightfields/Ayscough-1755-small-wikipedia.png){#fig-ayscough fig-alt="Alt Text" fig-align="center" width="80%"}

It is useful to write the environmental light field as an explicit function so that we can keep track of its various parameters. At each point in the volume $(x,y,z)$, rays are emitted in multiple directions that we specify by two angles $(\alpha,\beta)$, the azimuth and elevation. The light rays have wavelength $\lambda$ and polarization $\rho$. The intensity of each ray in the environment light field function, $L_E$, depends on these seven parameters.

$$L_E(x,y,z,\alpha,\beta,\lambda,\rho)$$

People in many fields are familiar with the concept of the light field, but they use different terminology. Physicists often describe the environmental light field as the spectral radiance of the enviroment. The term *light field* was introduced into optical engineering by @gershunLightField1939 as part of his work in understanding how to design lighting environments, such as the lighting in school rooms and public places[^lightfields-2]. The use of the term and the importance of the light field in practice was greatly amplified in the field of computer graphics by Marc Levoy and Pat Hanrahan. A related term, the *plenoptic* function, was introduced by @adelsonPlenopticFunctionElements1991. The plenoptic function was defined for a pinhole camera and a sensor that sums across all incident angles, making it equivalent to the spectral irradiance at the image sensor [@Wandell2021-Principles-Ashby].

[^lightfields-2]: I enjoyed the first sentence in the 1939 translation of Gershun's paper, from the translators, Moon and Timoshenko. They express frustration with the pace of advances in photometry. "Theoretical photometry constitutes a case of 'arrested development', and has remained basically unchanged since 1760 while the rest of physics has swept triumphantly ahead." Gershun himself writes "The problems of theoretical photometry were pushed aside from the main path of the development of physics."

In the following sections we continue to develop the light field terminology and follow the signal from the environment, to the entrance pupil of the camera, and then to the sensor.

## Incident light field

A simple imaging system, say the eye or a pinhole, only records the portion of the environmental light field that arrives at its entrance pupil (@fig-incident-lightfield). It is worth distinguishing those rays - the *incident light field* - from the environmental light field.

::: figure-wrap-right
![The incident light field is the subset of environmental light field rays that arrive at the image systems’ entrance pupil.](../images/lightfields/01-incident-lightfield.png){#fig-incident-lightfield fig-width="70%" fig-align="center" width="725"}
:::

The incident light field rays can be described with a slightly simpler function than the environmental light field because the entrance pupil is just a two-dimensional surface. Thus, the three dimensional position we use for the environment light field can be reduced two the two-dimensional position in the entrance pupil, $(u,v)$.

$$L_I(u,v,\alpha,\beta,\lambda,\rho)$$

If we wish to acquire information about more of the environmental light field we must measure at multiple locations. The human visual system makes two simultaneous measurements of the light field, one with each eye, and this information provides us with stereo vision. We acquire more information about the light field over time by rotating our eyes and moving our heads.

Various image systems have been built to extend how much of the environmental light field we measure. Acquiring this information enables us to reproduce a more complete experience of the environment in virtual reality environments, as a person changes position and direction of gaze. The cameras used in these image system arrays are more complex than pinholes. But the principle Leonardo identified holds: the light field is everywhere, and we can measure it by recording images at many different positions with cameras pointing in many different directions.

A large, rectangular camera array to measure the light field was built by Levoy and colleagues at the Stanford Graphics Lab (@fig-camera-array). The cameras in the rectangular array acquire rays from a large number of positions in the scene. In the figure, the cameras measure multiple views but all point in a similar direction. Arrays with different camera configurations (shown below) capture the 360 deg light field from a particular position. These camera arrays acquire enough information so that we can render images that match what a person would see if they look around. Camera arrays that acquire enough information for both 360 deg and slightly different positions have also been built (stacked omnistereo). These camera arrays acquire data that can be interpreted to estimate the light field. From this estimate, we calculate new images that from different points of view.

![The Stanford Multi-Camera Array (top) comprised 128 cameras that measured the environmental light field from many positions. With this configuration one can recreate the experience of moving side to side as well as back and forth. Other array configurations (bottom) have cameras pointing outward from a point. These capture the light field from many directions and can recreate the experience of looking around the room from the point.](../images/lightfields/01-camera-array.png){#fig-camera-array .margin-caption}

::: {.callout-note collapse="true" title="Interpreting the light field"}
### Inverse rendering {.unnumbered}

Measuring the light field data is the start of seeing. The next step interpreting the measurements to find the things and stuff, to identify material properties, and to understand the lighting. Algorithms to achieve these goals are part of the broad project of making computers that see.

One approach to making computers see is have them construct an internal model of the things, stuff, materials, and lights and then calculate what the light field would be for this model. The computer can compare the predicted light field with the measured light field, and then continually update the model until it agrees with the data. In computer graphic methods this approach is called *inverse rendering*, and it is at the frontier of what is currently possible (REFERENCES HERE).

Representing objects and lights is also valuable in graphics applications because it enables computer graphics to edit the scene by changing the shapes, materials, and lights. The approach is essential in robotics and driving applications where deriving information about the environment from the light field is essential to guiding action.

Building an object and lighting model from the encoded light field seems rather like what people do: we see the objects and materials, not the light per se. The basic idea was stated quite clearly nearly two centuries ago by the great vision scientist, Helmholtz, who wrote:

> The general rule determining the ideas of vision that are formed whenever an impression is made on the eye, is that *such objects are always imagined as being present in the field of vision as would have to be there in order to produce the same impression on the nervous mechanism* \[Italics in the original; Southall, Physiological Optics, Vol. III, p. 2, 1865\]

This book is written with the belief that achieving this goal will benefit significantly from understanding the light field and the instruments that measure it. This belief is not universally held.
:::

## Optical light field

The image system optics, whether a pinhole, thin lens, or multi-element lens, transforms the incident light field into a new light field that exists within the camera. We call this the *optical light field*. This light field originates in the environment, but it is reduced to the incident light field and then further transformed by the optics. The information in the optical light field is what the sensor measures. The data the sensor measures is where our interpretation begins. No optical system is perfect, and there is always some information that is lost or distorted by the optics. The sensing and perception systems that derive information from the light field must be designed to account for which information in the environmental light field makes it all the way through to the optical light field.

![The rays in the exit pupil of the optics are captured by the sensor (or film). These rays form a light field within the camera, the optical light field \$L_O\$. The rays of this light field that matter for the image system are those that are captured by sensor.](/images/lightfields/01-optical-lightfield.png){#fig-optical-lightfield .margin-caption}

The environment and incident light fields use parameters of position and angle for each ray. For the optical light field, however, we only measure rays that arrive at the sensor (or film). It is common, therefore, to replace the angles describing how the ray exits the optics with the position $(w,z)$ the ray will have when it arrives at the sensor. It is possible, of course, to convert these parameters to angular parameters.

$$L_O(u,v,w,z,\lambda,\rho)$$

::: {.callout-note collapse="true" title="Measuring a light field"}
### Light field sensors {.unnumbered}

The most common sensors in our cameras record only partial information about the optical light field. They rays at a single point on the sensor are summed together, no matter where the ray exited the optics. Rays with different wavelengths are also summed.

By capturing more information about the optical light field, it is possible to do more. This Adelson and XXX pointed out that ... And the Ren Ng's innovative company Lytro developed a sensor and algorithms that captured a high resolution light field and then enabled users to refocus images or change the depth of field after capture. In the chapter on sensors, we will describe ways in which sensors capture a more complete version of the light field.
:::

## Pinholes

Treating light as rays, it is easy to understand why pinhole optics renders a reasonable image on Leonardo's wall (@fig-ayscough). It is also straightforward to understand some of the basic properties of the pinhole camera from tracing the rays (@fig-pinhole-basic). This type of reasoning is helpful in many optics calculations, so let's do a few analyses.

![Pinhole camera geometry. (A) The rays from a distant, on-axis point arrive at in parallel at the pinhole. Were there no diffraction, they would simply continue on to form an image the size of the pinhole. (B) An off-axis point's rays also arrive in parallel, but they are at an angle to the pinhole aperture. Hence, fewer rays pass through the aperture so the image would be dimmer and smaller. If we can tolerate a relatively large point spread, this design is workable. But if we require a small point spread, diffraction makes this design unworkable.](../images/lightfields/pinholePSF.png){#fig-pinhole-basic fig-align="center" width="569"}

Consider a point (A) relatively far from the pinhole. The rays at the point may radiate in a wide range of directions, but only a small portion of those rays will arrive at the pinhole aperture. These rays will be parallel to one another (collimated). Setting diffraction aside for the moment, the collection of rays will pass straight through and form an image on the recording surface (image plane, such as Leonardo's wall). The image is called the point spread function. For the on-axis point, a pinhole, and ignoring diffraction, the point spread size is equal to the size of the pinhole aperture.

Consider an off-axis point (B). Again, its rays may spread in many directions but only a narrow, collimated subset of rays will arrive at the aperture. Because of the angle between the collimated rays of the incident light field a smaller fraction of the rays from (B) will pass through the pinhole aperture. Continuing to ignore diffraction, the point spread from (B) will be smaller and dimmer. If we can tolerate the reduced intensity and we do not mind the blurring from the pinhole, we will have a satisfactory image.

We can simulate this geometry, again ignoring diffraction, using computer graphics (@fig-pinhole-chessset). We render a scene of a chess set that is about 0.5 meters from the pinhole camera. Each chess piece is a few cm tall. We render four pictures, using different pinhole diameters. The upper left allows only one ray through from each location in the scene. The aperture diameters for the next three pictures are for increasing pinhole sizes. The images are brighter as the pinhole size increases, of course. For this scene geometry the image at the two smaller pinhole sizes are tolerable. Depending on your viewing distance from the screen or page, even the third pinhole image might be usable. But the image with image with the largests pinhole loses so much information that the individual pieces blur together. This loses much value of the image for many purposes.

![A simulated scene imaged with a pinhole camera. The size of the pinhole diameter increases, with the smallest in the upper left, and increasing (upper left to lower right). The relative intensity is preserved in the renderings so that the scene becomes brighter as the pinhole diameter increases. Rendered with PBRT and ISET3d.](../images/lightfields/01-pinhole-chessset.png){#fig-pinhole-chessset}

There are many ways to explore the impact of varying the pinhole size. We might also adjust the distance of the image plane from the pinhole, or we might change the field of view of the scene If you are new to reasoning about optics, some of the calculations in this ISETCam script may help you develop an intuition for these different parameters.

## Diffraction

The ray trace analysis of light passing through a pinhole is only half right. It is true that increasing the pinhole diameter blurs the image; but the ray tracing analysis fails dramatically in other ways. Consider the simple prediction of the ray model in @fig-ray-wave (A). The rays from a point source at a distance will be nearly parallel. They will pass through a pinhole, continuing along in a straight line. A person to the side of the pinhole would see it as dark because no rays are heading to his eyes. This proves that the rays do not simply continue on a straight line.

![A ray model failure. Many examples like this were published in a book by Grimaldi, and known in the time of Newton. Even so, the ray model was widely accepted for more than a century.](../images/lightfields/01-ray-wave.png){#fig-ray-wave}

The Dutch scientist Christiaan Huygens, working at the same time as Newton, suggested an alternative model: he proposed that we model light as a wave. He specifically suggested that we consider the light emitted from a point source as a spherical wave expanding in all directions. At each moment in time the points on the wavefront emit a secondary wavelet, also expanding in all directions. This concept leads to various quantitative predictions, including how light will be reflected from a mirror and how the light direction will change as the wave passes from one medium (air) into a second medium (glass) (@fig-huygens-waves).

![Light as a wave, rays as the wavefront. (a) A single source is shown emitting a circular (spherical if 3D) wave. Each point on the wavefront emits a new spherical wave. The leading edge of all these waves reinforce one another to form a spherical, expanding, wavefront. (b) At a distance from the source, the small part of the wavefront appears to be a plane wave. Again, each point on the plane emits a spherical wavefront. Because they are all aligned the wavefront remains planar. (c) When the wavefront arrives at a small aperture, like a pinhole, only a small part of the wave is allowed through. This is like the original source, so that the spherical wavefront from a small portion of the plane wave is revealed, and the light spreads in many directions.](../images/lightfields/01-huygens-wave.png){#fig-huygens-waves fig-cap-location="bottom"}

A consequence of the wave theory is that as the pinhole diameter size decreases, the wave properties of light cause the image to blur. This effect, called diffraction by Grimaldi in 1655, is very small when the pinhole is much larger than the wavelength of the light. But when the pinhole diameter is small we effectively see the spherical wave from a point on the wavefront (@fig-huygens-waves). In that case the light spreads in many directions and the image is strongly blurred.

::: {.callout-note collapse="true" title="What took us so long?"}
### Rays to waves

Some brief text here talking about the shift with Thomas Young's experiments.

Then ask why given Grimaldi and Huygens did so many (Brougham, for example) stick with rays for so long?

[My reading of the history.](../supplemental/optics-diffraction.html).

A lot about hero worship. A little about simplicity.
:::

### Calculating the two blurs

The numbers are in one of those YouTube videos. I sure hope I saved it!

The following might be a separate section about the history. For the main book we might simply build the calculation of blurring as per the YouTube video.

## Footnotes