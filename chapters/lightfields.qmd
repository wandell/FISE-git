# Light fields {#sec-lightfields}

Why do organisms sense and interpret their surroundings? From a biological perspective, sensory perception is fundamental for guiding movement, enabling animals to locate resources, avoid threats, and navigate their environments. Ambient electromagnetic radiation provides a rich and reliable stream of information, which mobile organisms—whether a hawk, a rabbit, or a human—analyze to make decisions and interact with their surroundings. For immobile life forms, such as trees and plants, this radiation serves primarily as a source of energy, rather than information.

In robotics, sensing serves a similar fundamental purpose: to enable movement and interaction within complex environments. By equipping robots and autonomous vehicles with imaging systems, we allow them to recognize objects, assess spatial relationships, and navigate with precision. The capability to capture and interpret visual data empowers these systems to identify, reach, and respond to targets in their surroundings, closely paralleling the biological role of sensory systems in guiding adaptive, goal-oriented actions.

Not all electromagnetic radiation is equally informative. High energy radiation (e.g., X-ray and Gamma ray bands) passes through most objects undisturbed. Low frequency radiation provides limited spatial resolution. The human eye detects electromagnetic radiation in the wavelength range from about 380 nm to 770 nm. We call this waveband "light", which is defined as "electromagnetic radiation that can be detected by the human eye." Electromagnetic radiation in this wavelength range is strongly absorbed and reflected by objects of interest, making it a very useful band for informing us about the environment.

Some image systems are designed to measure light because we want them to reproduce the signals that we might see. Others measure light because the signal is useful for their own function. And finally, some image systems measure electromagnetic radiation at wavelengths we do not see, exploring domains within the environment that are beyond our senses[^lightfields-1].

[^lightfields-1]: See [more about wavebands here.](../supplemental/lightfields-wavebands.html){target="_blank"}.

::: {.callout-note collapse="true" title="Who needs a brain?"}
### Movement requires a brain {.unnumbered}

The deep connection between mobility, sensing and the brain is illustrated by the remarkable life path of the [sea squirt](https://movementum.co.uk/journal/sea-squirts). It begins life as tadpole-like larvae, equipped with a brain and a tail for swimming. Once the tadpole finds a suitable spot to settle, it attaches to a hard surface like a rock or coral and becomes immobile.

As soon as the sea squirt fixes itself to the rock, with no more navigation or swimming, its brain is no longer needed. In a truly bizarre twist, the sea squirt digest its own brain. This adaptation reallocates energy resources from the now unnecessary brain to more vital functions.

Here are some [nice images of sea squirts!](https://goodheartextremescience.wordpress.com/2010/01/27/meet-the-creature-that-eats-its-own-brain/)\
<!-- Irv Weissmann has lots of neurodegeneration papers on this -->
:::

## The environmental light field

Electromagnetic radiation fills the environment, traveling in many directions and interacting with most materials. We call the ambient radiation in the visible band the *light field*. Leonardo Da Vinci's notebook [@davinciNotebooksLeonardoVinci1970] describes how we know that this field is everywhere. In a section of his notebook he describes placing a small hole in a wall of a windowless room. The room is adjacent to a brightly illuminated piazza (@fig-ayscough). The camera produces an image of the piazza (inverted) on the wall within the room. Leonardo further noted that an image is formed wherever the pinhole is placed. From this observation he concluded that the light field is "all everywhere"[^lightfields-2].

[^lightfields-2]: **PROVE HOW ALL OBJECTS, PLACED IN ONE POSITION, ARE ALL EVERYWHERE AND ALL IN EACH PART** <br> "I say that if the front of a building—or any open piazza or field—which is illuminated by the sun has a dwelling opposite to it, and if, in the front which does not face the sun, you make a small round hole, all the illuminated objects will project their images through that hole and be visible inside the dwelling on the opposite wall which may be made white; and there, in fact, they will be upside down, and if you make similar openings in several places in the same wall you will have the same result from each. Hence the images of the illuminated objects are all everywhere on this wall and all in each minutest part of it. The reason, as we clearly know, is that this hole must admit some light to the said dwelling, and the light admitted by it is derived from one or many luminous bodies. If these bodies are of various colours and shapes the rays forming the images are of various colours and shapes, and so will the representations be on the wall. (Leonardo's 1509 Notebook, curated by John Paul Richter)"

![Ayscough's book @ayscoughShortAccountEye1755 (“A short account of the eye and nature of vision. Chiefly ... to illustrate the use and advantage of spectacles”) opens with this illustration of Da Vinci’s pinhole camera. I superimposed the colored lines to indicate the courtyard light rays, emitted in all directions. The solid lines are selected by the pinhole and form the image in the dark room. The full collection of rays, solid and dotted, represents the *environment light field*. [Source: Wikipedia Camera Obscura](https://en.wikipedia.org/wiki/Camera_obscura#/media/File:1755_james_ayscough.jpg).](../images/lightfields/Ayscough-1755-small-wikipedia.png){#fig-ayscough width="95%" fig-alt="Alt Text" fig-align="center" width="80%"}

It is useful to write the environmental light field as an explicit function so that we can keep track of its various parameters. For the moment, it is simple and convenient to consider the radiation as comprising a large set of light rays. At each point in the volume of space, $(x,y,z)$, the rays are traveling in multiple directions. We specify these directions by two angles $(\alpha,\beta)$, the azimuth and elevation. Each ray has wavelength $\lambda$ and polarization $\rho$. Thus, the intensity of the environmental light field function, $L_E$, depends on seven parameters.

$$L_E(x,y,z,\alpha,\beta,\lambda,\rho)$$

People in many fields are familiar with the concept of the light field, but they use different terminology. Physicists often describe the environmental light field as the spectral radiance of the enviroment. The term *light field* was introduced into optical engineering by @gershunLightField1939 as part of his work in understanding how to design lighting environments, such as the lighting in school rooms and public places[^lightfields-3]. The use of the term and the importance of the light field in practice was greatly amplified in the field of computer graphics by Marc Levoy and Pat Hanrahan @Levoy-1996-light-field. I will continue to use the light field terminology to follow the signal from the environment, to the entrance pupil of the camera, and then to the sensor. I find the term "light" to be more familiar and engaging than spectral radiance or electromagnetic radiation. Perhaps this is because I am a psychologist at heart.

[^lightfields-3]: I enjoyed the first sentence in the 1939 translation of Gershun's paper, from the translators, Moon and Timoshenko. They express frustration with the pace of advances in photometry. "Theoretical photometry constitutes a case of 'arrested development', and has remained basically unchanged since 1760 while the rest of physics has swept triumphantly ahead." Gershun himself writes "The problems of theoretical photometry were pushed aside from the main path of the development of physics."

## Incident light field

An imaging system, say the eye or a pinhole, records only the portion of the environmental light field that arrives at its entrance pupil (@fig-incident-lightfield). It is worth distinguishing that part of the light field with a name: *incident light field*. For this definition, it is convenient to conceive of the light as comprising rays traveling in space.

::: figure-wrap-right
![The incident light field is the subset of environmental light field rays that arrive at the image systems’ entrance pupil.](../images/lightfields/01-incident-lightfield.png){#fig-incident-lightfield width="80%" fig-align="center"}
:::

The rays of the incident light field can be described with one fewer parameter than the environmental light field because the entrance pupil is just a two-dimensional surface. Thus, the three dimensional position is replaced by the two-dimensional position in the entrance pupil, $(u,v)$. There are, however, additional parameters that define the sensing system, not the light field. The aperture has a position in space $\mathbf{p}$ and and a viewing direction $\mathbf{n}$. We

$$L_I(u,v,\alpha,\beta,\lambda,\rho; ~\mathbf{p},\mathbf{n})$$

To acquire more information about the environmental light field, we measure at multiple locations and directions. The human visual system makes two simultaneous measurements of the light field, one with each eye, and this is stereo vision. We acquire more information about the light field over time by moving our heads, walking around, and rotating our eyes.

Special purpose image systems have been built to extend how much of the environmental light field we measure. This information enables us to reproduce a more complete experience of the environment in virtual reality environments, as a person changes position and direction of gaze. These image system usually use camera arrays. They rely on the principle Leonardo identified: the light field is everywhere, and we can measure it by recording images at many different positions using cameras pointing in many different directions.

A large, rectangular camera array to measure the light field was built by Levoy and colleagues at the Stanford Graphics Lab [@Levoy-1996-light-field] (@fig-camera-array). The cameras in the rectangular array acquire rays from a large number of positions in the scene. As illustrated in the figure, the cameras measure multiple views, with all the cameras pointing in the same direction. Arrays with different camera configuration, at the bottom of the figure, capture multiple directions from a particular position. These camera arrays acquire enough information so that we can render images that match what a person would see if they look around. Camera arrays that acquire enough information for both 360 deg and slightly different positions, sometimes called stacked omnistereo, have also been built [@thatte-2017-6dof]. From these data, along with an interpolation strategy, we can calculate images from a range of viewpoints.

![The Stanford Multi-Camera Array (top) comprised 128 cameras that measured the environmental light field from many positions. With this configuration one can recreate the experience of moving side to side as well as back and forth. Other array configurations (bottom) have cameras pointing outward from a point. These capture the light field from many directions and can recreate the experience of looking around the room from the point.](../images/lightfields/01-camera-array.png){#fig-camera-array .margin-caption width="70%"}

::: {.callout-note collapse="true" title="Interpreting the light field"}
### Inverse rendering {.unnumbered}

Measuring the light field data is a first step in seeing. A next step is interpreting the measurements to find the things and stuff, to identify material properties, and to understand the lighting. The discovery of algorithms that achieve these goals are part of the broad project of building computers that see.

Some general guidance in how to make computers see is have their software construct an internal model of the things, stuff, materials, and lights in the scene. The software can calculate the predicted light field for the model, comparing it with the measured light field. Deviations provide information to update the model in a virtuous cycle until the two agree. In computer graphic methods this approach is called *inverse rendering*, and it is at the frontier of what is currently possible [@nimier-david2019].

Representing things, stuff, materials and lights is valuable in many graphics applications because it enables users to edit the scene in terms of objects they understand. The approach is essential in robotics and driving applications where decision making about actors in the environment is essential to guiding action.

Building an object and lighting model from the encoded light field has long been believed to be what people do. Nearly two centuries ago by the great vision scientist, Helmholtz, wrote:

> The general rule determining the ideas of vision that are formed whenever an impression is made on the eye, is that *such objects are always imagined as being present in the field of vision as would have to be there in order to produce the same impression on the nervous mechanism* \[Italics in the original; Southall, Physiological Optics, Vol. III, p. 2, 1865\]

This book is written with the belief that achieving this goal will benefit significantly from understanding the light field and the instruments that measure it. I am aware, however, that this belief is not universally held.
:::

## Optical light field

The image system optics, whether a pinhole, thin lens, or multi-element lens, transforms the incident light field into a new light field within the camera. This *optical light field* is between the exit pupil of the optics and the sensor. It originates in the environment, is reduced to the incident light field, and then transformed by the optics. The camera sensor, or the retina, measures the optical light field.

![The rays emerging from the exit pupil of the optics are captured by the sensor (or film). These rays form a light field within the camera, the optical light field $L_O$. The rays of this light field that matter for the image system are those that are captured by sensor.](/images/lightfields/01-optical-lightfield.png){#fig-optical-lightfield .margin-caption width="80%"}

The environment and incident light fields use parameters of position and angle for each ray. For the optical light field, however, we only measure rays that arrive at the sensor (or film). It is helpful, therefore, to describe how the radiation exits the optics using in use the sensor position, $(w,z)$, for radiation exiting the optics. It is possible, of course, to convert these parameters to angular parameters.

$$L_O(u,v,w,z,\lambda,\rho)$$

The data recorded by the sensor is used to create a reproduction of the image, or as the basis for interpreting the image contents. The sensing and perception systems that derive information from the light field are designed to account for which information in the environmental light field makes it all the way through to the optical light field [^lightfields-4].

[^lightfields-4]: The *plenoptic* function, has been widely used to describe light fields [@adelson-plenoptic-1991]. The plenoptic function is defined for a pinhole camera and a sensor, removing two parameters $(u,v)$. In practice, the plenoptic function is equivalent to another familiar term, the spectral irradiance at the image sensor [@Wandell2021-Principles-Ashby].

For many years, the most common sensors in cameras record partial information about the optical light field. The rays arriving at a point on the sensor surface are summed together, no matter where the ray exited the optics. Measuring more information about the optical light field enables us to know and do more. For example, @adelson-wang-1992 demonstrated how to use this information to estimate depth. Ren Ng's innovative company, Lytro, developed a light field sensor and algorithms to enable users to refocus images or change the depth of field after capture. In @sec-sensors we describe how sensors capture a more complete version of the optical light field, and we will describe how modern sensors are now using this information for camera autofocus. 

## Pinholes

If we imagine light as a collection of ray, it is easy to understand why pinhole optics renders a reasonable image (@fig-ayscough). Some of the properties of the pinhole camera, such as why the image is inverted or how it is blurred, are also well-explained by tracing the rays. The formulation of light as rays, which was proposed by Isaac Newton, is a helpful approximation for many optics calculations.

![Pinhole camera geometry. (A) The rays from a distant, on-axis point arrive at in parallel at the pinhole. Were there no diffraction, they would simply continue on to form an image the size of the pinhole. (B) An off-axis point's rays also arrive in parallel, but they are at an angle to the pinhole aperture. Hence, fewer rays pass through the aperture so the image would be dimmer and smaller. If we can tolerate a relatively large point spread, this design is workable. But if we require a small point spread, diffraction makes this design unworkable.](../images/lightfields/pinholePSF.png){#fig-pinhole-basic fig-align="center" width="70%"}

For example, consider a point (A) relatively far from the pinhole (@fig-pinhole-basic). The rays at A radiate in a wide range of directions, but only a small portion of those rays arrive at the pinhole aperture. Because the pinhole forms a very narrow angle from the point the rays in the incident light field will be parallel to one another (collimated). According to the ray model of light, this collection of rays will pass straight through the pinhole and form an image on the recording surface (e.g., Leonardo's wall). The image formed by such a point is called the *point spread function*. For the on-axis point A, the point spread size is equal to the size of the pinhole aperture.

Consider an off-axis point, (B). Again, its rays will spread in many directions but only a narrow, collimated subset of rays will arrive at the aperture. Because of the angle between the collimated rays and the pinhole, a smaller fraction of the rays will pass through the pinhole aperture. The point spread from (B) will be smaller and dimmer. If we can tolerate the reduced intensity and we do not mind the blurring from the pinhole, we will have a satisfactory image.

It is common to simulate light as rays in computer graphics. In @fig-pinhole-chessset I rendered a chess set scene with the pieces, each a few cm tall, positioned about 0.5 meters from the pinhole camera. The four pictures were rendered using different pinhole diameters. The upper left allows only one ray through from each location in the scene. The aperture diameters for the next three pictures are for increasing pinhole sizes. The images are brighter as the pinhole size increases, of course. For this scene geometry the image at the two smaller pinhole sizes are tolerable. Depending on your viewing distance from the screen or page, even the third pinhole image might be usable. But the largest pinhole, which lets in the most light, also loses so much spatial information that the individual pieces blur together.

<!--# Maybe we do some simple geometric calculations in a script and callout here.  Possible ideas are showing how the distance of the image surface from the pinhole and the size of the image surface matter.  Also that the same light is spread over a smaller region when the surface is closer, and a wider region when the surface is further away.  These ideas will reappear below when we measure light levels and also in the optics chapter when we talk about magnification. -->

![A simulated scene imaged with a pinhole camera. The size of the pinhole diameter increases, with the smallest in the upper left, and increasing (upper left to lower right). The relative intensity is preserved in the renderings so that the scene becomes brighter as the pinhole diameter increases. Rendered with [@pharrPBRTVersion4] and [@iset3d2022].](../images/lightfields/01-pinhole-chessset.png){#fig-pinhole-chessset width="80%" fig-align="center"}

## Diffraction

Useful as the ray model has been, for more than 300 years scientists have known that describing light as rays is only half right. It is true that increasing the pinhole diameter blurs the image; but the ray tracing analysis fails dramatically in other ways. Consider the simple prediction of the ray model in @fig-ray-wave (A). The rays from a point source at a distance will be nearly parallel. They will pass through a pinhole, continuing along in a straight line. An image cast on a surface behind the pinhole should have the diameter of the pinhole; a person to the side of the pinhole would see it as dark because no rays are heading to his eyes. The Jesuit scientist, Grimaldi, demonstrated that the predictions of the ray model are violated [@Grimaldi1665-physico].

![@Grimaldi1665-physico introduced a pencil of light into a dark room through a hole. According to the ray theory of light, the image on the wall should be the same size as the pinhole, and the rays should not be visible to an observer looking from the side. Grimaldi found that under some conditions the image on the image plane was larger than the pinhole itself, inconsistent with the ray theory of light. Newton confirmed the experiment but continued to use the ray theory because it remained useful for many purposes (A History of Physics, Cajori, p. 88-89). In modern times, we accept the wave theory but often use the ray theory because it works well in many circumstances.](../images/lightfields/01-ray-wave.png){#fig-ray-wave width="80%" fig-align="center"}

### Huygens wave model

The Dutch scientist Christiaan Huygens, working at the same time as Newton, suggested an alternative model: he proposed modeling light as waves that expand to fill space rather than rays made up of particles that travel in a straight line. On Huygen's theory, the light emitted from a point expands as a spherical wave in all directions. At each moment in time the expansion, called the wavefront, could be considered as an array of points that each emitted a secondary wave. This model leads to many quantitative predictions, including how light will be reflected from a mirror and how the light direction will change as the wave passes from one medium (air) into a second medium (glass) (@fig-huygens-waves). We would think of a ray as being a plane wave, spread across space.

![Light modeled as a wavefront. (A) A point source is shown emitting a circular (or 3D, spherical) wave. Each point on the expanding wavefront emits a new spherical wave. The leading edge of these waves reinforce one another to form the expanding wavefront. (B) At a large distance from the source, the small part of the wavefront appears to be a plane wave. Again, each point on the plane emits a spherical wavefront. These are aligned and the wavefront remains planar. (C) When the wavefront arrives at a small aperture only a small part of the wave passes through. This is like the original source, revealing the spherical wavefront from that point.](../images/lightfields/01-huygens-wave.png){#fig-huygens-waves fig-cap-location="bottom" width="80%" fig-align="center"}

A consequence of the wave theory is that as light passes through a pinhole, or near an edge, the spherical waves from each point on the wavefront are revealed. The aperture limits the region of the plane wave that continues on to a much smaller number of points. When restricted in this way, the resulting wavefront is no longer a plane; instead, the sum of the spherical waves from the points sum to a new wavefront that spreads in multiple directions. This phenomenon was called diffraction by Grimaldi. When the pinhole diameter is small, and only a small portion of the plane wave compared to the wavelength of light enters the pinhole, the spherical wavefront becomes quite large and this blurs the image (@fig-huygens-waves).

::: {.callout-note collapse="true" title="What took us so long?"}
### Rays and waves

Although the two theories were proposed at about the same time, Newton's ray theory from 1704 was accepted for roughly a century. Widespread recognition of the value of Huygen's wave theory followed Thomas Young's 1804 demonstration, in England to the Royal Society, of the interference pattern created by light passing through two, coherent, nearby sources.

I find it useful to rembmer the [the history of these ideas.](../supplemental/optics-diffraction.html) There is real value in the ray model which remains widely used for many purposes: even though we know it is not correct in all cases. This very pragmatic approach - use the tool that is accurate enough for the problem at hand - is deeply ingrained in some fields. Approach practical measurements and theories with a toolbox of methods.

Some fields that I have worked in adopted a different philosophy: they rely on hypothesis testing and aim to prove ideas true or false. In this spirit, the phrase 'all models are wrong, some are useful', is widely quoted @box-science-1976. In physics and engineering hypotheses that are half full are still considered valuable, as long as one understands the scope over which the hypotheses can be reasonably applied.
:::

### Which effect blurs more?

The image is blurred by two effects: the pinhole size and diffraction. We can compare the size of these two effects numerically.

<!--#  Perfect for explaining which is larger, diffraction or the size of the hole. Short and clear. Robert Cruikshank. He has a direct calculation for when diffraction dominates the aperture size, and so forth. YouTube video on diffraction -->

First, suppose the pinhole diameter is $D$. For a far-away point, with the rays coming in parallel, we expect the blur size in the image to be $D$. This pinhole-blur, treating the light as rays, is the same no matter how far the image plane is from the pinhole.

Second, the light pattern we expect for a wave passing through a pinhole aperture can be calculated using a formula that was derived by George B. Airy [@airy-1835-diffraction]. The light pattern depends on the wavelength of the light $\lambda$, the diameter of the pinhole, and the distance from the pinhole to the image plane, $L$. The image is radially symmetric with a bright spot in the center, surrounded by a set of concentric rings of decreasing intensity. The central region is called the Airy disk, and it contains about 85% of the total energy. The mathematical notation for that formula is a bit complex, but the diameter of the disk, $d$, is given by a relatively simple formula

$$d= 2.44~L~(\lambda / D)$$ {#eq-airy1}

Suppose we calculate the size of the diffraction diameter when the image plane is 1 m away, so $L=1$, and for a wavelength of light is 550 nm. The diameter of the Airy disk, $d$, in meters will be $2.44 \times (550*10^{-9} / D)$. The diffraction spread dominates the pinhole spread when $d$ exceeds $D$. We do the calculation in the scripts in this [ISETCam script](https://htmlpreview.github.io/?https://github.com/wandell/FISE-git/blob/main/code/fise_diffraction.html).

::: {.callout-note collapse="true" title="Airy disk size formula"}
### Airy disk formula {.unnumbered}

In his paper entitled "On the Diffraction of an Object-glass with Circular Aperture", George Airy provided the mathematical description of the diffraction pattern [@airy-1835-diffraction], which includes the central bright spot (the Airy disk) surrounded by a series of concentric rings. The modern derivation of the full point spread function for both the circular aperture and other shapes can be found @goodman2022 (pages 88-94). The computation is implemented and frequently used in ISETCam. It is explained in this tutorial XXX.

In the main text, we expressed the formula for a particular distance from the pinhole to an image plane. For a pinhole, the formula is usually given in terms of the angle of the bundle of rays emerging from the pinhole.

$$sin(\theta) = 1.22 \lambda / D$$

When the angle is small, $\theta < 10 \deg$, the formula is very accurately approximated as

$$\theta = 1.22 \lambda / D$$

The size of the spot on the image plane depends on the distance how far away the pinhole is from the pinhole, as you can see in [@eq-airy1]. In @sec-optics we provide a formula for the Airy disk size for an ideal lens.
:::

## Light field measurement
Many applicationsrequire quantifying the light field.  We may wish to know whether a display is too bright, or not bright enough.  We may need to know the level of light to expect when we take a picture. In this section we introduce the fundamental concepts of light field quantification. Tracking the light levels and the signal levels is an important part of the ISETCam simulation software.

The engineering discipline of light field measurement, radiometry, the measurements are always made with separately for each wavelength. Light field quantification describes the amount of energy, or equivalently the number of photons, present in a portion of the light field. In the case of energy the basic unit is joules/sec, which is also called watts. In the case of photons the measure is photons per second. 

For people working with light, wavelength is commonly expressed with respect to nanometers [^lightfields-5]. Thus, in radiometry one typically finds the units watts per nanometer or photons/sec per nanometer as part of the measurement. We can described the energy for each wavelength because in most cases the energy at different wavelengths are independent: we can measure the energy at two wavelengths separately, mix the two lights, and the result is simply the sum of the individual measurements. This additivity property is a critical feature of light.

[^lightfields-5]: Units of micrometers (microns) are commonly used by people working in the infrared studies, units of angstroms in atomic and x-ray studies, and electron volts in high-energy photon research.

Radiometry defines standard units that depend on the geometry of the measurement. They are divided based on whether we are measuring light radiating from a source (radiance), or we are measuring a light field that is arriving at a surface (irradiance). Radiance is further divided depending on whether we are measuring from a small (point) source, or from an extended surface.

Finally, there is a set of light units that summarize the impact of the light on the human visual system. These measurements begin with the spectral radiometry units and form a weighted sum across wavelengths. The weights are selected to represent the relative visibility of each wavelength; the weights were measured in an experimental paradigm called 'flicker photometry'. The photometric terms that parallel radiance and irradiance are luminance and illuminance. In this section, we will describe the radiometric quantities. In @sec-human we explain the experimental basis for summing across wavelengths and how to convert the spectral radiometric quantities into photometric quantities.

![The geometry of radiometric measures.](../images/lightfields/01-radiometry.png){#fig-radiometry fig-cap-location="bottom" width="100%" fig-align="center"}

### Radiance from a point
@fig-radiometry illustrates the geometry for four radiometric measures.  At the upper left, we consider a point source.  The *radiant flux* measures the total energy emitted from the point in all directions. As for all radiometric measurements, the energy (watts) is specified separately with respect to wavelength (watts/nm).  

Often, we would like to measure the light emitted by a point source in a particular direction, the *radiant intensity*.  In that case we measure the energy within a cone of rays in a particular direction. We measure the cone angle in terms of steradians, the standard unit for angles in three-dimensions just as the radian is the standard unit for angles in two-dimensions. The radiant intensity measures watts  per steradian per nanometer (watts/sr/nm).  Thus, if the measurement instrument picks up light over an angle of 0.5 steradians, we divide our measurement by 0.5.  If it measures average 10 nm bands, we divide the energy by 10.  In this way, the radiant intensity normalizes the angle we measure to the unit steradian and per nanometer. 

::: {.callout-note collapse="false" title="Steradians"}

![Three-dimensional angle units are called steradians.  One steradian is defined as angle of the cone whose surface area on a sphere of radius $r$ is equal to $r^2$. Because the surface area of a sphere with radius $r$ is $4 \pi r^2$, it follows that the complete sphere surrounding a point is $4 \pi$ steradians.](../images/lightfields/01-radiometry-steradian.png){#fig-steradian width="40%" fig-cap-location="bottom"}
:::

### Radiance from a surface
In many applications we want to measure light from an extended, flat surface - such as a wall, the side of a large object, or perhaps a projector screen.  In that case, many individual points contribute energy to the measured light. To normalize the measurement, we divide by the area of the surface that is contributing to our measurement.  Thus, if the surface we are measuring is a square 0.1 meters on a side, we normalize the energy by $0.1^2 m^2$.  There is one additional geometric factor to consider: the extended surface may be at an angle to the measurement instrument.  To see why this is important consider looking at a surface that is fully facing you, and then start to turn it away.   In this case, we are measuring the *radiance*, which is watts per nanometer per unit area of the surface corrected by the angle of the surface with respect to the measurement instrument. 

### Irradiance



How do we know we are measuring from a point or a surface?

### Radiometric units
![The radiometric measurement units.](../images/lightfields/01-radiometry-units.png){#fig-radiometry-units fig-cap-location="bottom" width="100%" fig-align="center"}

## Footnotes