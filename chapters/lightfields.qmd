# Light fields {#sec-lightfields}

Both creatures of flesh and bone and machines of metal and code achieve many goals by sensing and perceiving their environment. Animals move in search of food and to avoid predators by relying on their ability to sense and interpret. Machines identify and grasp objects and navigate their environments by sensing and interpreting. For both the electromagnetic radiation in the environment is a main source of information. Immobile life forms, such as trees and plants, mainly use the radiation as a source of energy; but mobile life forms - a hawk, a rabbit, or a person - encode and interpret the radiation to glean information about the environment.

::: {.callout-note collapse="true" title="Who needs a brain?"}
### Movement requires a brain {.unnumbered}

The deep connection between mobility, sensing and the brain is illustrated by the remarkable life path of the [sea squirt](https://movementum.co.uk/journal/sea-squirts). It begins life as tadpole-like larvae, equipped with a brain and a tail for swimming. Once the tadpole finds a suitable spot to settle, it attaches to a hard surface like a rock or coral and becomes immobile.

As soon as the sea squirt fixes itself to the rock, with no more navigation or swimming, its brain is no longer needed. In a truly bizarre twist, the sea squirt digest its own brain. This adaptation reallocates energy resources from the now unnecessary brain to more vital functions.

Here are some [nice images of sea squirts!](https://goodheartextremescience.wordpress.com/2010/01/27/meet-the-creature-that-eats-its-own-brain/)\
<!-- Irv Weissmann has lots of neurodegeneration papers on this -->
:::

Not all electromagnetic radiation is equally informative. High energy radiation (e.g., X-ray and Gamma ray) passes through the objects. Low frequency radiation provides poor spatial resolution. Absorption and scattering of electromagnetic radiation in the wavelength range of a few hundred nanometers is useful for informing us about the environment.

The human eye encodes electromagnetic radiation in the wavelength range from about 400 nm to 770 nm; the word light means that portion of the electromagnetic radiation that humans see. Many image systems are designed to measure the radiation in this visible band, and some are designed to measure electromagnetic radiation at wavelengths we do not see, thus extending our sensing. We refer to the electromagnetic radiation that surrounds us and that we see as the light field. See [more about wavebands here.](../supplemental/lightfields-wavebands.html){target="_blank"}

The idea that the light field is everywhere is nicely described in Leonardo Da Vinci's 1509 notebook [@davinciNotebooksLeonardoVinci1970]. He described a pinhole camera (camera obscura) made by placing a small hole in a wall of a windowless room that is adjacent to a brightly illuminated piazza (@fig-ayscough). He observed an image of the piazza (inverted) on the wall within the room. Leonardo further noted that an image is formed wherever the pinhole is placed, and from this observation he concluded that the rays must be present at all of these positions[^lightfields-1].

[^lightfields-1]: **PROVE HOW ALL OBJECTS, PLACED IN ONE POSITION, ARE ALL EVERYWHERE AND ALL IN EACH PART** <br> "I say that if the front of a building—or any open piazza or field—which is illuminated by the sun has a dwelling opposite to it, and if, in the front which does not face the sun, you make a small round hole, all the illuminated objects will project their images through that hole and be visible inside the dwelling on the opposite wall which may be made white; and there, in fact, they will be upside down, and if you make similar openings in several places in the same wall you will have the same result from each. Hence the images of the illuminated objects are all everywhere on this wall and all in each minutest part of it. The reason, as we clearly know, is that this hole must admit some light to the said dwelling, and the light admitted by it is derived from one or many luminous bodies. If these bodies are of various colours and shapes the rays forming the images are of various colours and shapes, and so will the representations be on the wall. (Leonardo's 1509 Notebook, curated by John Paul Richter)"

![Ayscough's book @ayscoughShortAccountEye1755 (“A short account of the eye and nature of vision. Chiefly ... to illustrate the use and advantage of spectacles”) opens with this illustration of Da Vinci’s pinhole camera. I superimposed the colored lines to indicate the courtyard light rays, emitted in all directions. The solid lines are selected by the pinhole and form the image in the dark room. The full collection of rays, solid and dotted, represents the *environment light field*. [Source: Wikipedia Camera Obscura](https://en.wikipedia.org/wiki/Camera_obscura#/media/File:1755_james_ayscough.jpg).](../images/lightfields/Ayscough-1755-small-wikipedia.png){#fig-ayscough width="95%" fig-alt="Alt Text" fig-align="center" width="80%"}

It is useful to write the environmental light field as an explicit function so that we can keep track of its various parameters. At each point in the volume $(x,y,z)$, rays are emitted in multiple directions that we specify by two angles $(\alpha,\beta)$, the azimuth and elevation. The light rays have wavelength $\lambda$ and polarization $\rho$. The intensity of each ray in the environment light field function, $L_E$, depends on these seven parameters.

$$L_E(x,y,z,\alpha,\beta,\lambda,\rho)$$

People in many fields are familiar with the concept of the light field, but they use different terminology. Physicists often describe the environmental light field as the spectral radiance of the enviroment. The term *light field* was introduced into optical engineering by @gershunLightField1939 as part of his work in understanding how to design lighting environments, such as the lighting in school rooms and public places[^lightfields-2]. The use of the term and the importance of the light field in practice was greatly amplified in the field of computer graphics by Marc Levoy and Pat Hanrahan. A related term, the *plenoptic* function, was introduced by @adelsonPlenopticFunctionElements1991. The plenoptic function was defined for a pinhole camera and a sensor that sums across all incident angles, making it equivalent to the spectral irradiance at the image sensor [@Wandell2021-Principles-Ashby].

[^lightfields-2]: I enjoyed the first sentence in the 1939 translation of Gershun's paper, from the translators, Moon and Timoshenko. They express frustration with the pace of advances in photometry. "Theoretical photometry constitutes a case of 'arrested development', and has remained basically unchanged since 1760 while the rest of physics has swept triumphantly ahead." Gershun himself writes "The problems of theoretical photometry were pushed aside from the main path of the development of physics."

In the following sections we continue to develop the light field terminology and follow the signal from the environment, to the entrance pupil of the camera, and then to the sensor.

## Incident light field

A simple imaging system, say the eye or a pinhole, only records the portion of the environmental light field that arrives at its entrance pupil (@fig-incident-lightfield). It is worth distinguishing those rays - the *incident light field* - from the environmental light field.

::: figure-wrap-right
![The incident light field is the subset of environmental light field rays that arrive at the image systems’ entrance pupil.](../images/lightfields/01-incident-lightfield.png){#fig-incident-lightfield width="80%" fig-align="center" width="725"}
:::

The incident light field rays can be described with a slightly simpler function than the environmental light field because the entrance pupil is just a two-dimensional surface. Thus, the three dimensional position we use for the environment light field can be reduced two the two-dimensional position in the entrance pupil, $(u,v)$.

$$L_I(u,v,\alpha,\beta,\lambda,\rho)$$

If we wish to acquire information about more of the environmental light field we must measure at multiple locations. The human visual system makes two simultaneous measurements of the light field, one with each eye, and this information provides us with stereo vision. We acquire more information about the light field over time by rotating our eyes and moving our heads.

Various image systems have been built to extend how much of the environmental light field we measure. Acquiring this information enables us to reproduce a more complete experience of the environment in virtual reality environments, as a person changes position and direction of gaze. The cameras used in these image system arrays are more complex than pinholes. But the principle Leonardo identified holds: the light field is everywhere, and we can measure it by recording images at many different positions with cameras pointing in many different directions.

A large, rectangular camera array to measure the light field was built by Levoy and colleagues at the Stanford Graphics Lab (@fig-camera-array). The cameras in the rectangular array acquire rays from a large number of positions in the scene. In the figure, the cameras measure multiple views but all point in a similar direction. Arrays with different camera configurations (shown below) capture the 360 deg light field from a particular position. These camera arrays acquire enough information so that we can render images that match what a person would see if they look around. Camera arrays that acquire enough information for both 360 deg and slightly different positions have also been built (stacked omnistereo). These camera arrays acquire data that can be interpreted to estimate the light field. From this estimate, we calculate new images that from different points of view.

![The Stanford Multi-Camera Array (top) comprised 128 cameras that measured the environmental light field from many positions. With this configuration one can recreate the experience of moving side to side as well as back and forth. Other array configurations (bottom) have cameras pointing outward from a point. These capture the light field from many directions and can recreate the experience of looking around the room from the point.](../images/lightfields/01-camera-array.png){#fig-camera-array .margin-caption width="70%"}

::: {.callout-note collapse="true" title="Interpreting the light field"}
### Inverse rendering {.unnumbered}

Measuring the light field data is a first step in seeing. A next step is interpreting the measurements to find the things and stuff, to identify material properties, and to understand the lighting. The discovery of algorithms that achieve these goals are part of the broad project of building computers that see.

Some general guidance in how to make computers see is have their software construct an internal model of the things, stuff, materials, and lights in the scene. The software can calculate the predicted light field for the model, comparing it with the measured light field. Deviations provide information to update the model in a virtuous cycle until the two agree. In computer graphic methods this approach is called *inverse rendering*, and it is at the frontier of what is currently possible [@nimier-david2019].

Representing things, stuff, materials and lights is valuable in many graphics applications because it enables users to edit the scene in terms of objects they understand. The approach is essential in robotics and driving applications where decision making about actors in the environment is essential to guiding action.

Building an object and lighting model from the encoded light field has long been believed to be what people do. Nearly two centuries ago by the great vision scientist, Helmholtz, wrote:

> The general rule determining the ideas of vision that are formed whenever an impression is made on the eye, is that *such objects are always imagined as being present in the field of vision as would have to be there in order to produce the same impression on the nervous mechanism* \[Italics in the original; Southall, Physiological Optics, Vol. III, p. 2, 1865\]

This book is written with the belief that achieving this goal will benefit significantly from understanding the light field and the instruments that measure it. I am aware, however, that this belief is not universally held.
:::

## Optical light field

The image system optics, whether a pinhole, thin lens, or multi-element lens, transforms the incident light field into a new light field within the camera that is directed towards the sensor. This *optical light field* originates in the environment, is reduced to the incident light field, and then transformed by the optics. No optical system is perfect; some information will be lost or distorted by the optics. The camera sensor, or the retina, measures the optical light field.

![The rays emerging from the exit pupil of the optics are captured by the sensor (or film). These rays form a light field within the camera, the optical light field \$L_O\$. The rays of this light field that matter for the image system are those that are captured by sensor.](/images/lightfields/01-optical-lightfield.png){#fig-optical-lightfield .margin-caption width="80%"}

The environment and incident light fields use parameters of position and angle for each ray. For the optical light field, however, we only measure rays that arrive at the sensor (or film). It is common, therefore, to replace the angles describing how the ray exits the optics with the position $(w,z)$ the ray will have when it arrives at the sensor. It is possible, of course, to convert these parameters to angular parameters.

$$L_O(u,v,w,z,\lambda,\rho)$$

The data recorded by the sensor is used to create a reproduction of the image, or as the basis for interpreting the image contents. The sensing and perception systems that derive information from the light field are designed to account for which information in the environmental light field makes it all the way through to the optical light field.

::: {.callout-note collapse="true" title="Measuring a light field"}
### Light field sensors {.unnumbered}

The most common sensors in our cameras record only partial information about the optical light field. They rays at a single point on the sensor are summed together, no matter where the ray exited the optics. Rays with different wavelengths are also summed.

By capturing more information about the optical light field, it is possible to do more. This Adelson and XXX pointed out that ... And the Ren Ng's innovative company Lytro developed a sensor and algorithms that captured a high resolution light field and then enabled users to refocus images or change the depth of field after capture. In the chapter on sensors, we will describe ways in which sensors capture a more complete version of the light field.
:::

## Pinholes

Think of light as a collection of rays makes it easy to understand why pinhole optics renders a reasonable image (@fig-ayscough). Some of the properties of the pinhole camera, such as why the image is inverted or how it is blurred, are also well-explained by tracing the rays. The formulation of light as rays, which was proposed by Isaac Newton, is a helpful approximation for many optics calculations.

![Pinhole camera geometry. (A) The rays from a distant, on-axis point arrive at in parallel at the pinhole. Were there no diffraction, they would simply continue on to form an image the size of the pinhole. (B) An off-axis point's rays also arrive in parallel, but they are at an angle to the pinhole aperture. Hence, fewer rays pass through the aperture so the image would be dimmer and smaller. If we can tolerate a relatively large point spread, this design is workable. But if we require a small point spread, diffraction makes this design unworkable.](../images/lightfields/pinholePSF.png){#fig-pinhole-basic fig-align="center" width="70%"}

For example, consider a point (A) relatively far from the pinhole (@fig-pinhole-basic). The rays at A radiate in a wide range of directions, but only a small portion of those rays arrive at the pinhole aperture. Because the pinhole forms a very narrow angle from the point the rays in the incident light field will be parallel to one another (collimated). According to the ray model of light, this collection of rays will pass straight through the pinhole and form an image on the recording surface (e.g., Leonardo's wall). The image formed by such a point is called the *point spread function*. For the on-axis point A, the point spread size is equal to the size of the pinhole aperture.

Consider an off-axis point, (B). Again, its rays will spread in many directions but only a narrow, collimated subset of rays will arrive at the aperture. Because of the angle between the collimated rays and the pinhole, a smaller fraction of the rays will pass through the pinhole aperture. The point spread from (B) will be smaller and dimmer. If we can tolerate the reduced intensity and we do not mind the blurring from the pinhole, we will have a satisfactory image.

It is common to simulate this ray tracing using computer graphics. In @fig-pinhole-chessset I rendered a chess set scene with the pieces, each a few cm tall, positioned about 0.5 meters from the pinhole camera. The four pictures were rendered using different pinhole diameters. The upper left allows only one ray through from each location in the scene. The aperture diameters for the next three pictures are for increasing pinhole sizes. The images are brighter as the pinhole size increases, of course. For this scene geometry the image at the two smaller pinhole sizes are tolerable. Depending on your viewing distance from the screen or page, even the third pinhole image might be usable. But the image with image with the largests pinhole loses so much information that the individual pieces blur together.

![A simulated scene imaged with a pinhole camera. The size of the pinhole diameter increases, with the smallest in the upper left, and increasing (upper left to lower right). The relative intensity is preserved in the renderings so that the scene becomes brighter as the pinhole diameter increases. Rendered with [@pharrPBRTVersion4] and [@iset3d2022].](../images/lightfields/01-pinhole-chessset.png){#fig-pinhole-chessset width="80%" fig-align="center"}

There are many ways to explore the impact of varying the pinhole size. We might adjust the distance of the image plane from the pinhole, or we might change the field of view of the scene. If you are new to reasoning about optics, some of the calculations in this ISETCam script may help you develop an intuition for these parameters.

::: text-center
Some code here?
:::

## Diffraction

The ray trace analysis of light passing through a pinhole is only half right. It is true that increasing the pinhole diameter blurs the image; but the ray tracing analysis fails dramatically in other ways. Consider the simple prediction of the ray model in @fig-ray-wave (A). The rays from a point source at a distance will be nearly parallel. They will pass through a pinhole, continuing along in a straight line. A person to the side of the pinhole would see it as dark because no rays are heading to his eyes. This simple demonstration proves that the rays do not simply continue on a straight line.

![Grimaldi (Physico-mathesis de lumine, 1666) described introducing a pencil of light into a dark room through a hole. The illuminated circle was larger than it should have been; Newton repeated the experiment but stuck to the ray theory (A History of Physics, Cajori, p. 88-89).](../images/lightfields/01-ray-wave.png){#fig-ray-wave width="80%" fig-align="center"}

The Dutch scientist Christiaan Huygens, working at the same time as Newton, suggested an alternative model: he proposed modeling light as a wave. On his theory, the light emitted from a point source would expand as a spherical wave in all directions. At each moment in time the expansion, called the wavefront, could be considered as an array of points that each emitted a secondary wave. This model leads to many quantitative predictions, including how light will be reflected from a mirror and how the light direction will change as the wave passes from one medium (air) into a second medium (glass) (@fig-huygens-waves).

![Light modeled as a wavefront. (A) A point source is shown emitting a circular (or 3D, spherical) wave. Each point on the expanding wavefront emits a new spherical wave. The leading edge of these waves reinforce one another to form the expanding wavefront. (B) At a large distance from the source, the small part of the wavefront appears to be a plane wave. Again, each point on the plane emits a spherical wavefront. These are aligned and the wavefront remains planar. (C) When the wavefront arrives at a small aperture only a small part of the wave passes through. This is like the original source, revealing the spherical wavefront from that point.](../images/lightfields/01-huygens-wave.png){#fig-huygens-waves fig-cap-location="bottom" width="80%" fig-align="center"}

A consequence of the wave theory is that as the pinhole diameter decreases, the spherical wave dominates and causes the image to blur. This effect, called diffraction by Grimaldi in 1655, is very small when the pinhole is large compared to wavelength of the light. But when the pinhole diameter is small, we see the spherical wavefront (@fig-huygens-waves).

A typical image is formed from many different plane waves (rays), arriving from many different directions. The light from each of these plane waves is turned into a spherical wave at the pinhole. This explains why the image is blurred when the pinhole diameter becomes small.

::: {.callout-note collapse="true" title="What took us so long?"}
### Rays to waves

The shift from Newton's ray theory to Huygen's wave theory began following Thomas Young's demonstration to the Royal Society of the interference pattern created by light passing through two, coherent, nearby sources.

[My reading of the history.](../resources/optics-diffraction.html).

A lot about hero worship. A little about simplicity.
:::

### Calculating the two blurs

The numbers are in one of those YouTube videos. I sure hope I saved it!

The following might be a separate section about the history. For the main book we might simply build the calculation of blurring as per the YouTube video.

## Footnotes