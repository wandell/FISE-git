# Sensors {#sec-sensors}

We have been considering the properties of the CMOS imager one pixel at a time. In this section, we consider properties of the array, such as variations between pixels, the color filter array, and the microlens array. First, I describe the basic principles—the foundations—of sensors. Then, I describe recent innovations in pixel and sensor circuitry, which is an active and fascinating field.

## Sensor noise {#sec-sensor-noise}

When manufacturing an array comprising millions of pixels, there will be minor differences between them. For example, the area of the photodiode might differ, or the capacitance of the floating diffusion node, or perhaps the material doping concentration. There is not a lot of variation—foundries are amazing—but there will be some. Because these are variations between the pixels, they are called **sensor noise**. Some of these variations do not change over time; for example, the relative photodiode area of different pixels. Thus, these variations are called **fixed pattern noise (FPN)**.

![Sensor fixed pattern noise. On average, the pixel response is linear, but manufacturing variations introduce slight differences in the offset and slope. Dark signal nonuniformities (DSNU) are variations in the offset. These can be due to different amounts of dark current or unaccounted for bias in the reset noise. Photoresponse nonuniformities (PRNU) are variations in the slope of the response. These can be due to differences in the photodiode area or in the transparency of the path from the imaging lens to the pixel.](images/sensors/03-sensor-noise.png){#fig-sensor-noise width="60%"}

### Dark signal nonuniformity {#sec-sensor-dsnu}

**Dark signal nonuniformity (DSNU)** refers to the variation in the dark current across the pixels. Some pixels may accumulate dark current at slightly different rates. As a result, when we consider their response to increasing amounts of light, they will start at slightly different levels. This is illustrated in the variation of the starting positions at zero illumination level in Figure @fig-sensor-noise. Recall that the pixel dark current depends on several factors, including temperature and exposure time (@sec-pixel-darkcurrent). Hence, the DSNU will also depend on these factors.

### Photoresponse nonuniformity {#sec-sensor-prnu}

The **photoresponse nonuniformity (PRNU)** refers to the variation in the slope (gain) of the pixel responses (@fig-sensor-noise). There are various reasons why the response curve might differ between pixels. The photodiode area might be slightly larger in one pixel than another, or the effectiveness of the microlens at focusing the light on the photodiode might be slightly different. Also, the media, such as the color filter, may differ slightly between two pixels of the same type.

## Evolution of pixel structures {#sec-sensor-evolution}

The original sensor stack is called front-side illumination (FSI), in which multiple metal layers are located above the semiconductor substrate (@fig-pixel-layers). Even at the time of the original implementation, it was clear that forcing light to pass through the metal layers to reach the light-sensitive photodiode created many problems. Additionally, both the photodiodes and their associated circuitry share the same substrate, so that the circuitry reduced the area of the photodiode.

<!--# https://www.pcmag.com/how-to/whats-the-difference-between-cmos-bsi-cmos-and-stacked-cmos 
https://news.skhynix.com/evolution-of-pixel-technology-in-cmos-image-sensor
-->

:::{.callout-note title="Timeline of Pixel Evolution" collapse="true"}

The evolution of image sensor pixel technology has enabled dramatic improvements in image quality, sensitivity, and device functionality. Key milestones include:

<!--
| Technology                | Commercialization | Key Benefit                                 |
|---------------------------|------------------|----------------------------------------------|
| **Front-Side Illumination (FSI)**   | 1970s–2000s      | Simplicity, but limited by wiring obstruction|
| **Back-Side Illumination (BSI)**    | ~2007–2010       | Higher QE, better low-light, smaller pixels  |
| **Deep Trench Isolation (DTI)**     | ~2010s           | Reduced crosstalk, higher pixel density      |
| **Deep Photodiode**                 | ~2015+           | Higher sensitivity, dynamic range            |
| **Stacked Sensor**                  | ~2015+           | Advanced features, compact design            |


- **Front-Side Illumination (FSI):** Early standard; light passes through wiring before reaching the photodiode.
- **Back-Side Illumination (BSI):** Introduced ~2010; light enters from the back, improving efficiency for small pixels.
- **Deep Trench Isolation (DTI):** Early 2010s; insulating trenches reduce crosstalk between pixels.
- **Deep Photodiode:** Mid-2010s; deeper photodiodes increase sensitivity and dynamic range.
- **Stacked Sensor:** Mid-2010s onward; sensor and circuitry are stacked for advanced features and compact design.
-->

- **Front-Side Illumination (FSI) (1970s–2000s)**
    - Standard architecture for early CCD and CMOS sensors.
    - Light enters from the front, passing through metal wiring and transistors before reaching the photodiode.
    - Wiring blocks some light, reducing quantum efficiency, especially as pixel sizes shrink.

- **Back-Side Illumination (BSI) (Late 2000s, commercialization ~2007–2010)**
    - Sensor wafer is thinned and flipped so light enters from the back, directly reaching the photodiode.
    - Dramatically improves quantum efficiency, especially for small pixels.
    - First widely adopted in smartphone cameras (e.g., iPhone 4 in 2010).

- **Deep Trench Isolation (DTI) (Early 2010s)**
    - Introduced to reduce electrical and optical crosstalk between pixels.
    - Trenches filled with insulating material are etched between pixels, confining photo-generated electrons.
    - Enables higher pixel density and better color fidelity.

- **Deep Photodiode (Mid 2010s)**
    - Photodiodes are engineered to extend deeper into the silicon substrate.
    - Increases the volume for photon absorption, improving sensitivity and full-well capacity.
    - Particularly beneficial for small pixels and high dynamic range imaging.

- **Stacked Sensor (3D Stacking) (Mid–Late 2010s)**
    - Sensor and circuitry are fabricated on separate silicon layers and then stacked (bonded) together.
    - Allows for more complex circuitry (e.g., faster readout, on-chip memory, AI processing) without sacrificing pixel area.
    - Now common in high-end smartphone and professional sensors.
:::

### Back-Side illuminated (BSI) sensor {#sec-sensor-bsi}
After years of incremental improvements, engineers developed a new architecture to overcome the limitations of front-side illuminated (FSI) sensors. The resulting design, known as **Back-Side illuminated (BSI)** CMOS, rearranges the device layers to improve sensitivity. In BSI sensors, the photodiode and wiring layers are flipped; the light no longer needs to pass through the wires.  

![The back-side illuminated pixel. (a) The front-side illuminated pixel has metal lines above the silicon substrate. (b) The back side illuminated sensor flips the metal lines and silicon substrate.  Source: <a href="https://scientificimaging.com/knowledge-base/front-side-illuminated-and-back-side-illuminated-imagers" target="_blank">Scientific Imaging.</a>](images/sensors/03-sensor-bsi2.png){#fig-sensor-bsi2 width="60%"}

In addition to flipping the order of the silicon and wires, it was necessary to change the thickness of the substrate (@fig-sensor-bsi2). In the FSI pixel, the photodiode is embedded in a relatively thick layer of silicon. Flipping this structure would require the light to pass through a substantial amount of silicon prior to reaching the photodiode. This is problematic because the silicon absorbs light, and particularly short-wavelength light (@sec-sensor-wavelength). Thus, it was necessary to find a way to thin the silicon substrate and still have a working photodiode and circuitry. 

::: {#sensor-bsi .callout-note collapse="true"}
## BSI technology development

[Wikipedia article](https://en.wikipedia.org/wiki/Back-illuminated_sensor)

Eric Fossum first proposed the concept of back-side illuminated (BSI) CMOS image sensors in 1994, recognizing the need to reduce metal line obstruction and improve sensitivity. However, it took more than a decade for the technology to mature and reach commercial viability. OmniVision Technologies was among the first to develop BSI sensors, introducing the OV8810 (1.4 μm pixels, 8 megapixels) in September 2008. Sony followed soon after, launching the first widely adopted BSI sensors in 2009.

```{=html}
<!-- Read this.  Evolution of stacked and BI.
@Oike2022-stackedevolution -->
```
:::

### Deep photodiodes {#sec-sensor-deepdiode}
@fig-sensor-skhynix illustrates a further improvement in the BSI design. SK Hynix, Omnivision and other manufacturers recognized that simply flipping the sensor (BSI) improved light capture. But as pixel sizes continued to shrink there was relatively little ability to store electrons.  Thus the well capacity was reduced and which limited the pixel's dynamic range (@sec-pixel-wellcapacity). Moreover, the sensitivity to longer wavelengths, which penetrates deeper into silicon, became a challenge (@sec-sensor-wavelength). 

To overcome these limitations, many vendors invented methods to build photodiodes that extend significantly deeper into the silicon (deep photodiode technology). Increasing the volume of the photodiode has two benefits.  First, the photodiode can gather more charge before saturating.  Second, the photodiode has higher long-wavelength sensitivity. 

![Deep photodiodes in the BSI. (a) A super pixel of a front-side illuminated CMOS sensor, including the microlenses, color filters, metal lines, and silicon. (b) A back-side illuminated super pixel with deep photodiode technology. Source: <a href="https://news.skhynix.com/evolution-of-pixel-technology-in-cmos-image-sensor/" target="_blank">Skhynix.</a>](images/sensors/03-sensor-bsi.png){#fig-sensor-skhynix width="70%"}

### Stacked sensor {#sec-sensor-stacked}
In the original planar technology, photodiodes are adjacent to the transistor circuits within the silicon substrate. Thus, photodiodes must compete for space with the transistors.  Reducing the photodiode area means less light efficiency; reducing the transistor size means noisier performance. Youse pays yer money and yer makes yer choice.

The **stacked sensor** design directly addresses the trade-off between the light collection area (fill factor) and the necessary space for readout circuitry. By relocating the processing circuitry to a separate, underlying layer, the stacked structure maximizes the light-sensitive area for a given pixel size, allowing for more efficient photon capture.Furthermore, this separation permits the use of larger, more stable transistors within the logic layer, which significantly contributes to noise reduction and improved imaging properties.

![Pixels in the stacked sensor are based on die stacking technology. Several layers of silicon that are bonded and connected using Through-Silicon Via (TSV) technology. The technology began with 2-layers, but when have you ever known Silicon Valley to stop?](images/sensors/03-pixel-stacked.png){#fig-sensor-stacked width="50%"}

**Stacked sensors** were the next advance, following BSI. The new innovation was finding a method to place the photodiode and its circuitry on different layers. The key technology is called **die stacking**. The different layers of silicon are bonded together and connected electrically with **Through-Silicon Vias (TSVs)**. As you can imagine, with the features being so small, this is a challenging technology.

Placing the photodiode and circuitry on different layers means they no longer compete for space.  This approach both improves light sensitivity and also establishes the foundation for stacking additional processing circuitry beneath the photodiode layer. 


<!-- Stacked Pixel Structures Google doc 
A significant step towards multi-layer integration was taken by Mitsumasa Koyanagi of Tohoku University, who pioneered the technique of wafer-to-wafer bonding with TSV in 1989
-->


::: {.callout-note title="Stacked sensor applications" collapse="true"}

This stacked sensor expanded the capabilities of image sensors well beyond basic light capture, enabling advanced features such as high dynamic range (HDR) video, faster frame rates, and the integration of on-chip artificial intelligence (AI) processing.

Stacked CMOS chips improve upon the BSI CMOS concept. They place components in a similar arrangement, but the design also stacks the image signal processor and its ultra-fast DRAM memory into the same silicon. This makes readout speeds even faster. The first mainstream Stacked CMOS camera, the Sony a9 from 2019, made waves by offering an interruption-free photographic experience—you can use it to fire off photos at 20fps without losing view of your scene.

:::

## Sensor components: Space {#sec-sensor-space}
Conventional CMOS imagers include additional components that are critical for producing high quality images (@fig-pixel-overview). The microlens array and deep trench isolation both increase sensitivity and reduce unwanted crosstalk between the pixels. Spectral filters, which are included in sensors for consumer photography and many other types of cameras, are used to captur information about the scene spectral radiance. 

### Microlens arrays {#sec-sensor-microlens}

The microlens array is a layer of tiny lenses placed above the color filter array. These lenses serve to redirect the light from the main lens onto the photodiode, increasing the light-gathering efficiency. The microlens also reduces **pixel cross-talk**. This refers to the case in which light from the imaging lens might arrive at a relatively large angle at the color filter. A ray may end up generating electrons in a pixel that has a different color filter. Over time, microlens arrays have evolved to support additional functions, such as optimizing focus and enabling advanced imaging techniques for light field capture that I describe later.

![Electron micrograph cross section illustrating the position of the color filters and microlenses above the photodiode (from @elgamalCMOSImageSensors2005)](images/sensors/03-microlens-cfa-SEM.png){#fig-components-microlens width="60%"}

At the center of the sensor array, the incidence of the chief ray from the center of the lens is close to perpendicular to the photodiode array. There, a lens centered above the photodiode works well to simply concentrate the rays. At the edges of the array, however, the angle of the chief ray is relatively steep, say 35 degrees. To redirect the rays toward the photodiode, the microlens and the color filter are shifted laterally. The decentering of the microlens and color filters improves the redirection the rays so that more light arrives at the proper photodiode, and less light is incorrectly absorbed by adjacent photodiodes.

![De-centering the microlens and color filter above photodiodes at the sensor edge)](images/sensors/03-microlens-shifted.png){#fig-components-shiftedmicrolens width="100%"}

The microlens was important for classic, frontside illuminated sensors when the path from the lens through the metal layers was quite long and the angle of the chief ray at the edge was fairly large. For large pixels, and the initial sensor pixels were 6 or more microns, microlens technology was quite effective.
@Hwang2023-microlens-alignment @Hwang2023-microlens-quad

### Deep trench isolation {#sec-sensor-dti}
As pixel sizes shrank to around 1.5 microns or less, diffraction effects became more significant. Light entering these tiny pixel apertures would spread out, causing photons to be absorbed not only by the intended photodiode but also by neighboring ones. This led to two main issues: reduced image sharpness due to optical blur, and decreased color accuracy because light passing through, for example, a green filter could generate electrons in a photodiode beneath a red filter.

**Deep trench isolation (DTI)** technology addresses these challenges. Engineers introduced narrow trenches at the boundaries between pixels and filled them with insulating materials such as silicon dioxide or silicon nitride (@fig-sensor-deeptrench). These trenches act as barriers, preventing light from spreading laterally between adjacent pixels. As a result, DTI improves both spatial resolution and color fidelity in modern image sensors.

![Deep trench isolation revealed by Chipworks from a 8MP Samsung ISOCELL imager in the Galaxy S5 (2014). Light passes through color filters (bottom) to the silicon photodiodes. They are separated by deep, poly-filled trenches in the substrate. Source: <a href="https://image-sensors-world.blogspot.com/2014/04/chipworks-shows-samsung-isocell-cross.html" target="_blank">Image Sensors World, April 1, 2014</a>](images/sensors/03-sensor-deeptrench.png){#fig-sensor-deeptrench width="70%"}

@Han2020-deeptrench
@Tournier2011-deeptrench
@Park2007-deeptrench -->

## Sensor components: Wavelength {#sec-sensor-wavelength}
Silicon-based CMOS image sensors are sensitive to photons with wavelengths up to about 1100 nm. In contrast, the human eye detects only a narrower range, from roughly 380 nm to 700 nm. For consumer cameras, the goal is to capture images that look natural to people, so sensors are designed to record light mainly within the visible spectrum. Since human vision uses three types of broadband photoreceptors, most cameras only need to capture three broad spectral bands.

To achieve this, image sensors use two types of spectral filters: one to block unwanted infrared (IR) and ultraviolet (UV) light, and another to divide the visible spectrum into three color channels. The resulting data are processed to produce images that closely match human color perception. In this section, I describe these filters. After reviewing the relevant properties of human vision (@sec-human), I explain how sensor color channels are processed to render accurate or visually pleasing images (@sec-imgprocessing).

### Spectral blocking filter {#sec-sensor-irfilter}
Most consumer cameras include a spectral filter that blocks wavelengths longer than 680–700 nm (IR) and shorter than 380–400 nm (UV) from reaching the photodiode. These thin **UV/IR blocking filters** are placed above the microlenses on the sensor (@fig-sensor-uvir)[^irfilters].

[^irfilters]: Although these filters block both UV and IR, they are often called **IR blocking filters**. This may be because the glass in the optics already blocks much of the UV light.

![The UV/IR blocking filter is placed above the microlens. These filters allow only visible light to reach the photodiodes. Source: <a href="https://commons.wikimedia.org/wiki/File:Removed_EOS_350D_IR-blocking_filter.jpg" target="_blank">Removed EOS 350D IR-blocking filter</a>](images/sensors/03-sensor-uvirfilter.png){#fig-sensor-uvir width="80%"} 

Early digital cameras often included an additional laminated layer called an **optical low pass filter (OLPF)**, which slightly blurred the image. The OLPF ensured that light from a point in the scene would be spread over a small area (such as a 2x2 pixel region). This blur was useful because adjacent pixels have different color filters, but their outputs are combined and treated as coming from a single point in the scene. In recent cameras, pixel sizes have become so small that diffraction already spreads the light across multiple pixels (@fig-blur-comparison), making the OLPF unnecessary.

### Color filter array {#sec-sensor-cfa}
Below the UV/IR blocking filter, color cameras have a **color filter array (CFA)** (@fig-pixel-layers). Typically, the CFA consists of three types of filters arranged in a repeating pattern. The smallest repeating unit is called a **super pixel**. Pixels behind each filter type form a mosaic that samples the image, and together these mosaics create three interleaved **color channels**.

For consumer photography, the most common CFA pattern is the Bayer pattern, which uses two green filters, one red, and one blue in each super pixel (@fig-cfa-pattern). This arrangement is named after its inventor (@bayerColorImagingArray1976).

![Bayer RGB pattern. Source: <a href="https://www.nature.com/articles/s41467-022-31019-7/figures/1" target="_blank">Pixel-level Bayer-type colour router based on metasurfaces, Zou et al. 2022</a>](images/sensors/03-sensor-cfapattern.png){#fig-cfa-pattern width="70%" fig-align="center"}

::: {#sensor-bayer .callout-note collapse="true"}
## The Bayer pattern
Say something about human vision motivation.

Also, isn't there another Bryce Bayer related patent?  Maybe for displays or printers or something?

:::

The data from these mosaics are combined to create a full-color image with red, green, and blue values at every pixel location. This process is called **demosaicking**. Demosaicking is especially important when images are coarsely sampled. For modern sensors with many small pixels—where the entire super pixel fits within the lens point spread function—demosaicking becomes less critical.

@fig-cfa-spectral shows the spectral transmission of three color filters from a consumer camera. Different spectral filters and physical methods are used in other applications, such as medical, scientific, or industrial cameras.

![Spectral sensitivity of the color filters alone. Notice that the red filter passes long-wavelength light (IR). Light beyond 680 nm, however, does not reach the photodiode because it is blocked by the UV/IR filters. Source: <a href="https://evidentscientific.com/en/microscope-resource/knowledge-hub/digital-imaging/cmosimagesensors" target="_blank">Evident Scientific.</a>](images/sensors/03-sensor-cfaspectral.png){#fig-cfa-spectral width="70%" fig-align="center"}

The spectral sensitivity of a color channel depends on more than just the color filter. Light must pass through the lens, the UV/IR filter, the microlens, the color filter, and finally be absorbed in the silicon. Each component affects the probability that a photon is absorbed, and their combined effect determines the overall spectral responsivity of the color channel.

![Spectral sensitivity of the color channels in a typical modern sensor.](images/sensors/03-sensor-channel-spectral.png){#fig-channel-spectral width="70%" fig-align="center"}


<!--  -->

 