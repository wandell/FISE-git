# Pixel arrays {#sec-pixel-arrays}
The discussion so far has focused on the properties of CMOS imagers at the level of individual pixels. This section shifts the perspective to the pixel array as a whole, exploring how pixel-to-pixel variations, the color filter array, and the microlens array influence overall performance. It begins with the foundational principles of sensor operation and then highlights recent innovations in pixel and sensor circuitry—a rapidly evolving and fascinating field. These array-level effects can limit image quality, especially in low-light or high-dynamic-range scenes.

## Array noise {#sec-array-noise}

When manufacturing an array comprising millions of pixels, there will be minor differences between them. For example, the area of the photodiode might differ, or the capacitance of the floating diffusion node, or perhaps the material doping concentration. Although modern foundries achieve remarkable uniformity, some variation is inevitable. These are variations between pixels and are often referred to as **sensor noise**. Some of these sensor variations do not change over time; for example, the relative photodiode area of different pixels. These variations across the sensor are called **fixed pattern noise (FPN)**.

![Sensor fixed pattern noise. On average, the pixel response is linear, but manufacturing variations introduce slight differences in the offset and slope. Dark signal nonuniformities (DSNU) are variations in the offset. These can be due to different amounts of dark current or unaccounted for bias in the reset noise. Photoresponse nonuniformities (PRNU) are variations in the slope of the response. These can be due to differences in the photodiode area or in the transparency of the path from the imaging lens to the pixel.](images/sensors/03-sensor-noise.png){#fig-sensor-noise width="60%"}

### Dark signal nonuniformity {#sec-sensor-dsnu}

**Dark signal nonuniformity (DSNU)** refers to the variation in the dark current across the pixels. Some pixels may accumulate dark current at slightly different rates. As a result, when we consider their response to increasing amounts of light, they will start at slightly different levels. This is illustrated in the variation of the starting positions at zero illumination level in Figure @fig-sensor-noise. Recall that the pixel dark current depends on several factors, including temperature and exposure time (@sec-pixel-darkcurrent). Hence, the DSNU will also depend on these factors.

### Photoresponse nonuniformity {#sec-sensor-prnu}

The **photoresponse nonuniformity (PRNU)** refers to the variation in the slope (gain) of the pixel responses (@fig-sensor-noise). There are various reasons why the response curve might differ between pixels. The photodiode area might be slightly larger in one pixel than another, or the effectiveness of the microlens at focusing the light on the photodiode might be slightly different. Also, the media, such as the color filter, may differ slightly between two pixels of the same type.

## Evolution of pixel structures {#sec-sensor-evolution}

There has been a large engineering effort to reduce various sources of noise, including temporal and fixed pattern noise, as well as other limitations in the sensor design relating to color and dynamic range. The engineering effort was supported by the enormous demand for CMOS sensors. In 2020, approximately 6.5 billion sensors were shipped.  In 2025, we expect about 13 billion sensors to be produced. That's a lot of sensors and the demand motivates a lot of engineering. This section covers the evolution of the pixel and sensor design.

The sensor architecture I have described is known as **front-side illumination (FSI)**. The architecture has the obvious challenge of placing multiple metal layers above the semiconductor substrate (@fig-pixel-layers). Even at the time of the original implementation, it was clear that requiring light to pass through the metal layers to reach the light-sensitive photodiode created many problems. 

Additionally, both the photodiodes and their associated circuitry share the same substrate. Requiring the circuitry and photodiode to share space reduced the sensitivity the sensor. Finally, the original pixels were fairly large, $6 \mu \text{m}$ or more. As technology scaled and pixels became smaller -many CMOS imagers have pixels as small as $0.8 \mu \text{m}$ a reduction in area of $50x$- additional problems arose. Different opportunities arose as well.

These challenges, and the huge success of CMOS sensors, were met by a massive, world-wide, engineering effort to improve the system. The evolution of image sensor pixel technology that came from this effort enabled dramatic improvements in image quality, sensitivity, and device functionality. The following sections describe the key milestones that were achieved through academic and industry partnerships.

<!--# https://www.pcmag.com/how-to/whats-the-difference-between-cmos-bsi-cmos-and-stacked-cmos 
https://news.skhynix.com/evolution-of-pixel-technology-in-cmos-image-sensor
-->

:::{.callout-note title="Pixel Evolution timeline" collapse="true"}
Here is a timeline for the key milestones achieved in the sections below.

| Technology                | Commercialization | Key Benefit                                 |
|---------------------------|------------------|----------------------------------------------|
| **Front-Side Illumination (FSI)**   | 1990s–2000s      | Simplicity, but limited by wiring obstruction|
| **Back-Side Illumination (BSI)**    | ~2007–2010       | Higher QE, better low-light, smaller pixels  |
| **Deep Trench Isolation (DTI)**     | ~2010s           | Reduced crosstalk, higher pixel density      |
| **Deep Photodiode**                 | ~2015+           | Higher sensitivity, dynamic range            |
| **Stacked Sensor**                  | ~2015+           | Advanced features, compact design            |

<!--
- **Front-Side Illumination (FSI):** Early standard; light passes through wiring before reaching the photodiode.
- **Back-Side Illumination (BSI):** Introduced ~2010; light enters from the back, improving efficiency for small pixels.
- **Deep Trench Isolation (DTI):** Early 2010s; insulating trenches reduce crosstalk between pixels.
- **Deep Photodiode:** Mid-2010s; deeper photodiodes increase sensitivity and dynamic range.
- **Stacked Sensor:** Mid-2010s onward; sensor and circuitry are stacked for advanced features and compact design.


- **Front-Side Illumination (FSI) (1970s–2000s)**
    - Standard architecture for early CCD and CMOS sensors.
    - Light enters from the front, passing through metal wiring and transistors before reaching the photodiode.
    - Wiring blocks some light, reducing quantum efficiency, especially as pixel sizes shrink.

- **Back-Side Illumination (BSI) (Late 2000s, commercialization ~2007–2010)**
    - Sensor wafer is thinned and flipped so light enters from the back, directly reaching the photodiode.
    - Dramatically improves quantum efficiency, especially for small pixels.
    - First widely adopted in smartphone cameras (e.g., iPhone 4 in 2010).

- **Deep Trench Isolation (DTI) (Early 2010s)**
    - Introduced to reduce electrical and optical crosstalk between pixels.
    - Trenches filled with insulating material are etched between pixels, confining photo-generated electrons.
    - Enables higher pixel density and better color fidelity.

- **Deep Photodiode (Mid 2010s)**
    - Photodiodes are engineered to extend deeper into the silicon substrate.
    - Increases the volume for photon absorption, improving sensitivity and full-well capacity.
    - Particularly beneficial for small pixels and high dynamic range imaging.

- **Stacked Sensor (3D Stacking) (Mid–Late 2010s)**
    - Sensor and circuitry are fabricated on separate silicon layers and then stacked (bonded) together.
    - Allows for more complex circuitry (e.g., faster readout, on-chip memory, AI processing) without sacrificing pixel area.
    - Now common in high-end smartphone and professional sensors.
  
  -->
:::

### Back-Side illuminated (BSI) sensor {#sec-sensor-bsi}
After years of incremental improvements, engineers developed a new architecture to overcome the limitations of front-side illuminated (FSI) sensors. The resulting design, known as **Back-Side illuminated (BSI)** CMOS, rearranges the device layers to improve sensitivity. In BSI sensors, the photodiode and wiring layers are flipped; the light no longer needs to pass through the wires.  

![The back-side illuminated pixel. (a) The front-side illuminated pixel has metal lines above the silicon substrate. (b) The back side illuminated sensor flips the metal lines and silicon substrate.  Source: <a href="https://scientificimaging.com/knowledge-base/front-side-illuminated-and-back-side-illuminated-imagers" target="_blank">Scientific Imaging.</a>](images/sensors/03-sensor-bsi2.png){#fig-sensor-bsi2 width="60%"}

In addition to flipping the order of the silicon and wires, it was necessary to change the thickness of the substrate (@fig-sensor-bsi2). In the FSI pixel, the photodiode is embedded in a relatively thick layer of silicon. Flipping this structure would require the light to pass through a substantial amount of silicon prior to reaching the photodiode. This is problematic because the silicon absorbs light, and particularly short-wavelength light (@sec-sensor-wavelength). Thus, it was necessary to find a way to thin the silicon substrate and still have a working photodiode and circuitry. 

::: {#sensor-bsi .callout-note collapse="true" title="BSI technology development"}

[Wikipedia article](https://en.wikipedia.org/wiki/Back-illuminated_sensor)

The concept of back-side illuminated (BSI) CMOS image sensors was first proposed by Eric Fossum in 1994 to address the limitations caused by metal wiring obstructing incoming light and to improve sensitivity. While the benefits of rearranging the sensor layers were clear to many, developing a practical manufacturing process proved challenging. It took more than a decade for BSI technology to become commercially viable. OmniVision Technologies was among the first to introduce a commercial BSI sensor, releasing the OV8810 (1.4 μm pixels, 8 megapixels) in September 2008. Sony soon followed, launching the first widely adopted BSI sensors in 2009.

```{=html}
<!-- Read this.  Evolution of stacked and BI.
@Oike2022-stackedevolution -->
```
:::

### Deep photodiodes {#sec-sensor-deepdiode}
@fig-sensor-skhynix illustrates a further improvement in the BSI design. SK Hynix, Omnivision and other manufacturers recognized that simply flipping the sensor (BSI) improved light capture. But as pixel sizes continued to shrink there was relatively little ability to store electrons.  Thus the well capacity was reduced and which limited the pixel's dynamic range (@sec-pixel-wellcapacity). Moreover, the sensitivity to longer wavelengths, which penetrates deeper into silicon, became a challenge (@sec-sensor-wavelength). 

To overcome these limitations, many vendors invented methods to build photodiodes that extend significantly deeper into the silicon (deep photodiode technology). Increasing the volume of the photodiode has two benefits.  First, the photodiode can gather more charge before saturating.  Second, the photodiode has higher long-wavelength sensitivity. 

![Deep photodiodes in the BSI. (a) A super pixel of a front-side illuminated CMOS sensor, including the microlenses, color filters, metal lines, and silicon. (b) A back-side illuminated super pixel with deep photodiode technology. Source: <a href="https://news.skhynix.com/evolution-of-pixel-technology-in-cmos-image-sensor/" target="_blank">Skhynix.</a>](images/sensors/03-sensor-bsi.png){#fig-sensor-skhynix width="70%"}

### Stacked sensor {#sec-sensor-stacked}
In the original planar technology, photodiodes are adjacent to the transistor circuits within the silicon substrate. Thus, photodiodes must compete for space with the transistors.  Reducing the photodiode area means less light efficiency; reducing the transistor size means noisier performance. Youse pays yer money and yer makes yer choice.

The **stacked sensor** design directly addresses the trade-off between the light collection area (fill factor) and the necessary space for readout circuitry. By relocating the processing circuitry to a separate, underlying layer, the stacked structure maximizes the light-sensitive area for a given pixel size, allowing for more efficient photon capture.Furthermore, this separation permits the use of larger, more stable transistors within the logic layer, which significantly contributes to noise reduction and improved imaging properties.

![Pixels in the stacked sensor are based on die stacking technology. Several layers of silicon that are bonded and connected using Through-Silicon Via (TSV) technology. The technology began with 2-layers, but when have you ever known Silicon Valley to stop?](images/sensors/03-pixel-stacked.png){#fig-sensor-stacked width="50%"}

**Stacked sensors** were the next advance, following BSI. The new innovation was finding a method to place the photodiode and its circuitry on different layers. The key technology is called **die stacking**. The different layers of silicon are bonded together and connected electrically with **Through-Silicon Vias (TSVs)**. As you can imagine, with the features being so small, this is a challenging technology.

Placing the photodiode and circuitry on different layers means they no longer compete for space.  This approach both improves light sensitivity and also establishes the foundation for stacking additional processing circuitry beneath the photodiode layer. 

<!-- Stacked Sensor Structures Google doc 
A significant step towards multi-layer integration was taken by Mitsumasa Koyanagi of Tohoku University, who pioneered the technique of wafer-to-wafer bonding with TSV in 1989
-->

::: {.callout-note title="Stacked sensor applications" collapse="true"}
This stacked sensor expanded the capabilities of image sensors well beyond basic light capture, enabling advanced features such as high dynamic range (HDR) video, faster frame rates, and the integration of on-chip artificial intelligence (AI) processing.

Stacked CMOS chips improve upon the BSI CMOS concept. They place components in a similar arrangement, but the design also stacks the image signal processor and its ultra-fast DRAM memory into the same silicon. This makes readout speeds even faster. The first mainstream Stacked CMOS camera, the Sony a9 from 2019, made waves by offering an interruption-free photographic experience—you can use it to fire off photos at 20fps without losing view of your scene.
:::

## Sensor components: Space {#sec-sensor-space}
Conventional CMOS imagers include additional components that are critical for producing high quality images (@fig-pixel-overview). The microlens array and deep trench isolation both increase sensitivity and reduce unwanted crosstalk between the pixels. Spectral filters, which are included in sensors for consumer photography and many other types of cameras, are used to captur information about the scene spectral radiance. 

### Microlens arrays {#sec-sensor-microlens}
The microlens array is a layer of tiny lenses placed above the color filter array. These lenses serve to redirect the light from the main lens onto the photodiode, increasing the light-gathering efficiency. The microlens also reduces **pixel cross-talk**. This refers to the case in which light from the imaging lens might arrive at a relatively large angle at the color filter. A ray may end up generating electrons in a pixel that has a different color filter. Over time, microlens arrays have evolved to support additional functions, such as optimizing focus and enabling advanced imaging techniques for light field capture that I describe later.

![Electron micrograph cross section illustrating the position of the color filters and microlenses above the photodiode (from @elgamalCMOSImageSensors2005)](images/sensors/03-microlens-cfa-SEM.png){#fig-components-microlens width="60%"}

At the center of the sensor array, the incidence of the chief ray from the center of the lens is close to perpendicular to the photodiode array. There, a lens centered above the photodiode works well to simply concentrate the rays. At the edges of the array, however, the angle of the chief ray is relatively steep, say 35 degrees. To redirect the rays toward the photodiode, the microlens and the color filter are shifted laterally. The decentering of the microlens and color filters improves the redirection the rays so that more light arrives at the proper photodiode, and less light is incorrectly absorbed by adjacent photodiodes.

![Geometric logic for de-centering the microlens and color filter above photodiodes at the sensor edges.](images/sensors/03-microlens-shifted.png){#fig-components-shiftedmicrolens width="100%"}

The microlens was important for classic, frontside illuminated sensors when the path from the lens through the metal layers was quite long and the angle of the chief ray at the edge was fairly large. For large pixels, and the initial sensor pixels were 6 or more microns, microlens technology was quite effective.
@Hwang2023-microlens-alignment @Hwang2023-microlens-quad

### Deep trench isolation {#sec-sensor-dti}
As pixel sizes shrank to around 1.5 microns or less, diffraction effects became more significant. Light entering these tiny pixel apertures would spread out, causing photons to be absorbed not only by the intended photodiode but also by neighboring ones. This led to two main issues: reduced image sharpness due to optical blur, and decreased color accuracy because light passing through, for example, a green filter could generate electrons in a photodiode beneath a red filter.

**Deep trench isolation (DTI)** technology addresses these challenges. Engineers introduced narrow trenches at the boundaries between pixels and filled them with insulating materials such as silicon dioxide or silicon nitride (@fig-sensor-deeptrench). These trenches act as barriers, preventing light from spreading laterally between adjacent pixels. As a result, DTI improves both spatial resolution and color fidelity in modern image sensors.

![Deep trench isolation revealed by Chipworks from a 8MP Samsung ISOCELL imager in the Galaxy S5 (2014). Light passes through color filters (bottom) to the silicon photodiodes. They are separated by deep, poly-filled trenches in the substrate. Source: <a href="https://image-sensors-world.blogspot.com/2014/04/chipworks-shows-samsung-isocell-cross.html" target="_blank">Image Sensors World, April 1, 2014</a>](images/sensors/03-sensor-deeptrench.png){#fig-sensor-deeptrench width="70%"}

@Han2020-deeptrench
@Tournier2011-deeptrench
@Park2007-deeptrench -->

## Sensor components: Wavelength {#sec-sensor-wavelength}
Silicon-based CMOS image sensors are sensitive to photons with wavelengths up to about 1100 nm. In contrast, the human eye detects only a narrower range, from roughly 380 nm to 700 nm. For consumer cameras, the goal is to capture images that look natural to people, so sensors are designed to record light mainly within the visible spectrum. Since human vision uses three types of broadband photoreceptors, most cameras only need to capture three broad spectral bands.

To achieve this, image sensors use two types of spectral filters: one to block unwanted infrared (IR) and ultraviolet (UV) light, and another to divide the visible spectrum into three color channels. The resulting data are processed to produce images that closely match human color perception. In this section, I describe these filters. After reviewing the relevant properties of human vision (@sec-human), I explain how sensor color channels are processed to render accurate or visually pleasing images (@sec-imgprocessing).

### Spectral blocking filter {#sec-sensor-irfilter}
Most consumer cameras include a spectral filter that blocks wavelengths longer than 680–700 nm (IR) and shorter than 380–400 nm (UV) from reaching the photodiode. These thin **UV/IR blocking filters** are placed above the microlenses on the sensor (@fig-sensor-uvir)[^irfilters].

[^irfilters]: Although these filters block both UV and IR, they are often called **IR blocking filters**. This may be because the glass in the optics already blocks much of the UV light.

![The UV/IR blocking filter is placed above the microlens. These filters allow only visible light to reach the photodiodes. Source: <a href="https://commons.wikimedia.org/wiki/File:Removed_EOS_350D_IR-blocking_filter.jpg" target="_blank">Removed EOS 350D IR-blocking filter</a>](images/sensors/03-sensor-uvirfilter.png){#fig-sensor-uvir width="80%"} 

Early digital cameras often included an additional laminated layer called an **optical low pass filter (OLPF)**, which slightly blurred the image. The OLPF ensured that light from a point in the scene would be spread over a small area (such as a 2x2 pixel region). This blur was useful because adjacent pixels have different color filters, but their outputs are combined and treated as coming from a single point in the scene. In recent cameras, pixel sizes have become so small that diffraction already spreads the light across multiple pixels (@fig-blur-comparison), making the OLPF unnecessary.

### Color filter array {#sec-sensor-cfa}
Below the UV/IR blocking filter, color cameras have a **color filter array (CFA)** (@fig-pixel-layers). Typically, the CFA consists of three types of filters arranged in a repeating pattern. The smallest repeating unit is called a **super pixel**. Pixels behind each filter type form a mosaic that samples the image, and together these mosaics create three interleaved **color channels**. For consumer photography, the most common CFA pattern is the Bayer pattern [-@bayerColorImagingArray1976], which uses two green filters, one red, and one blue in each super pixel (@fig-cfa-pattern).

![Bayer RGB pattern. Source: <a href="https://www.nature.com/articles/s41467-022-31019-7/figures/1" target="_blank">Pixel-level Bayer-type colour router based on metasurfaces, Zou et al. 2022</a>](images/sensors/03-sensor-cfa-pattern.png){#fig-cfa-pattern width="60%" fig-align="center"}

::: {#sensor-bayer .callout-note collapse="true" title="Bayer pattern"}
<a href="https://en.wikipedia.org/wiki/Bryce_Bayer" target="_blank">Bryce Bayer</a> worked at Kodak Research Labs and invented the Bayer color filter array. 

In the mid-1960s, Bayer’s group worked alongside another team focused on psychophysics and human color perception. Kodak was among the first companies to design and market digital cameras, and he was consulted on how to arrange the color filters.  The Bayer filter configuration was designed to mimic aspects of human vision: specifically, he proposed using twice as many green filters as red or blue, reflecting the human eye’s greater spatial resolution to these wavelengths.

In March 1975, Kodak filed a patent application titled "Color imaging array," naming Bayer as the sole inventor. The patent (U.S. 3,971,065, issued July 1976) became foundational for digital imaging. Kodak typically licensed its patents as a bundle, with the Bayer filter patent being a key part of this portfolio. While it is difficult to assign a specific value to this single patent, it contributed significantly to the value of Kodak’s licensing program, which generated billions of dollars. Despite this, Kodak ultimately struggled to adapt to the digital era (@sec-preface-film).
:::

@fig-cfa-spectral shows the spectral transmission of three color filters from a consumer camera. Over the years, the number of different color filters used in color photography has converged onto a relatively small set with common properties (@tominagaMeasurementEstimationSpectral2021). Different spectral filters and physical methods for sampling the spectrum are used in for other applications, such as medical, scientific, or industrial cameras.

![Spectral transmission profiles typical color filters. Notice that the red filter passes long-wavelength light (IR). But light beyond 680 nm does not reach the photodiode because it is blocked by the UV/IR filter. Source: <a href="https://evidentscientific.com/en/microscope-resource/knowledge-hub/digital-imaging/cmosimagesensors" target="_blank">Evident Scientific.</a>](images/sensors/03-sensor-cfa-spectral.png){#fig-cfa-spectral width="70%" fig-align="center"}

The spectral sensitivity of a color channel depends on the entire light path, not just the color filter. The sensitivity depends on the light transmitted by the lens, the UV/IR filter, the microlens, the color filter, and finally the spectral quantum efficiency of the photodiode. Each of these components can affect the probability that a photon is absorbed; their combined effect determines the overall spectral responsivity of the color channel.

![Spectral quantum efficiency sensitivity of three color channels in a typical modern sensor.  The channel properties are determined by the combination of media in the light path.](images/sensors/03-sensor-channel-spectral.png){#fig-channel-spectral width="70%" fig-align="center"}

 When using a color filter array, we obtain one spectral sample at each position.  To represent a color image, however, we represent three spectral samples at each location.  Thus, we must convert the data from these interleaved mosaics to create a full-color image by assigning a red, green, and blue value at every pixel location. This process, called **demosaicking** can be very useful when a sensor has relatively large pixels.  In that case, the images are coarsely sampled and the image appearance is significantly improved by interpolating the color channels, essentially upsampling the measurements. 
 
 There is a principle we can use when evaluating whether a pixel is 'large'. The imaging lens has a point spread function (@sec-pointspread and @sec-optics-linear), and if its spread is two times larger than the pixel size we consider the pixel small. On the other hand, if the pixel is equal to or larger than the point spread, we consider the pixel large. 

For modern sensors, particularly in consumer imaging, pixel sizes can be quite small, often less than $1~\mu \text{m}$. This pixel size is smaller than the ideal point spread function of a diffraction limited lens (@sec-airy-pattern), and often considerably smaller than the point spread function of the true system. For such sensors many small pixels —and even the entire super pixel- fit within the point spread function. Demosaicking is less critical in this case; we can simply group the measured values of the super pixel into a single point in the RGB image. We will quantify the impact of optics, pixel size and demosaicking for image quality in @sec-imgprocessing.

<!--  -->

