# Sensors {#sec-sensors}

We have been considering the properties of the CMOS imager one pixel at a time. In this section, we consider properties of the array, such as variations between pixels, the color filter array, and the microlens array. First, I describe the basic principles—the foundations—of sensors. Then, I describe recent innovations in pixel and sensor circuitry, which is an active and fascinating field.

## Sensor noise {#sec-sensor-noise}

When manufacturing an array comprising millions of pixels, there will be minor differences between them. For example, the area of the photodiode might differ, or the capacitance of the floating diffusion node, or perhaps the material doping concentration. There is not a lot of variation—foundries are amazing—but there will be some. Because these are variations between the pixels, they are called **sensor noise**. Some of these variations do not change over time; for example, the relative photodiode area of different pixels. Thus, these variations are called **fixed pattern noise (FPN)**.

![Sensor fixed pattern noise. On average, the pixel response is linear, but manufacturing variations introduce slight differences in the offset and slope. Dark signal nonuniformities (DSNU) are variations in the offset. These can be due to different amounts of dark current or unaccounted for bias in the reset noise. Photoresponse nonuniformities (PRNU) are variations in the slope of the response. These can be due to differences in the photodiode area or in the transparency of the path from the imaging lens to the pixel.](images/sensors/03-sensor-noise.png){#fig-sensor-noise width="60%"}

### Dark signal nonuniformity {#sec-sensor-dsnu}

**Dark signal nonuniformity (DSNU)** refers to the variation in the dark current across the pixels. Some pixels may accumulate dark current at slightly different rates. As a result, when we consider their response to increasing amounts of light, they will start at slightly different levels. This is illustrated in the variation of the starting positions at zero illumination level in Figure @fig-sensor-noise. Recall that the pixel dark current depends on several factors, including temperature and exposure time (@sec-pixel-darkcurrent). Hence, the DSNU will also depend on these factors.

### Photoresponse nonuniformity {#sec-sensor-prnu}

The **photoresponse nonuniformity (PRNU)** refers to the variation in the slope (gain) of the pixel responses (@fig-sensor-noise). There are various reasons why the response curve might differ between pixels. The photodiode area might be slightly larger in one pixel than another, or the effectiveness of the microlens at focusing the light on the photodiode might be slightly different. Also, the media, such as the color filter, may differ slightly between two pixels of the same type.

## Evolution of pixel structures {#sec-sensor-evolution}

The original sensor stack is called front-side illumination (FSI), in which multiple metal layers are located above the semiconductor substrate (@fig-pixel-layers). Even at the time of the original implementation, it was clear that forcing light to pass through the metal layers to reach the light-sensitive photodiode created many problems. Additionally, both the photodiodes and their associated circuitry share the same substrate, so that the circuitry reduced the area of the photodiode.

<!--# https://www.pcmag.com/how-to/whats-the-difference-between-cmos-bsi-cmos-and-stacked-cmos 
https://news.skhynix.com/evolution-of-pixel-technology-in-cmos-image-sensor
-->

### Back illuminated (BI) sensor {#sec-sensor-bsi}

After years of incremental improvements, engineers developed a new architecture to overcome the limitations of front-side illuminated (FSI) sensors. The resulting design, known both as **Back Illuminated (BI)** and **Backside illuminated (BSI)** CMOS, rearranges the device layers to improve sensitivity (@fig-sensor-bsi). In BSI sensors, the photodiode and wiring layers are flipped, so the light no longer needs to pass through the wires.  To enable this, notice it was necessary to change the position of the photodiode in the substrate.  If you just flip the wires and silicon, the light would need to pass through the silicon substrate prior to arriving at the photodiode. Thus it was necessary to thin the silicon substrate. This configuration increases the amount of light reaching the photodiode, significantly improving pixel and sensor sensitivity.

![The evolution of pixel designs. (a) The original front-side illuminated CMOS sensor. (b) The back side illuminated sensor. ](images/sensors/03-sensor-bsi.png){#fig-sensor-bsi width="80%"}

The development of BSI CMOS image sensors was closely tied to advances in **die stacking** technology. Manufacturing BSI sensors requires thinning the silicon wafer and, in advanced designs, separating the photodiode and readout circuitry onto different layers. These layers are then bonded together using direct oxide bonding and connected electrically with **Through-Silicon Vias (TSVs)**. This approach not only improved light sensitivity but also established the foundation for stacking more complex readout and processing circuitry beneath the photodiode layer, paving the way for the stacked pixel architectures that followed.

![The evolution of pixel designs. (a) The original front-side illuminated CMOS sensor. (b) The back side illuminated sensor. ](images/sensors/03-sensor-bsi2.png){#fig-sensor-bsi2 width="80%"}


::: {#sensor-bsi-stacked .callout-note collapse="true"}
## BSI technology development

[Wikipedia article](https://en.wikipedia.org/wiki/Back-illuminated_sensor)

Eric Fossum first proposed the concept of back-illuminated (BSI) CMOS image sensors in 1994, recognizing the need to reduce metal line obstruction and improve sensitivity. However, it took more than a decade for the technology to mature and reach commercial viability. OmniVision Technologies was among the first to develop BSI sensors, introducing the OV8810 (1.4 μm pixels, 8 megapixels) in September 2008. Sony followed soon after, launching the first widely adopted BSI sensors in 2009.

```{=html}
<!-- Read this.  Evolution of stacked and BI.
@Oike2022-stackedevolution -->
```
:::

### Stacked sensor {#sec-sensor-stacked}

<!-- Stacked Pixel Structures Google doc 
A significant step towards multi-layer integration was taken by Mitsumasa Koyanagi of Tohoku University, who pioneered the technique of wafer-to-wafer bonding with TSV in 1989
-->

The **stacked pixel** architecture goes further by using multiple semiconductor layers (@fig-sensor-evolution (c)). The development and widespread adoption of Back-Illuminated Sensors (BSI) were not merely parallel advancements but a fundamental prerequisite for the widespread adoption of the stacked pixel structure. BSI technology addressed the critical problem of light blockage by circuitry in traditional planar sensors, physically reorienting the light path. The manufacturing processes involved in BSI, specifically the thinning and flipping of wafers, directly led to the capability and motivation for stacking the photodiode layer on top of a separate logic layer. This created a situation where the optimization of light capture (via BSI) opened the door for a radical architectural change (stacking), demonstrating a profound causal link between these two innovations.


The stacked design directly addresses inherent limitations of traditional planar CMOS designs, particularly the persistent trade-off between the light collection area (fill factor) and the necessary space for readout circuitry. In conventional small pixels, the integration of extensive circuitry on the same plane as the photodiode often leads to higher source follower noise and a diminished fill factor, negatively impacting dynamic range and signal-to-noise ratio.11 By relocating the processing circuitry to a separate, underlying layer, the stacked structure maximizes the light-sensitive area for a given pixel size, allowing for more efficient photon capture.1 Furthermore, this separation permits the use of larger, more stable transistors within the logic layer, which significantly contributes to noise reduction and improved imaging properties.4 This architectural flexibility has expanded the capabilities of image sensors well beyond basic light capture, enabling advanced features such as high dynamic range (HDR) video, faster frame rates, and the integration of on-chip artificial intelligence (AI) processing.1


Stacked CMOS chips improve upon the BSI CMOS concept. They place components in a similar arrangement, but the design also stacks the image signal processor and its ultra-fast DRAM memory into the same silicon. This makes readout speeds even faster. The first mainstream Stacked CMOS camera, the Sony a9 from 2019, made waves by offering an interruption-free photographic experience—you can use it to fire off photos at 20fps without losing view of your scene.

![Stacked pixel architecture explained.](images/sensors/03-sensor-stackedpixel.png){#fig-sensor-stacked width="50%"}

## Sensor components: Space {#sec-sensor-space}
Conventional CMOS imagers include additional components that are critical for producing high quality images (@fig-pixel-overview). The microlens array and deep trench isolation both increase sensitivity and reduce unwanted crosstalk between the pixels. Spectral filters, which are included in sensors for consumer photography and many other types of cameras, are used to captur information about the scene spectral radiance. 

### Microlens arrays {#sec-sensor-microlens}

The microlens array is a layer of tiny lenses placed above the color filter array. These lenses serve to redirect the light from the main lens onto the photodiode, increasing the light-gathering efficiency. The microlens also reduces **pixel cross-talk**. This refers to the case in which light from the imaging lens might arrive at a relatively large angle at the color filter. A ray may end up generating electrons in a pixel that has a different color filter. Over time, microlens arrays have evolved to support additional functions, such as optimizing focus and enabling advanced imaging techniques for light field capture that I describe later.

![Electron micrograph cross section illustrating the position of the color filters and microlenses above the photodiode (from @elgamalCMOSImageSensors2005)](images/sensors/03-microlens-cfa-SEM.png){#fig-components-microlens width="70%"}

At the center of the sensor array, the incidence of the chief ray from the center of the lens is close to perpendicular to the photodiode array. There, a lens centered above the photodiode works well to simply concentrate the rays. At the edges of the array, however, the angle of the chief ray is relatively steep, say 35 degrees. To redirect the rays toward the photodiode, the microlens and the color filter are shifted laterally. The decentering of the microlens and color filters improves the redirection the rays so that more light arrives at the proper photodiode, and less light is incorrectly absorbed by adjacent photodiodes.

![De-centering the microlens and color filter above photodiodes at the sensor edge)](images/sensors/03-microlens-shifted.png){#fig-components-shiftedmicrolens width="70%"}

The microlens was important for classic, frontside illuminated sensors when the path from the lens through the metal layers was quite long and the angle of the chief ray at the edge was fairly large. For large pixels, and the initial sensor pixels were 6 or more microns, microlens technology was quite effective.
@Hwang2023-microlens-alignment @Hwang2023-microlens-quad

### Deep trench isolation {#sec-sensor-dti}
As pixel sizes became smaller, on the order of 1.5 or 1 micron, the impact of diffraction became increasingly important. Light passing through these small pixel apertures would spread beyond the photodiode, and significant amounts of energy would be deposited in the photodiodes of adjacent pixels. This caused two problems. First, the spread of the light reduced the sharpness of the image. Second, the color accuracy of the image was reduced because light that passed through, say a green filter, would produce electrons in the photodiode beneath a red filter.

**Deep trench isolation (DTI)** technology was developed to reduce this problem. Designers developed methods of placing trenches at the pixel margins and then filling the trenches with an insulating material (like silicon dioxide or nitride). This formed very thin walls within the pixel that would stop the radiation from spreading between pixels (@fig-sensor-deeptrench). Deep trench isolation improved both the spatial resolution and the color accuracy. 

![Deep trench isolation images.)](images/sensors/03-sensor-deeptrench.png){#fig-sensor-deeptrench width="50%"}

Alternative DTI as well as BSI.

![Alternative BSI image.](images/sensors/BSI_DTI_Pixel_Technology.png){#fig-sensor-evolution width="80%"}

@Han2020-deeptrench
@Tournier2011-deeptrench
@Park2007-deeptrench -->

## Sensor components: Wavelength {#sec-sensor-wavelength}

The bandgap of silicon enables CMOS image sensors to convert photons at wavelengths shorter than 1100 nm into electrons. The human eye, however, only encodes radiation in a narrower range, from about 380 nm to 700 nm. To reproduce a scene for a person, which is the goal of consumer photography, the image sensors need to record spectral radiation in th visible band. Furthermore, the human visual system coarsely samples the spectral signal, using only three broadband photoreceptors. Although it is possible to record the spectral radiance more finely than the human eye, it is not necessary. Thus most cameras are designed to capture only three broadband samples in the visible range.

Two types of spectral filters are used to roughly match the wavelength encoding of consumer cameras to the human eye. The three color channels of recorded data are then processed to render a satisfactory reproduction. In this section, I describe the color filters. After describing the essential properties of the human eye in more detail (@sec-human), I describe how the color channels of the sensor are processed to reproduce an accurate or at least pleasant rendering of the scene (@sec-imgprocessing).

### Spectral blocking filter {#sec-sensor-irfilter}

Many consumer cameras incorporate a spectral filter that blocks wavelengths longer than 680-700 nm (infrared, **IR**) and shorter than 380-400 nm (ultraviolet, **UV**) from reaching the photodiode. These thin **UV/IR blocking filters** are integrated onto the sensor, above the microlenses (@fig-sensor-uvir)[^irfilters].

[^irfilters]: Although the filters block both UV and IR they are also called **IR blocking filters**. This might be because it is simpler and in some cases the UV light is already blocked by the glass material in the optics.]

![The UV/IR blocking filter.](images/sensors/03-sensor-uvirfilter.png){#fig-sensor-uvir width="80%"} 

In the first few generations of cameras, the spectral filters had an additional laminated layer that blurred the image slightly. This **optical low pass filter (OLPF)** was included to make sure that light from a point in the scene would be spread over a small, say 2x2 array, of pixels. This blur is useful because adjacent pixelshave different color filters, but we combine their outputs and treat them as arising from a single point in the scene. In more recent generations, the pixel size has shrunk and is often below the spread due to diffraction (@fig-blur-comparison). Hence, the optics spreads the light from a point across multiple pixels and there is no need for the OLPF.

### Color filter array {#sec-sensor-cfa}
The UV/IR filter defines the range of wavelengths that arrive at the photodiodes. Below this filter, many cameras have an additional **color filter array (CFA)** (@fig-pixel-layers). The color filters are typically arranged in a repeating pattern, and the smallest repeating unit of the color filter array is called a **super pixel**. The consequence of this array is that the pixels form multiple interleaved mosaics, each capturing a different part of the wavelength spectrum. Each of these spatial mosaics is usually called a **color channel**. It is common to have three channels in consumer photography; different choices about the spectral encoding are made for other applications in, say, medical, scientific, or industrial applications.

![Spectral sensitivity of the color filters alone. Notice that the red filter passes long wavelength light (IR). Light beyond 680nm, however, does not reach the photodiode because it is blocked by the UV/IR filters. <a href="https://evidentscientific.com/en/microscope-resource/knowledge-hub/digital-imaging/cmosimagesensors" target="_blank">Link to original.</a>](images/sensors/03-sensor-cfaspectral.png){#fig-cfa-spectral width="70%" fig-align="center"}


The most common CFA pattern is was patented by Bryce Bayer, at Kodak (Bayer patent here). which uses red, green, and blue filters distributed across the sensor. This allows the sensor to sample the visible spectrum at different locations, and the resulting data can be combined to reconstruct a full-color image. Some sensors use alternative filter patterns or additional spectral bands for specialized applications. Together, microlens arrays and color filter arrays are essential for the function and versatility of modern digital cameras.

![Bayer RGB pattern.](images/sensors/03-sensor-cfapattern.png){#fig-cfa-pattern width="70%" fig-align="center"}

<!-- https://www.nature.com/articles/s41467-022-31019-7/figures/1 -->

 