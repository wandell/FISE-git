# Sensors {#sec-sensors}
We have been considering the properties of the CMOS imager one pixel at a time. In this section we consider properties of the array, such as the variations between pixels, the color filter array, and the microlens array. Here again, First, I describe the basic principles, the foundations, of sensors. Then, I describe recent innovations in pixel and sensor circuitry, which is an active and fascinating field.

## Sensor noise
When manufacturing an array comprising millions of pixels, there will be minor differences between them. For example the area of the photodiode might differ, or the capacitance of the floating diffusion node, or perhaps the material doping concentration. There is not much variation -foundries are amazing- but there will be some. These variations across the array are called **sensor noise**.  The source of the variations do not change over time, and thus they are called **fixed pattern noise (FPN)**.  

![Sensor fixed pattern noise. On average, the pixel responses will be linear.  But variations in the pixel manufacturing will make their responses differ somewhat in the offset and slope.  Variations in the offset, generally due to different amounts of dark current are shown in the DSNU region. They may also differ in the slope of the linear response, perhaps because the photodiode area is slightly different or perhaps there is some slight difference in the transparent media above the different pixels. These slope variations are illustrated by the PRNU arrow.](images/sensors/03-sensor-noise.png){#fig-sensor-noise width="60%"}

Both of these sensor noise characteristics

### Dark signal nonuniformity {#sensor-dsnu}
**Dark signal nonuniformity (DSNU)** refers to the variation in the dark current across the pixels.  Some pixels may accumulate dark current at a slightly higher or lower rate. As a result, when we consider their response to increasing amount of light, they will start at slightly different levels.  This is illustrated in the variation of the starting positions at zero illumination level in Figure @fig-sensor-noise.  Recall that the pixel dark current depends on several factors, including temperature and exposure time (@sec-pixel-darkcurrent).  Hence, the DSNU will also depend on these factors.

### Photoresponse nonuniformity {#sensor-prnu}
The **photoresponse nonuniformity (PRNU) ** refers to the variation in the slope (gain) of the pixel responses (@fig-sensor-noise). There are various reasons why the response curve might differ between pixels. The photodiode area might be slightly larger in one pixel than another, or the effectiveness of the microlens at focusing the light on the photodiode might be slightly different.  

## Sensor components
Standard CMOS imagers include a few additional components 


### UV/IR blocking filter {#sec-sensor-irfilter}

### Color filter array {#sec-sensor-cfa}
Sensors use a color filter array (CFA), placed above the photodiodes, to acquire the spectral information necessary to render a color image.The CFA ensures that each pixel is has its own spectral sensitivity. The most common CFA pattern is the Bayer arrangement, which uses red, green, and blue filters distributed across the sensor. This allows the sensor to sample the visible spectrum at different locations, and the resulting data can be combined to reconstruct a full-color image. Some sensors use alternative filter patterns or additional spectral bands for specialized applications. Together, microlens arrays and color filter arrays are essential for the function and versatility of modern digital cameras.

<!--# https://www.pcmag.com/how-to/whats-the-difference-between-cmos-bsi-cmos-and-stacked-cmos -->

![The color filter array and microlens array above the photodiode array. From the El Gamal and Eltoukhy article](images/sensors/03-microlens-cfa.png){#fig-components-overview2 width=50%}


### Microlens arrays {#sec-sensor-microlens}

Importance for capture on a single pixel  The light field sensor will be described later.  Not sure how to group.

The microlens array is a layer of tiny lenses placed above the color filter array. Each microlens is precisely positioned relative to the main lens, color filter, and photodiode. It serves to focus incoming light from the main lens onto the photodiode within the pixel. The microlens increases light-gathering efficiency, which is especially important as pixel sizes shrink. It also prevents cross-talk, a condition in which light passes at a large angle through a color filter and ends up generating electrons in a pixel that has a different color filter. Over time, microlens arrays have evolved to support additional functions, such as optimizing focus and enabling advanced imaging techniques like light field capture.

## Sensor innovations {#sec-sensor-innovations}
Over time, as technology has scaled, more complex electrical circuitry has been placed on the sensor. Modern image sensors frequently include circuitry that performs local processing to increase the dynamic range of the sensor (well recycling), or to reduce the intrinsic noise (correlated double sampling). Some of this processing is *adaptive*, that is the circuit actions depend on the property of the input image. Consequently, the sensor output can depend upon both the control parameters set by the user and the image content.

### Global shutter {#sec-sensor-globalshutter}
Instead of using a floating diffusion as a memory element, Aptina has utilized a surface-pinned storage node in the pixel to address dark current challenges. Available in its newest global shutter sensor, the MT9M031, the storage node also enables using a true correlated double sampling technique to reduce readout noise to four electrons, resulting in excellent low-light performance. The combination of the effective use of an anti-reflective metal light shield in close proximity to the memory node and careful doping and potential profile design results in a high GSE.

Global Shutter Pixel Technologies and CMOS Image Sensors – A Powerful Combination – (Aptina white paper)

### Stacked pixel structures {#sec-sensor-stacked}
The original sensors were built using a technology called front-side illumination (FSI), in which multiple metal layers are located above the semiconductor substrate. In this architecture, photons incident on the pixel must pass through these metal layers to reach the light-sensitive photodiode (@fig-sensor-evolution (a) and @fig-sensor-pixel-layers). Additionally, both the photodiodes and their associated circuitry share the same substrate, forcing a trade-off: the area used for circuitry reduces the area available for the photodiode.

The **back-side illuminated (BSI)** sensor addresses this limitation by placing the metal wiring below the substrate, allowing photons to reach the photodiode more directly (@fig-sensor-evolution (b)). The **stacked pixel** architecture goes further by using multiple semiconductor layers (@fig-sensor-evolution (c)). The first layer contains the photodiode array, while the readout circuitry is placed on a separate layer below. Advanced interconnect technologies electrically connect these layers. This multi-layer approach improves light sensitivity and enables more complex on-sensor processing, supporting advanced features and higher performance.

The improved design, Backside Illuminated (BSI) CMOS, is similar in concept to regular CMOS, but these chips arrange the components differently. In short, the photosites are further forward on the die and the line-by-line readout speed is brisker. This change enables practical advantages: Generally speaking, a BSI CMOS is around an f-stop better when it comes to image noise. That means a BSI CMOS shows as much noise at ISO 12800 as a similar CMOS chip would at ISO 6400. It also means that APS-C and Micro Four Thirds cameras with BSI chips play on more even footing with full-frame CMOS cameras. Those aren't hard-and-fast rules, but they're good guidelines to follow. Stacked CMOS chips improve upon the BSI CMOS concept. They place components in a similar arrangement, but the design also stacks the image signal processor and its ultra-fast DRAM memory into the same silicon. This makes readout speeds even faster. The first mainstream Stacked CMOS camera, the Sony a9 from 2019, made waves by offering an interruption-free photographic experience—you can use it to fire off photos at 20fps without losing view of your scene.

![The evolution of pixel designs. (a) The original front-side illuminated CMOS sensor. (b) The back side illuminated sensor.  (c) The stacked pixel design. ](images/sensors/03-sensor-evolution.png){#fig-sensor-evolution width=80%}

::: {#sensor-bsi-stacked .callout-note collapse="true"}
## Technology development
Notably, Eric Fossum is credited with creating back-illuminated (BSI) CMOS technology in 1994, more than a decade before its widespread adoption in consumer cameras. OmniVision Technologies was an early adopter, sampling their first BSI sensors in 2007, with their widely used OV8810 BSI sensor announced in September 2008. 

Back-illuminated sensor. (Wikipedia)

Read this.  Evolution of stacked and BI.
@Oike2022-stackedevolution


:::

