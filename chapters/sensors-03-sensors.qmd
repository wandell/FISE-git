# Sensors {#sec-sensors}

We have been considering the properties of the CMOS imager one pixel at a time. In this section, we consider properties of the array, such as variations between pixels, the color filter array, and the microlens array. First, I describe the basic principles—the foundations—of sensors. Then, I describe recent innovations in pixel and sensor circuitry, which is an active and fascinating field.

## Sensor noise {#sec-sensor-noise}

When manufacturing an array comprising millions of pixels, there will be minor differences between them. For example, the area of the photodiode might differ, or the capacitance of the floating diffusion node, or perhaps the material doping concentration. There is not a lot of variation—foundries are amazing—but there will be some. Because these are variations between the pixels, they are called **sensor noise**. Some of these variations do not change over time; for example, the relative photodiode area of different pixels. Thus, these variations are called **fixed pattern noise (FPN)**.

![Sensor fixed pattern noise. On average, the pixel response is linear, but manufacturing variations introduce slight differences in the offset and slope. Dark signal nonuniformities (DSNU) are variations in the offset. These can be due to different amounts of dark current or unaccounted for bias in the reset noise. Photoresponse nonuniformities (PRNU) are variations in the slope of the response. These can be due to differences in the photodiode area or in the transparency of the path from the imaging lens to the pixel.](images/sensors/03-sensor-noise.png){#fig-sensor-noise width="60%"}

### Dark signal nonuniformity {#sec-sensor-dsnu}

**Dark signal nonuniformity (DSNU)** refers to the variation in the dark current across the pixels. Some pixels may accumulate dark current at slightly different rates. As a result, when we consider their response to increasing amounts of light, they will start at slightly different levels. This is illustrated in the variation of the starting positions at zero illumination level in Figure @fig-sensor-noise. Recall that the pixel dark current depends on several factors, including temperature and exposure time (@sec-pixel-darkcurrent). Hence, the DSNU will also depend on these factors.

### Photoresponse nonuniformity {#sec-sensor-prnu}

The **photoresponse nonuniformity (PRNU)** refers to the variation in the slope (gain) of the pixel responses (@fig-sensor-noise). There are various reasons why the response curve might differ between pixels. The photodiode area might be slightly larger in one pixel than another, or the effectiveness of the microlens at focusing the light on the photodiode might be slightly different. Also, the media, such as the color filter, may differ slightly between two pixels of the same type.

## Evolution of pixel structures {#sec-sensor-evolution}

The original sensor stack is called front-side illumination (FSI), in which multiple metal layers are located above the semiconductor substrate (@fig-pixel-layers). Even at the time of the original implementation, it was clear that forcing light to pass through the metal layers to reach the light-sensitive photodiode created many problems. Additionally, both the photodiodes and their associated circuitry share the same substrate, so that the circuitry reduced the area of the photodiode.

<!--# https://www.pcmag.com/how-to/whats-the-difference-between-cmos-bsi-cmos-and-stacked-cmos 
https://news.skhynix.com/evolution-of-pixel-technology-in-cmos-image-sensor
-->

### Back-Side illuminated (BSI) sensor {#sec-sensor-bsi}
After years of incremental improvements, engineers developed a new architecture to overcome the limitations of front-side illuminated (FSI) sensors. The resulting design, known as **Back-Side illuminated (BSI)** CMOS, rearranges the device layers to improve sensitivity. In BSI sensors, the photodiode and wiring layers are flipped; the light no longer needs to pass through the wires.  

![The back-side illuminated pixel. (a) The front-side illuminated pixel has metal lines above the silicon substrate. (b) The back side illuminated sensor flips the metal lines and silicon substrate.  Source: <a href="https://scientificimaging.com/knowledge-base/front-side-illuminated-and-back-side-illuminated-imagers" target="_blank">Scientific Imaging.</a>](images/sensors/03-sensor-bsi2.png){#fig-sensor-bsi2 width="60%"}

In addition to flipping the order of the silicon and wires, it was necessary to change the thickness of the substrate (@fig-sensor-bsi2). In the FSI pixel, the photodiode is embedded in a relatively thick layer of silicon. Flipping this structure would require the light to pass through a substantial amount of silicon prior to reaching the photodiode. This is problematic because the silicon absorbs light, and particularly short-wavelength light (@sec-sensor-wavelength). Thus, it was necessary to find a way to thin the silicon substrate and still have a working photodiode and circuitry. 

::: {#sensor-bsi .callout-note collapse="true"}
## BSI technology development

[Wikipedia article](https://en.wikipedia.org/wiki/Back-illuminated_sensor)

Eric Fossum first proposed the concept of back-side illuminated (BSI) CMOS image sensors in 1994, recognizing the need to reduce metal line obstruction and improve sensitivity. However, it took more than a decade for the technology to mature and reach commercial viability. OmniVision Technologies was among the first to develop BSI sensors, introducing the OV8810 (1.4 μm pixels, 8 megapixels) in September 2008. Sony followed soon after, launching the first widely adopted BSI sensors in 2009.

```{=html}
<!-- Read this.  Evolution of stacked and BI.
@Oike2022-stackedevolution -->
```
:::

### Deep photodiodes {#sec-sensor-deepdiode}
@fig-sensor-skhynix illustrates a further improvement in the BSI design. SK Hynix, Omnivision and other manufacturers recognized that simply flipping the sensor (BSI) improved light capture. But as pixel sizes continued to shrink there was relatively little ability to store electrons.  Thus the well capacity was reduced and which limited the pixel's dynamic range (@sec-pixel-wellcapacity). Moreover, the sensitivity to longer wavelengths, which penetrates deeper into silicon, became a challenge (@sec-sensor-wavelength). 

To overcome these limitations, many vendors invented methods to build photodiodes that extend significantly deeper into the silicon (deep photodiode technology). Increasing the volume of the photodiode has two benefits.  First, the photodiode can gather more charge before saturating.  Second, the photodiode has higher long-wavelength sensitivity. 

![Deep photodiodes in the BSI. (a) A super pixel of a front-side illuminated CMOS sensor, including the microlenses, color filters, metal lines, and silicon. (b) A back-side illuminated super pixel with deep photodiode technology. Source: <a href="https://news.skhynix.com/evolution-of-pixel-technology-in-cmos-image-sensor/" target="_blank">Skhynix.</a>](images/sensors/03-sensor-bsi.png){#fig-sensor-skhynix width="70%"}

### Stacked sensor {#sec-sensor-stacked}



In the original planar technology, photodiodes are adjacent to the transistor circuits within the silicon substrate. Thus, photodiodes must compete for space with the transistors.  Reducing the photodiode area means less light efficiency; reducing the transistor size means noisier performance. Youse pays yer money and yer makes yer choice.

![Pixels in the stacked sensor are based on die stacking technology. Several layers of silicon that are bonded and connected using Through-Silicon Via (TSV) technology. The technology began with 2-layers, but when have you ever known Silicon Valley to stop?](images/sensors/03-pixel-stacked.png){#fig-sensor-stacked width="50%"}

**Stacked sensors** were the next advance, following BSI. The new innovation was finding a method to place the photodiode and its circuitry on different layers. The key technology is called **die stacking**. The different layers of silicon are bonded together and connected electrically with **Through-Silicon Vias (TSVs)**. As you can imagine, with the features being so small, this is a challenging technology.

Placing the photodiode and circuitry on different layers means they no longer compete for space.  This approach both improves light sensitivity and also establishes the foundation for stacking additional processing circuitry beneath the photodiode layer. 

::: {#sensor-stacked .callout-note collapse="true"}
## Stacked technology development

<!-- Stacked Pixel Structures Google doc 
A significant step towards multi-layer integration was taken by Mitsumasa Koyanagi of Tohoku University, who pioneered the technique of wafer-to-wafer bonding with TSV in 1989
-->

To edit.

The stacked design directly addresses inherent limitations of traditional planar CMOS designs, particularly the persistent trade-off between the light collection area (fill factor) and the necessary space for readout circuitry. In conventional small pixels, the integration of extensive circuitry on the same plane as the photodiode often leads to higher source follower noise and a diminished fill factor, negatively impacting dynamic range and signal-to-noise ratio.11 By relocating the processing circuitry to a separate, underlying layer, the stacked structure maximizes the light-sensitive area for a given pixel size, allowing for more efficient photon capture.1 Furthermore, this separation permits the use of larger, more stable transistors within the logic layer, which significantly contributes to noise reduction and improved imaging properties.4 This architectural flexibility has expanded the capabilities of image sensors well beyond basic light capture, enabling advanced features such as high dynamic range (HDR) video, faster frame rates, and the integration of on-chip artificial intelligence (AI) processing.1

Stacked CMOS chips improve upon the BSI CMOS concept. They place components in a similar arrangement, but the design also stacks the image signal processor and its ultra-fast DRAM memory into the same silicon. This makes readout speeds even faster. The first mainstream Stacked CMOS camera, the Sony a9 from 2019, made waves by offering an interruption-free photographic experience—you can use it to fire off photos at 20fps without losing view of your scene.
:::

## Sensor components: Space {#sec-sensor-space}
Conventional CMOS imagers include additional components that are critical for producing high quality images (@fig-pixel-overview). The microlens array and deep trench isolation both increase sensitivity and reduce unwanted crosstalk between the pixels. Spectral filters, which are included in sensors for consumer photography and many other types of cameras, are used to captur information about the scene spectral radiance. 

### Microlens arrays {#sec-sensor-microlens}

The microlens array is a layer of tiny lenses placed above the color filter array. These lenses serve to redirect the light from the main lens onto the photodiode, increasing the light-gathering efficiency. The microlens also reduces **pixel cross-talk**. This refers to the case in which light from the imaging lens might arrive at a relatively large angle at the color filter. A ray may end up generating electrons in a pixel that has a different color filter. Over time, microlens arrays have evolved to support additional functions, such as optimizing focus and enabling advanced imaging techniques for light field capture that I describe later.

![Electron micrograph cross section illustrating the position of the color filters and microlenses above the photodiode (from @elgamalCMOSImageSensors2005)](images/sensors/03-microlens-cfa-SEM.png){#fig-components-microlens width="70%"}

At the center of the sensor array, the incidence of the chief ray from the center of the lens is close to perpendicular to the photodiode array. There, a lens centered above the photodiode works well to simply concentrate the rays. At the edges of the array, however, the angle of the chief ray is relatively steep, say 35 degrees. To redirect the rays toward the photodiode, the microlens and the color filter are shifted laterally. The decentering of the microlens and color filters improves the redirection the rays so that more light arrives at the proper photodiode, and less light is incorrectly absorbed by adjacent photodiodes.

![De-centering the microlens and color filter above photodiodes at the sensor edge)](images/sensors/03-microlens-shifted.png){#fig-components-shiftedmicrolens width="70%"}

The microlens was important for classic, frontside illuminated sensors when the path from the lens through the metal layers was quite long and the angle of the chief ray at the edge was fairly large. For large pixels, and the initial sensor pixels were 6 or more microns, microlens technology was quite effective.
@Hwang2023-microlens-alignment @Hwang2023-microlens-quad

### Deep trench isolation {#sec-sensor-dti}
As pixel sizes became smaller, on the order of 1.5 or 1 micron, the impact of diffraction became increasingly important. Light passing through these small pixel apertures would spread beyond the photodiode, and significant amounts of energy would be deposited in the photodiodes of adjacent pixels. This caused two problems. First, the spread of the light reduced the sharpness of the image. Second, the color accuracy of the image was reduced because light that passed through, say a green filter, would produce electrons in the photodiode beneath a red filter.

**Deep trench isolation (DTI)** technology was developed to reduce this problem. Designers developed methods of placing trenches at the pixel margins and then filling the trenches with an insulating material (like silicon dioxide or nitride). This formed very thin walls within the pixel that would stop the radiation from spreading between pixels (@fig-sensor-deeptrench). Deep trench isolation improved both the spatial resolution and the color accuracy. 

![Deep trench isolation images.)](images/sensors/03-sensor-deeptrench.png){#fig-sensor-deeptrench width="50%"}

@Han2020-deeptrench
@Tournier2011-deeptrench
@Park2007-deeptrench -->

## Sensor components: Wavelength {#sec-sensor-wavelength}

The bandgap of silicon enables CMOS image sensors to convert photons at wavelengths shorter than 1100 nm into electrons. The human eye, however, only encodes radiation in a narrower range, from about 380 nm to 700 nm. To reproduce a scene for a person, which is the goal of consumer photography, the image sensors need to record spectral radiation in th visible band. Furthermore, the human visual system coarsely samples the spectral signal, using only three broadband photoreceptors. Although it is possible to record the spectral radiance more finely than the human eye, it is not necessary. Thus most cameras are designed to capture only three broadband samples in the visible range.

Two types of spectral filters are used to roughly match the wavelength encoding of consumer cameras to the human eye. The three color channels of recorded data are then processed to render a satisfactory reproduction. In this section, I describe the color filters. After describing the essential properties of the human eye in more detail (@sec-human), I describe how the color channels of the sensor are processed to reproduce an accurate or at least pleasant rendering of the scene (@sec-imgprocessing).

### Spectral blocking filter {#sec-sensor-irfilter}

Many consumer cameras incorporate a spectral filter that blocks wavelengths longer than 680-700 nm (infrared, **IR**) and shorter than 380-400 nm (ultraviolet, **UV**) from reaching the photodiode. These thin **UV/IR blocking filters** are integrated onto the sensor, above the microlenses (@fig-sensor-uvir)[^irfilters].

[^irfilters]: Although the filters block both UV and IR they are also called **IR blocking filters**. This might be because it is simpler and in some cases the UV light is already blocked by the glass material in the optics.]

![The UV/IR blocking filter.](images/sensors/03-sensor-uvirfilter.png){#fig-sensor-uvir width="80%"} 

In the first few generations of cameras, the spectral filters had an additional laminated layer that blurred the image slightly. This **optical low pass filter (OLPF)** was included to make sure that light from a point in the scene would be spread over a small, say 2x2 array, of pixels. This blur is useful because adjacent pixelshave different color filters, but we combine their outputs and treat them as arising from a single point in the scene. In more recent generations, the pixel size has shrunk and is often below the spread due to diffraction (@fig-blur-comparison). Hence, the optics spreads the light from a point across multiple pixels and there is no need for the OLPF.

### Color filter array {#sec-sensor-cfa}
The UV/IR filter defines the range of wavelengths that arrive at the photodiodes. Below this filter, many cameras have an additional **color filter array (CFA)** (@fig-pixel-layers). The color filters are typically arranged in a repeating pattern, and the smallest repeating unit of the color filter array is called a **super pixel**. The consequence of this array is that the pixels form multiple interleaved mosaics, each capturing a different part of the wavelength spectrum. Each of these spatial mosaics is usually called a **color channel**. It is common to have three channels in consumer photography; different choices about the spectral encoding are made for other applications in, say, medical, scientific, or industrial applications.

![Bayer RGB pattern. Source: <a href="https://www.nature.com/articles/s41467-022-31019-7/figures/1" target="_blank">Pixel-level Bayer-type colour router based on metasurfaces, Zou et al. 2022</a>](images/sensors/03-sensor-cfapattern.png){#fig-cfa-pattern width="70%" fig-align="center"}

The most common CFA pattern is was patented by Bryce Bayer, at Kodak (Bayer patent here). which uses red, green, and blue filters distributed across the sensor. This allows the sensor to sample the visible spectrum at different locations, and the resulting data can be combined to reconstruct a full-color image. Some sensors use alternative filter patterns or additional spectral bands for specialized applications. Together, microlens arrays and color filter arrays are essential for the function and versatility of modern digital cameras.

![Spectral sensitivity of the color filters alone. Notice that the red filter passes long wavelength light (IR). Light beyond 680nm, however, does not reach the photodiode because it is blocked by the UV/IR filters. Source: <a href="https://evidentscientific.com/en/microscope-resource/knowledge-hub/digital-imaging/cmosimagesensors" target="_blank">Evident Scientific.</a>](images/sensors/03-sensor-cfaspectral.png){#fig-cfa-spectral width="70%" fig-align="center"}

The spectral sensitivity of a color channel does not depend on the color filter alone. The light must pass through the lens, the UV/IR filter, the microlens, the color filter, and then be absorbed in the silicon.  Each of these components can have an impact on the likelihood of a photon being absorbed, and their combination is what determines the spectral responsivity of the color channel.

![Spectral sensitivity of the color channels in a typical modern sensor.](images/sensors/03-sensor-channel-spectral.png){#fig-channel-spectral width="70%" fig-align="center"}


<!--  -->

 