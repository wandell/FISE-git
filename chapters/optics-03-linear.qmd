# Optics and Linear Systems {#sec-optics-linear}
Geometric optics is valuable for understanding and characterizing many properties of optical systems. Its concepts are closely linked to the physical characteristics of lenses, making geometric optics especially useful in lens design.

Another perspective is to view an optical system as a transformer: it converts the radiation entering the aperture into radiation at the sensor surface. This approach treats the optics as mapping one spectral image (the incident light field) into another (the irradiance at the sensor). Thinking of the system in terms of signal processing is a powerful method for system simulation and analysis.

## Linear Systems
This perspective is particularly important because many optical systems are approximately linear. Specifically, they satisfy the **Principle of Superposition**, a property that can be tested experimentally. Linear systems are common in science and engineering, and we will encounter this concept repeatedly throughout the book.

### Principle of Superposition
When analyzing a system, scientists and engineers often hope its input-output relationship is linear. A system $L$ is linear if it satisfies the superposition rule:

$$
L(x + y) = L(x) + L(y).
$$ {#eq-superposition}

Here, $x$ and $y$ are two possible inputs, and $L(x)$ is the system's response. For an optical system, $x$ might represent the incident light field at the entrance aperture, and $L(x)$ the spectral irradiance at the sensor surface.

No real system is perfectly linear—extreme inputs will always cause failure—but many systems are linear over a significant range of inputs. Others are locally linear, meaning they behave linearly for small signals. Linear and locally linear systems are fundamental in mathematics, physics, and engineering.

### Principle of Homogeneity
A special case of superposition is **homogeneity**:

$$
L(\alpha x) = \alpha L(x)
$$ {#eq-homogeneity}

Homogeneity follows from superposition. For example, adding an input to itself:

$$\begin{aligned}
L(x + x) &= L(2x) \nonumber  \\ 
&= L(x) + L(x) \nonumber \\
&= 2L(x). \nonumber
\end{aligned}
$$

This generalizes to any integer $m$:

$$
L(m x) = m L(x).
$$

::: {.callout-note title="Extension to Rational Numbers" collapse="true"}

For real-valued systems that obey superposition, homogeneity holds for any rational number. If $\alpha = \frac{p}{q}$ is rational:

$$
L(\alpha x) = L\left(\frac{p}{q}x\right) = p L\left(\frac{x}{q}\right).
$$

Let $x' = x/q$, so

$$
L(x) = L(q x') = q L(x') \implies L(x') = \frac{1}{q} L(x).
$$

Therefore,

$$
L(\alpha x) = p L(x') = p \frac{1}{q} L(x) = \frac{p}{q} L(x) = \alpha L(x).
$$

To extend this to all real numbers, continuity is required. But for most engineering purposes, this is sufficient.
:::

It is important to note that a system can be homogeneous without being linear. For example, $f(x) = |x|$ and the vector length function

$$
f(\mathbf{x}) = \|\mathbf{x}\|_2 = \sqrt{x_1^2 + x_2^2 + \cdots + x_n^2}
$$

are homogeneous but not linear. The function $ReLU(x) = \max(0, x)$, widely used in neural networks, is also homogeneous for $\alpha > 0$ but not linear:

$$
1 = ReLU(2 + -1) \neq ReLU(2) + ReLU(-1) = 2 + 0.
$$


### Point spread functions

Superposition has a really useful consequence when we think about how optics turn a scene into an image. Imagine the scene as a bunch of points, each with its own intensity $I(\mathbf{p})$, where $\mathbf{p} = (p_x, p_y, p_z)$ is the location of each point in the scene. For each of these points, the optics spreads out the light in a certain way—this is called the point spread function, or $PSF_\mathbf{p}(x, y)$. Here, $(x, y)$ are positions in the image.

If we know all these point spread functions, and if the system is linear (so superposition holds), we can figure out what the image will look like for any scene. We just add up the contribution from every point in the scene. The formula looks like this:

$$
O(x, y) = \sum_{\mathbf{p}} I(\mathbf{p})\, PSF_\mathbf{p}(x, y)
$$ {#eq-pointspread-1}

Here, $O(x, y)$ is the brightness at each spot in the image. Each point in the scene sends out its own little blur, and the image is just the sum of all those blurs.

::: {.callout-note title="A limitation of PSFs" collapse="true"}
There’s a catch: in real scenes, objects can block each other (occlusion). That means the point spread from one spot might be blocked or changed by something else in the scene, so the formula above isn’t always perfect.

But if all the points are on a flat surface, or if everything is far away (so nothing blocks anything else), this approach works well. The main idea is that you can build up the whole image by adding up the point spreads from each part of the scene. It’s a great starting point for understanding how images form.
:::

A limitation with this observation is that we might have to measure a lot of different point spread functions. For some systems, that is true.  But for many systems, and certainly within small regions of the scene for most systems, across many positions, $\mathbf{p}$; the point spread function shifts but its shape remains the same.  This makes the system calibration and calculations much easier.

## Shift-invariant linear systems
When the shape of the point spread function is the same for different points, except for a shift in position, we say the system is **shift invariant**When a system is shift-invariant, we can model and characterize it and very efficiently, based on only a few measurements.

There are a large number of shift invariant linear systems in all branches of science and engineering. Optical linear systems usually refers to shift invariant in space; and there are many other linear systems that are a shift invariant in time. The simplifications of shift invariance make it a very useful tool, and some linear systems courses focus almost entirely on shift-invariant linear systems, with little time spent on the general linear system. 

The image formation component (optics) of most image systems are typically linear, but they are not fully shift-invariant.  Even so, one often finds a region near the main optical axis to be spatially shift invariant. My colleagues in optics call these spatially shift invariant regions **isoplanatic**.  

Some mathematical notation is useful, as a reminder. Suppose we express image translation as a function, say expressing translation of an image, $x$, by an amount, $\delta$, as $T(x,\delta)$. The linear relationship from the input to the output of the optics is $L(x)$. An isoplanatic region means that the linear transform of a translated image is approximately equal to the translation of the linear response. 

$$L(T(x,\delta)) \approx T(L(x),\alpha \delta)$$.  

Notice that the size of the shift of the image differs on the input, $x$, and output $L(x)$, by a scale factor, $\alpha$. This scale factor depends on the image magnfication.

### Point spread functions


Relationship between point spread and line spread. psf2lsf easy, but to go back you need to assume something about the circular symmetry.

### Harmonics 
Almost eigenfunctions.  Not quite because of phase.  If we treat the harmonics as 2-dimensional vectors, then they are eigenfunctions.

### Optical Transfer Function

### Modulation Transfer

### Contrast sensitivity

## Chromatic aberration

This is already in, earlier.  Maybe an example here? Explain transverse and longitudinal chromatic aberration here? 





