<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>pcc</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="PCC_files/libs/clipboard/clipboard.min.js"></script>
<script src="PCC_files/libs/quarto-html/quarto.js"></script>
<script src="PCC_files/libs/quarto-html/popper.min.js"></script>
<script src="PCC_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="PCC_files/libs/quarto-html/anchor.min.js"></script>
<link href="PCC_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="PCC_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="PCC_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="PCC_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="PCC_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">




<p>[1996/06/01]</p>
<section id="principles-and-consequences-of-the-initial-visual-encoding" class="level1">
<h1>Principles and Consequences of the Initial Visual Encoding</h1>
<blockquote class="blockquote">
<p>‘Only infrequently is it possible to subject the manifold phenomena of life to simple and strict forms of mathematical treatment without forcing the data and encountering contradiction, probably never without a certain abandonment of the immense multiplicity of details to which those phenomena owe their aesthetic attractiveness. Nevertheless, however, it has often proved to be possible and useful to establish, for wide fields of biological processes and organic arrangements, comparatively simple mathematical formulas which, though they are probably not applicable with absolute accuracy, nevertheless simulate to a certain approximation a large number of phenomena. Such representations not only offer preliminary orientation in a field that at first seems completely incomprehensible, but they also often direct research into a correct course, inasmuch as first an insight into those fundamental formulations is sought, and then the deviations from their strict validity, which become apparent here and there, are made the subject of special investigations. Among the fields of physiology which have permitted the establishment of such guiding formulas the theory of visual sensations and of color mixture assumes a particularly distinguished position <span class="citation" data-cites="vonKries1902Chromatic">[@vonKries1902Chromatic]</span>.’</p>
</blockquote>
<p>(Von Kries quotation re-written by ChatGPT)</p>
<blockquote class="blockquote">
<p>It’s rare to describe the complexity of life using simple, strict mathematical formulas without oversimplifying the data or losing the beauty of the details. However, in many cases, it’s been possible and helpful to create relatively simple mathematical models for biological processes and systems. While these formulas may not be perfectly accurate, they often do a good job of approximating a wide range of phenomena. These models serve two purposes: they give us a starting point to understand complicated systems, and they guide research by highlighting areas where the models don’t fully match reality, which can lead to new discoveries. In physiology, one of the best examples of this approach is in the study of visual sensations and color mixing, where mathematical formulas have been especially useful and influential.</p>
</blockquote>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Vision research has many purposes. Medical investigators aim to diagnose and repair visual disorders ranging from optical focus to retinal dysfunction to cortical lesions. Psychologists aim to identify and quantify the systematic rules of perception, including models of visual sensitivity, image quality, and the laws that predict percepts such as brightness, color, motion, size and depth. Systems neuroscientists seek to relate visual experience and performance to the neural signals in the visual pathways, and computational investigators seek principles and models of perceptual and neural processes. Image systems engineers ask how to design sensors and processing to provide effective artificial vision systems.</p>
<p>Vision science draws upon findings from many fields, including biology, computer science, electrical engineering, neuroscience, psychology and physics. Clear communication among people trained in different disciplines is not always straightforward. One of the ways that vision science has flourished is by using the language of mathematics to communicate core ideas. Vision science uses many types of mathematics; here we describe methods that have been used for many decades. These are certain linear methods, descriptions of noise distributions, and Bayesian inference. Many other linear methods (e.g., principal components, Fourier and Gabor bases, and independent components analysis) and nonlinear methods (e.g., linear-nonlinear cascades, normalization, information theory, and neural networks) can be found throughout the vision science literature. For this chapter, we focus on a few core mathematical methods and the complementary role of computation.</p>
<p>Physics - the field that quantifies the input to the visual system - provides mathematical representations of the light signal and definitions of physical units. The field of physiological optics quantifies the optical and biological properties of the lens. These properties are summarized as a mathematical transformation that maps the physical stimulus to the image focused on the retina, generally referred to as the retinal image. At each retinal location the image is characterized as the spectral irradiance (power per unit area as a function of wavelength). Retinal anatomy and electrophysiology identify the properties of the rod and cone photoreceptors, enabling us to calculate the photopigment excitations from the retinal image using linear algebraic methods.</p>
<p>Perhaps the most famous use of mathematics in vision science is at the intersection of physics and psychology: The laws of color matching formalize the relationship between the physics of light and certain aspects of color appearance. The mathematical principles of color matching are also deeply connected to Thomas Young’s biological insight that there are only three types of cone photopigment <span class="citation" data-cites="Young1802OnTheTheory">[@Young1802OnTheTheory]</span>. This insight implies a low-dimensional biological encoding of the high-dimensional spectral light. The linear algebraic techniques used to describe the laws of color matching were developed by the mathematician Grassmann. Indeed, he developed vector spaces in part for this purpose <span class="citation" data-cites="Grassmann1853ZurTheorie">[@Grassmann1853ZurTheorie]</span>. The mathematics he introduced remains central to color imaging technologies and throughout science and engineering.</p>
<p>While acknowledging the importance of mathematical foundations, it is also important to recognize that there is much to be gained by building computational methods that account for specific system properties. The added value of computations is clear in many different fields, not just vision science. The laws of gravity are simple, but predicting the tides at a particular location on earth is not done via analytic application of Newton’s formulas. Similarly, that color vision is three-dimensional is a profound principle, yet precise stimulus control requires accounting for many factors, such as variations of the inert pigments across the retinal surface <span class="citation" data-cites="Whitehead2006Macular CIE2007Fundamental">[@Whitehead2006Macular; @CIE2007Fundamental]</span> and the wavelength-dependent blur of chromatic aberration <span class="citation" data-cites="Marimont1994-ys">[@Marimont1994-ys]</span>. The mathematical principles guide, but we need detailed computations to predict precisely how color matches vary from central to peripheral vision.</p>
<p>We hope this chapter helps the reader value principles expressed by equations and the computations embodied in software. Establishing the principles first provides a foundation for implementing accurate computations. Historically, our knowledge about vision has been built up by developing principles, testing them against experiments, and combining them with computation; this remains a useful and important approach. Indeed, we believe the goal of vision science includes not only producing models that account for performance and enable engineering advances, but also leveraging those models to extract new principles that help us think about how visual circuits work.</p>
<p>There are competing views: Some would argue that large data sets combined with analyses using machine learning provide the best way forward to understanding, and recent years have seen impressive engineering advances achieved with this approach <span class="citation" data-cites="Cox2014-hp">[@Cox2014-hp]</span>. We are certainly interested in the performance of such black-box models as a point of departure, but here we emphasize principles and data-guided computational implementations of these principles.</p>
<p>This chapter begins by describing the representation of the visual stimulus, and how light rays in the scene pass through the optics of the eye and arrive at the retina. Next, we explain how the retinal photoreceptors (a) transform the retinal spectral irradiance into photoreceptor excitations, and (b) spatially sample the retinal image. Each of these steps can be expressed by a crisp mathematical formulation. To describe the real system with quantitative precision, we implemented software that models specific features of the scene, optics, and retina (ISETBio; <span class="citation" data-cites="Cottaris2019AComputational">@Cottaris2019AComputational</span>, <span class="citation" data-cites="Cottaris2019AComputational">[-@Cottaris2019AComputational]</span>; <span class="citation" data-cites="Cottaris2020AComputational">@Cottaris2020AComputational</span>, <span class="citation" data-cites="Cottaris2020AComputational">[-@Cottaris2020AComputational]</span>; https://github.com/isetbio/isetbio/wiki), and we illustrate the use of these models in several examples.</p>
<p>The frontiers of vision science use mathematics to understand visual percepts, which provide a useful basis for thought and action. The information provided by light-driven photopigment excitations is used to create these percepts, but knowledge of the excitations alone falls far short of describing visual perception. The brain makes inferences about the external world from the retinal encoding of light, and throughout the history of vision science many investigators have suggested that the role of neural computation is to implement the principles that underlie these inferences. This point was emphasized as early as Helmholtz who wrote</p>
<blockquote class="blockquote">
<p>“The general rule determining the ideas of vision that are formed whenever an impression is made on the eye, is that such objects are always imagined as being present in the field of vision as would have to be there in order to produce the same impression on the nervous mechanism (<span class="citation" data-cites="Helmholtz1866Handbuch">@Helmholtz1866Handbuch</span>, <span class="citation" data-cites="Helmholtz1866Handbuch">[-@Helmholtz1866Handbuch]</span>; English translation <span class="citation" data-cites="Helmholtz1896Southall">@Helmholtz1896Southall</span>, <span class="citation" data-cites="Helmholtz1896Southall">[-@Helmholtz1896Southall]</span>).”</p>
</blockquote>
<p>Within psychology this idea is called ‘unconscious inference’, a phrase that emphasizes that we are not aware of the neural processes that produce our conscious experience, an idea was important to Helmholtz. Perhaps more important in this context is the principle that the percepts represent critical properties of external objects in the field of view, such as depth, reflectance, shape, and motion.</p>
<p>The mathematics of perceptual inferences can take many forms, and in common scientific practice the mathematics of inference depend on what is known about the input signal. If the scene properties are not uniquely determined by the sensory measurements, such as when only three spectral classes of cones sample the spectral irradiance of the retinal image, probabilistic reasoning about the likely state of the world is inevitable. In vision science, linear methods combined with the mathematical tools of probabilistic inference are commonly used to understand how the brain interprets the mosaic of photoreceptor excitations to see objects, depth, and color. In the final part of this chapter we close the loop between sensory measurements and perceptual inference by introducing the mathematics of such inferences, focusing on two specific examples relevant to the study of the initial visual encoding. The principles we introduce, however, apply generally.</p>
</section>
<section id="scene-to-retinal-image" class="level2">
<h2 class="anchored" data-anchor-id="scene-to-retinal-image">Scene to retinal image</h2>
<section id="light-field" class="level3">
<h3 class="anchored" data-anchor-id="light-field">Light field</h3>
<p>Light is the most important visual stimulus.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> The word light means the electromagnetic ‘radiation that is visible to the human eye.’<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> The mathematical representation of light has been developed through a series of famous experiments over many centuries, and these experiments provide several different ways to think about light. Several properties of how light is encoded by the eye can be understood by treating light as comprising rays of many different wavelengths.</p>
<figure id="fig:lightfield" class="figure">
<img src="PCC_images/lightfield.png" style="width:12cm" class="figure-img">
<p>
Figure reproduced from <span class="citation" data-cites="Ayscough1755-uf"></span>. <span id="fig:lightfield" data-label="fig:lightfield"></span>
</p>
<figcaption>
<strong>Light field geometry</strong>. The complete set of rays in the environment is the light field. The rays that arrive at the imaging system, in this figure a large pinhole camera, are the incident light field. If the imaging system includes a lens, rather than just a pinhole, the incident light field is described by the positions and angles of the rays at the lens aperture.
</figcaption>
</figure>
<p>In a passage in his 1509 notebook <span class="citation" data-cites="Da_Vinci1970-fj">[@Da_Vinci1970-fj]</span>, Leonardo da Vinci noted that an illuminated scene is filled with rays that travel in all directions<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>. As evidence, he described a pinhole camera (camera obscura) made by placing a small hole in a wall of a windowless room (Figure&nbsp;<a href="#fig:lightfield" data-reference-type="ref" data-reference="fig:lightfield">1.1</a>). The wall is adjacent to a brightly illuminated piazza; an image of the piazza (inverted) appears on a wall within the room. Leonardo noted that an image is formed wherever the pinhole is placed, and he concluded that the rays needed to form an image must be present at all of these positions. Leonardo compared the space-filling light rays to the traveling waves that arise after dropping a rock in a pond.</p>
<p>The Russian physicist, Gershun, provided a mathematical representation of the geometry of these rays, which he called the light field <span class="citation" data-cites="Gershun1939-lu">[@Gershun1939-lu]</span>. The mathematical representation of the light field quantifies the properties of the light rays at each position in space (Equation <a href="#eqn:lightfield" data-reference-type="ref" data-reference="eqn:lightfield">[eqn:lightfield]</a>). Each ray travels from a location (x,y,z) in a direction (<span class="math inline">\(\alpha,\beta\)</span>) and has a wavelength and polarization (<span class="math inline">\(\lambda,\rho\)</span>). To know these parameters and the intensity of every ray is to know the light field at a given moment in time:</p>
<p><span class="math display">\[LF(x,y,z,\alpha,\beta,\lambda,\rho) .
\label{eqn:lightfield}\]</span></p>
<p>The light field representation does not capture some phenomena of electromagnetic radiation such as interference (waves) or the Poisson character of light absorption by the photoreceptors (photons). Even so, the light field representation provides an excellent model to describe the ways in which light interacts with surfaces, and the geometric description of the light field is important in the mathematics of computer graphics, a technology that is important for illumination engineering, photography and cinema <span class="citation" data-cites="Pharr2016-yb Wald2003-vz Wald2006-ej">[@Pharr2016-yb; @Wald2003-vz; @Wald2006-ej]</span>.</p>
</section>
<section id="the-incident-light-field" class="level3">
<h3 class="anchored" data-anchor-id="the-incident-light-field">The incident light field</h3>
<p>An eye - or a camera - records the small subset of the light field, those rays arriving at the pupil or entrance aperture. We call these the incident light field. In Figure <a href="#fig:lightfield" data-reference-type="ref" data-reference="fig:lightfield">1.1</a> the dashed and solid lines are the light field and the solid lines are the incident light field. The natural parameterization of the incident light differs from the general light field. We can represent the incident light field using only the position (<span class="math inline">\(u,v\)</span>) and angle (<span class="math inline">\(\alpha,\beta\)</span>) of the rays at the entrance aperture of the imaging system:</p>
<p><span class="math display">\[ILF(u,v,\alpha,\beta,\lambda,\rho,t) .
\label{eqn:ilf}\]</span></p>
<p>Equation <a href="#eqn:ilf" data-reference-type="ref" data-reference="eqn:ilf">[eqn:ilf]</a> also represents time (<span class="math inline">\(t\)</span>) explicitly, which allows it to describe effects of motion both in the scene and by the eye.</p>
</section>
<section id="spectral-irradiance-and-the-plenoptic-function" class="level3">
<h3 class="anchored" data-anchor-id="spectral-irradiance-and-the-plenoptic-function">Spectral irradiance and the plenoptic function</h3>
<p>The eye and most cameras do not measure the incident light field. Rather, the rays are focused to form an image and the photodetectors respond to the sum of the rays across all directions. Adelson and Bergen <span class="citation" data-cites="Adelson1991Plenoptic">[-@Adelson1991Plenoptic]</span> introduced the term plenoptic function, a simplified version of the incident light field, that was chosen to guide thinking about the computations carried out in the human visual pathways (their Equation 2). First, they approximated the eye as a pinhole camera; with this approximation all rays have the same entrance position <span class="math inline">\(p\)</span>, and the retinal/sensor surface faces the scene in a single direction <span class="math inline">\(d\)</span> through the pinhole. For pinhole optics, specifying the two angles of a ray at the pinhole is equivalent to specifying the location where the ray intersects the retinal/sensor surface, <span class="math inline">\((r_x,r_y)\)</span>. Finally, Adelson and Bergen ignored polarization as unimportant for human perception. With these restrictions, the plenoptic function for human vision is no more than the retinal spectral irradiance, over time (t), with the added parameters defining the pinhole position (p) and the viewing direction (d):</p>
<p><span class="math display">\[E(r_x,r_y,\lambda,t,p,d).\]</span></p>
<p>Understanding the progression from light field to incident light field to plenoptic function/retinal image is useful for understanding how the information available for visual processing relates to the complete set of potential information that could be sensed by a visual system.</p>
<p>Adelson and Bergen note that by placing the pinhole at many different positions and viewing directions, we can estimate the full light field from the set of spectral irradiances. Thus an array of cameras can return an estimate of the light field. Similarly, a way to estimate the incident light field is to use a lens and insert a microlens array over the photodetector array, placing multiple detectors behind each microlens. Both cameras and microscopes have using this technology to measure the incident light field, not just the spectral irradiance. These specialized devices support depth estimation <span class="citation" data-cites="Adelson1992-bt">[@Adelson1992-bt]</span> and they enable controlling focus and depth of field in post-processing <span class="citation" data-cites="Ng2005-yh">[@Ng2005-yh]</span>.</p>
<p>Such general purpose light-field cameras are not currently in wide use <span class="citation" data-cites="Wikipedia_contributors2021-hz">[@Wikipedia_contributors2021-hz]</span>, but many modern cameras do acquire partial information about the incident light field. Specifically, dual pixel autofocus technology is designed to distinguish the light rays incident at different sides of the entrance aperture <span class="citation" data-cites="Canon_USA_Inc2017-xx Mlinar2016-ev">[@Canon_USA_Inc2017-xx; @Mlinar2016-ev]</span>. This is accomplished by inserting a microlens array over pairs of photodetectors, so that rays from the left and right sides of the lens are captured by different but adjacent detectors. The images from rays arriving from the two sides of the lens are compared and used to set the lens focus or estimate depth.</p>
</section>
<section id="the-initial-visual-encoding" class="level3">
<h3 class="anchored" data-anchor-id="the-initial-visual-encoding">The initial visual encoding</h3>
<p>Computational models of the early visual pathways define a series of transformations that characterize how the incident light field becomes a neural response. In this chapter, we introduce the mathematics used to characterize the initial visual encoding in the context of the first few of these transformations (Figure <a href="#fig:computations" data-reference-type="ref" data-reference="fig:computations">1.2</a>; see also <span class="citation" data-cites="Wandell1995Foundations">@Wandell1995Foundations</span>, <span class="citation" data-cites="Wandell1995Foundations">[-@Wandell1995Foundations]</span>; <span class="citation" data-cites="Rodieck1998FirstSteps">@Rodieck1998FirstSteps</span>,<span class="citation" data-cites="Rodieck1998FirstSteps">[-@Rodieck1998FirstSteps]</span>; <span class="citation" data-cites="Brainard2010Colorimetry">@Brainard2010Colorimetry</span>, <span class="citation" data-cites="Brainard2010Colorimetry">[-@Brainard2010Colorimetry]</span>; <span class="citation" data-cites="Packer2003Light">@Packer2003Light</span>, <span class="citation" data-cites="Packer2003Light">[-@Packer2003Light]</span>). We focus on the encoding of the spectral radiance by the photoreceptors - this encoding provides the sensory measurements upon which subsequent neural processing operates.</p>
<p>A visual scene’s light field is generated by the properties and locations of the light sources and objects, and how the rays from the light sources are absorbed and reflected by the objects. Here we consider the special case of scenes presented on a flat display, so that in the idealized case where the display is the only object and there are no other light sources, the full light field is determined just by the spectral radiance emitted at each location of the display. Elsewhere, we consider the more general case of modeling the formation of the retinal spectral irradiance, given a description of the light sources and objects in a three-dimensional scene <span class="citation" data-cites="Lian2019Ray">[@Lian2019Ray]</span>.</p>
<p>The optics of the eye collect the incident light field and focus the rays to produce the spectral irradiance arriving at the retina. Factors such as diffraction and aberrations in the eye’s optics mean that this image is blurred relative to the displayed image. In addition, wavelength-selective absorption of short-wavelength light by the lens and and inert macular pigment also affect the spectral irradiance. Of note (but not illustrated in Figure <a href="#fig:computations" data-reference-type="ref" data-reference="fig:computations">1.2</a>) the density the macular pigment is high in the central area of the retina and falls off rapidly with increasing eccentricity.</p>
<p>Photoreceptors spatially sample the retinal image. Excitations of photopigment molecules in these photoreceptors provides the information available to the visual system for making perceptual inferences about the scene. Here we consider the cone photoreceptors, which operate at light levels typical of daylight. There are three spectral classes of cones, each characterized by its own spectral sensitivity. That there are three classes leads to the trichromatic nature of human color vision. Figure&nbsp;<a href="#fig:computations" data-reference-type="ref" data-reference="fig:computations">1.2</a> illustrates a patch of cone mosaic from the central region of the human retina. The properties of the mosaic are quite interesting. For example, there are no S cones in the very center of the retina, and many properties of the mosaic (e.g., cone density, cone size, cone photopigment optical density) vary systematically with eccentricity <span class="citation" data-cites="Hofer2014Color Brainard2015Color">[@Hofer2014Color; @Brainard2015Color]</span>.</p>
<div id="fig:computations" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="PCC_images/computations.png" class="img-fluid figure-img" style="width:12cm"></p>
<figcaption><strong>The initial encoding of light by the visual system</strong>. <em>Scene</em>: An image on a display surface is characterized by the spectral radiance at each display location. Images of the display spectral radiance are shown at a few sample wavelengths, along with a rendering of the image. <em>Optics</em>: The incident light field enters the pupil of the eye and a spectral irradiance image is formed on the retina. The <em>retinal image</em> is blurred relative to the displayed image, and the spectral irradiance is affected by lens and macular pigment absorptions. <em>Cone mosaic</em>: The retinal image is spatially sampled by the L, M and S cone mosaics. <em>Cone excitations</em>: The retinal image irradiance, spectrally weighted by each cone photopigment absorptance function, is integrated within the cone’s aperture and temporally integrated over the exposure duration to produce a pattern of cone excitations. We thank Nicolas Cottaris for the figure.</figcaption>
</figure>
</div>
<p>Not considered here is a separate mosaic of highly-sensitive rod photoreceptors that is interleaved with the cone mosaic. The rods mediate human vision at low light levels <span class="citation" data-cites="Rodieck1998FirstSteps">[@Rodieck1998FirstSteps]</span>. We also ignore the melanopsin containing intrinsically-sensitive retinal ganglion cells <span class="citation" data-cites="Hattar2002Melanopsin Van_Gelder2016-dm Gamlin2007Human">[@Hattar2002Melanopsin; @Van_Gelder2016-dm; @Gamlin2007Human]</span>. The principles we develop, however, also apply to modeling the excitations of these receptors.</p>
<p>Computational modeling of the initial linear encoding is well-understood, and we explain these mathematical principles next, using a simplified representation of the light stimulus. Advanced modeling of the subsequent neural processes includes non-linearities, the mathematical principles and computational methods we introduce are a fundamental part of the full description. After explaining the mathematical principles, we illustrate how to extend them through computational modeling that harnesses the power of computers to characterize biological reality in more detail than is possible with analytic calculations alone.</p>
</section>
</section>
<section id="mathematical-principles" class="level2">
<h2 class="anchored" data-anchor-id="mathematical-principles">Mathematical principles</h2>
<section id="linear-systems" class="level3">
<h3 class="anchored" data-anchor-id="linear-systems">Linear systems</h3>
<p>Linear systems and the tools of linear algebra are the most important mathematical methods used in vision science. Indeed, when trying to characterize a system, the scientist and engineer’s first hope is that the system can be approximated as linear. A system, <span class="math inline">\(L\)</span>, is linear if it follows the superposition rule: <span class="math display">\[L(x + y) = L(x) + L(y).\]</span> Here <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are two possible inputs to the system and <span class="math inline">\(x+y\)</span> represents their superposition. The homogeneity rule of linear systems follows from the superposition rule. Consider that</p>
<p><span class="math display">\[\begin{aligned}
L(x + x) &amp;= L(2x) \nonumber  \\
&amp;= L(x) + L(x) \nonumber \\
&amp;= 2L(x). \nonumber
\end{aligned}\]</span></p>
<p>This is easily generalized for any integer <span class="math inline">\(m\)</span> to show that:<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<p><span class="math display">\[L(m x) = m  L(x).\]</span></p>
<p>No physical system can be linear over an infinite range - if you put enough energy into a system it will blow up! But many systems are linear over a meaningful range of input values.</p>
</section>
<section id="linearity-example-cone-excitations-and-color-matching" class="level3">
<h3 class="anchored" data-anchor-id="linearity-example-cone-excitations-and-color-matching">Linearity example: Cone excitations and color matching</h3>
<p>Vision is initiated when a photopigment molecule absorbs a photon of light. The absorption can cause the photopigment, a protein, to change conformation, an event we refer to as a photopigment excitation. The excitation initiates a molecular cascade inside the photoreceptor that changes the ionic currents at the photoreceptor membrane. The change in current modulates the voltage at the photoreceptor’s synapse and causes a release of neurotransmitter <span class="citation" data-cites="Rodieck1998FirstSteps">[@Rodieck1998FirstSteps]</span>.</p>
<p>The transformation from the spectral energy of light, <span class="math inline">\(E(\lambda)\)</span>, incident upon a cone to the number of photopigment excitations, <span class="math inline">\(n\)</span>, produced by that light is an important, early vision, linear system. Consider two different spectra, denoted by <span class="math inline">\(E_1(\lambda)\)</span> and <span class="math inline">\(E_2(\lambda)\)</span>. Let <span class="math inline">\(L\)</span> represent the system that describes the transformation between spectra and excitations. This system obeys the superposition rule: <span class="math display">\[\begin{aligned}
L(E_1(\lambda) + E_2(\lambda)) &amp;= L(E_1(\lambda))  + L(E_2(\lambda))
\label{eq:linearity}
\end{aligned}\]</span> This linearity holds well over a wide range of light levels typical of daylight natural environments <span class="citation" data-cites="Burns1987-kt">[@Burns1987-kt]</span>.</p>
<p>An important feature of photopigment excitations is that their effect on the membrane current and transmitter release does not differ with the wavelength of the exciting photon. Such differences might have existed because different wavelengths are preferentially absorbed at different locations within the cone outer segment, or because photons of different wavelengths carry different amounts of energy. The observation that all excitations have the same impact is called the Principle of Univariance. As Rushton wrote:</p>
<blockquote class="blockquote">
<p>“The output of a receptor depends upon its quantum catch, but not upon what quanta are caught <span class="citation" data-cites="Rushton1972Pigments">[@Rushton1972Pigments]</span>.”</p>
</blockquote>
<p>The color-typical human retina contains three distinct classes of cones, which are referred to as the L (long-wavelength sensitive), M (middle-wavelength sensitive) and S (short-wavelength sensitive) cones. While the effects of photopigment excitations are univariant, the probability of a photopigment excitation is wavelength-dependent. The wavelength-dependent probability that an incident photon leads to an excitation is characterized by the pigment’s spectral absorbtance<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> The absorbtance depends on the specific density of the photopigment within the cone’s outer segment, as well as on the outer segment length; details are elaborated elsewhere (<span class="citation" data-cites="Rodieck1998FirstSteps">@Rodieck1998FirstSteps</span>, <span class="citation" data-cites="Rodieck1998FirstSteps">[-@Rodieck1998FirstSteps]</span>; see also <span class="citation" data-cites="Pugh1988Vision">@Pugh1988Vision</span>, <span class="citation" data-cites="Pugh1988Vision">[-@Pugh1988Vision]</span>; <span class="citation" data-cites="Packer2003Light">@Packer2003Light</span>, <span class="citation" data-cites="Packer2003Light">[-@Packer2003Light]</span>).</p>
<p>It is difficult to measure the light at the retinal surface in the living eye, but it is straightforward to measure the light incident at the cornea. Hence, it is typical to specify the absorptance with respect to the spectrum of the light incident at the cornea. This convention effectively combines the effects of absorption of light by the lens and the inert retinal macular pigment with absorption by photopigment. The cornea-referred absorbtance function is called the cone fundamental.</p>
<p>The three (L-, M-, and S-) cone fundamentals define for each cone type the probability of excitation given the spectrum of light entering the eye. The human cone fundamentals have been carefully measured and tabulated (Figure <a href="#fig:fundamentals" data-reference-type="ref" data-reference="fig:fundamentals">1.3</a>; <span class="citation" data-cites="Stockman1999SpectralShort">@Stockman1999SpectralShort</span>, <span class="citation" data-cites="Stockman1999SpectralShort">[-@Stockman1999SpectralShort]</span>; <span class="citation" data-cites="Stockman2000SpectralLongMiddle">@Stockman2000SpectralLongMiddle</span>, <span class="citation" data-cites="Stockman2000SpectralLongMiddle">[-@Stockman2000SpectralLongMiddle]</span>; http://www.cvrl.org) and are an international standard <span class="citation" data-cites="CIE2007Fundamental">[@CIE2007Fundamental]</span>.</p>
<p>To compute the number of cone excitations we use linear formulas. Suppose that a cone’s fundamental is given by <span class="math inline">\(C(\lambda)\)</span>. Using linearity and continuous mathematics, we compute the number of excitations at a single location as</p>
<p><span class="math display">\[N(r_x,r_y) = \int C(\lambda) E(r_x,r_y,\lambda) d\lambda .\]</span></p>
<p>The discrete form of this integral, commonly used in computational methods, is the inner product of the cone fundamental with the cornea-referred spectral irradiance incident upon a retinal location:<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></p>
<p><span class="math display">\[N(r_x,r_y) = \sum_{\lambda_i} C(\lambda_i) E(r_x,r_y,\lambda_i) \Delta \lambda .\]</span></p>
<p>Here the <span class="math inline">\(\lambda_i\)</span> are a set of <span class="math inline">\(w\)</span> discretely sampled wavelengths, and <span class="math inline">\(\Delta \lambda\)</span> is the wavelength sample spacing.</p>
</section>
<section id="matrix-formulation-of-linearity" class="level3">
<h3 class="anchored" data-anchor-id="matrix-formulation-of-linearity">Matrix formulation of linearity</h3>
<p>We can calculate cone excitations by a matrix multiplication. The matrix <span class="math inline">\(\mathbf{C}\)</span> combines the three discretized cone fundamentals, <span class="math inline">\(C_L(\lambda_i)\)</span>, <span class="math inline">\(C_M(\lambda_i)\)</span>, and <span class="math inline">\(C_S(\lambda_i)\)</span> into its rows, so that its dimension is <span class="math inline">\(3 \times w\)</span>. Similarly, we write the spectral irradiance at a position, <span class="math inline">\(E(r_x,r_y,\lambda)\)</span>, as a <span class="math inline">\(w \times 1\)</span> vector <span class="math inline">\(\mathbf{e}(r_x,r_y)\)</span>. The L-, M-, and S-cone excitations available at a retinal location are described by a <span class="math inline">\(3\)</span>-dimensional column vector,</p>
<p><span class="math display">\[\mathbf{n}(r_x,r_y) = \mathbf{C} \mathbf{e}(r_x,r_y).
\label{eqn:conematrix}\]</span></p>
<p>The vector field <span class="math inline">\(\mathbf{n}(r_x,r_y)\)</span> describes the potential information available to the visual system from the cones at a moment in time. This representation replaces the dependence of the spectral irradiance on wavelength with the excitations of the three classes of cones. As we describe in more detail below, not all of this potential information is sensed by the visual system, since the cones discretely sample <span class="math inline">\(\mathbf{n}(r_x,r_y)\)</span>.</p>
<p>It is worth reflecting on the implication of the linearity expressed by Equation <a href="#eqn:conematrix" data-reference-type="ref" data-reference="eqn:conematrix">[eqn:conematrix]</a>. If we measure the cone fundamentals at each of the sample wavelengths <span class="math inline">\(\lambda_i\)</span>, we can predict the cone excitations to any spectrum <span class="math inline">\(E(r_x,r_y,\lambda_i)\)</span>. Thus, linearity implies that we can compute the system response to any input after making enough measurements to determine the system matrix <span class="math inline">\(\mathbf{C}\)</span>. The ability to delineate the set of measurements required for complete system characterization is an important consequence of linearity, and this observation applies to linear systems in general, not just to computation of cone excitations.</p>
<p>A second implication of Equation <a href="#eqn:conematrix" data-reference-type="ref" data-reference="eqn:conematrix">[eqn:conematrix]</a> concerns which spectral radiances appear to be the same. This was first established quantitatively by the color matching experiment, which dates back to James Clerk Maxwell <span class="citation" data-cites="Maxwell1860OnTheTheory">[-@Maxwell1860OnTheTheory]</span>. He built an instrument to measure pairs of spectral irradiance functions, <span class="math inline">\(\mathbf{e_1}\)</span> and <span class="math inline">\(\mathbf{e_2}\)</span>, that appear the same to humans despite being physically different; these pairs are called metamers. Thomas Young&nbsp;<span class="citation" data-cites="Young1802OnTheTheory">[-@Young1802OnTheTheory]</span> had proposed that metamers arise if two lights produce the same set of cone excitations. This implies that the difference between a metameric pair is in the null space<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> of the matrix, <span class="math inline">\(\mathbf{C}\)</span>. That is, <span class="math inline">\(\mathbf{e_1}\)</span> and <span class="math inline">\(\mathbf{e_2}\)</span> must satisfy</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{C} \mathbf{e_1} &amp;= \mathbf{C} \mathbf{e_2}  \nonumber \\
\mathbf{0} &amp;= \mathbf{C}(\mathbf{e_2} - \mathbf{e_1}).
\end{aligned}\]</span></p>
<p>Wyszecki (<span class="citation" data-cites="Wyszecki1958Evaluation">[-@Wyszecki1958Evaluation]</span>; see also <span class="citation" data-cites="WyseckiStiles1982Book">@WyseckiStiles1982Book</span>, <span class="citation" data-cites="WyseckiStiles1982Book">[-@WyseckiStiles1982Book]</span>) referred to vectors in the null space of <span class="math inline">\(\mathbf{C}\)</span> as metameric black spectra. Adding a metameric black to any spectrum produces a metamer.</p>
<p>Displays and printers do not reproduce the original physical stimulus; rather, they create lights that aspire to be metamers to the original. Thus, calculation of metamers is central to color reproduction technologies; knowledge of the matrix <span class="math inline">\(\mathbf{C}\)</span> is sufficient to compute metamers. Although the dimensionality of the space of metamers is quite high (<span class="math inline">\(w-3\)</span>), metamers produced by devices are constrained by the spectra that the device can produce. The computation of metamers for color reproduction applications is discussed in detail elsewhere (<span class="citation" data-cites="Hunt2004Reproduction">@Hunt2004Reproduction</span>, <span class="citation" data-cites="Hunt2004Reproduction">[-@Hunt2004Reproduction]</span>; <span class="citation" data-cites="Brainard2010Colorimetry">@Brainard2010Colorimetry</span>, <span class="citation" data-cites="Brainard2010Colorimetry">[-@Brainard2010Colorimetry]</span>).</p>
<p>Notice that the null space of <span class="math inline">\(\mathbf{C}\)</span> is the same as the null space of <span class="math inline">\(\mathbf{T} = \mathbf{M C}\)</span>, for any invertible <span class="math inline">\(3 \times 3\)</span> matrix, <span class="math inline">\(\mathbf{M}\)</span>. Thus any such matrix <span class="math inline">\(\mathbf{T}\)</span> may be used to predict physically different lights that match perceptually. The rows of <span class="math inline">\(\mathbf{T}\)</span>, when viewed as functions of wavelength, are referred to as a set of color matching functions. The technology for creating metamers relies on standardized color-matching functions <span class="citation" data-cites="CIE1986Colorimetry CIE2007Fundamental">[@CIE1986Colorimetry; @CIE2007Fundamental]</span>. How color matching functions may be obtained directly from perceptual color matching experiments, without explicit reference to the cone fundamentals, is treated in many sources <span class="citation" data-cites="WyseckiStiles1982Book Wandell1995Foundations Brainard2010Colorimetry">[@WyseckiStiles1982Book; @Wandell1995Foundations; @Brainard2010Colorimetry]</span>. Indeed, high-quality measurements of behavioral color matching (e.g. <span class="citation" data-cites="Stiles1959NPL">@Stiles1959NPL</span>, <span class="citation" data-cites="Stiles1959NPL">[-@Stiles1959NPL]</span>) provide key data that constrain modern estimates of human cone fundamentals.</p>
<p>There are a number of properties of the eye that must be modeled if we are to compute a true estimate of cone mosaic excitations. For example, only one type of cone is present at each position, so we must specify a cone spatial sampling scheme. That is the reason that we use the term <em>potential information</em> to describe cone excitations <span class="math inline">\(\mathbf{n}(r_x,r_y)\)</span> as a function of retinal location - not all of that information is sampled by the cone mosaic. Also, as noted above, the density of both inert pigments and photopigments varies with retinal location, as does the size of the cone apertures. Enough is known about these properties to enable us to compute a reasonable approximation to the cone mosaic excitations across the retina.</p>
<figure id="fig:fundamentals" class="figure">
<img src="PCC_images/Fundamentals.png" style="width:12cm" class="figure-img">
<p>
. <span id="fig:fundamentals" data-label="fig:fundamentals"></span>
</p>
<figcaption>
<strong>Human cone fundamentals</strong>. The left panel shows estimates of the L-, M- and S-cone fundamentals for foveal viewing. The fundamentals are the probability of excitation per photon entering the cone’s entrance aperture, but with pre-retinal absorption taken into account. Note the large difference between the L- and M-cone fundamentals compared to the S-cone fundamental. This difference is due partly to the selective absorption of short-wavelength light by the lens and macular pigment. The right panel shows estimates for cones at 10° eccentricity. The S-cone fundamental is relatively higher at 10°, because there is little or no macular pigment at that eccentricity; and for the same reason there is a slight change in the relative values of the L- and M-cone fundamentals. In addition, the cone outer segment lengths decrease with eccentricity, leading to the lower peak probability of excitation in the periphery. This reduction, however, is more than compensated for by an increase in the size of the cone apertures with eccentricity. The impact of the aperture is not shown in these plots, but see Figure <a href="#fig:noise" data-reference-type="ref" data-reference="fig:noise">1.4</a>
</figcaption>
</figure>
</section>
<section id="noise-in-the-sensory-measurements" class="level3">
<h3 class="anchored" data-anchor-id="noise-in-the-sensory-measurements">Noise in the sensory measurements</h3>
<p>Measurement noise is fundamental in the physical sciences and engineering. Two types of noise are used throughout the sensory sciences: Gaussian (Normal) noise and Poisson noise. Gaussian noise has two parameters (a mean and variance) but the Poisson distribution has a single parameter (the Poisson mean equals its variance). The formulas for Gaussian and Poisson noise, along with example probability density/mass functions, are shown in Figure <a href="#fig:noise" data-reference-type="ref" data-reference="fig:noise">1.4</a>.</p>
<p>The Gaussian and Poisson distributions can be compared by setting the Gaussian mean equal to its variance. For small values, the Gaussian has values below zero. As the Poisson mean increases, the matched Gaussian is extremely similar (Figure <a href="#fig:noise" data-reference-type="ref" data-reference="fig:noise">1.4</a>).</p>
<p>There is an important conceptual difference between how these noise distributions are used in applications. There are many theorems about additive Gaussian noise, and thus it is common to introduce noise in a model with such noise using a fixed mean (<span class="math inline">\(\mu\)</span>) and standard deviation (<span class="math inline">\(\sigma\)</span>). The added noise has the the same distribution for all values of the signal (signal-independent noise).</p>
<p>For typical sensor measurements, including the cone excitations, the noise depends on the signal. Specifically, for the cones and many other measurement devices, the noise is Poisson distributed and the Poisson parameter is equal to the mean number of excitations (signal-dependent noise). The difference between signal-independent and signal-dependent noise can be quite significant (Figure&nbsp;<a href="#fig:noise" data-reference-type="ref" data-reference="fig:noise">1.4</a>).</p>
<div class="center">
<div id="fig:noise" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="PCC_images/noise.png" class="img-fluid figure-img" style="width:12cm"></p>
<figcaption><strong>The number of cone excitations is inescapably noisy, following a signal-dependent Poisson distribution</strong>. (Top). For mean values greater than 10, the Poisson distribution is reasonably approximated by a Gaussian distribution with a mean equal to the variance. For smaller values, it is necessary to clip the negative values for the Gaussian to achieve a good approximation. Low excitation rates are common under low-light conditions and for nearly all conditions when assessing the S-cones and rods <span class="citation" data-cites="Hecht1942Energy Baylor1979Responses">[@Hecht1942Energy; @Baylor1979Responses]</span>. (Bottom) The signal-dependent nature of Poisson noise is important; simply adding Gaussian noise with a fixed mean is not a good approximation if there is a substantial range in the mean excitation values. The images illustrate the excitations in response to a series of bars spanning a large range of mean excitation using a signal-independent clipped Gaussian noise (left) and a Poisson noise (right). The Gaussian distribution added to the signal has zero mean and variance equal to the number of excitations in the brightest bar (blue arrows); this approximates Poisson noise for that bar. The inset trace, which shows excitations across a row of the image, illustrates that the Gaussian noise is far too large for the dark bars. Had the variance been set to match the noise at the dark bar, the clipped Gaussian would be far too small for the brightest bar. The simulation was created for an array of M-cones in the central fovea, a 2 ms exposure duration, achromatic bars of increasing intensity, and the brightest bar with a luminance of 300 cd/m<sup>2</sup>.</figcaption>
</figure>
</div>
</div>
</section>
<section id="image-formation" class="level3">
<h3 class="anchored" data-anchor-id="image-formation">Image formation</h3>
<p>The linear system principles described for one-dimensional spectral functions can be extended to two-dimensional functions, such as images. We use linear system methods to analyze how the cornea and lens form the retinal image. An important, but simple, case occurs for an image confined to a plane, such as a visual display or an optometrist’s eye chart. For such images we can estimate the spectral irradiance at the cone apertures using a two-dimensional linear system computation.</p>
<p>The image emitted from a visual display is a function of position <span class="math inline">\((x,y)\)</span> and wavelength <span class="math inline">\(\lambda\)</span>. As a first approximation, the display emits the same density of rays over a wide angle, which is why the display appears to be approximately the same when seen from different positions. The image from the display is called the spectral radiance, <span class="math inline">\(I(x,y,\lambda)\)</span>, and it has units of <span class="math inline">\(W/sr/{m^2}/nm\)</span>.</p>
<p>The spectral irradiance at the retina, <span class="math inline">\(E(r_x,r_y,\lambda)\)</span>, is formed from the cone of rays that are captured by the pupil. In this case, <span class="math inline">\(r_x\)</span> and <span class="math inline">\(r_y\)</span> specify retinal location and the units of the image are those of spectral irradiance, <span class="math inline">\(W/m^2/nm\)</span>, which result from integration over the solid angle of the pupil.</p>
<p>The key linear system idea (Equation&nbsp;<a href="#eq:linearity" data-reference-type="ref" data-reference="eq:linearity">[eq:linearity]</a>) holds for retinal image formation <span class="citation" data-cites="Wandell1995Foundations">[@Wandell1995Foundations]</span>. If two input images <span class="math inline">\(I_1(x,y,\lambda)\)</span> and <span class="math inline">\(I_2(x,y,\lambda)\)</span>, produce two retinal images <span class="math inline">\(E_1(r_x,r_y,\lambda)\)</span> and <span class="math inline">\(E_2(r_x,r_y,\lambda)\)</span>, then the superposition of the input images, <span class="math inline">\(I_1(x,y,\lambda) = I_1(x,y,\lambda) + I_2(x,y,\lambda)\)</span>, produces the superposition of the retinal images,</p>
<p><span class="math display">\[E(r_x,r_y,\lambda) = E_1(r_x,r_y,\lambda) + E_2(r_x,r_y,\lambda).\]</span></p>
<p>It follows that if the input image is the weighted sum of two input images, <span class="math inline">\(I(x,y,\lambda) = \alpha I_1(x,y,\lambda) + \beta I_2(x,y,\lambda)\)</span>, the output retinal image will be the weighted sum of the two corresponding retinal images <span class="math display">\[E(r_x,r_y,\lambda) = \alpha E_1(r_x,r_y,\lambda) + \beta E_2(r_x,r_y,\lambda)\]</span></p>
<p>As noted above, an important consequence of linearity is that it tells us how to generalize. When we know the response to an image <span class="math inline">\(I_k\)</span>, measuring the response to a second image, <span class="math inline">\(I_j\)</span>, enables us to predict the responses to an entire class of new images, all images of the form <span class="math inline">\(\alpha I_k + \beta I_j\)</span>.</p>
</section>
<section id="shift-invariance-and-convolution" class="level3">
<h3 class="anchored" data-anchor-id="shift-invariance-and-convolution">Shift-invariance and convolution</h3>
<p>We saw in our treatment of color matching (Equation <a href="#eqn:conematrix" data-reference-type="ref" data-reference="eqn:conematrix">[eqn:conematrix]</a>) that a discrete linear system may be characterized as a matrix multiplication. This is also true for retinal image formation, but in this case the number of measurements required to determine the requisite matrix is very large for typical image sizes. For this reason, we consider an additional special and simplifying property linear systems can have: shift-invariance. These are linear systems such that shifting the position of the input stimulus correspondingly shifts the position of the output, without changing its form<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>. It is possible to measure whether a system is shift-invariant by a simple experiment. Take an input image, say <span class="math inline">\(I(x,y,\lambda)\)</span>, and measure the retinal image <span class="math inline">\(E(r_x,r_y,\lambda)\)</span>. Then shift the input, <span class="math inline">\(I(x-\delta x, y- \delta y,\lambda)\)</span> and measure the retinal image again. If for all choices of <span class="math inline">\(I(x,y,\lambda)\)</span> and <span class="math inline">\(\delta\)</span>, the output corresponding to <span class="math inline">\(I(x-\delta x, y- \delta y,\lambda)\)</span> is a shifted version of <span class="math inline">\(E(r_x,r_y,\lambda)\)</span>, <span class="math inline">\(E(r_x-\delta r_x, r_y- \delta r_y, \lambda)\)</span>, then the system is shift-invariant.</p>
<p>We can express linearity and shift-invariance using the convolution formula. For simplicity, we choose one wavelength and suppress <span class="math inline">\(\lambda\)</span>. Suppose <span class="math inline">\(P(r_x,r_y)\)</span> is the retinal image from an image that is just a single point. The image <span class="math inline">\(P(r_x,r_y)\)</span> plays a central role in the characterization of optical systems: it is called the point spread function.<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> The point spread function is all we need to compute the retinal image for any input image. The idea is to treat the input image as a set of points, and to add shifted copies of the point spread function, each weighted by the input image intensity:</p>
<p><span class="math display">\[E(r_x,r_y) = \int_u \int_v I(u,v)P(r_x-u,r_y-v) du dv .\]</span></p>
<p>The importance of linear shift-invariance is that we characterize the system fully by one measurement, <span class="math inline">\(P(r_x,r_y)\)</span>. We use the convolution formula and this measurement to compute the responses of a linear shift-invariant system to any input.</p>
<p>While shift-invariance and convolution are important concepts, the eye’s optics deviates significantly from this ideal. Shift-invariance is a good approximation of human retinal image formation in local regions, say spanning a few degrees of visual angle and a change in wavelength of 20-50 nm. Properties of the photoreceptor sampling mosaic further limit the accuracy of the shift-invariant approximation of the visual encoding (see Figure&nbsp;<a href="#fig:sampling" data-reference-type="ref" data-reference="fig:sampling">1.6</a>). Thus the convolutional approximation is helpful for thinking about encoding over small regions, but it is not an accurate depiction when one considers a larger field of view. A realistic approximation requires computational modeling.</p>
</section>
</section>
<section id="computational-model-of-the-initial-encoding" class="level2">
<h2 class="anchored" data-anchor-id="computational-model-of-the-initial-encoding">Computational model of the initial encoding</h2>
<p>The mathematical principles described above tell us how to compute the retinal image and the noisy cone excitations from a displayed image; the calculations are straightforward for a single retinal location. But an accurate model of the visual system must account for variations in the optics, pigments, and sampling properties of the cone mosaic with visual field location. These are substantial and impact the information available to the brain for making perceptual inferences about the visual scene. Parameters with significant spatial variation across the visual field include the optical point spread function, density and size of the cones in the mosaic, the distribution of different cone types within the overall mosaic, and the cone fundamentals. To make a realistic calculation requires implementing a computational model of the visual transformations.</p>
<section id="the-value-of-computational-modeling" class="level3">
<h3 class="anchored" data-anchor-id="the-value-of-computational-modeling">The value of computational modeling</h3>
<p>Carefully validated computer simulation of the initial visual encoding has the potential to support advances in understanding many aspects of visual function. We use image-computable models to build upon the mathematical characterizations - earned through 400 years of experimental and theoretical work in vision science - and estimate the initial visual signals. Such knowledge is an essential foundation to use when modeling less well understood visual processes. The models help us separate effects attributable to known factors of the initial encoding from effects of factors that arise in later processing. For example, understanding cortical visual processing requires representing the input to the cortex. Without accurate modeling of the input, we risk attributing features of the cortical signals to the wrong neural mechanisms.</p>
<p>Because of the central role computational modeling plays in understanding vision, we have invested in developing a set of freely available software tools to model retinal image formation and cone excitations (Image Systems Engineering Tools for Biology - ISETBio; https://github.com/isetbio/isetbio.git; <span class="citation" data-cites="Cottaris2019AComputational">@Cottaris2019AComputational</span>, <span class="citation" data-cites="Cottaris2019AComputational">[-@Cottaris2019AComputational]</span>; <span class="citation" data-cites="Cottaris2020AComputational">@Cottaris2020AComputational</span>, <span class="citation" data-cites="Cottaris2020AComputational">[-@Cottaris2020AComputational]</span>). The tools can be used for images presented on planar displays and for a full three-dimensional descriptions of the objects and light sources in the scene (Image Systems Engineering Tools 3D; https://github.com/iset/iset3d.git; <span class="citation" data-cites="Lian2019Ray">@Lian2019Ray</span>, <span class="citation" data-cites="Lian2019Ray">[-@Lian2019Ray]</span>). In this section we briefly illustrate some basic calculations enabled by ISETBio. We are not advocating uniquely for our implementation, but we do believe that the field needs to develop trusted open-science tools for computational modeling.</p>
</section>
<section id="shift-varying-and-wavelength-dependent-pointspreads" class="level3">
<h3 class="anchored" data-anchor-id="shift-varying-and-wavelength-dependent-pointspreads">Shift-varying and wavelength-dependent pointspreads</h3>
<p>The point spread functions from a single subject, measured at different retinal locations and wavelengths, differ significantly (Figure&nbsp;<a href="#fig:pointspread" data-reference-type="ref" data-reference="fig:pointspread">1.5</a>). The variation with retinal location occurs because the optical aberrations depend on the direction of the rays incident at the retina. The ISETBio tools explicitly represent the full incident light field and can calculate these effects from a model eye <span class="citation" data-cites="Lian2019Ray">[@Lian2019Ray]</span>. Improvement of eye models is an active area of investigation, and in some cases ISETBio uses empirical measurements of the eye’s optics to estimate the point spread function for a range of retinal field locations <span class="citation" data-cites="Jaeken2012-bc Polans2015-de">[@Jaeken2012-bc; @Polans2015-de]</span>.</p>
<p>The point spread function varies with pupil diameter and wavelength in addition to visual field position. The dependence on pupil diameter, which varies with the light level of the scene, occurs for two reasons. As the pupil opens the aberrations vary because more of the imperfectly-shaped corneal and lens surfaces refract the light. As the pupil closes, diffraction starts to be a significant factor. The wavelength dependence is explained by the refractive indices of the cornea and lens. These chromatic aberrations are the largest of all the aberrations&nbsp;<span class="citation" data-cites="Thibos1990-sq Wandell1995Foundations">[@Thibos1990-sq; @Wandell1995Foundations]</span>.</p>
<div id="fig:pointspread" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="PCC_images/pointspread.png" class="img-fluid figure-img" style="width:12cm"></p>
<figcaption><strong>The human point spread function</strong>. The diagram at the left shows simple ways to estimate degrees of visual angle: Hold your hand at arms length and make the gesture. The images in the top row show the point spread functions at 550 nm from a typical subject measured at three different visual eccentricities. The point spread increases with eccentricity. The bottom images show the point spread but for light at 450 nm. The human eye cannot focus these two wavelengths at the same time because the index of refraction in the lens and cornea is wavelength-dependent. For many people, chromatic aberration is the largest aberration. Diagram on the left reproduced from <span class="citation" data-cites="Branwyn2016-sc Branwyn2016-sc">@Branwyn2016-sc [-@Branwyn2016-sc]</span>.</figcaption>
</figure>
</div>
</section>
<section id="shift-varying-sampling" class="level3">
<h3 class="anchored" data-anchor-id="shift-varying-sampling">Shift-varying sampling</h3>
<p>Figure&nbsp;<a href="#fig:sampling" data-reference-type="ref" data-reference="fig:sampling">1.6</a> shows the spatial arrangement of cones at different locations within the retina. The cone density is highest in the central fovea where the cones are tightly packed. Moving away from the center, cone density falls off and the cone apertures become larger. As cone density decreases, rod photoreceptors (the smaller receptors in the peripheral images) appear and fill the gaps between the cones. In addition, not apparent in the figure, cones become shorter away from the fovea. The shortening affects the spectral absorptance.</p>
<p>The sampling density reduction means that less spatial information about the retinal image is extracted at retinal locations away from the central fovea. The relative density of the different cone types also varies with eccentricity. Indeed, as noted above, there are no S-cones in the very central fovea <span class="citation" data-cites="Williams1981Foveal">[@Williams1981Foveal]</span>, so that vision in this small retinal region is dichromatic rather than trichromatic. Perhaps this region is specialized for high-resolution vision and omitting a few S-cones, which see a blurry retinal image at short wavelengths because of the chromatic aberrations, maximizes the information transmitted to the brain about spatial structure <span class="citation" data-cites="Williams1991Cost Hofer2014Color Brainard2015Color Garrigan2010Design Zhang2021Reconstruction">[@Williams1991Cost; @Hofer2014Color; @Brainard2015Color; @Garrigan2010Design; @Zhang2021Reconstruction]</span>.</p>
<div id="fig:sampling" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="PCC_images/sampling.png" class="img-fluid figure-img" style="width:12cm"></p>
<figcaption><strong>Human cone and rod sampling mosaics</strong>. The en face images show the photoreceptor inner segments, where light enters the cones, at four retinal eccentricities. In the central region, all of the receptors are cones. At 4 deg and beyond, the large apertures are the cones and the smaller apertures are the rods. The cone sampling density and cone aperture sizes differ substantially between the central fovea and other visual eccentricities. The reduced sampling density limits the spatial resolving power of the eye. The larger cone apertures increase the rate of photon excitations per cone. Scale bar is 10 um. Reproduced from Curcio <span class="citation" data-cites="Curcio1990Photoreceptor">[-@Curcio1990Photoreceptor]</span>.</figcaption>
</figure>
</div>
<p>The impact of the cone size and density, along with variations in the inert pigments described above, mean that calculating the cone excitations is shift-varying: The calculation is linear, but the parameters change with eccentricity. These eccentricity-dependent calculations are included in the ISETBio simulations. There is little value in expressing the full complexity of these calculations in pure mathematical form.</p>
<p>The impact of the several eccentricity-dependent factors on the cone excitations is substantial and illustrated in Figure&nbsp;<a href="#fig:excitations" data-reference-type="ref" data-reference="fig:excitations">1.7</a>. The images in the left column illustrate calculations in central fovea and the images in the right column illustrate the same calculations at 10 deg in the periphery. The top image shows the differences in the size and density of the cone photoreceptor apertures. Also, notice the absence of S-cones in the small region of the very central fovea. The images inset in the top show the size of the point spread function for an infocus wavelength: there are many more cones within the foveal point spread than within the 10 deg point spread.</p>
<p>The images in the middle row represent the number of cone excitations in response to a relatively low frequency grating pattern. There are more excitations per cone at 10 deg than in the fovea, and there are many more cones representing the stimulus in the fovea. The third row shows the effect of increasing the stimulus spatial frequency. The foveal mosaic samples densely enough to preserve the regular pattern, but at 10 deg the spatial samples look like a wobbly form of the stimulus.</p>
<div id="fig:excitations" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="PCC_images/excitations.png" class="img-fluid figure-img" style="width:12cm"></p>
<figcaption><strong>Excitation calculations</strong>. The two columns represent two retinal eccentricities, each about 1 deg<sup>2</sup>. (Top): The interleaved L, M and S cone mosaics are shown as red, green and blue dots, are shown at the top. The inset shows an expanded view of the point spread function in the same region. The rods are not represented. (Middle and Bottom): The gray level n these images shows the estimated cone excitations for a 6 c/deg harmonic and a 12 c/deg harmonic. The scale for the foveal location runs between 0 and 250, while that for the peripheral location runs from 0 to 1000. Peripheral cones have more excitations to the same stimulus because the cone apertures are larger. Courtesy of Nicolas Cottaris.</figcaption>
</figure>
</div>
<p>Finally, notice that there are many cones that have a very low excitation level to this achromatic stimulus. These cones appear as the quasi-regular array of black dots that are easy to see at 10 deg. They are also present, but harder to see in excitations for the central location. These cones are the S-cones, which absorb many fewer photons than the L- and M-cones. This lower excitation rate is partly due the spectral transmission of the lens (and in the central region the macular pigment), which absorbs a great deal of short-wavelength light.</p>
<p>In summary, the principles of linearity and shift-invariance are useful guides for thinking about cone excitations. These principles were part of our toolkit as we built a specific model of the human eye, and so they would be for any model. However, the deviations from shift-invariance in the human eye are substantial. In addition, there are significant differences between people that may be important for explaining between subject differences. For these reasons, a computational model is essential for applications that aim to create realistic estimates of the cone excitations.</p>
<p>Finally, an essential ingredient for building a computational model is data sets that quantify the critical model parameters (e.g.&nbsp;how the optical PSF and cone density vary with visual field position) and how these parameters are likely to vary across individuals. The computational implementation has benefited enormously from data collected and shared by many investigators. Conversely, the exercise of building computational models often highlights the need for data sets that do not yet exist (e.g.&nbsp;across individuals, do are optical quality and cone density independent, or do they covary in some systematic way?).</p>
<p>At this point in the chapter, the reader might find it useful to re-read the quote at the start of this chapter which was written by Von Kries, Helmholtz’ greatest disciple <span class="citation" data-cites="Cahan1993-mm">[@Cahan1993-mm]</span>, more than a century ago.</p>
</section>
<section id="spatial-derivatives-of-the-cone-excitations-mosaic" class="level3">
<h3 class="anchored" data-anchor-id="spatial-derivatives-of-the-cone-excitations-mosaic">Spatial derivatives of the cone excitations mosaic</h3>
<p>In addition to describing the plenotpic function, Adelson and Bergen <span class="citation" data-cites="Adelson1991Plenoptic">[-@Adelson1991Plenoptic]</span> observed that the partial derivatives of the spectral irradiance correspond to computations performed by neurons in the early visual system. Figure&nbsp;<a href="#fig:derivativeslater" data-reference-type="ref" data-reference="fig:derivativeslater">1.8</a> illustrates these derivatives for several cases that they highlight: derivatives with respect to spatial position, wavelength, and viewpoint (i.e.&nbsp;across the viewpoints provided by the left and right eyes). Receptive fields that respond to these derivatives include neurons that are pattern-selective <span class="citation" data-cites="shapley1985spatial priebe2016mechanisms">[@shapley1985spatial; @priebe2016mechanisms]</span>, cone-opponent <span class="citation" data-cites="Solomon2997Machinery Shevell2017Color">[@Solomon2997Machinery; @Shevell2017Color]</span>, and stereo disparity-selective <span class="citation" data-cites="cumming2001physiology">[@cumming2001physiology]</span>. Partial derivatives with respect to time describe motion-selective neurons <span class="citation" data-cites="wei2018neural pasternak2020linking">[@wei2018neural; @pasternak2020linking]</span>.</p>
<p>The emphasis that Adelson and Bergen <span class="citation" data-cites="Adelson1991Plenoptic">[-@Adelson1991Plenoptic]</span> place on derivatives is consistent with the generally-accepted idea that it is the local change (contrast) in the spectral irradiance, not the absolute level of that irradiance, that provides the critical information used for perception <span class="citation" data-cites="Shapley1986Importance">[@Shapley1986Importance]</span>. Later in the chapter, we analyze psychophysical measurements of contrast sensitivity, which characterize quantitatively how small changes in spatial contrast are encoded by human vision.</p>
<p>An additional advantage of representations based on derivatives is that they are a highly compressible representation of naturally occurring spectral irradiance. The reason for this is that natural radiances tend to vary slowly, and thus many of the partial derivatives are small. A distribution with many small values may be compressed by coding the small values with tokens specified with a small number of bits, reserving tokens specified with a large number of bits for rarely occurring larger values <span class="citation" data-cites="Cover1991Elements Wandell1995Foundations">[@Cover1991Elements; @Wandell1995Foundations]</span>.</p>
<div id="fig:derivativeslater" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="PCC_images/derivatives.png" class="img-fluid figure-img" style="width:12cm"></p>
<figcaption><strong>Derivatives of the retinal image</strong>. A scene (top) is represented as spectral irradiance hypercubes for the left and right eye. The responses of neurons that compute the local differences, as indicated by several oval pairs with <span class="math inline">\(\pm\)</span>, approximate local partial derivatives. Differences can be taken across spatial location, across wavelength, across the spectral radiance measured by the two eyes, and across time (not shown). The original color image was kindly provided by David Sparks.</figcaption>
</figure>
</div>
</section>
</section>
<section id="perceptual-inference" class="level2">
<h2 class="anchored" data-anchor-id="perceptual-inference">Perceptual inference</h2>
<section id="ambiguity-and-perceptual-processing" class="level3">
<h3 class="anchored" data-anchor-id="ambiguity-and-perceptual-processing">Ambiguity and Perceptual Processing</h3>
<p>An important and consistent take-away from the analysis of sensory encoding is that the information available to the brain about the state of the external world is ambiguous: many different physical configurations produce the same sensory representation. A classic example is metamerism: There are only three classes of cone photoreceptors and different spectra produce identical triplets of responses in the L, M and S cones. Another well-known example is depth reconstruction: The three spatial dimensions of the light field are projected onto a two dimensional retina, and many 3D shapes produce the same retinal image. Such many-to-one mappings are a reason why Helmholtz (<span class="citation" data-cites="Helmholtz1866Handbuch">[-@Helmholtz1866Handbuch]</span>; <span class="citation" data-cites="Helmholtz1896Southall">[-@Helmholtz1896Southall]</span>) emphasized perceptual inference: The brain decodes the sensory representation to produce perceptions that are a likely guess about the state of the external world. Perception is an unconscious inference.</p>
</section>
<section id="mathematical-principles-of-inference" class="level3">
<h3 class="anchored" data-anchor-id="mathematical-principles-of-inference">Mathematical principles of inference</h3>
<p>The mathematical formulation of perceptual inference can be developed within a Bayesian probabilistic framework. Suppose <span class="math inline">\(\mathbf{x}\)</span> is a vector that describes some aspect of a scene. The entries of <span class="math inline">\(\mathbf{x}\)</span> might represent the spectral power density of a light entering the eye at a set of discretely sampled wavelengths, the pixel values of a displayed stimulus image, the optical flow vectors corresponding to a viewed dynamic scene, or a full 3D scene description input to a computer graphics package. Now, suppose <span class="math inline">\(\mathbf{y}\)</span> is the sensory representation at some stage of the visual system produced when an observer views the scene described by <span class="math inline">\(\mathbf{x}\)</span>. The entries of <span class="math inline">\(\mathbf{y}\)</span> might describe the retinal image, the excitations of each cone in the retinal mosaic, or the action potentials in a class of retinal ganglion cells.</p>
<p>Because sensory measurements are noisy, the relation between <span class="math inline">\(\mathbf{y}\)</span> and <span class="math inline">\(\mathbf{x}\)</span> is described by a conditional probability distribution, <span class="math inline">\(p(\mathbf{y} | \mathbf{x})\)</span>. This distribution is referred to as the likelihood function. The likelihood can be estimated using a forward model that relates the scene parameters <span class="math inline">\(\mathbf{x}\)</span> to the sensory representation <span class="math inline">\(\mathbf{y}\)</span>.</p>
<p>Within the Bayesian framework, the perceptual representation results from a choice the brain makes about the most likely scene given the observed sensory representation. Indeed, we can reverse the likelihood function, <span class="math inline">\(p(\mathbf{y} | \mathbf{x})\)</span>, to obtain a conditional probability distribution <span class="math inline">\(p(\mathbf{x} | \mathbf{y})\)</span>, which is called the posterior distribution. The posterior defines which are the more or less likely scenes, given the sensory measurements. To obtain the posterior, we use Bayes Rule <span class="citation" data-cites="Lee1989Bayeisan Bishop2006Pattern">[@Lee1989Bayeisan; @Bishop2006Pattern]</span> <span class="math display">\[p(\mathbf{x} | \mathbf{y}) = K( \mathbf{y}) p( \mathbf{y} |  \mathbf{x}) p(\mathbf{x}),
\label{eqn:bayesrule}\]</span></p>
<p>where <span class="math inline">\(K( \mathbf{y})\)</span> is a normalizing factor that depends on <span class="math inline">\(\mathbf{y}\)</span> but not <span class="math inline">\(\mathbf{x}\)</span>. This factor ensures that the posterior integrates to 1 for any value of <span class="math inline">\(\mathbf{y}\)</span>. For many applications, our interest is in how the posterior depends on <span class="math inline">\(\mathbf{x}\)</span>, and it is not necessary to compute <span class="math inline">\(K(\mathbf{y})\)</span>.</p>
<p>Critically, <span class="math inline">\(p(\mathbf{x})\)</span> is a prior distribution that describes the statistical regularities of the scenes; how likely it is <em>a priori</em> that the world is in the state <span class="math inline">\(\mathbf{x}\)</span>. A prior is essential because many scenes might have produced the same sensory measurements. Bayes Rule specifies how to combine the prior with the likelihood. Sometimes little is known about the prior. In these cases, using the Bayesian formulation directs our attention to learn more about it. The Bayesian formulation also forces us to make the forward model explicit in the form of the likelihood.</p>
<p>The posterior is a distribution over possible <span class="math inline">\(\mathbf{x}\)</span>. We need a means of selecting a specific one, say <span class="math inline">\(\hat{\mathbf{x}}\)</span>, to generate the percept. One common way to make a choice is to select a value <span class="math inline">\(\hat{\mathbf{x}}\)</span> that is most likely: the maximum a posterior (MAP) estimate. Other possibilities, such as the mean of the posterior, are also commonly used. The interested reader is referred to the literature on Bayesian decision theory (e.g., <span class="citation" data-cites="Berger1985Statistical">@Berger1985Statistical</span>, <span class="citation" data-cites="Berger1985Statistical">[-@Berger1985Statistical]</span>) for more on this topic.</p>
<p>It is helpful to consider a simple example. Above we explained that the mean cone excitations at a location are a linear function of the radiance of a displayed image. Suppose we treat the spectral radiance on a display as the state of the world <span class="math inline">\(\mathbf{x}\)</span>, with the entries of <span class="math inline">\(\mathbf{x}\)</span> appropriately ordered, and we denote the noisy cone excitations as <span class="math inline">\(\mathbf{y}\)</span>. Then, <span class="math display">\[\mathbf{y} = \mathbf{C} \mathbf{x} + \mathbf{\epsilon}\]</span></p>
<p>for an appropriately arranged matrix <span class="math inline">\(\mathbf{C}\)</span>. and where the noise in cone excitations is represented by the random variable <span class="math inline">\(\mathbf{\epsilon}\)</span>. If we approximate <span class="math inline">\(\mathbf{\epsilon}\)</span> as with signal-independent zero-mean Gaussian distribution, we have:</p>
<p><span class="math display">\[p(\mathbf{y} | \mathbf{x}) = norm(\mathbf{C} \mathbf{x} , \sigma_y^2 \mathbf{I}_y) .\]</span></p>
<p>where <span class="math inline">\(norm()\)</span> denotes the multivariate Gaussian distribution, <span class="math inline">\(\sigma_y^2\)</span> is the variance of the noise added to each mean cone excitation, which will be equal to the mean for the Poisson approximation. The symbol <span class="math inline">\(\mathbf{I}_y\)</span> denotes the identity matrix with the same dimensionality as the vector <span class="math inline">\(\mathbf{y}\)</span>.</p>
<p>We can also use a Gaussian distribution to describe a prior over <span class="math inline">\(x\)</span></p>
<p><span class="math display">\[p(\mathbf{x}) = norm(\mathbf{\boldsymbol{\mu}}_x,\mathbf{\Sigma}_x)\]</span></p>
<p>where the vector <span class="math inline">\(\boldsymbol{\mu}_x\)</span> and matrix <span class="math inline">\(\mathbf{\Sigma_x}\)</span> represent the mean and covariance of the prior.</p>
<p>Given the Gaussian likelihood and prior, the posterior is also Gaussian; its mean and covariance matrix may be computed analytically from the mean and covariance matrices of the likelihood and prior. This result follows from a standard identity that the product of two multivariate Gaussian distibutions is also a multivariate Gaussian (see <span class="citation" data-cites="RasmussenGaussian2006">@RasmussenGaussian2006</span>, <span class="citation" data-cites="RasmussenGaussian2006">[-@RasmussenGaussian2006]</span>; <span class="citation" data-cites="Brainard1995AnIdeal">@Brainard1995AnIdeal</span>, <span class="citation" data-cites="Brainard1995AnIdeal">[-@Brainard1995AnIdeal]</span> provides the derivation in the context of the Bayesian posterior). In the case where the posterior is a multivariate Gaussian, its mean <span class="math inline">\(\boldsymbol{\mu}_{x|y}\)</span> provides the estimate of <span class="math inline">\(\mathbf{x}\)</span> that corresponds to both the posterior mean and the MAP estimate.</p>
<div id="fig:bayesexample" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="PCC_images/bayesexample.png" class="img-fluid figure-img" style="width:12cm"></p>
<figcaption><strong>Bayes reconstruction</strong>. See description in text. For the prior and posterior, probability is given as the probability mass for a region of size <span class="math inline">\(0.01^2\)</span> in the pixel radiance plane. Matlab code to produce this figure is available at https://github.com/DavidBrainard/BrainardFigListings.git (subdirectory scripts/MathPsychChapter/FigLinBayesExample, script Example.m).</figcaption>
</figure>
</div>
<p>Figure <a href="#fig:bayesexample" data-reference-type="ref" data-reference="fig:bayesexample">1.9</a> illustrates the idea for a simple example case. Suppose that the display has only two pixels and emits at only one wavelength. Then <span class="math inline">\(\mathbf{x} = [x_1, x_2]^T\)</span>. We will assume that the radiance at each pixel of the display can range between 0 and 1. For natural images, there is a strong correlation between the radiance at neighboring pixels at the same wavelength <span class="citation" data-cites="Burton1987Color Tkacik2011Natural">[@Burton1987Color; @Tkacik2011Natural]</span>. A bivariate Gaussian prior distribution with this property is illustrated in the left panel of Figure <a href="#fig:bayesexample" data-reference-type="ref" data-reference="fig:bayesexample">1.9</a>. The mean of the prior is <span class="math inline">\(\mathbf{x} = [0.5, 0.5]\)</span> while the covariance matrix <span class="math inline">\(\mathbf{\Sigma}_x\)</span> corresponds to standard deviation of 0.127 and a correlation across the two pixels of 0.89. The strong correlation in the prior restricts the best guesses about the values of <span class="math inline">\(\mathbf{x}\)</span> relative to the full available range.</p>
<p>To compute a likelihood we need to know the nature of the sensory measurements. We suppose that there is just one cone and that it is equally sensitive to the radiance at the two display pixels. This gives us <span class="math inline">\(\mathbf{C} = [0.5, 0.5]\)</span>. We assume that the mean excitation of the cone is perturbed by zero-mean Gaussian noise with standard deviation <span class="math inline">\(\sigma_y = 0.01\)</span>. The middle panel of Figure <a href="#fig:bayesexample" data-reference-type="ref" data-reference="fig:bayesexample">1.9</a> illustrates the likelihood for the specific cone excitation <span class="math inline">\(\mathbf{y} = 0.3\)</span>: the likelihood <span class="math inline">\(p(\mathbf{y} = 0.3 | \mathbf{x})\)</span> is plotted as a function of <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>. This likelihood is highest along the ridge where the weighted sum of the pixel radiances sums to the observed cone excitation of <span class="math inline">\(0.3\)</span>. The likelihood falls off away from this ridge, with the rate of falloff determined by the magnitude of the noise. If the noise were smaller, the falloff would be faster and the likelihood ridge thinner, and conversely if the noise were larger. The likelihood alone tells us that <span class="math inline">\(\mathbf{x}\)</span> is unlikely to lie far from that ridge. At the same time, the likelihood makes explicit the ambiguity about <span class="math inline">\(\mathbf{x}\)</span> remaining after observing <span class="math inline">\(\mathbf{y}\)</span>, with many values of <span class="math inline">\(\mathbf{x}\)</span> equally likely.</p>
<p>Bayes’ rule specifies that the prior and likelihood should be combined using point-by-point multiplication over the pixel radiance plane (Equation <a href="#eqn:bayesrule" data-reference-type="ref" data-reference="eqn:bayesrule">[eqn:bayesrule]</a>), and then normalized, to form the posterior. The right panel of Figure <a href="#fig:bayesexample" data-reference-type="ref" data-reference="fig:bayesexample">1.9</a> illustrates the result of this multiplication. The same result may be obtained directly by application of the analytic formulae for the posterior.</p>
<p>The posterior makes intuitive sense: It is large where both the prior and likelihood are large, and the resulting distribution is more concentrated than either the prior or likelihood alone. Although there is still uncertainty remaining in the posterior, it captures what we know about the scene when we combine the statistical regularities of the displayed images with the sensory measurement provided by the cone excitation.</p>
</section>
<section id="thresholds-and-ideal-observer-theory" class="level3">
<h3 class="anchored" data-anchor-id="thresholds-and-ideal-observer-theory">Thresholds and ideal observer theory</h3>
<p>In this and the next sections, we show how ideas of perceptual inference as implemented through Bayes rule help us understand perceptual processing. We begin with analysis of threshold measurements. A threshold is the minimum difference required for an observer to correctly discriminate between two stimuli, and threshold measurements are fundamental to the psychophysical approach to characterizing perceptual performance and making inferences about the mechanisms underlying this performance.</p>
<p>Consider, for example, discrimination between a uniform field and a contrast grating (see Figure <a href="#fig:csf" data-reference-type="ref" data-reference="fig:csf">1.10</a>). In a typical experiment, the observer is shown the uniform field and the grating in sequence, with the order randomized on each trial. The observer’s task is to indicate which was presented first. In the experiment the stimulus contrast is titrated to a level at which the observer is correct, say, 80% of the time. The resulting contrast is the threshold.</p>
<p>Threshold measurements quantify the information needed by the visual system to make a basic perceptual decision: namely, that two stimuli differ. They inovlve small perturbations of the visual stimulus, and they may be thought of as assessing sensitivity to derivatives of the retinal image. In this way, thresholds are connected to the ideas introduced above about the importance of derivatives of the spectral radiance as a basis for visual processing.</p>
<p>Figure&nbsp;<a href="#fig:csf" data-reference-type="ref" data-reference="fig:csf">1.10</a> shows the threshold for contrast gratings measured as a function of grating spatial frequency, in the form of the spatial contrast sensitivity function (CSF). When measured with static or very slowly moving gratings, the human CSF has an inverted U-shape: the highest contrast sensitivity is between 3-6 cycles per degree, with lower sensitivity at higher and lower spatial frequencies. Because any image may be synthesized by a weighted superposition of sinusoidal gratings <span class="citation" data-cites="Bracewell1978Fourier">[@Bracewell1978Fourier]</span>, the CSF characterizes the sensitivity to basic stimulus components. Because the visual system as a whole is neither shift-invariant nor linear, however, the CSF is a useful but not complete description of sensitivity.</p>
<p>We would like to understand how the human contrast sensitivity function is limited by the properties of the visual components described in this chapter. Bayes’ rule provides a way to build this understanding, by linking the initial encoding to performance on the psychophysical threshold detection task. Analyses of this sort are called ideal observer theory <span class="citation" data-cites="Geisler1989Sequential">[@Geisler1989Sequential]</span>. Ideal observer theory allows us to understand the extent to which observer performance is limited by the factors in early visual encoding that we have outlined above. These include blurring by the eye’s optics, which reduces the retinal contrast of a grating stimulus, spatial sampling by the cone mosaic, and the Poisson variability in the cone excitations. Of particular interest is separating aspects of visual performance that are tightly coupled to these factors from aspects that are limited by processes not incorporated into the ideal observer calculation.</p>
<div id="fig:csf" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="PCC_images/csf.png" class="img-fluid figure-img" style="width:12cm"></p>
<figcaption><strong>Modeling the human contrast sensitivity function</strong>. A. Sensitivity, defined as the inverse of threshold contrast, is plotted as a function of spatial frequency. The stimuli were small, equal-sized patches of contrast gratings. Reproduced from De Valois et al. <span class="citation" data-cites="de1974psychophysical">[-@de1974psychophysical]</span>. The thumbnails below the graph illustrate contrast grating patches at different spatial frequencies. B. Triangles and black line: Human contrast sensitivity function for two observers, data from Banks et al. <span class="citation" data-cites="Banks1987Physical">[-@Banks1987Physical]</span>. Grey circles/line: Contrast sensitivity of an ideal observer implemented at the level of the Poisson limited cone excitations, from Banks et al. <span class="citation" data-cites="Banks1987Physical">[-@Banks1987Physical]</span>. Red circles/line: Ideal observer CSF with recent estimates of optics and mosaic properties. Blue circles/line: Computational observer CSF with decision rule determined using supervised machine learning. Green circles/line: Computational observer CSF additionally accounting for fixational drift. Purple circles/line: Computational observer CSF additionally incorporating a model of the transformation from excitations to photocurrent. After Figure 6 of Cottaris et al. <span class="citation" data-cites="Cottaris2020AComputational">[-@Cottaris2020AComputational]</span>.</figcaption>
</figure>
</div>
<p>So, how do we use Bayes to predict performance in the two-interval forced choice task described above? We’ll use the terms reference stimulus and comparison stimulus to describe the two stimuli being discriminated. In this example the reference stimulus will be a spatially uniform field and the comparison stimulus will be a patch of contrast grating with known spatial frequency, orientation, size, and contrast; but, the ideas we develop here apply to any two stimuli being discriminated.</p>
<p>Using the computational methods described in this chapter, we can compute the mean cone excitations to the reference and comparison stimuli. Let <span class="math inline">\(\mathbf{u}_r\)</span> be the vector of mean cone mosaic excitations in response to the reference stimulus and let <span class="math inline">\(\mathbf{u}_c\)</span> be the vector of mean cone excitations in response to the comparison stimulus. In the two-interval forced choice task, the observer must indicate whether the reference came first followed by the comparison, or the other way around. We thus form two concatenated vectors, <span class="math inline">\(\mathbf{u}_1 = [\mathbf{u}_r, \mathbf{u}_c]\)</span> and <span class="math inline">\(\mathbf{u}_2 = [\mathbf{u}_c, \mathbf{u}_r]\)</span>.</p>
<p>To apply Bayes’ rule to this problem, we can think of the scene as described by the binary random variable. This variable, <span class="math inline">\(x\)</span>, can take on value <span class="math inline">\(1\)</span> or <span class="math inline">\(2\)</span>. These values represent the reference first and reference second possibilities that can occur on each trial. The prior probability <span class="math inline">\(p(x)\)</span> is given by</p>
<p><span class="math display">\[p(x = 1) = 0.5; \hspace{1mm} p(x = 2) = 0.5.\]</span></p>
<p>The data available to the observer to make a response of <span class="math inline">\(x = 1\)</span> or <span class="math inline">\(x = 2\)</span> are the pattern of observed cone excitations across the two intervals, which we will denote by <span class="math inline">\(\mathbf{y}\)</span>. We know that for <span class="math inline">\(x = 1\)</span>, each entry of <span class="math inline">\(\mathbf{y}\)</span> is an independent Poisson random variable with mean given by the corresponding entry of <span class="math inline">\(\mathbf{u}_1\)</span>, while for <span class="math inline">\(x = 2\)</span> the means are given by the corresponding entries of <span class="math inline">\(\mathbf{u}_2\)</span>. From this, we have for the posterior:</p>
<p><span class="math display">\[p(x = 1 | \mathbf{y}) = p(\mathbf{y} | x = 1) p(x = 1) = \prod_{i} { (p(y_i | x = 1)}  p(x = 1)\]</span></p>
<p>where <span class="math inline">\(y_i\)</span> denotes the <span class="math inline">\(i_{th}\)</span> entry of <span class="math inline">\(\mathbf{y}\)</span> and we have explicitly expressed the joint distribution of independent random variables as the product of their individual distributions.</p>
<p>We substitute the expression for the probability mass function of a Poisson random variable and the value of <span class="math inline">\(p(x = 1)\)</span> to obtain</p>
<p><span class="math display">\[p(x = 1 | \mathbf{y}) = \prod_{i} { \frac {{{u_1}_i}^{y_i}  e^{-{{u_1}_i}} } { {y_i}! } } 0.5\]</span></p>
<p>where <span class="math inline">\({u_1}_i\)</span> denotes the <span class="math inline">\(i^{th}\)</span> entry of <span class="math inline">\(\mathbf{u}_1\)</span>. Similarly, we have</p>
<p><span class="math display">\[p(x = 2 | \mathbf{y}) = \prod_{i} { \frac {{{u_2}_i}^{y_i}  e^{-{{u_2}_i}} } { {y_i}! } } 0.5.\]</span></p>
<p>To maximize percent correct on the task, the observer should compare <span class="math inline">\(p(x = 1 | \mathbf{y})\)</span> with <span class="math inline">\(p(x = 2 | \mathbf{y})\)</span> and indicate <span class="math inline">\(1\)</span> or <span class="math inline">\(2\)</span> according to which is larger. It is instructive to implement this comparison in terms of the difference of the logs of <span class="math inline">\(p(x = 1 | \mathbf{y})\)</span> and <span class="math inline">\(p(x = 2 | \mathbf{y})\)</span>, with a response of <span class="math inline">\(1\)</span> corresponding to a difference greater than or equal to <span class="math inline">\(0\)</span> and a response of <span class="math inline">\(2\)</span> corresponding to a difference less than <span class="math inline">\(0\)</span>. Writing the difference of logs explicitly and simplifying, we have decision variable <span class="math display">\[\delta = \sum_{i} {y_i {log( \frac {{u_1}_i} {{u_2}_i}}) } + \sum_{i} ({ {u_2}_i - {u_1}_i)  }\]</span> An observer who responds according to the sign of <span class="math inline">\(\delta\)</span> will maximize percent correct. The value of percent correct depends on how <span class="math inline">\(\delta\)</span> is distributed when <span class="math inline">\(x = 1\)</span> and <span class="math inline">\(x = 2\)</span>. Geisler <span class="citation" data-cites="Geisler1984Physical">[-@Geisler1984Physical]</span> provides a Gaussian approximation to these distributions, which may be used to obtain the corresponding percent correct. As with the human psychophysical experiment, contrast may be titrated to find the ideal observer threshold contrast, that which leads to the ideal observer having the criterion percent correct.</p>
<p>Figure <a href="#fig:csf" data-reference-type="ref" data-reference="fig:csf">1.10</a>B shows the ideal observer contrast sensitivity for human foveal viewing (gray circles/line), along with psychophysical measurements of human contrast sensitivity at spatial frequencies increasing from 5 cpd, and with the measurements (triangles/black line) made with stimuli matched to those used in the ideal observer calculations <span class="citation" data-cites="Banks1987Physical">[@Banks1987Physical]</span>. As with the human data at higher spatial frequencies, the ideal observer contrast sensitivity function falls off as spatial frequency increases, and indeed the slope of this falloff closely resembles that of the human observer. This correspondence suggests that the factors that cause the human falloff share basic features with those included in the ideal observer calculation. Here the primary factor is blur from the eye’s optics and cone apertures, both of which reduce the contrast captured by spatial variation in the cone excitations.</p>
<p>The ideal observer CSF differs from the human measurements. One differeince is that the overall sensitivity of the ideal observer is markedly higher than that of the human observer. The Poisson noise in the cone excitations limits the ideal observer sensitivity. The fact that incorporating only this noise source leads to an ideal observer more sensitive than the human tells us that additional factors limit human sensitivity, and motivates study of what these additional factors are.</p>
<p>In many cases, a single "efficiency" parameter representing an omnibus loss of information by the actual visual system relative to an ideal observer calculation is sufficient to bring ideal observer predictions into alignment with measured human performance <span class="citation" data-cites="Burge2020ARVSReview">[@Burge2020ARVSReview]</span>, as is true in the case of the ideal and human CSF rolloff at high spatial frequencies. The efficiency parameter can be thought of as capturing the effect of additional noise in the human visual system, not included in the ideal observer calculation, whose effect on performance is stimulus-independent.</p>
<p>It is important to note, however, that it is not always possible to account for a difference between ideal and human with a single efficiency parameter. For example, the ideal observer CSF does not roll off at low spatial frequencies but the human CSF does. The factors that produce the measured low-spatial-frequency roll off are not included in the ideal observer calculations presented here. As with the difference in overall sensitivity, the difference between ideal and human CSF at low spatial frequencies motivates investigation of what additional factors in the human visual system account for the difference.</p>
</section>
<section id="computational-observers" class="level3">
<h3 class="anchored" data-anchor-id="computational-observers">Computational observers</h3>
<p>The ideal observer calculation used by Banks et al. <span class="citation" data-cites="Banks1987Physical">[-@Banks1987Physical]</span> employed a simplified model of the eye’s point spread function and cone mosaic, and this simplification enabled efficient computation of ideal observer performance. In two recent papers, Cottaris et al.&nbsp;(<span class="citation" data-cites="Cottaris2019AComputational">[-@Cottaris2019AComputational]</span>; <span class="citation" data-cites="Cottaris2020AComputational">@Cottaris2020AComputational</span>, <span class="citation" data-cites="Cottaris2020AComputational">[-@Cottaris2020AComputational]</span>) employed computational methods to examine the effect of more recent estimates of the point spread function <span class="citation" data-cites="Thibos2002Statistical">[@Thibos2002Statistical]</span> and a more detailed model of the foveal mosaic on performance. These had only a modest effect on the predictions (Figure <a href="#fig:csf" data-reference-type="ref" data-reference="fig:csf">1.10</a>B, red circles/line).</p>
<p>The ideal observer developed above has full knowledge of mean cone excitations and Poisson structure of the noise, so that the observer’s performance is not degraded by stimulus uncertainty <span class="citation" data-cites="Pelli1985Uncertainty Geisler2018Psychometric">[@Pelli1985Uncertainty; @Geisler2018Psychometric]</span>. Cottaris et al. <span class="citation" data-cites="Cottaris2019AComputational">[-@Cottaris2019AComputational]</span> relaxed this assumption by replacing the ideal observer decision rule with a decision rule based on a trained linear classifier <span class="citation" data-cites="Manning2008Introduction scholkopf2002learning">[@Manning2008Introduction; @scholkopf2002learning]</span>. The classifier measured the match of the data to a template that had the same spatial structure as the stimuli. The decision boundary was optimized in the presence of noise. The need to partially learn the decision rule reduced the absolute level of ideal observer performance while retaining the same CSF shape (blue circles/line in Figure <a href="#fig:csf" data-reference-type="ref" data-reference="fig:csf">1.10</a>B). Cottaris et al. <span class="citation" data-cites="Cottaris2020AComputational">[-@Cottaris2020AComputational]</span> then introduced a computational model of fixational eye movements (<span class="citation" data-cites="Mergenthaler2007Modeling">@Mergenthaler2007Modeling</span>, <span class="citation" data-cites="Mergenthaler2007Modeling">[-@Mergenthaler2007Modeling]</span>; see also <span class="citation" data-cites="Engert2004Microsaccades">@Engert2004Microsaccades</span>, <span class="citation" data-cites="Engert2004Microsaccades">[-@Engert2004Microsaccades]</span>) and showed that an approach to handling the stimulus blur introduced by these movements further reduced performance (green circles/line). Finally, Cottaris et al. <span class="citation" data-cites="Cottaris2020AComputational">[-@Cottaris2020AComputational]</span> introduced a computational model of the transformation from excitations to electrical photocurrent, which included both gain control and additional noise. Accounting for this transformation brought computational observer performance into approximate alignment with the human measurements at the higher spatial frequncies (purple circles/line).</p>
<p>This analysis outlines a set of factors that together provide an account of the high-spatial frequency limb of the human spatial CSF, capturing both the shape and absolute level of this important measure of performance. For purposes of the present chapter, we emphasize less the specific elements of the account, which will surely be refined by future research, but rather the way the mathematical principles are combined with computational modeling with the goal of accounting for the full richness of the visual system. The combination of principles and computations accounts for factors that are beyond what is possible using analytic calculations alone.</p>
</section>
<section id="image-reconstruction" class="level3">
<h3 class="anchored" data-anchor-id="image-reconstruction">Image reconstruction</h3>
<p>The ideal observer and computational observer development above applies Bayesian inference to the analysis of threshold measurements. Thresholds characterize the limits of visual performance, and the analyses illustrate how threshold performance can be linked to quantitative measurements of physiological optics, retinal anatomy, and retinal physiology. Not all vision is threshold vision, however. Sometimes we are interested in predicting what clearly visible stimuli look like (e.g., "that apple looks red") or how similar easily distinguishable objects appear (e.g., "the color of the apple appears more similar to the color of the tomato than it does to the color of the banana"). There are a number of methods for studying suprathreshold vision. These include asymmetric matching <span class="citation" data-cites="Burnham1957 Brainard1992Asymmetric Wandell1995Foundations">[@Burnham1957; @Brainard1992Asymmetric; @Wandell1995Foundations]</span> and various scaling techniques <span class="citation" data-cites="Knoblauch2012 MaloneyMLDS2003 CoxMDS2001">[@Knoblauch2012; @MaloneyMLDS2003; @CoxMDS2001]</span>. We will not treat these methods here. Below, however, we illustrate how Bayesian methods can be used to understand how the initial visual encoding shapes the perceptual inferences that can be made about supra-threshold stimuli.</p>
<p>In our introduction to Bayes Rule, we illustrated the core ideas by considering reconstruction of a two pixel image from the excitations of a single cone, using both a Gaussian prior and a Gaussian likelihood. As computer power has increased, these same Bayesian principles have been applied to increasingly large perceptual problems. As we illustrate here, it is now possible to reconstruct an estimate of a full displayed color image from a realistic model of cone excitations using the Poisson likelihood <span class="citation" data-cites="Zhang2021Reconstruction">[@Zhang2021Reconstruction]</span>.</p>
<p>The forward computation starts with the displayed image, <span class="math inline">\(\mathbf{x}\)</span> and computes the cone excitations <span class="math inline">\(\mathbf{y}\)</span>. The vector <span class="math inline">\(\mathbf{x}\)</span> can be thought of as the concatenation of the linearized and rasterized pixel values for each of the red, green, and blue channels of the display. Using the Poisson noise model of the cone excitations, we compute the likelihood of observed cone excitations <span class="math inline">\(p(\mathbf{y} | \mathbf{x})\)</span>. Here the vector <span class="math inline">\(\mathbf{y}\)</span> is simply a list of the excitations of each cone in the mosaic. Because the mean cone excitations are a linear function of of the display pixel values, we can write for these mean excitations</p>
<p><span class="math display">\[\mathbf{\bar{y}} = \mathbf{R} \mathbf{x}\]</span></p>
<p>for some matrix <span class="math inline">\(\mathbf{R}\)</span>. Each column of this matrix may be computed as the vector of cone excitations produced when one pixel is at its maximum value for one color channel, with the display values for all other pixels and color channels are set to zero, and these computations may be implemented in software such as ISETBio to determine explicitly the matrix <span class="math inline">\(\mathbf{R}\)</span> <span class="citation" data-cites="Zhang2021Reconstruction">[@Zhang2021Reconstruction]</span>. This yields for the likelihood</p>
<p><span class="math display">\[p(\mathbf{y} | \mathbf{x}) = Poisson(\mathbf{R} \mathbf{x}),\]</span></p>
<p>where Poisson() denotes the result of Poisson noise applied independently to its vector argument by taking each entry of the argument as corresponding Poisson mean.</p>
<p>Next, we specify a prior distribution <span class="math inline">\(p(\mathbf{x})\)</span> for natural images. Natural images have a great deal of structure <span class="citation" data-cites="Simoncelli2005Statistical">[@Simoncelli2005Statistical]</span>, and a full statistical description of this structure is not currently available. There are two robust regularities of natural images, however, that can be described by a multivariate Gaussian. The first is that within a single wavelength band, the spectral radiances at nearby image locations are highly correlated <span class="citation" data-cites="Pratt1978Digital Field1987Relations Ruderman1988Statistics">[@Pratt1978Digital; @Field1987Relations; @Ruderman1988Statistics]</span>. The second regularity is that at a single position, values in nearby wavelength bands are highly correlated <span class="citation" data-cites="Burton1987Color Tkacik2011Natural">[@Burton1987Color; @Tkacik2011Natural]</span>. This is a consequence of the relatively smooth spectral functions one observes in nature <span class="citation" data-cites="Cohen1964Dependency Maloney1986Evaluation Vrhel1994Measurement">[@Cohen1964Dependency; @Maloney1986Evaluation; @Vrhel1994Measurement]</span>. These two observations may be used to construct a covariance matrix for a multivariate Gaussian that describes the second order statistics of natural images. Together with the average image, these provide a Gaussian image prior.</p>
<p>With the likelihood and prior, we can construct an estimate of the image given a vector of cone excitations. As with many calculations described in this chapter, the principles of Bayesian estimation guide the way, but once we introduce the Poisson likelihood, numerical computational methods are used to find the solution.</p>
<p>We used ISETBio to reconstruct images from cone excitations, with the Poisson likelihood and Gaussian image prior described above. We reconstructed images for retinal patches at various visual field eccentricities. As visual field eccentricity increases, the point spread of the retinal image becomes more blurred and the density with which the cones sample the image decreases(Figures <a href="#fig:pointspread" data-reference-type="ref" data-reference="fig:pointspread">1.5</a>, <a href="#fig:sampling" data-reference-type="ref" data-reference="fig:sampling">1.6</a>, and <a href="#fig:excitations" data-reference-type="ref" data-reference="fig:excitations">1.7</a>). Thus, less information becomes available to the visual system in the peripheral visual field. The effect of this loss for reconstruction depends on the prior. Although the information loss means that two images whose cone excitations are different in the fovea can produce the same cone excitations in the periphery, this ambiguity need not degrade the reconstructions if the probability that one of the two images will occur is small.</p>
<p>The reconstructions in Figure&nbsp;<a href="#fig:reconstructions" data-reference-type="ref" data-reference="fig:reconstructions">1.11</a> show the effect of information loss at the level of the cone excitations, in the context of the Gaussian image prior. The reconstructed image quality in the periphery is worse than in the fovea, but many objects remain recognizable from the peripheral reconstructions. Moreover, there are interesting interactions between the likelihood and prior. For example, the recovery of color can be better in the fovea and more peripheral locations than it is in the mid-periphery (see images of strawberries in Figure&nbsp;<a href="#fig:reconstructions" data-reference-type="ref" data-reference="fig:reconstructions">1.11</a>, for example). Zhang et al. <span class="citation" data-cites="Zhang2021Reconstruction">[-@Zhang2021Reconstruction]</span> describe the reconstruction approach to analyzing the initial visual encoding in more detail, extending the ideas to a more realistic prior than the Gaussian, and showing a number of calculations that use image reconstruction to examine how prior and likelihood interact to support both color and spatial vision (see also <span class="citation" data-cites="Brainard2008Trichromatic">@Brainard2008Trichromatic</span>, <span class="citation" data-cites="Brainard2008Trichromatic">[-@Brainard2008Trichromatic]</span>).</p>
<p>Image reconstruction computations provide useful insights about how statistical regularities in natural scenes interact with the sensory measurements to guide perception. But, it is important to bear in mind that reconstruction of displayed images is not the task for which visual perception evolved. Rather, we view the task of perception to reconstruct the properties and positions of objects in the three-dimension environment. The Bayesian ideas presented here have applicability to this task as well <span class="citation" data-cites="KnillRichards1996">[@KnillRichards1996]</span>, but a computational solution that is as effective as human vision currently remains elusive. This is an area where recent progress in machine learning and deep neural networks may provide new insights.</p>
</section>
<section id="optimizing-sensory-measurements" class="level3">
<h3 class="anchored" data-anchor-id="optimizing-sensory-measurements">Optimizing sensory measurements</h3>
<p>Earlier in this chapter, we explained that the visual system appears to extract information about motion, color, and pattern from the pattern of cone excitations by estimating the local derivatives of various quantities <span class="citation" data-cites="Adelson1991Plenoptic">[@Adelson1991Plenoptic]</span>. The Bayesian framework provides a quantitative framework for addressing how to optimize which signals should be transduced by a sensory system when the goal is reconstruction of the state of the environment, as well as how the sensory signals should be summarized (e.g., in the form of local derivatives) for further processing. Indeed, the Bayesian image reconstruction methods developed here point towards the ingredients required for a full analysis of such questions. To know what measurements we should make, we first need to know the prior distribution over the environmental states that an organism will encounter. We then need a parameterized set of candidate likelihood functions, each of which describes a feasible arrangement of the sensory apparatus and (if desired) associated early processing. This information allows us to compute the posterior over the environmental states for any candidate likelihood function, and we can ask how well different sensory measurements constrain the posterior, averaging this information over the environmental states described by the prior. Developing a parameterized set of candidate likelihood functions requires an understanding of what biological constraints apply to the sensory system. Also required is an understanding of the cost of different types of error in the resultant perceptual representation (the loss function; <span class="citation" data-cites="Berger1985Statistical">@Berger1985Statistical</span>, <span class="citation" data-cites="Berger1985Statistical">[-@Berger1985Statistical]</span>), as well as how the cost of error should be balanced against the energetic cost of making and processing the sensory measurements <span class="citation" data-cites="Laughlin2001Energy Balasubramanian2001Metabolically Koch2004Efficiency">[@Laughlin2001Energy; @Balasubramanian2001Metabolically; @Koch2004Efficiency]</span>. A number of authors have pursued questions of optimizing sensory measurements in this manner <span class="citation" data-cites="Levin2008Understanding Manning2009Optimal Garrigan2010Design Zhang2021Reconstruction">[@Levin2008Understanding; @Manning2009Optimal; @Garrigan2010Design; @Zhang2021Reconstruction]</span>.<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> It would be interesting to compare the results of an analysis of this sort to the Adelson/Bergen conjecture that approximations to local derivatives represent an optimal measurement set.</p>
<div id="fig:reconstructions" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="PCC_images/reconstruction.png" class="img-fluid figure-img" style="width:12cm"></p>
<figcaption><strong>Image reconstructions from cone excitations at three retinal eccentricities</strong>. Each row shows reconstructions of 7 images using the Bayesian method and Poisson likelihood and multivariate Gaussian prior. The reconstructions at 1 degree eccentricity are close to veridical, with increasing distortions seen at the 10 and 18 degree locations. Each original and reconstructed image was represented at a pixel resolution of 128 by 128, and the extent of each image on the retina was 1 degree by 1 degree. The mean excitation of the cones was <span class="math inline">\(~10^5\)</span> excitations per cone, so the simulation corresponds to a relatively high signal-to-noise regime. The parameters of the Gaussian prior were fit to 16 by 16 pixel patches of images from the ImageNet ILSVRC dataset (https://www.image-net.org), and extended in an overlapping block-wise fashion to the higher image pixel resolution. Figure courtesy Lingqi Zhang. See Zhang et al. <span class="citation" data-cites="Zhang2021Reconstruction">[-@Zhang2021Reconstruction]</span> for a more extended discussion of Bayesian image reconstruction and the general methods used to produce this figure.</figcaption>
</figure>
</div>
</section>
</section>
<section id="summary-and-conclusions" class="level2">
<h2 class="anchored" data-anchor-id="summary-and-conclusions">Summary and conclusions</h2>
<p>To focus on the mathematics of the initial visual encoding, we introduce vision science from the point of view of a forward calculation: physics of the stimulus, image formation, and quantitative system modeling. The key mathematical principles are linear algebra, shift-invariant linear systems, and specification of sensory noise. The mathematics of vision science shares much in common with the mathematics of many fields of science and engineering.</p>
<p>After expressing and implementing the forward calculations, we explore the mathematics of Helmholtz’ hypothesis: People see a stimulus that is the most likely explanation of the cone excitations. We use Bayesian inference methods to clarify the uncertainty about the encoded signal. This approach requires that we confront the problem of establishing priors on the signal. There is a close connection between Helmholtz’ unconscious inference and Bayesian inference; the latter may be thought of as a quantitative implementation of Helmholtz’s idea.</p>
<p>The approach we describe has a long and accomplished tradition. But, it is not the only valid way to make progress in vision science; several other approaches are important. A quantitative study of behavioral rules can be very informative. For example, color appearance matching was a largely behavioral exploration at first; an understanding of the physics of the signal and the biological underpinnings followed later. Also, neurobiological measures can be helpful. Anatomical and functional measurements that characterize the properties of multiple pathways within the visual system - including multiple types of retinal ganglion cells and multiple pathways through the visual cortex - are useful guides to understanding visual specializations and computations, particularly for stages of vision beyond the initial encoding.. Finally, engineering work to build functional artificial visual systems continues to be very helpful in understanding vision: A classic principle states that the best way to demonstrate you understand a system is to build one that does the same thing. Engineering efforts continue to clarify features that we might look for in the nervous system as well as why certain behavioral patterns emerge.</p>
<p>The field of vision science is large and vigorous enough that there is no need to choose a single approach. We are inspired by the fact that different investigators adopt different approaches, all seeking to gain some understanding. To the student thinking about how to approach vision science, we offer advice from an American philosopher who commented about making difficult decisions: ‘When you come to a fork in the road, take it (Yogi Berra).’</p>
</section>
<section id="further-reading" class="level2">
<h2 class="anchored" data-anchor-id="further-reading">Further Reading</h2>
<p>This chapter introduces key mathematical and computational approaches to understanding the initial visual encoding. A number of the mathematical ideas we present here are developed in more detail by Wandell <span class="citation" data-cites="Wandell1995Foundations">[-@Wandell1995Foundations]</span>, and the classic treatment of visual perception by Cornsweet <span class="citation" data-cites="Cornsweet1970">[-@Cornsweet1970]</span> remains a valuable introduction to the field, as does Rodieck <span class="citation" data-cites="Rodieck1998FirstSteps">[-@Rodieck1998FirstSteps]</span>. Principles of ray tracing are introduced in many computer graphics texts (e.g. <span class="citation" data-cites="Pharr2016-yb">@Pharr2016-yb</span>, <span class="citation" data-cites="Pharr2016-yb">[-@Pharr2016-yb]</span>); similarly many texts introduce optics (e.g., Hecht, <span class="citation" data-cites="Hech2017Optics">[-@Hech2017Optics]</span>). In the context of the retinal image and cone excitations specifically, Yellott et al. <span class="citation" data-cites="Yellot1984Beginnings">[-@Yellot1984Beginnings]</span>, Pugh <span class="citation" data-cites="Pugh1988Vision">[-@Pugh1988Vision]</span>, and Packer and Williams <span class="citation" data-cites="Packer2003Light">[-@Packer2003Light]</span> are useful. Brainard and Stockman <span class="citation" data-cites="Brainard2010Colorimetry">[-@Brainard2010Colorimetry]</span> elaborate in more detail on using linear algebra in support of colorimetric applications. Although we do not treat the Fourier transform and frequency domain representations in this chapter, the reader who wishes to specialize in this field will want to learn about these ideas. Two useful sources are Bracewell <span class="citation" data-cites="Bracewell1978Fourier">[-@Bracewell1978Fourier]</span> and Pratt <span class="citation" data-cites="Pratt1978Digital">[-@Pratt1978Digital]</span>. Useful introductions to statistical inference include Bishop <span class="citation" data-cites="Bishop2006Pattern">[-@Bishop2006Pattern]</span> and Duda, Hart and Stork <span class="citation" data-cites="DudaHart2001">[-@DudaHart2001]</span>.</p>
</section>
<section id="acknowledgments" class="level2">
<h2 class="anchored" data-anchor-id="acknowledgments">Acknowledgments</h2>
<p>We thank Nicolas Cottaris and Lingqi Zhang for providing figures for this chapter. We also thank Amy Ni, Nicolas Cottaris, Lingqi Zhang, Joyce Farrell, Eline Kupers, Heiko Schutt, and Greg Ashby for useful comments on the manuscript.</p>
<hr>
</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Mechanical force on the retina (pressure phosphenes) and injecting current into the retina or brain (electrical phosphenes) can also cause a visual sensation.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>https://www.merriam-webster.com/dictionary/light<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>From the section “prove how all objects, placed in one position, are all everywhere”<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>It is an exercise for the reader to show that real-valued systems that follow the superposition rule also obey the homogeneity rule, not just for integers, but for any real scalar. The converse is not true. A real-valued system can obey the homogeneity rule but not the superposition rule. For example, <span class="math inline">\(f(x) = \lvert x \rvert\)</span>, or <span class="math inline">\(f(x) = x^2\)</span> both satisfy homogeneity, but not superposition.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Strictly speaking, the absorptance spectrum tells us the probability that a photon is absorbed. Not all absorbed photons lead to an excitation, so an additional factor specifying the quantal efficiency (probability of excitation given absorption) needs to be included in the calculation. Current estimates put the quantal efficiency of human cone photopigment at about 67%. In addition, the calculation of cone excitations from spectral irradiance requires taking into account the size of the cone’s light collecting aperture.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>In this mathematical formulation, we do not make the spatial extent of the cone acceptance aperture explicit. This aperture introduces additional blur into the retinal image. Computational models such as the one shown in Figure <a href="#fig:excitations" data-reference-type="ref" data-reference="fig:excitations">1.7</a>) do take this factor, which is significant, into account.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>The null space of a matrix <span class="math inline">\(\mathbf{C}\)</span> is the space of vectors <span class="math inline">\(\mathbf{v}\)</span> such that <span class="math inline">\(\mathbf{C} \mathbf{v} = 0\)</span>. If a matrix has column dimension <span class="math inline">\(n\)</span> and rank <span class="math inline">\(r\)</span>, its null space has dimension <span class="math inline">\(n-r\)</span>.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>When describing optics a shift-invariant region within the visual field is called an isoplanatic region.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>The point spread function is the spatial analog of the impulse response function used to characterize time-invariant linear systems.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>The formalism used in these analyses is interestingly similar to that underlying Bayesian adaptive psychophysical procedures <span class="citation" data-cites="Watson1983QUEST Watson2017QUEST">[@Watson1983QUEST; @Watson2017QUEST]</span>.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>