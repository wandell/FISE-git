# Preface {.unnumbered}

The rapid development image systems technology over the last few decades has impacted many parts of society. For engineers who understand imaging principles, keeping up with these rapid advances is exhilarating and challenging. The main purpose of this book is to explain the fundamental principles of image systems to people who are entering the field, or to people who are simply curious about how image systems work.

Image systems comprise many parts that work together. This book introduces the reader to tools that simulate and characterize the performance of the whole image system. Any image system will have multiple components, and to understand the system we must understand the original signal (scene) as well as the components - optics, sensors, image processing, display - and properties of the human visual system. It is impossible for me to keep up with every new advance in the component technologies. On the other hand, even as technology advances. each component implements certain principles that persist across generations of technology. This book aims to describe principles that remain invariant, even as technology evolves. Hence, the title is: **Foundations of Image Systems Engineering**.

![Image Systems Engineering Slide.](images/preface/ee_image-systems-engineering_ee292e.jpg){#fig-ise-overview fig-align="center" width="339"}

We have one further aim: we would like to help the reader apply this ideas in his or her work. One of the great strengths of image systems engineering is that we often know enough so that we can compute from the input (scene) to the sensor data and then we can calculate a displayed image, or use an algorithm to interpret the sensor data. There are many details that go into these computations, and to help the reader we provide open-source software tools to perform these calculations. In this way, we hope to make it easy for the reader to experiment with different parameters and to try different system designs. The foundations we present will be illustrated by code snippets with links to runnable scripts.

This is an introductory book; each of the topics we cover can be studied in much greater depth. Throughout we provide the reader with external links to additional material, and in some cases we take advantage of this web format to link to videos or text of our own that explains an idea or a specific technology in greater detail. The main thread of this book, however, presents image system foundations. We provide this system overview because we believe it is a good way to welcome people - whether students at school or a new team member in a commercial venture - to the exciting field of image systems.

## The first image systems

In 1826, Joseph Nicéphore Niépce took the first recorded image, the "View from the Window at Le Gras". He used the simplest optics: a pinhole (camera obscura), and he recorded the image on a light-sensitive material (film). The color in the rendering was a consequence of the processing, and not an accurate representation of the scene itself. The spatial pattern was blurry, and barely interpretable - but it was inspirational. By 1840, Daguerre and then Talbot improved upon the film chemistry to enable the first commercially viable monochrome film.

![The first recorded image ("View from the Window at Le Gras"). This monochrome image was made by Joseph Nicéphore Niépce.](images/preface/ViewFromWindowLeGras-OriginalRotated.png){#fig-window-le-gras width="50%" fig-align="center"}

By 1860 James Clerk Maxwell had worked out fundamental experiments of human wavelength encoding and established the mathematical principles of human color encoding. The principles and experiments are the foundations of modern color science. In collaboration with Thomas Sutton, Maxwell created the first color image. The image shows a colored Tartan ribbon. Maxwell went on to other matters - such as clarifying the connection between electricity and magnetism and showing that light is an electromagnetic wave. Wow.

In 1888 George Eastman invented and commercialized a camera that used rolls of film, enabling users to capture multiple images in a convenient, handheld device. His company, Kodak, also provided the technology for developing the roll of film into a series of pictures. Simplifying film development was critical for the widespread adoption, and the company became a huge success. The Kodak image system included optics, a recording medium, and a processing procedure to render and print the images.

The delay between acquiring a picture and seeing it printed was usually days, or even weeks, and many people wanted to see the reproduction quickly. In 1948 Edwin Land created a chemical process for rapidly and developing the film without going to a lab; these pictures could be viewed within minutes of acquisition. His company, Polaroid, was also a huge success.

The first color image was produced by James Clerk Maxwell and XXX Sutton. The image system was designed using principles of human color vision that were established by Maxwell in a series of papers.

![The first color image was produced by James Clerk Maxwell and XXX Sutton. The image system was designed using principles of human color vision that were established by Maxwell in a series of papers.](images/preface/Maxwell_Sutton_Tartan_Ribbon.jpg){#fig-maxwell-sutton width="50%" fig-align="center"}

People were passionate about their film cameras, and taking pictures was a common part of the family holiday and many special occasions. Images with Kodak film - which were higher quality than Polaroid - dominated the market and its brand was dominant. Film and the chemistry needed for film development required consumables, which made the technology expensive but also profitable. The ability to store the pictures, typically in photoalbums, was effortful but also became part of many family's meaningful history. Editing pictures was difficult, and thus not a thing - apart from propagandists. Quantitative image analysis was also very limited.

In 1969 Willard S. Boyle and George E. Smith at Bell Labs described an entirely different approach to image capture: They described an image sensor based on semiconductor technology. Their invention, which was reduced to practice by Tompsett in 1970, was called a charge-coupled device (CCD). This light sensor revolutionized the field of imaging by enabling a new way of recording the electromagnetic radiation. There are virtually no consumables when acquiring images with a solid-state sensor. The digital output of the sensor made it straightforward to integrate image acquisition with computers, which simplified storing, sharing, editing and analyzing images. A new business model1 was required [^index-1].

[^index-1]: Kodak and Polaroid did not adapt and both ultimately reorganized under bankruptcy. Their principal function is to market their brand name. Fujifilm, an excellent but late entry into the film market, adapted and has a relatively high valuation and a broad range of products.

At first, cameras based on CCDs replaced film in a few specialized digital cameras: they found application in science and engineering for telescopes, and microscopes. The power requirements of CCDs, necessary for the method of reading out the pixel data, made CCDs impractical for lightweight consumer cameras. In 1993 this changed with Fossum's invention, at the Jet Propulsion Labs, of a solid state sensor based on CMOS which operated with lower voltages and power requirements. The first CMOS sensors were noisy and low resolution compared to the CCDs - they had a 25 year head-start! But substantial engineering efforts led to commercial CMOS imagers with excellent sensitivity, low noise, and low power. By 2020 about 200 CMOS image sensors were built every second, and 6 billion were solid per year. Today, about 7 billion people carry a smartphone with one or more CMOS sensors, most of these people keep the camera in a pocket or purse throughout the day. These sensors record still and video digital image sequences. The raw image data are immediately transformed by computer algorithms, can be viewed immediately, stored in digital memory, and easily edited, shared or analyzed.

These sensor advances spurred the development of other hardware components of image systems. Many of these changes arose because the system needed to fit within the slim form factor of the phone. New optics were designed and new displays for viewing the image prior to capture also evolved. But perhaps the biggest change was the growth of image processing methods. The acquisition with a semiconductor that was part of a computational device has motivated the development of an industry of image processing applications that add great value to the image system.

## Image systems and human vision

The great 20th century vision scientist, William Rushton, wrote - "You will need no words of mine to convince you how precious are your eyes." Understanding how the eye encodes light, and how the brain interprets this encoding, is important for understanding many aspects of human behavior. Vision helps with many of life's critical activities: moving from place to place, interpreting a facial expression, selecting food, reading a page.

Image systems, too, encode and interpret the spatio-temporal pattern of electromagnetic radiation. When we design, build, or analyze image systems, we must account both for how the signals are encoded and how they will be interpreted. A general principle about this relationship comes from the great 19th century scientist, Hermann von Helmholtz:

> The general rule determining the ideas of vision that are formed whenever an impression is made on the eye, is that such objects are always imagined as being present in the field of vision as would have to be there in order to produce the same impression on the nervous mechanism \[Italics in the original; Southall, Physiological Optics, Vol. III, p. 2, 1865\]

Helmholtz' principle is that the brain uses the image encoded by the eye to make an inference about the properties of the scene2. This is also an important principle for designing, building, and using modern image systems. A robot uses image data to infer the location and orientation of a package on the table; a microscopist uses image data to infer the properties of cells in a biopsy. An autonomous vehicle uses image data to identify nearby traffic. The early pioneers in imaging systems developed the means of encoding the light - film, sensors, optics. The modern pioneers are combining the encoding with advanced interpretations.

This book is grouped into sections that review the foundations of image formation, sensor encoding, and image processing. Even as we describe the components, we always consider their properties within the context of a larger image system. Many of the requirements for a component can be traced to their status as part of a system. For example, the size requirements on optics changed as image systems became an important component of cell phones. Similarly, the development of low power CMOS image sensors enabled the integration of image systems into these devices. Further, the design requirements for these components and the system as a whole can depend on how the system is used.

The human visual system is an essential context for all image systems related to consumer photography. Deep neural networks must be considered for many robotics and autonomous driving applications. The properties of biological substrates are a critical context when building medical imaging systems. The diversity of applications and the ability to match the physics to these applications make the field of image systems engineering interesting and exciting.

## Image systems engineering at Stanford

Stanford's involvement in the image systems accelerated in the early 1990s. Dr. Joyce Farrell was leading a group at Hewlett-Packard Laboratories (HPL), and imaging was a relatively new direction for HP. It was a phase of the company when they were inventing new things, and they had developed a type of printing technology (inkjet). They were also building image capture devices based on flatbed scanners. Professor Brian Wandell worked at Stanford, an institution that encouraged collaborations with industrial partners, and his focus was on human visual perception, particularly color. The reproduction of images depended on basic knowledge of human visual perception, and members of the HP staff needed to hire people to design and evaluate new types of image systems.

Dr. Farrell thought that Stanford might set up an image systems engineering training program to provide a source of new employees. She approached Professor Joseph Goodman and me, suggesting that we initiate such a training program. Professor John Hennessy, who was Dean of the School of Engineering at the time, encouraged us to go ahead, and so we did (Goodman and Wandell, 1995). When Joyce moved from HP to Stanford to become executive director, and Professor Bernd Girod joined the Stanford faculty, they transformed the training program into a broader industry affiliates program, [SCIEN](https://scien.stanford.edu/). For twenty-five years this organization served as a watering hole where skilled engineers and scientists have met for talks and conferences about new developments in image systems. Many of the talks from that seminar series are available to [view online](https://scien.stanford.edu/index.php/events/scien-seminars/).

Around that same time [Professor Abbas El Gamal](https://profiles.stanford.edu/abbas-el-gamal) returned to Stanford from a spell in industry. Along with Dr. Boyd Fowler, he chose to explore different circuit designs from CMOS imagers, extending the work initiated by Eric Fossum at JPL. Joe introduced me to Abbas, who invited me to meet regularly with his group to discuss how different sensor designs might produce images with extended dynamic range. As we set about raising funds to support the students, postdocs and research costs, we encountered the same resistance that Eric Fossum mentions (see above). Responding to that resistance was fun and sharpened our thinking.

The collaboration between Abbas' group and mine included people with a diverse set of skills. We all had some comfort with basic electrical engineering mathematics, but there were many specializations related to imaging and visual perception. That is, the physics and biology represented by the mathematical models were not part of an existing Stanford course curriculum. The research, which included sensor tapeouts, calibration, image display, required that we communicate with precision. This led us to create software that forced us to be very explicit about what we meant. This same software could also be used to try out new ideas before going to the trouble and cost of building the system. Image system simulation remains an important concept, and these days the idea applied broadly to many types of systems is often called a 'digital twin'.

Others in the imaging industry also needed tools to communicate between team members building image systems: These teams included experts in optics and sensors, or experts in sensors and displays. The software helps engineers understand how varying the specifications of individual imaging components will impact overall system performance. The simulation software was initially developed by Ting Chen, and me with a focus on the sensor. Its scope grew extensively to model the input (three-dimensional spectral radiance of scenes) various types of optics, and the control systems that manage image acquisition. With the assitance and comments from students and commercial collaborators over the years we now have a suite of tools that have been numerically validated. The tools include a core library of functions that emphasize physical descriptions of scenes, optics, sensors, displays, and color metrics. We call these tools the Image Systems Engineering Toolbox. We use that software in courses, and we will use it in this book.

Stanford is a teaching institution, although there are times we are distracted by other pursuits. The research we do is closely coupled to the courses that we teach, and before long Dr. Farrell and I initiated a course called 'Image Systems Engineering.' The course mission is to teach students about the foundations of image systems engineering; these are the ideas that will apply to all image systems even as technology implementations evolve. For example, we think that understanding the physics of the original scene is essential implementing an image system. Certain principles of optics will remain essential, as well as critical ideas about image processing, and displays. Perhaps more than most, we also think that understanding how the human visual system encodes color, adapts to changing environmental conditions, and judges position and sharpness will be fundamental for many image systems.

This book is a summary of what we have learned, and what we teach, in our image systems course at Stanford. To help students understand the concepts and calculate practical examples, we use the Image Systems Engineering Toolbox for Cameras (ISETCam). This software package enables a large variety of calculations starting with the light in the scene, calculating through the optics, arriving at the sensor, and then processed for either visualization or machine learning applications. The entire repository is open and extensively documented. Code written for this book is included in a separate repository that relies on the ISETCam routines.