<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>characterizing-artal</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">




<section id="characterization-of-visual-stimuli-using-the-standard-display-model" class="level1">
<h1>Characterization of Visual Stimuli using the Standard Display Model</h1>
<p>Joyce E. Farrell 1<br>
Haomiao Jiang 1<br>
Brian A. Wandell 1,2</p>
<p>1 Department of Electrical Engineering<br>
2 Psychology Department<br>
Stanford University, Stanford, CA 94305</p>
<p><strong>Corresponding author:</strong> J.E. Farrell, <a href="mailto:joyce_farrell@stanford.edu">joyce_farrell@stanford.edu</a></p>
<p><strong>Key Words:</strong> Display technology, calibration, modeling, simulation, visual stimuli, psychophysics, pixel point spread function, spectral power distribution</p>
</section>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>Human vision science advances by experiments that measure how sensations and perceptions arise from carefully controlled visual stimuli. Progress depends in large part on the type of display technology that is available to generate visual stimuli. In this chapter, we first describe the strengths and limitations of the display technologies that are currently used to study human vision. We then describe a standard display model that guides the calibration and characterization of visual stimuli on these displays <a href="https://paperpile.com/c/rDmune/gGuZ+0yTJ">(Brainard et al., 2002; Post, 1992)</a>. We illustrate how to use the standard display model to specify the spatial-spectral radiance of any stimulus rendered on a calibrated display. This model is used by engineers to assess the tradeoffs in display design, and by scientists to specify stimuli. The ability to quantify the stimulus is essential for reproducible research and to support the development of image-computable vision models. Finally, we discuss trends for new display systems that integrate light generation with optics, sensors, and computation. Calibrating and characterizing such systems will require extending the standard display model through digital twin simulation, advancing both technology and science.</p>
</section>
<section id="display-technologies-for-vision-science" class="level1">
<h1>Display technologies for vision science</h1>
<p>An ideal display system for science and commerce would deliver the complete spectral, spatial, directional, and temporal distribution of light rays, as if these rays were generated by a real three-dimensional scene. The full radiometric description of light rays in the three-dimensional scene is called the “light field” <a href="https://paperpile.com/c/rDmune/yLhf">(Gershun, 1939)</a>. For vision science, the simplified and related stimulus is the portion of the environmental light field that is incident at the cornea - this is the only part of the stimulus that the retina encodes. The incident light field changes as the head and eyes move.</p>
<p>Most displays fall short of reproducing the incident light field and lack the capability to dynamically adjust the displayed image in response to an observer’s head and eye movements. Despite these limitations, modern displays create a very compelling perceptual experience that captures many important perceptual elements of a real three-dimensional scene. The ability to control these displays with computers that have digital frame buffers and graphics cards has greatly enlarged the range of stimuli used in visual psychophysics compared to the optical benches and tachistoscopes used by previous generations.</p>
<p>All modern color displays are designed with three types of subpixels whose spectral power distributions (SPD) peak in the long-, middle- and short-wavelength regions of the visible spectrum (Figure 1). Each type of subpixel is called a display primary and the whole mosaic is called a color channel. The relative SPD of each primary is designed to be invariant as its intensity changes (spectral homogeneity). In normal operation, the intensities of the three subpixels are set to match the color appearance of an experimental stimulus. Three primaries are used because human color-matching experiments show that subjects can match the color appearance of a wide range of spectral power distributions using the mixture of just three independent light sources <a href="https://paperpile.com/c/rDmune/b1Ta+2zzl+E40X">(Maxwell, 1860; Wandell, 1995; Wyszecki &amp; Stiles, 1982)</a>. Modern displays effectively comprise a very large number of color-matching experiments, one for each pixel in every frame.</p>
<p>The three main display technologies used in vision experiments today are CRTs (cathode ray tubes), LCDs (liquid-crystal displays) and OLEDs (organic light emitting diodes). Color CRTs were developed by RCA in the 1950s <a href="https://paperpile.com/c/rDmune/Cc3X">(Law, 1976)</a> and were the nearly universal display technology for several decades. They remain an important display technology for vision researchers, although now they are rarely sold as consumer products. Invented at RCA labs in the 1970s <a href="https://paperpile.com/c/rDmune/Fz8U">(Kawamoto, 2002)</a>, LCDs were introduced as small mobile displays in digital watches, calculators and other handheld devices; later they enabled the widespread adoption of laptop computers. OLEDs were invented at Kodak in the 1980s <a href="https://paperpile.com/c/rDmune/Vf1l">(Tang &amp; Vanslyke, 1987)</a> and were first introduced as displays for digital cameras. Large OLED displays are expensive, but they have some advantages over LCDs: they achieve a deeper black and they have better temporal resolution.</p>
<p>In the realm of vision research, CRTs have long been valued for their ability to accurately control primary color intensities beyond 10 bits <a href="https://paperpile.com/c/rDmune/gGuZ">(Brainard et al., 2002)</a>, a critical feature for precise psychophysical experiments. The discontinuation of CRT mass production has led scientists to develop software solutions like ViewPixx and Display++ that enable researchers to achieve 10 bits of intensity resolution from LCD monitors <a href="https://paperpile.com/c/rDmune/fO5b">(Ghodrati et al., 2015)</a>. In recent years, OLED displays have gained popularity due to their superior contrast ratios and higher refresh rates, making them ideal for experiments requiring precise temporal control and high-fidelity visual stimuli <a href="https://paperpile.com/c/rDmune/SSHP">(Cooper et al., 2013)</a>.</p>
<p>Digital light projection (DLP) displays <a href="https://paperpile.com/c/rDmune/zQ5t">(Hornbeck, 1991)</a> are spatial light modulators that use an array of small, deformable mirrors. They are not used widely in visual psychophysics. However, they have been adapted for use in studies of color constancy <a href="https://paperpile.com/c/rDmune/VIgL+g9Yn">(Brainard, 1998; Brainard et al., 1997)</a>, in vitro primate retina intracellular recordings <a href="https://paperpile.com/c/rDmune/sh8P">(Packer et al., 2001)</a>, and functional magnetic resonance imaging <a href="https://paperpile.com/c/rDmune/Ie9F">(Engel et al., 1997)</a>. The advantages of DLP technology, such as high contrast ratios, fast response times, MRI compatibility, and flexibility in stimulus presentation, have made it a useful tool in these specialized research applications.</p>
<p>MicroLEDs are a promising technology for the next-generation of displays <a href="https://paperpile.com/c/rDmune/K9T7">(Jiang et al., 2002)</a>. The initial report has been followed up by many groups, both in academia and industry. At present, microLEDs are primarily used in compact displays for personal devices like smartwatches, smartphones, and head-mounted displays. Several manufacturers have demonstrated the capability to produce large-scale microLED displays, but widespread consumer adoption hinges on lowering manufacturing costs.</p>
<section id="cathode-ray-tubes-crt" class="level2">
<h2 class="anchored" data-anchor-id="cathode-ray-tubes-crt"><strong>Cathode Ray Tubes (CRT)</strong></h2>
<p>CRTs create light by directing an electron beam through a metal shadow mask onto one of three different types of phosphors <a href="https://paperpile.com/c/rDmune/RRqX+Cc3X">(Castellano, 1992; Law, 1976)</a>. When irradiated by electrons, each of the phosphors emits light with a spectral radiance distribution that is unique to that phosphor. The CRT phosphors are painted on a transparent glass surface in a pattern of alternating dots or stripes (Figure 2A), and they are selected to emit predominantly in the long (red), middle (green) and short (blue) wavebands (Figure 1A). The amount of light from each type of phosphor is controlled by the intensity of the electron beam that is incident on the phosphor. The spatial properties of the display are determined by the size and spacing of the phosphor dots or stripes.</p>
<p>The temporal properties of the display are determined by the frequency with which each phosphor is stimulated by electrons and the rate at which the phosphorescence decays (see Figure 3B). The refresh rate is determined by how fast an electron beam can scan across the many rows of pixels in a display. The more rows there are, the more time it takes for the electron beam to return to the same phosphor dot. When the refresh rate is slow and the phosphor decay is fast, the display appears to flicker. Longer phosphor decay times reduce the visibility of flicker, but increase the visibility of motion blur <a href="https://paperpile.com/c/rDmune/CYWd+jlep+7M0a+sI2C">(Becker, 2019; J. E. Farrell, 1986; Travis, n.d.; Y. Zhang et al., 2007)</a>.</p>
<p>In addition to scanning through many rows of pixels, the electron beam intensity modulates as the beam traverses phosphors within each row. The electron beam modulation rate, referred to as slew rate, is not fast enough to change perfectly as the beam moves between adjacent pixels. Consequently, the ability to control the light from adjacent pixels within a row is not perfectly independent <a href="https://paperpile.com/c/rDmune/QEri">(Lyons &amp; Farrell, 1989)</a>. We will explain the consequence of this slew rate limitation on the use of a standard display model later in this chapter.</p>
</section>
<section id="liquid-crystal-displays" class="level2">
<h2 class="anchored" data-anchor-id="liquid-crystal-displays"><strong>Liquid Crystal Displays</strong></h2>
<p>LCD displays function as an array of light valves, with each pixel regulating the amount of light transmitted from a constantly illuminated backlight. Traditional LCD backlights employ either a white fluorescent tube or a row of white LEDs positioned along the edge of the LC array, known as edge-lit configuration. In 2013, Sony introduced the first LCD television featuring a quantum dot (QD) backlight, a technology that has since become prevalent in most high-end displays and some laptops. An LCD QD backlight utilizes a row of blue LEDs that emit light through a film containing quantum dots. As the blue light traverses this quantum dot film, a portion is converted into red and green light, resulting in a combination that produces a white light that reportedly increases the color gamut of the LCD display <a href="https://paperpile.com/c/rDmune/BBSB+XFvh">(Luo et al., 2014; Xu et al., 2025)</a>. Regardless of the method used to create the white light, it is uniformly distributed across the display surface using diffusing and brightness-enhancing films, ensuring consistent illumination throughout the screen.</p>
<p>The backlight passes through a polarization filter, a layer of liquid crystal material, a second polarization filter, and then a color filter (Figure 4). The ability of photons to traverse this path is controlled by the alignment of the liquid crystals which determines the polarization of the photons and thus how much light passes through the two polarization filters. The state of the liquid crystal in each pixel is determined by an electric field that is controlled by digital values in a frame-buffer, under software control. Even when the liquid crystal is in a state that permits transmission (open), only a small fraction (about 3 percent) of the backlight photons pass through the two polarizers, color filter, and electronics.</p>
<p>The spectral radiance of an LCD pixel is determined by the SPD of the backlight and the transmissivity of the optical elements (polarizers, LC, and color filters). The spatial properties of an LCD are determined by the dimensions of a panel of thin film transistors (TFT) that controls the voltage for each pixel component and the size and arrangement of each individual filter in the color filter array. The temporal properties of an LCD are determined by the modulation rate of the backlight and the temporal response of the liquid crystal <a href="https://paperpile.com/c/rDmune/XJuv">(Yang &amp; Wu, 2014)</a>. LCDs use sample and hold circuitry that keep the liquid crystals in their “open” or “closed” state (see Figure 4B). This means that flicker is not visible, but a negative consequence of the slow dynamics is that LCDs can produce visible motion blur. Furthermore, liquid crystals respond asymmetrically to an increase or decrease in voltage (changing the alignment of the liquid crystals). For example, it is often the case that a change from white to black is faster than a change from black to white. Some LCD manufacturers have introduced circuitry to “overdrive” and “undershoot” the voltage delivered to each pixel. This additional circuitry reduces the visible motion blur but makes it difficult for the user to have precise control in the timing of visual stimuli <a href="https://paperpile.com/c/rDmune/K1CN">(Elze &amp; Tanner, 2012)</a>.</p>
<p>Another limitation of LCDs is that in the “off” state, photons from the backlight still find their way through the filters to the viewer. Consequently, LCDs do not achieve a complete black background which limits their dynamic range. Manufacturers introduced LED backlit panels that can be locally dimmed in different regions. In this way, one portion of the image can be much brighter than another, and a portion of the display can be nearly black, extending the image dynamic range. Such LCD displays are difficult to use in calibrated experiments because of the proprietary software and control circuitry that can vary with the displayed image.</p>
</section>
<section id="digital-light-projectors" class="level2">
<h2 class="anchored" data-anchor-id="digital-light-projectors"><strong>Digital light projectors</strong></h2>
<p>The digital light projector (DLP) display technology is a micro-electro-mechanical system (MEMS) consisting of an array of microscopically small mirrors arranged in a matrix on a semiconductor chip- one mirror for each pixel <a href="https://paperpile.com/c/rDmune/9Bo1+abHF">(Florence &amp; Yoder, 1996; Younse, 1993)</a>. Each mirror is on the order of 15 microns in size, and the deformable mirror arrays can have many different formats (e.g., 4K, 1080p, etc.). Like the LCDs, this is a light valve technology. The system includes a constant backlight, and each mirror can be in one of two states: it either reflects the backlight photons towards or away from the viewer. A typical pixel can change states at a high rate (kilohertz), though some devices can change much faster.</p>
<p>The intensity at each pixel is controlled by varying the percentage of time the mirror is directing light towards the viewer. In the single-chip DLP, color is controlled using a rapidly spinning color wheel that interposes different color filters between the light source. The color wheel rotation is synchronized with the control signals sent to the chip. While most display technologies use subpixel primaries that are adjacent in space, the DLP color primaries are adjacent in time - a technique called field-sequential color. Some DLP devices include only three (red, green and blue) primaries, while others include a fourth (white or clear) primary. The white primary increases the maximum display brightness, but at the highest brightness levels the display has a vanishingly small color gamut <a href="https://paperpile.com/c/rDmune/pjWz">(Kelley et al., 2009)</a>.</p>
<p>A problem with the single-chip DLP design is that field-sequential color can produce visible color artifacts when the eye moves rapidly across the image. High speed eye movements cause the sequential red, green and blue images to project to different retinal positions <a href="https://paperpile.com/c/rDmune/ayA6">(X. Zhang &amp; Farrell, 2003)</a>. A more expensive three-chip DLP design is often used in home and movie theatres. The three-chip design simultaneously projects red, green and blue images that are co-registered; hence, these DLPs do not produce the sequential color artifacts.</p>
</section>
<section id="organic-light-emitting-diodes-oled" class="level2">
<h2 class="anchored" data-anchor-id="organic-light-emitting-diodes-oled"><strong>Organic Light emitting diodes (OLED)</strong></h2>
<p>Conceptually, OLEDs are a much simpler display technology. Rather than being a light valve, each pixel in an OLED display is a light source, and thus they do not require a backlight. The device consists of two layers of organic molecules that are sandwiched between a cathode and an anode (Figure 5). OLED pixels emit light when an electric current is applied to the electroluminescent layer of organic molecules.</p>
<p>There are several ways to produce the different color primaries: (1) Each diode can be made from a different substance that emits light in a distinct wavelength band, (2) color filters can be placed in front of a single type of diode, or (3) a single type of OLED (usually blue) can be use to excite different types of phosphors <a href="https://paperpile.com/c/rDmune/eUhb">(Tsujimura, 2017)</a> or quantum dots <a href="https://paperpile.com/c/rDmune/zuiR">(Fan et al., 2024)</a>. This is the same quantum dot technology that is used to create QD-LCD backlights, except that in this context, the red, green, and blue light sources serve as emissive pixels. A significant advantage of OLED displays is their ability to achieve a very high dynamic range, as each OLED pixel can be completely black. This characteristic allows for exceptional contrast and deep blacks in the displayed image.</p>
<p>The spatial properties of an OLED display are determined by the size and arrangement of the OLED pixels that are deposited onto glass. Today, OLED pixels can be as small as 10 micrometers but researchers are experimenting with OLEDs as small as 100 nanometers <a href="https://paperpile.com/c/rDmune/fGqn">(Marcato et al., 2024)</a>. The OLED pixel size varies considerably depending on the type of display. For example, pixels can be 100 micrometers or larger on large format televisions. Some types of OLEDs (polymer OLEDs or PLEDs) can be printed onto plastic using a modified inkjet printer <a href="https://paperpile.com/c/rDmune/wEPT">(Bale et al., 2006)</a>, or a 3D printer with a spray nozzle <a href="https://paperpile.com/c/rDmune/ENGQ">(Su et al., 2022)</a>. These advances in OLED fabrication techniques are driving progress in display technology, paving the way for higher resolutions, larger screens, and potentially novel form factors in future display applications <a href="https://paperpile.com/c/rDmune/gCx2">(Singh et al., 2023)</a>.</p>
<p>The temporal properties of an OLED display are determined by the rate at which the pixel intensities can be changed (update rate) and the rate at which the screen is refreshed (refresh rate). OLEDs can be turned on and off extremely rapidly (update rate). Hence, the update rate limits the motion velocities that can be represented <a href="https://paperpile.com/c/rDmune/hv8D">(Watson et al., 1986)</a>. To reduce the visibility of flicker and motion blur, OLEDs can be refreshed at a rate that exceeds the update rate (see Figure 5B). This method is referred to as “pulse-width modulation” <a href="https://paperpile.com/c/rDmune/BHJc">(Dimigen &amp; Stein, 2024)</a>.</p>
</section>
<section id="microleds" class="level2">
<h2 class="anchored" data-anchor-id="microleds"><strong>MicroLEDs</strong></h2>
<p>MicroLED displays are a new generation of screen technology. The key difference from OLED displays is that MicroLEDs use inorganic materials like gallium nitride (GaN) rather than organic compounds. When current flows through these tiny LEDs, electrons combine with “holes” (areas lacking electrons) to produce light. The intensity of this light depends on how much current is delivered; this relationship isn’t linear. The current delivered to each pixel in a MicroLED display is controlled by a thin-film transistor (TFT) in the display’s backplane. MicroLEDs produce color primaries using the same methods as OLEDs - direct emission, color filters, or color conversion using quantum dots.</p>
<p>MicroLEDs offer several significant advantages compared to current OLED technology. Their lifespan is remarkably long, about 106 hours compared to OLEDs’ 3 x 104 hours. They’re also approximately 1,000 times brighter and respond faster to electrical signals. Perhaps most impressively, MicroLEDs can be made very small, around 1 micrometer <a href="https://paperpile.com/c/rDmune/tvR7+H7Zs">(Hsiang et al., 2021; Smith et al., 2020)</a>.</p>
<p>MicroLED technology is just appearing in consumer products, including near-eye displays <a href="https://paperpile.com/c/rDmune/bTso+Ay2a">(Bandari &amp; Schmidt, 2024; Huang et al., 2020)</a>. Manufacturers have also successfully created large displays using this technology <a href="https://paperpile.com/c/rDmune/i16o">(MiniMicroLED, 2024)</a>. Companies are currently developing MicroLED displays for smartphones and virtual reality headsets. The main challenge now is reducing production costs to make these displays affordable.</p>
</section>
</section>
<section id="the-standard-display-model-and-stimulus-characterization" class="level1">
<h1>The standard display model and stimulus characterization</h1>
<section id="overview" class="level2">
<h2 class="anchored" data-anchor-id="overview"><strong>Overview</strong></h2>
<p>Display technologies are differentiated by two main factors: the mechanism used to generate light and the spatial configuration of pixels and subpixels. When designing commercial displays, manufacturers prioritize various performance metrics such as energy efficiency, brightness, spatial resolution, contrast ratio (darkness), color gamut, and refresh/update rates. The relative importance of these parameters varies depending on the intended application.</p>
<p>Despite the diverse array of display architectures, it is possible to identify a few fundamental principles that describe the relationship between electronic control signals and the spectral radiance generated by a display. These widely adopted principles form the basis of a standard display model <a href="https://paperpile.com/c/rDmune/0yTJ+gGuZ">(Brainard et al., 2002; Post, 1992)</a>. This model is phenomenological in nature, meaning it describes observable relationships without necessarily explaining the underlying physical processes. Importantly, this standard display model is applicable not only to current popular displays but also to anticipated future developments in display technology.</p>
<p>The standard display model’s parameters can be determined through a limited number of calibration measurements. The model, combined with the calibration data, enables precise control over the display’s radiance output. A model is necessary because there are far too many images to calibrate individually <a href="https://paperpile.com/c/rDmune/4u7G">(Brainard, 1989)</a>. To illustrate, an 8-bit display can produce 2^24 distinct RGB combinations for a single static image. Furthermore, a 1024x1024 pixel display (2^20 pixels) has the potential to render an astronomical 2^480 different images. The standard display model efficiently addresses this complexity by defining a compact set of calibration measurements, which can then be used to predict the spectral radiance for a wide array of images.</p>
<p>Several key measurements are necessary to specify a model for any particular display. First, each subpixel type has a characteristic spectral power distribution (SPD, Figure 1). The model assumes that the SPD is the same for all subpixels of a given type and is invariant when normalized for intensity level. Thus, the normalized SPD can be measured using a spectroradiometer that averages the spectral radiance emitted from a region of the display surface.</p>
<p>Second, the absolute level (peak radiance) of the SPD is set by the frame buffer value. The relationship between the frame buffer value and the SPD level is referred to as the gamma-curve. The gamma-curve is assumed to be the same for all subpixels of a given type (shift-invariant), independent of the image content, and monotone increasing.</p>
<p>Third, the standard display model describes the spatial distribution of light emitted by each type of subpixel, called the point spread function (PSF). The standard display model assumes that the PSF is the same for subpixels of a given type (shift-invariant) and independent of the image content.</p>
<p>Finally, most displays refresh the image (frame) at a rate between 30 and 240 times per second. Within each frame, the subpixel intensity can rise and fall, and the frame repetitions and pixel dynamics influence the visibility of motion and flicker. The standard display model assumes that each subpixel has a simple time-invariant impulse response function that is independent of image content. This assumption is frequently violated because of the extensive engineering to control the dynamics of displays (see previous sections on LCDs, CRTs and OLEDs). Characterizing the display dynamics is particularly important for experiments involving rapidly changing high contrast targets (e.g.&nbsp;random dots).</p>
<p>The standard display model clarifies the measurements needed to calibrate a display. The first two are to measure (a) the normalized spectral radiance distributions for each of the display primaries, and (b) the gamma-curve that specifies the absolute level of the spectral radiance given a particular frame buffer value. It is less common for scientists to measure the subpixel PSFs. These can be measured using a macro-lens and the linear output of a calibrated digital camera <a href="https://paperpile.com/c/rDmune/fUuF">(J. Farrell et al., 2008)</a>, but in most cases the function is treated as a single point (impulse). Characterizing the PSF can be meaningful for measurements of fine spatial resolution (e.g., quality of fonts, vernier resolution) where there are significant effects of human optics on retinal image formation. In the next section, we offer specific advice about making these calibration measurements and combining them into a computational implementation of the standard display model.</p>
</section>
<section id="spectral-radiance-and-gamma-curves" class="level2">
<h2 class="anchored" data-anchor-id="spectral-radiance-and-gamma-curves"><strong>Spectral radiance and gamma curves</strong></h2>
<p>It is common to use a spectral radiometer to measure the spectral radiance emitted by each of the three types of primaries. The standard display model assumes that for each primary the spectral power distribution takes the form I(F) P(), where P()is the spectral power distribution of the display when the frame buffer is set to its maximum value and 0 &lt; I(F) &lt; 1 is the relative intensity for a frame buffer value of F.</p>
<p>To estimate I(F)and P(), we measure the spectral radiance for a series of different frame buffer levels. An important detail is this: In most displays there is some stray light present even when F=0. This light is usually treated as a fixed offset, B()and subtracted from the calibration data <a href="https://paperpile.com/c/rDmune/gGuZ">(Brainard et al., 2002)</a>. Hence, the measured spectral radiance curves have the form</p>
<p>R(,F)=I(F)P() + B()</p>
<p>The term I is the relative intensity of the primary and F is the frame buffer value. When F is set to the maximum value, the value of I is equal to 1. If one subtracts the background spectral power distribution, then when I(0)=0 and the relative intensity is typically modeled as a simple power law <a href="https://paperpile.com/c/rDmune/x1cy">(Poynton, 1993)</a> which gives the curve its name.</p>
<p>I=F [1]</p>
<p>For most displays B() is difficult to measure because it is small and negligible compared to the experimental stimuli. In such cases, the radiance is modeled by including a small, wavelength-independent, offset in the gamma curve</p>
<p>R(,F)=I(F)P() [2]<br>
I=F+B0 [3]</p>
<p>Historically, the value of ɣ in manufactured displays been between 1.8 and 2.4, which is quite significant. If one changes the ɣ of a display from 1.8 to 2.4, the same frame buffer values will produce very different spectral radiance distributions. Pixels set to the same frame buffer (R,G,B) produce spectral radiances that differ by as much as 10 CIELAB ΔE units (median ~ 6 ΔE). In recent years, manufacturers have converged to a function that is linear at small values, close to ɣ = 2.4 at high values, and overall similar to ɣ = 2.2 (sRGB 2015).</p>
<p>The analytical gamma function is an approximation to the true I(F). In modern computers, this approximation can be avoided by building a lookup table that stores the nonlinear relationship between the digital control values and the display output, I(F).</p>
<p>This nonlinearity will continue across technologies because programmers prefer that equal spacing of the digital frame buffer values correspond to equal perceptual spacing <a href="https://paperpile.com/c/rDmune/x1cy+8fWH">(Poynton, 1993; Poynton &amp; Funt, 2014)</a>. To preserve this perceptual relationship, the display intensity must be nonlinearly related to the frame buffer value <a href="https://paperpile.com/c/rDmune/2zzl+L5vo">(Stevens, 1957; Wandell, 1995)</a>.</p>
</section>
<section id="the-subpixel-point-spread-functions" class="level2">
<h2 class="anchored" data-anchor-id="the-subpixel-point-spread-functions"><strong>The subpixel point spread functions</strong></h2>
<p>The spatial distribution of light from each subpixel is described by a point spread function, P(x,y,). The spatial spread of the light from each subpixel can be measured using a high resolution digital camera with a close-up lens <a href="https://paperpile.com/c/rDmune/fUuF">(J. Farrell et al., 2008)</a>. Furthermore, the spectral and spatial parts of the point spread function are separable.</p>
<p>P(x,y,)=s(x,y)w() [4]</p>
<p>The subpixel point spread is assumed to have the same form across display positions, that is the subpixel point spread function at pixel (u,v) is s(x-u,y-v)w(). And finally, the shape scales with intensity I s(x-u,y-v)w().</p>
<p>The standard display model assumes that point spread functions from adjacent pixels sum. This linearity is an ideal - no display is precisely linear. But display designs generally aim to satisfy these principles and implementations are close enough so that these principles are a good basis for display characterization and simulation.</p>
</section>
<section id="linearity" class="level2">
<h2 class="anchored" data-anchor-id="linearity"><strong>Linearity</strong></h2>
<p>Apart from the static, nonlinear gamma curve, the standard display model is a shift-invariant linear system. That is, given the intensity of each subpixel we compute the expected display spectral radiance as the weighted sum of the subpixel point spread functions. If the subpixel intensities for one image are I1with corresponding spectral radiance R1(x,y,), and a second image is I2with corresponding spectral radiance R2(x,y,), then the radiance when the image is I1 + I2 will be R1(x,y,) + R2(x,y,).</p>
<p>The calibration process should test the additivity assumption. Simple tests include checking that the light emitted from the <em>i</em>th subpixel does not depend on the intensity of other subpixels <a href="https://paperpile.com/c/rDmune/QEri+Qfii+fUuF">(J. Farrell et al., 2008; Lyons &amp; Farrell, 1989; Pelli, 1997)</a>.</p>
</section>
<section id="model-summary" class="level2">
<h2 class="anchored" data-anchor-id="model-summary"><strong>Model summary</strong></h2>
<p>The standard display model for a steady-state image can be expressed as a simple formula that maps the frame buffer values, F, to the display spatial-spectral radiance R(x,y,).</p>
<p>Suppose the gamma function, point spread function, and spectral power distribution of the jth subpixel type are Ij (v), pj (x,y), and wj (). Suppose the frame buffer values for the jth subpixel type is Fj(u,v). Then the display spectral radiance across space is predicted to be</p>
<p>R(x,y,)=u,v j=1,3 Ij(Fj(u,v)) sj(x-u,y-v) wj() [5]</p>
</section>
</section>
<section id="display-calibration" class="level1">
<h1>Display calibration</h1>
<p>If the standard display model describes the device under test, then calibration requires a very small set of display measurements - gamma, spd, PSF and temporal response - to fully describe the physical radiance of displayed stimuli. Display calibration can be conceived as (a) measuring how well the key model assumptions hold (spectral homogeneity, pixel independence, spatial homogeneity), and (b) using the measurements to estimate the model parameters.</p>
<section id="pixel-independence" class="level2">
<h2 class="anchored" data-anchor-id="pixel-independence"><strong>Pixel independence</strong></h2>
<p>The radiance emitted by a subpixel should depend only on the digital frame buffer value controlling that subpixel. Equivalently, the radiance emitted by a collection of pixels must not change as the digital values of other pixels change. Displays often satisfy this pixel independence principle for a large range of stimuli <a href="https://paperpile.com/c/rDmune/fUuF">(J. Farrell et al., 2008)</a>, but there are displays and certain types of stimuli that fail this test <a href="https://paperpile.com/c/rDmune/QEri+K1CN">(Elze &amp; Tanner, 2012; Lyons &amp; Farrell, 1989)</a>.</p>
<p>For example, CRTs must sweep the intensity of the electron beam very rapidly across each row of pixels. There are limits to how rapidly the beam intensity can change (a maximum “slew rate”). If a very different intensity is required for a pair of adjacent row pixels, the beam may not be able to adjust in time and independence is violated, and the standard display model will not be useful for characterizing the spatial-spectral radiance of such stimuli <a href="https://paperpile.com/c/rDmune/QEri+OZwX">(Lyons &amp; Farrell, 1989; Naiman &amp; Makous, 1993)</a>.</p>
<p>LCDs are limited by the rate at which liquid crystals can change their state in response to a change in voltage polarity, as well as the asymmetry in their response to the “on” or “off” states. LCDs typically combine sample and hold circuitry to switch between different LC states and a flickering backlight to minimize the visibility of both motion blur. LCDs with these features (sample and hold circuitry with flickering LED or fluorescent backlights) can be modeled as a linear system <a href="https://paperpile.com/c/rDmune/fUuF">(J. Farrell et al., 2008)</a>. Departure from display linearity occurs, however, when LCD manufacturers introduce “overdrive” and “undershoot” circuitry to minimize the visibility of motion blur or when they locally dim LED backlight panels to increase dynamic range. These new features make it very difficult to control and calibrate visual stimuli, particularly for studies that require precise control of timing <a href="https://paperpile.com/c/rDmune/K1CN">(Elze &amp; Tanner, 2012)</a>.</p>
<p>There are several ways to test pixel independence <a href="https://paperpile.com/c/rDmune/QEri+Qfii+fUuF">(J. Farrell et al., 2008; Lyons &amp; Farrell, 1989; Pelli, 1997)</a>, but the general principle is simple. Separately measure the radiance from the middle of a large patch of pixels. Make the measurement with a few different digital values. Then create spatial patterns that are made up with half the pixels at one digital value and half at the other. The radiance from these mixed patches should be the average of the radiance from the large patches, measured individually.</p>
<p>A key assessment is to evaluate how well independence is satisfied for the planned experimental stimuli. For example, CRTs often fail pixel independence for high spatial frequency stimuli because of the finite slew rate of the electron beam. Nonetheless, CRTs are very useful for visual experiments that use low frequency stimuli, such as studies of human color vision. The standard display model, like any useful model, will have some compliance range, and the practical question is whether the model can be used given a specific experimental plan.</p>
<p>OLEDs are excellent devices for vision research because they can meet the requirements of the standard display model <a href="https://paperpile.com/c/rDmune/SSHP">(Cooper et al., 2013)</a>. Display electronics control the rate at which the pixel intensities can be changed (the update rate), but OLED pixels can be rapidly turned on and off. Thus while the update rate limits the motion velocities that can be represented, the higher refresh rates minimize the visibility of motion blur and flicker. And, unlike the LCDs that modulate the intensity of a backlight, OLED pixels can be turned off, creating a very dark background.</p>
<p>Given these benefits, and the fact that the cost of manufacturing OLED displays is decreasing, one<br>
might consider these displays to be ideal devices for vision research. There are, however, potential challenges to consider. OLED display manufacturers are experimenting with different types of color pixel patterns and developing proprietary methods for rendering images on these new displays. For instance, one manufacturer incorporated a hardware-based edge enhancement algorithm, though fortunately, this feature can be disabled to maintain pixel independence <a href="https://paperpile.com/c/rDmune/SSHP">(Cooper et al., 2013)</a>. Unless it is possible to turn off or at least control the proprietary display rendering, which may vary depending on the presented image, it may be difficult to know the spatial distribution of the spectral energy in displayed stimuli.</p>
</section>
<section id="spectral-homogeneity" class="level2">
<h2 class="anchored" data-anchor-id="spectral-homogeneity"><strong>Spectral homogeneity</strong></h2>
<p>The relative spectral radiance from a subpixel should be the same as its intensity is varied. Any change in the relative spectral radiance will be manifest as an unwanted color shift, and the display will be difficult to calibrate. Recall that the intensity of the light from an LC display depends on the rotation of the polarization angle caused by the birefringent liquid crystal. In some displays, the polarization effect is wavelength dependent and this violates the spectral homogeneity assumption <a href="https://paperpile.com/c/rDmune/PVJo">(Wandell &amp; Silverstein, 2003)</a>. These failure occurs because the LC polarization is not precisely<br>
the same for all wavelengths and also as a result of spectral variations in polarizer extinction.</p>
<p>A second deviation from the standard display model occurs when the display emission is angle-dependent. In fact, the first-generation of LCDs had a very large angle-dependence so that even small changes in the viewing position had a large impact on the spectral radiance at the cornea. The reason for this strong dependence is that the path followed by a ray through the LC and the polarizers has an influence on the likelihood of transmission, and this function is wavelength dependent <a href="https://paperpile.com/c/rDmune/bmaJ">(Silverstein, L.D., Fiske, T.G., 1993)</a>. Manufacturers have reduced these viewing angle dependencies by placing retardation films in the optical path <a href="https://paperpile.com/c/rDmune/R0sH">(Yakovlev et al., 2015)</a>.</p>
<p>For visual psychophysics experiments, it is typical to fix the subject’s head position relative to the screen, typically by using a chin-rest or a bite bar placed on-axis facing the middle of the display. Instruments used for display calibration should be placed at this position. If the spectrophotometer and the eye are located at any other angle, the spectral radiance from the display may different.</p>
</section>
<section id="spatial-homogeneity-shift-invariance" class="level2">
<h2 class="anchored" data-anchor-id="spatial-homogeneity-shift-invariance"><strong>Spatial homogeneity (shift invariance)</strong></h2>
<p>When a subject is close to the display surface, the angle-dependence of the spectral radiance appears as a spatial inhomogeneity: the spectral radiance at the cornea differs between on-axis (center) and off-axis (edge) pixels. At further distances, say 1m away, the angle between the center and edge is smaller and the spatial homogeneity is better.</p>
<p>A second source of spatial inhomogeneity arises from the fact that it is difficult to maintain perfect uniformity of the pixels across the relatively large display surfaces. Such non-uniformities are referred to as “mura”, which is a Japanese word for “unevenness”. For LCDs, there are several sources of mura, including non-uniformity in the TFT thickness, LC material density, color filter variations, backlight illumination, and variations in the optical filters. Additional possible sources are impurities in the LC material, non-uniform gaps between substrates and warped light guides.</p>
<p>On LCDs, mura appears as blemishes and dark spots; manufacturers attempt to eliminate these sources during the manufacturing process. For OLEDs, mura is mainly due to non-uniformity in the currents in spatially adjacent diodes that appear as black lines, blotches, dots, and faint stains that are more visible in the dark areas of an image. This can be mitigated during the manufacturing process by introducing feedback circuitry that adjusts the pixel transistor current during a calibration procedure <a href="https://paperpile.com/c/rDmune/J0ki">(McCreary, 2014)</a>.</p>
</section>
</section>
<section id="applications-of-the-standard-display-model" class="level1">
<h1>Applications of the standard display model</h1>
<p>The standard display model is a versatile tool in the fields of vision science and display engineering. Its primary functions can be summarized in three key areas. First, it serves as a guide for calibrating visual stimuli, which is essential for scientists to accurately characterize and share experimental stimuli, thereby facilitating the replication of scientific studies <a href="https://paperpile.com/c/rDmune/4u7G+gGuZ">(Brainard, 1989; Brainard et al., 2002)</a>. Second, the model supports the advancement of computational models for human vision by enabling researchers to calculate the irradiance incident at the eye, a critical factor in understanding visual perception <a href="https://paperpile.com/c/rDmune/q9T7+cFeb+29lo+jeJF">(Cottaris et al., 2019, 2020; X. Ding et al., 2019; J. E. Farrell et al., 2014)</a>. Third, it is valuable in the engineering design process, allowing for the simulation and evaluation of different display types and rendering algorithms <a href="https://paperpile.com/c/rDmune/fUuF">(J. Farrell et al., 2008)</a>.</p>
<p>Key assumptions of the standard display model is that the light generated by each display subpixel is additive, independent and shift invariant. These assumptions, referred to as spectral homogeneity, pixel independence and spatial homogeneity, can be tested in the calibration process. A particular display may not meet these conditions for all stimuli, yet the model may still be used to predict the spatial-spectral radiance of a restricted class of visual stimuli. As an example, the standard display model does not predict the spectral radiance of high frequency gratings presented on a CRT <a href="https://paperpile.com/c/rDmune/QEri+Qfii+fUuF">(J. Farrell et al., 2008; Lyons &amp; Farrell, 1989; Pelli, 1997)</a>, but the model does predict the spectral-spectral radiance of large uniform colors <a href="https://paperpile.com/c/rDmune/gGuZ+0yTJ">(Brainard et al., 2002; Post, 1992)</a>. The standard display model can predict the steady-state spatial-spectral radiance of high frequency gratings and text rendered on many LCD displays <a href="https://paperpile.com/c/rDmune/fUuF">(J. Farrell et al., 2008)</a>, particularly in the absence of complex circuitry to overdrive or undershoot pixel intensity <a href="https://paperpile.com/c/rDmune/Zk9Z+woLm">(B.-W. Lee et al., 2001; S.-W. Lee et al., 2006)</a> and locally dim LED backlights <a href="https://paperpile.com/c/rDmune/Vla5">(Seetzen et al., 2004)</a>. The standard display can predict the spatial-spectral radiance of many visual stimuli rendered on OLED displays when image-adaptive edge enhancement algorithms that are typically implemented in hardware are disabled <a href="https://paperpile.com/c/rDmune/SSHP">(Cooper et al., 2013)</a>. Even head-mounted displays (HMDs) which present unique challenges for calibration due to their proprietary rendering and updating software, can be modified so that spectral homogeneity and pixel independence are achievable <a href="https://paperpile.com/c/rDmune/Ids2+w7FM+RZzA">(Gil Rodríguez et al., 2022; Toscani et al., 2019; Zaman et al., 2023)</a>.</p>
<p>In the following sections, we present two examples that illustrate how the standard model, coupled with color discrimination metrics, can analyze display capabilities. The first demonstrates the necessity of 10-bit intensity resolution for measuring psychophysical discrimination functions. The second analyzes the impact of different subpixel PSFs on font discriminations. These examples illustrate how the standard display model can be applied to analyze display capabilities under specific experimental conditions, highlighting its versatility and practical applications in display technology research and development.</p>
<section id="color-discriminations-the-impact-of-bit-depth" class="level2">
<h2 class="anchored" data-anchor-id="color-discriminations-the-impact-of-bit-depth"><strong>Color discriminations: the impact of bit-depth</strong></h2>
<p>First, we consider how the number of digital steps (frame buffer levels) limits the ability to make threshold color and luminance discrimination measurements. We calculated the CIE XYZ values for each of 27 different RGB levels, and we then calculated the CIELAB ΔE value between each of these 27 points and all of its neighbors within 2 digital steps. We repeated this calculation simulation assuming a frame buffer with 10-bits (1024 levels), the actual display resolution, and a coarser step size of 8-bits (256 levels) but equivalent gamma.</p>
<p>The distributions of CIELAB ΔE differences for the 10-bit and 8-bit displays are shown in the upper and lower histograms of Figure 6, respectively. For a 10-bit display, the signals within two digital steps are below ΔE=1. In this case, the visual discriminability is small enough to measure a psychophysical discrimination curve. If the display has only 8-bits of intensity resolution, the two digital steps frequently exceed ΔE=1. This explains why threshold measurements are impractical on 8-bit displays. For commercial purposes, however, one step is about ΔE=1, which explains why 8-bits renders a reasonable reproduction.</p>
</section>
<section id="spatial-spectral-discriminations" class="level2">
<h2 class="anchored" data-anchor-id="spatial-spectral-discriminations"><strong>Spatial-spectral discriminations</strong></h2>
<p>Next, we analyzed the visual impact of changing the subpixel point spread function (see Figure 2). In this example, we compared two displays with the same primaries and spatial resolution (96 dots per inch), but with different pixel point spread functions. In one case, the point spread function is the conventional set of three parallel stripes (Dell LCD Display Model 1905FP), while in the second case the point spread is three adjacent chevrons (Dell LCD Display Model 1907FPc). We used the standard display model to calculate the spatial-spectral radiance of the 52 upper and lower case letters on both displays. The spatial-spectral radiance image data are represented as 3D matrices or hypercubes where each plane in the hypercube contains the stimulus intensity for points sampled across the display (x,y) for each of the sampled wavelengths (ƛ). To visualize the data, we map the vector describing the spectral radiance for each pixel into CIE XYZ values and convert these into sRGB display values (see inset in Figure 7).</p>
<p>We used the spatial-spectral radiance data to calculate the Spatial CIELAB (S-CIELAB) ΔE difference <a href="https://paperpile.com/c/rDmune/zQ8e">(X. Zhang &amp; Wandell, 1997)</a> between each letter simulated on the two displays and viewed from different distances. Figure 7 plots the median S-CIELAB ΔE value as a function of viewing distance. The analysis predicts no visible differences between pairs of letters rendered on the two displays at any of the viewing distances. And indeed, we did not find significant differences between subject’s judgments about the quality of letters rendered on the two different displays <a href="https://paperpile.com/c/rDmune/ZbJD">(J. Farrell et al., 2009)</a>.</p>
</section>
</section>
<section id="the-future-display-systems-and-simulation" class="level1">
<h1>The future: Display systems and simulation</h1>
<p>The standard display model is used in vision science in two important ways. First, it is used to control and characterize the stimuli used in visual psychophysical experiments. Second, it is used to calculate the irradiance at the eye and integrated into computational models of human vision<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.</p>
<p>A third use of the standard display model, which is important for industry, is its use in simulation to soft prototype displays and quantify their performance. Simulation is a powerful tool in the imaging industry and an important part of our own research<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>. Many research groups and companies are developing simulation software to support the design of imaging components including lenses, sensors and light-emitting devices. Soft prototyping is used to evaluate the integration of those components into imaging systems. Similarly, the standard display model will need to evolve and be integrated into image systems simulation software as displays become an integral part of more complex imaging systems.</p>
<p>To understand the evolution of display imaging systems, it’s helpful to consider the concept of the environmental light field—a complete radiometric description of light rays within a three-dimensional scene. Recreating this full light field is a significant challenge, currently beyond our technological capabilities. An ideal system would emit light rays with precise intensity, direction, and wavelength from every point within a large volume, effectively replicating how light propagates from a real 3D scene. This approach promises accurate depth cues—including focus, parallax, and occlusion—allowing viewers to perceive lifelike 3D images without special glasses or head tracking, a key objective for many display technology researchers and engineers.</p>
<p>Free-standing displays approximate portions of the environmental light field. Early systems, primarily for teleconferencing, were expensive and complex. Commercial examples included life-size displays with ultra-high definition (e.g., Cisco TelePresence, Poly RealPresence, Google’s Project Starline), designed to present remote participants at their actual size. Some used curved or wrap-around displays for greater immersion, attempting a better approximation of the light field. More recent displays offer a closer approximation of the light field <a href="https://paperpile.com/c/rDmune/CnGQ">(Wang et al., 2024)</a> within a volume of space (e.g., Holografika, Light Field Lab, Leia). These systems integrate optics with light generation to control the intensity, color, and direction of emitted rays. The light field itself can be generated using computer graphics or captured with a light field camera (e.g., Raytrix). These are often referred to as “far-field” displays.</p>
<p>An alternative approach uses simpler displays but dynamically updates the image based on the viewer’s head and eye position. The development of small displays like OLEDs and microLEDs has enabled compact, wearable “near-field” displays in the form of glasses or goggles. These systems use information about both the environmental light field and the viewer’s head and eye position. A computer then calculates and renders the appropriate display image. This approach was pioneered in virtual reality (VR) devices and is now being developed for augmented reality (AR) and mixed reality (MR) (e.g., Apple’s Vision Pro, Meta’s Quest Pro). The near-field devices involve integration between many different sensors and extensive computation, along with highly specialized optics <a href="https://paperpile.com/c/rDmune/WUgk+ICdX">(Y. Ding et al., 2023; Rolland &amp; Goodsell, 2024)</a> to enable viewing the image on the small, near display. For example, the Apple Vision Pro includes twenty different sensors, a micro-OLED display with 23 million pixels and custom optics, and an M2 processor<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> .</p>
<p>Future display development will likely explore both far-field devices, aiming to present a larger portion of the environmental light field, and near-field devices, focusing on dynamically updating the incident light field. However, the near-field devices must address two key human factors challenges: the vergence-accommodation conflict (VAC) and latency <a href="https://paperpile.com/c/rDmune/ylUK+ySCm+ipUJ">(Bhowmik, 2024; Jerald, 2015; Kramida, 2016)</a>.</p>
<p>The VAC arises from a mismatch between vergence and accommodation, two normally coupled eye responses. In natural vision, our eyes converge (or diverge) to fixate on an object, and simultaneously adjust focus (accommodate) to maintain a sharp image. In most stereoscopic displays, however, the eyes are always focused on the fixed display plane, regardless of the virtual object’s apparent distance. This creates a conflict: while the eyes verge to align with virtual objects at varying depths, accommodation remains fixed at the screen distance. This unnatural decoupling can lead to visual discomfort, eye strain, and difficulty fusing stereoscopic images, especially for near objects. Latency, the delay between user head/eye movements and corresponding display updates, can disrupt presence and induce motion sickness.</p>
<p>Addressing the vergence-accommodation conflict (VAC) and latency requires close collaboration between engineering and vision science researchers. Implementing and evaluating both far- and near-field displays necessitates new approaches to device calibration, characterization, and understanding the impact of engineering design choices on the appearance. The standard display model will likely be replaced by sophisticated simulation software capable of modeling each system component, including light-emitting elements, light guides, and various optical components. Increased emphasis will be placed on precise timing to understand latency requirements. This will enable prediction of the dynamic light field generated by these displays and, consequently, the irradiance at the viewer’s retinas. This shift from a standard display model to comprehensive system simulation demands new ideas in both vision science and display technology. We are confident that scientists and engineers will develop robust calibration and simulation methodologies, enabling researchers to effectively integrate these new technologies into scientific practice and generate novel insights into vision and cognition.</p>
</section>
<section id="references" class="level1">
<h1>References</h1>
<p><a href="http://paperpile.com/b/rDmune/wEPT">Bale, M., Carter, J. C., Creighton, C. J., Gregory, H. J., Lyon, P. H., Ng, P., Webb, L., &amp; Wehrum, A. (2006). Ink‐jet printing: The route to production of full‐color P‐OLED displays. <em>Journal of the Society for Information Display</em>, <em>14</em>(5), 453–459.</a><br>
<a href="http://paperpile.com/b/rDmune/bTso">Bandari, V. K., &amp; Schmidt, O. G. (2024). A bright future for micro-LED displays. <em>Light, Science &amp; Applications</em>, <em>13</em>(1), 317.</a><br>
<a href="http://paperpile.com/b/rDmune/7M0a">Becker, M. E. (2019). 50‐1: Flicker from electronic displays ‐ reconsidering the confusion. <em>Digest of Technical Papers. SID International Symposium</em>, <em>50</em>(1), 687–690.</a><br>
<a href="http://paperpile.com/b/rDmune/ySCm">Bhowmik, A. K. (2024). Virtual and augmented reality: Human sensory‐perceptual requirements and trends for immersive spatial computing experiences. <em>Journal of the Society for Information Display</em>, <em>32</em>(8), 605–646.</a><br>
<a href="http://paperpile.com/b/rDmune/4u7G">Brainard, D. H. (1989). Calibration of a computer controlled color monitor. <em>Color Research and Application</em>, <em>14</em>(1), 23–34.</a><br>
<a href="http://paperpile.com/b/rDmune/g9Yn">Brainard, D. H. (1998). Color constancy in the nearly natural image. 2. Achromatic loci. <em>Journal of The Optical Society of America A-Optics Image Science and Vision</em>, <em>15</em>(2), 307–325.</a><br>
<a href="http://paperpile.com/b/rDmune/VIgL">Brainard, D. H., Brunt, W. A., &amp; Speigle, J. M. (1997). Color constancy in the nearly natural image. 1. Asymmetric matches. <em>JOSA A</em>, <em>14</em>(9), 2091–2110.</a><br>
<a href="http://paperpile.com/b/rDmune/gGuZ">Brainard, D. H., Pelli, D. G., &amp; Robson, T. (2002). Display characterization. <em>Signal Processing Algorithms, Architectures, Arrangements, and Applications Conference Proceedings</em>, <em>80</em>, 2–067.</a><br>
<a href="http://paperpile.com/b/rDmune/RRqX">Castellano, J. A. (1992). <em>Handbook of display technology</em>.</a> <a href="https://books.google.com/books?hl=en&amp;lr=&amp;id=p68XfA0MHpoC&amp;oi=fnd&amp;pg=PR13&amp;dq=+Handbook+Of+Display+Technology+Castellano&amp;ots=yt-mSACsLQ&amp;sig=pQWBgKMXlbNGo-Z6iZwydyCAkJY">https://books.google.com/books?hl=en&amp;lr=&amp;id=p68XfA0MHpoC&amp;oi=fnd&amp;pg=PR13&amp;dq=+Handbook+Of+Display+Technology+Castellano&amp;ots=yt-mSACsLQ&amp;sig=pQWBgKMXlbNGo-Z6iZwydyCAkJY</a><br>
<a href="http://paperpile.com/b/rDmune/SSHP">Cooper, E. A., Jiang, H., Vildavski, V., Farrell, J. E., &amp; Norcia, A. M. (2013). Assessment of OLED displays for vision research. <em>Journal of Vision</em>, <em>13</em>(12), 16.</a><br>
<a href="http://paperpile.com/b/rDmune/cFeb">Cottaris, N. P., Jiang, H., Ding, X., Wandell, B. A., &amp; Brainard, D. H. (2019). A computational-observer model of spatial contrast sensitivity: Effects of wave-front-based optics, cone-mosaic structure, and inference engine. <em>Journal of Vision</em>, <em>19</em>(4), 8.</a><br>
<a href="http://paperpile.com/b/rDmune/jeJF">Cottaris, N. P., Wandell, B. A., Rieke, F., &amp; Brainard, D. H. (2020). A computational observer model of spatial contrast sensitivity: Effects of photocurrent encoding, fixational eye movements, and inference engine. <em>Journal of Vision</em>, <em>20</em>(7), 17.</a><br>
<a href="http://paperpile.com/b/rDmune/BHJc">Dimigen, O., &amp; Stein, A. (2024). A high-speed OLED monitor for precise stimulation in vision, eye-tracking, and EEG research. In <em>bioRxiv</em>. https://doi.org/</a><a href="http://dx.doi.org/10.1101/2024.09.13.612866">10.1101/2024.09.13.612866</a><br>
<a href="http://paperpile.com/b/rDmune/29lo">Ding, X., Radonjic, A., Cottaris, N. P., Jiang, H., Wandell, B. A., &amp; Brainard, D. H. (2019). Computational-observer analysis of illumination discrimination. <em>Journal of Vision</em>, <em>19</em>(7), 11.</a><br>
<a href="http://paperpile.com/b/rDmune/ICdX">Ding, Y., Yang, Q., Li, Y., Yang, Z., Wang, Z., Liang, H., &amp; Wu, S.-T. (2023). Waveguide-based augmented reality displays: perspectives and challenges. <em>eLight</em>, <em>3</em>(1), 1–34.</a><br>
<a href="http://paperpile.com/b/rDmune/K1CN">Elze, T., &amp; Tanner, T. G. (2012). Temporal properties of liquid crystal displays: implications for vision science experiments. <em>PloS One</em>, <em>7</em>(9), e44048.</a><br>
<a href="http://paperpile.com/b/rDmune/Ie9F">Engel, S. A., Glover, G. H., &amp; Wandell, B. A. (1997). Retinotopic organization in human visual cortex and the spatial precision of functional MRI. <em>Cerebral Cortex (New York, N.Y.: 1991)</em>, <em>7</em>(2), 181–192.</a><br>
<a href="http://paperpile.com/b/rDmune/zuiR">Fan, J., Han, C., Yang, G., Song, B., Xu, R., Xiang, C., Zhang, T., &amp; Qian, L. (2024). Recent progress of quantum dots light-emitting diodes: Materials, device structures, and display applications. <em>Advanced Materials (Deerfield Beach, Fla.)</em>, <em>36</em>(37), e2312948.</a><br>
<a href="http://paperpile.com/b/rDmune/CYWd">Farrell, J. E. (1986). An analytical method for predicting perceived flicker. <em>Behaviour &amp; Information Technology</em>, <em>5</em>(4), 349–358.</a><br>
<a href="http://paperpile.com/b/rDmune/q9T7">Farrell, J. E., Jiang, H., Winawer, J., Brainard, D. H., &amp; Wandell, B. A. (2014). 27.2: <em>distinguished paper</em>: Modeling visible differences: The computational observer model. <em>Digest of Technical Papers. SID International Symposium</em>, <em>45</em>(1), 352–356.</a><br>
<a href="http://paperpile.com/b/rDmune/fUuF">Farrell, J., Ng, G., Ding, X., Larson, K., &amp; Wandell, B. (2008). A display simulation toolbox for image quality evaluation. <em>Journal of Display Technology</em>, <em>4</em>(2), 262–270.</a><br>
<a href="http://paperpile.com/b/rDmune/ZbJD">Farrell, J., Xu, J., Larson, K., &amp; Wandell, B. (2009). 47.2: Visual preference for ClearType technology. <em>Digest of Technical Papers. SID International Symposium</em>, <em>40</em>(1), 702–705.</a><br>
<a href="http://paperpile.com/b/rDmune/abHF">Florence, J. M., &amp; Yoder, L. A. (1996). Display system architectures for digital micromirror device (DMD)-based projectors. In M. H. Wu (Ed.), <em>Projection Displays II</em>. SPIE. https://doi.org/</a><a href="http://dx.doi.org/10.1117/12.237004">10.1117/12.237004</a><br>
<a href="http://paperpile.com/b/rDmune/yLhf">Gershun, A. (1939). The Light Field. <em>Journal of Mathematics and Physics</em>, <em>18</em>(1-4), 51–151.</a><br>
<a href="http://paperpile.com/b/rDmune/fO5b">Ghodrati, M., Morris, A. P., &amp; Price, N. S. C. (2015). The (un)suitability of modern liquid crystal displays (LCDs) for vision research. <em>Frontiers in Psychology</em>, <em>6</em>, 303.</a><br>
<a href="http://paperpile.com/b/rDmune/RZzA">Gil Rodríguez, R., Bayer, F., Toscani, M., Guarnera, D. ’ya, Guarnera, G. C., &amp; Gegenfurtner, K. R. (2022). Colour Calibration of a Head Mounted Display for Colour Vision Research Using Virtual Reality. <em>SN Computer Science</em>, <em>3</em>(1), 22.</a><br>
<a href="http://paperpile.com/b/rDmune/zQ5t">Hornbeck, L. J. (1991). Spatial light modulator and method (USPTO Patent No.&nbsp;5061049). In <em>US Patent</em> (No.&nbsp;5061049).</a> <a href="https://patents.google.com/patent/US5061049A/en">https://patents.google.com/patent/US5061049A/en</a><br>
<a href="http://paperpile.com/b/rDmune/H7Zs">Hsiang, E.-L., Yang, Z., Yang, Q., Lan, Y.-F., &amp; Wu, S.-T. (2021). Prospects and challenges of mini‐LED, OLED, and micro‐LED displays. <em>Journal of the Society for Information Display</em>, <em>29</em>(6), 446–465.</a><br>
<a href="http://paperpile.com/b/rDmune/Ay2a">Huang, Y., Hsiang, E.-L., Deng, M.-Y., &amp; Wu, S.-T. (2020). Mini-LED, Micro-LED and OLED displays: present status and future perspectives. <em>Light, Science &amp; Applications</em>, <em>9</em>(1), 105.</a><br>
<a href="http://paperpile.com/b/rDmune/ipUJ">Jerald, J. (2015). <em>The VR book: Human-centered design for virtual reality</em>. ACM Books.</a> <a href="https://www.google.com/books/edition/The_VR_Book/ZEBiDwAAQBAJ?hl=en&amp;gbpv=1&amp;dq=Jason+Jerald&amp;pg=PR11&amp;printsec=frontcover">https://www.google.com/books/edition/The_VR_Book/ZEBiDwAAQBAJ?hl=en&amp;gbpv=1&amp;dq=Jason+Jerald&amp;pg=PR11&amp;printsec=frontcover</a><br>
<a href="http://paperpile.com/b/rDmune/K9T7">Jiang, H., Lin, J., Jin, S., &amp; Li, J. (2002). Micro-size LED and detector arrays for minidisplay, hyper-bright light emitting diodes, lighting, and UV detector and imaging sensor applications (USPTO Patent No.&nbsp;6410940). In <em>US Patent</em> (No.&nbsp;6410940).</a> <a href="https://patents.google.com/patent/US6410940B1/en">https://patents.google.com/patent/US6410940B1/en</a><br>
<a href="http://paperpile.com/b/rDmune/Fz8U">Kawamoto, H. (2002). The history of liquid-crystal displays. <em>Proceedings of the IEEE. Institute of Electrical and Electronics Engineers</em>, <em>90</em>(4), 460–500.</a><br>
<a href="http://paperpile.com/b/rDmune/pjWz">Kelley, E. F., Lang, K., Silverstein, L. D., &amp; Brill, M. H. (2009). <em>17.5: Projector Flux from Color Primaries</em>.</a> <a href="https://www.nist.gov/publications/projector-flux-color-primaries">https://www.nist.gov/publications/projector-flux-color-primaries</a><br>
<a href="http://paperpile.com/b/rDmune/ylUK">Kramida, G. (2016). Resolving the vergence-accommodation conflict in head-mounted displays. <em>IEEE Transactions on Visualization and Computer Graphics</em>, <em>22</em>, 1912–1931.</a><br>
<a href="http://paperpile.com/b/rDmune/Cc3X">Law, H. B. (1976). The shadow mask color picture tube: How it began—An eyewitness account of its early history. <em>IEEE Transactions on Electron Devices</em>, <em>23</em>(7), 752–759.</a><br>
<a href="http://paperpile.com/b/rDmune/Zk9Z">Lee, B.-W., Park, C., Kim, S., Jeon, M., Heo, J., Sagong, D., Kim, J., &amp; Souk, J. (2001). 51.2: Reducing gray‐level response to one frame: Dynamic Capacitance Compensation. <em>Digest of Technical Papers. SID International Symposium</em>, <em>32</em>(1), 1260–1263.</a><br>
<a href="http://paperpile.com/b/rDmune/woLm">Lee, S.-W., Kim, M., Souk, J. H., &amp; Kim, S. S. (2006). Motion artifact elimination technology for liquid‐crystal‐display monitors: Advanced dynamic capacitance compensation method. <em>Journal of the Society for Information Display</em>, <em>14</em>(4), 387–394.</a><br>
<a href="http://paperpile.com/b/rDmune/XFvh">Luo, Z., Xu, D., &amp; Wu, S.-T. (2014). Emerging Quantum-Dots-Enhanced LCDs. <em>Journal of Display Technology</em>, <em>10</em>(7), 526–539.</a><br>
<a href="http://paperpile.com/b/rDmune/QEri">Lyons, N. P., &amp; Farrell, J. E. (1989). Linear systems analysis of CRT displays. <em>SID Digest</em>, <em>10</em>, 220–223.</a><br>
<a href="http://paperpile.com/b/rDmune/fGqn">Marcato, T., Oh, J., Lin, Z.-H., Shivarudraiah, S. B., Kumar, S., Zeng, S., &amp; Shih, C.-J. (2024). Nanomolecular OLED Pixelization Enabling Electroluminescent Metasurfaces. In <em>arXiv [physics.optics]</em>. arXiv.</a> <a href="http://arxiv.org/abs/2404.05336">http://arxiv.org/abs/2404.05336</a><br>
<a href="http://paperpile.com/b/rDmune/b1Ta">Maxwell, J. C. (1860). IV. On the theory of compound colours, and the relations of the colours of the spectrum. <em>Philosophical Transactions of the Royal Society of London</em>, <em>150</em>(0), 57–84.</a><br>
<a href="http://paperpile.com/b/rDmune/J0ki">McCreary, J. L. (2014). Correction of TFT non-uniformity in AMOLED display (USPTO Patent No.&nbsp;8624805). In <em>US Patent</em> (No.&nbsp;8624805).</a> <a href="https://patents.google.com/patent/US8624805B2/en">https://patents.google.com/patent/US8624805B2/en</a><br>
<a href="http://paperpile.com/b/rDmune/i16o">MiniMicroLED, D. (2024, May 15). Highlights Of Micro LED Innovations At 2024 SID Display Week. <em>Mini/MicroLED Insights | Shaping the Future of Mini &amp; Micro LED Displays</em>.</a> <a href="https://www.minimicroled.com/2024-sid-display-week-highlights-of-micro-led-innovations-from-12-companies/">https://www.minimicroled.com/2024-sid-display-week-highlights-of-micro-led-innovations-from-12-companies/</a><br>
<a href="http://paperpile.com/b/rDmune/OZwX">Naiman, A. C., &amp; Makous, W. (1993). Undetected gray strips displace perceived edges nonlinearly. <em>Journal of the Optical Society of America. A, Optics and Image Science</em>, <em>10</em>(5), 794–803.</a><br>
<a href="http://paperpile.com/b/rDmune/sh8P">Packer, O., Diller, L. C., Verweij, J., Lee, B. B., Pokorny, J., Williams, D. R., Dacey, D. M., &amp; Brainard, D. H. (2001). Characterization and use of a digital light projector for vision research. <em>Vision Research</em>, <em>41</em>(4), 427–439.</a><br>
<a href="http://paperpile.com/b/rDmune/Qfii">Pelli, D. G. (1997). Pixel independence: measuring spatial interactions on a CRT display. <em>Spatial Vision</em>, <em>10</em>(4), 443–446.</a><br>
<a href="http://paperpile.com/b/rDmune/0yTJ">Post, D. L. (1992). Colorimetric measurement, calibration, and characterization of self-luminous displays. In <em>Color in Electronic Displays</em> (pp.&nbsp;299–312). Springer US.</a><br>
<a href="http://paperpile.com/b/rDmune/x1cy">Poynton, C. (1993). Gamma and its disguises : The nonlinear mappings of intensity in perception, CRTs, film, and video. <em>Smpte Journal</em>, <em>102</em>, 1099–1108.</a><br>
<a href="http://paperpile.com/b/rDmune/8fWH">Poynton, C., &amp; Funt, B. (2014). Perceptual uniformity in digital image representation and display. <em>Color Research and Application</em>, <em>39</em>(1), 6–15.</a><br>
<a href="http://paperpile.com/b/rDmune/WUgk">Rolland, J. P., &amp; Goodsell, J. (2024). Waveguide-based augmented reality displays: a highlight. <em>Light, Science &amp; Applications</em>, <em>13</em>(1), 22.</a><br>
<a href="http://paperpile.com/b/rDmune/Vla5">Seetzen, H., Heidrich, W., Stuerzlinger, W., Ward, G., Whitehead, L., Trentacoste, M., Ghosh, A., &amp; Vorozcovs, A. (2004). High dynamic range display systems. <em>ACM Transactions on Graphics</em>, <em>23</em>(3), 760–768.</a><br>
<a href="http://paperpile.com/b/rDmune/bmaJ">Silverstein, L.D., Fiske, T.G. (1993). Colorimetric and photometric modeling of liquid crystal displays. <em>Color and Imaging Conference. Vol. 1993. No.&nbsp;1</em>. Color and Imaging Conference., Scottsdale, AZ.</a> <a href="https://library.imaging.org/admin/apis/public/api/ist/website/downloadArticle/cic/1/1/art00038">https://library.imaging.org/admin/apis/public/api/ist/website/downloadArticle/cic/1/1/art00038</a><br>
<a href="http://paperpile.com/b/rDmune/gCx2">Singh, L., Dubey, R., &amp; Rai, R. N. (2023). <em>Organic light emitting diode (OLED) toward smart lighting and displays technologies: Material design strategies, challenges and future perspectives</em> (L. Singh, R. Dubey, &amp; R. N. Rai (eds.)). CRC Press.</a> <a href="https://books.google.com/books?hl=en&amp;lr=&amp;id=TqMIEQAAQBAJ&amp;oi=fnd&amp;pg=PP1&amp;dq=Organic+Light+Emitting+Diode+(OLED)+Toward+Smart+Lighting+and+Displays+Technologies&amp;ots=44pGRTvA2V&amp;sig=80HUNiFW_xl0giEpVt2YdscHB30">https://books.google.com/books?hl=en&amp;lr=&amp;id=TqMIEQAAQBAJ&amp;oi=fnd&amp;pg=PP1&amp;dq=Organic+Light+Emitting+Diode+(OLED)+Toward+Smart+Lighting+and+Displays+Technologies&amp;ots=44pGRTvA2V&amp;sig=80HUNiFW_xl0giEpVt2YdscHB30</a><br>
<a href="http://paperpile.com/b/rDmune/tvR7">Smith, J. M., Ley, R., Wong, M. S., Baek, Y. H., Kang, J. H., Kim, C. H., Gordon, M. J., Nakamura, S., Speck, J. S., &amp; DenBaars, S. P. (2020). Comparison of size-dependent characteristics of blue and green InGaN microLEDs down to 1 <em>μ</em> m in diameter. <em>Applied Physics Letters</em>, <em>116</em>(7), 071102.</a><br>
<a href="http://paperpile.com/b/rDmune/L5vo">Stevens, S. S. (1957). On the psychophysical law. <em>Psychology Review</em>, <em>64</em>(3), 153–181.</a><br>
<a href="http://paperpile.com/b/rDmune/ENGQ">Su, R., Park, S. H., Ouyang, X., Ahn, S. I., &amp; McAlpine, M. C. (2022). 3D-printed flexible organic light-emitting diode displays. <em>Science Advances</em>, <em>8</em>(1), eabl8798.</a><br>
<a href="http://paperpile.com/b/rDmune/Vf1l">Tang, C., &amp; Vanslyke, S. (1987). Organic Electroluminescent Diodes. <em>Applied Physics Letters</em>, <em>51</em>, 913–915.</a><br>
<a href="http://paperpile.com/b/rDmune/w7FM">Toscani, M., Gil, R., Guarnera, D., Guarnera, G., Kalouaz, A., &amp; Gegenfurtner, K. R. (2019). Assessment of OLED head mounted display for vision research with virtual reality. <em>2019 15th International Conference on Signal-Image Technology &amp; Internet-Based Systems (SITIS)</em>, 738–745.</a><br>
<a href="http://paperpile.com/b/rDmune/sI2C">Travis, D. (n.d.). <em>ISO 9241: Part 3</em>. Retrieved December 30, 2024, from</a> <a href="https://www.userfocus.co.uk/resources/iso9241/part3.html?t">https://www.userfocus.co.uk/resources/iso9241/part3.html?t</a><br>
<a href="http://paperpile.com/b/rDmune/eUhb">Tsujimura, T. (2017). <em>OLED Display Fundamentals and Applications</em> (2nd ed.) [PDF]. John Wiley &amp; Sons.</a><br>
<a href="http://paperpile.com/b/rDmune/2zzl">Wandell, B. A. (1995). <em>Foundations of vision: Behaviour, neuroscience, and computation</em>. Sinauer Associates.</a><br>
<a href="http://paperpile.com/b/rDmune/PVJo">Wandell, B. A., &amp; Silverstein, L. D. (2003). Digital Color Reproduction. In <em>The Science of Color</em> (Vol. 2, pp.&nbsp;281–316). Elsevier.</a><br>
<a href="http://paperpile.com/b/rDmune/CnGQ">Wang, T., Yang, C., Chen, J., Zhao, Y., &amp; Zong, J. (2024). Naked-eye light field display technology based on mini/micro light emitting diode panels: a systematic review and meta-analysis. <em>Scientific Reports</em>, <em>14</em>(1), 24381.</a><br>
<a href="http://paperpile.com/b/rDmune/hv8D">Watson, A. B., J., A. J. A., &amp; Farrell, J. E. (1986). Window of visibility: a psychophysical theory of fidelity in time-sampled visual motion displays. <em>Journal of the Optical Society of America A</em>, <em>3</em>(3), 300–307.</a><br>
<a href="http://paperpile.com/b/rDmune/E40X">Wyszecki, G., &amp; Stiles, W. S. (1982). <em>Color science</em> (Vol. 8). Wiley New York.</a><br>
<a href="http://paperpile.com/b/rDmune/BBSB">Xu, B., Zhou, J., Zhang, C., Chang, Y., &amp; Deng, Z. (2025). Research progress on quantum dot-embedded polymer films and plates for LCD backlight display. <em>Polymers</em>, <em>17</em>(2), 233.</a><br>
<a href="http://paperpile.com/b/rDmune/R0sH">Yakovlev, D. A., Chigrinov, V. G., &amp; Kwok, H.-S. (2015). <em>Modeling and optimization of LCD optical performance: Yakovlev/modeling</em> (D. A. Yakovlev, V. G. Chigrinov, &amp; H.-S. Kwok (eds.); 1st ed.) [PDF]. Wiley-Blackwell.</a><br>
<a href="http://paperpile.com/b/rDmune/XJuv">Yang, D.-K., &amp; Wu, S.-T. (2014). <em>Fundamentals of liquid crystal devices</em> (2nd ed.) [EPUB]. Wiley-Blackwell.</a><br>
<a href="http://paperpile.com/b/rDmune/9Bo1">Younse, J. (1993). Mirrors on a chip. <em>IEEE Spectrum</em>, <em>30</em>, 27–31.</a><br>
<a href="http://paperpile.com/b/rDmune/Ids2">Zaman, N., Sarker, P., &amp; Tavakkoli, A. (2023). Calibration of head mounted displays for vision research with virtual reality. <em>Journal of Vision</em>, <em>23</em>. https://doi.org/</a><a href="http://dx.doi.org/10.1167/jov.23.6.7">10.1167/jov.23.6.7</a><br>
<a href="http://paperpile.com/b/rDmune/ayA6">Zhang, X., &amp; Farrell, J. E. (2003). Sequential color breakup measured with induced saccades. In B. E. Rogowitz &amp; T. N. Pappas (Eds.), <em>Human Vision and Electronic Imaging VIII</em>. SPIE. https://doi.org/</a><a href="http://dx.doi.org/10.1117/12.497843">10.1117/12.497843</a><br>
<a href="http://paperpile.com/b/rDmune/zQ8e">Zhang, X., &amp; Wandell, B. A. (1997). A spatial extension of CIELAB for digital color‐image reproduction. <em>Journal of the Society for Information Display</em>, <em>5</em>(1), 61–63.</a><br>
<a href="http://paperpile.com/b/rDmune/jlep">Zhang, Y., Song, W., &amp; Teunissen, K. (2007). A tradeoff between motion blur and flicker visibility of electronic display devices. In L. Zhou (Ed.), <em>International Symposium on Photoelectronic Detection and Imaging 2007: Related Technologies and Applications</em>. SPIE. https://doi.org/</a><a href="http://dx.doi.org/10.1117/12.790748">10.1117/12.790748</a></p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>https://github.com/isetbio/isetbio/wiki<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>https://github.com/iset/isetcam/wiki<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>https://www.apple.com/apple-vision-pro/specs/<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>