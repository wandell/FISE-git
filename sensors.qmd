# Sensors

## Historical sensor types

Film was the first major commercial technology for capturing and storing image irradiance, and it relied heavily on chemistry. Light-sensitive molecules were coated onto flexible rolls of plastic, which were then loaded into a camera.

![Film was the dominant image sensor and recording material for a hundred years. The chemistry of film for both encoding and developing into images was the core scientific discipline. Film could not be reused, producing a steady commercial profit for large companies, such as Kodak, Fuji, Polaroid and Agfa.](images/sensors/film.png){#fig-film width="60%"}

Inside the camera, the film was mechanically advanced so that a small, flat section would be positioned at the focal plane of the lens. This section of the film was exposed to the focused light (electromagnetic radiation) that passed through the cameraâ€™s optics. The light altered the molecular structure of the film, with more molecules being changed in areas of higher light intensity and fewer in areas of lower intensity. These molecular changes represented the local brightness of the captured image. Using sophisticated chemical processes, different light-sensitive molecules could be designed to respond to specific wavelengths of light, selectively ignoring others. After exposure, additional chemical treatments developed the film and transformed it into a printed image. This process was resource-intensive, requiring a variety of chemical consumables, as the film could not be reused, and specialized labs were needed for both development and printing.

A completely different approach to capturing radiance emerged with the discovery by Willard S. Boyle and George E. Smith at Bell Labs that semiconductor technology could also be used to record irradiance. Light striking silicon releases electrons (photoelectric effect), and the spatial pattern of these electrons could be stored and measured. This breakthrough was first implemented in 1970 by Michael Tompsett, who developed the image sensor known as the charge-coupled device (CCD).

The semiconductor based sensor revolutionized imaging, moving it from a field based on chemistry to electrical engineering. This method required no consumables (chemicals). Importantly, the digital readout from these electrical devices could be immediately integrated with computers. Images could be stored, shared, edited and analyzed directly from the camera output. The step from the CCD sensor to the CMOS sensor was another major advance that unleashed a massive adoption of electronic image sensors around the world.

![Image sensors include many different subsystems and features.](images/sensors/olympusCMOSSensorImage.png){#fig-olympus-overview style=".float-right" width="489"}

This chapter is about the properties of the CMOS image sensors used to measure radiance. In later sections we will also describe how semiconductor sensor can be used to estimate depth, as well. Because there are significant differences in these technologies and the information they acquire, we treat them in separate sections.

## Radiance sensors: CMOS imagers

A typical CMOS image sensor, just on its own, is a complex system. The star actor of the image sensor is the photodiode array. The photodiodes convert the irradiance at the sensor into an array of electrons that are stored in local capacitors. The actions of these photodiodes are controlled by local circuitry built on the sensor. This circuitry manages the timing of the acquisition and the transmission of the electrons to the analog to digital converters that deliver the sensor output. For many years, the digital read out from the sensor was a one-to-one match with the signals measured at individual photodiodes.

Over time, as technology has scaled, more complex electrical circuitry could be fit onto the sensor. In modern CMOS image sensors the circuitry can perform various types of local processing, and the output is no longer a one-to-one match with the number of electrons at each photodiode. The processing is adaptive, so that the same sensor can produce different types of outputs depending upon control parameters delivered to the sensor from other image system components.

There are also other non-electrical components of most image sensors. These are small lens (lenslet) arrays and small color filter arrays that influence which part of the irradiance is capable of producing electrons from each photodiode.

The sensor records the light that exits from the back of the imaging optics. The photodiodes are small and they are located at the bottom of small holes in the sensor. It is necessary to guide the irradiance from the camera lens with an array of microlenses that account for the position of the pixel with respect to the center of the lens. The original role of the microlens array was to compensate for the challenges inherent in this geometry. Overtime, the microlens array has been designed to achieve additional objectives including determining how to focus the image and estimate the scene light field.

To make a color image, the sensor must measure some information about the relative intensity of different wavelengths. To make this measurement small color filters are placed in front of each pixel, so that different pixels are sensitive to different wavelengths. These spectral encoding differences enable the system to reconstruct a color image that is convincing to the human visual system.

### Silicon detectors

Photoelectric effect is linear

Wavelength dependency

Poisson statistics

### Color filter arrays

### Microlens arrays