
@book{knill_perception_1996,
	address = {Cambridge},
	title = {Perception as {Bayesian} {Inference}},
	publisher = {Cambridge University Press},
	author = {Knill, D. C. and Richards, W.},
	year = {1996},
}

@book{helmholtz_physiological_1896,
	address = {New York},
	title = {Physiological {Optics}},
	publisher = {Dover Publications, Inc.},
	author = {Helmholtz, H.},
	year = {1896},
	annote = {Translated by J. P. C. Southall},
}

@article{manning_optimal_2009,
	title = {Optimal design of photoreceptor mosaics: {Why} we do not see color at night},
	volume = {26},
	number = {1},
	journal = {Visual Neuroscience},
	author = {Manning, J. R. and Brainard, D. H.},
	year = {2009},
	pages = {5--19},
}

@book{bishop_pattern_2006,
	address = {New York},
	title = {Pattern {Recognition} and {Machine} {Learning}},
	publisher = {Springer Science + Business Media LLC},
	author = {Bishop, C. M.},
	year = {2006},
}

@book{duda_pattern_2001,
	address = {New York},
	title = {Pattern classification and scene analysis (2nd {Edition})},
	publisher = {John Wiley \& Sons},
	author = {Duda, R.O. and Hart, P.E. and Stork, D.G.},
	year = {2001},
}

@book{hecht_optics_2017,
	address = {Boston},
	edition = {5th},
	title = {Optics},
	publisher = {Pearson},
	author = {Hecht, E.},
	year = {2017},
}

@article{young_theory_1802,
	title = {On the theory of light and colours},
	volume = {92},
	journal = {Philosophical Transactions of the Royal Society of London},
	author = {Young, T.},
	year = {1802},
	pages = {20--71},
}

@article{maxwell_theory_1860,
	title = {On the theory of compound colours and the relations of the colours of the spectrum},
	volume = {150},
	journal = {Philosophical Transactions of the Royal Society of London},
	author = {Maxwell, J.C.},
	year = {1860},
	pages = {57--84},
}

@article{jaeken_optical_2012,
	title = {Optical quality of emmetropic and myopic eyes in the periphery measured with high-angular resolution},
	volume = {53},
	number = {7},
	journal = {Investigative Ophthalmology and Visual Science},
	author = {Jaeken, Bart and Artal, Pablo},
	year = {2012},
	pages = {3405--3413},
}

@article{van_gelder_ocular_2016,
	title = {Ocular {Photoreception} for {Circadian} {Rhythm} {Entrainment} in {Mammals}},
	volume = {2},
	journal = {Annual Review of Vision Science},
	author = {Van Gelder, Russell N and Buhr, Ethan D},
	year = {2016},
}

@article{cox_neural_2014,
	title = {Neural networks and neuroscience-inspired computer vision},
	volume = {24},
	number = {18},
	journal = {Current Biology},
	author = {Cox, David Daniel and Dean, Thomas},
	month = sep,
	year = {2014},
	pages = {R921--R929},
}

@article{stiles_npl_1959,
	title = {{NPL} colour-matching investigation: {Final} report (1958)},
	volume = {6},
	journal = {Optica Acta},
	author = {Stiles, W.S. and Burch, J.M.},
	year = {1959},
	pages = {1--26},
}

@article{wei_neural_2018,
	title = {Neural mechanisms of motion processing in the mammalian retina},
	volume = {4},
	journal = {Annual Review of Vision Science},
	author = {Wei, Wei},
	year = {2018},
	note = {Publisher: Annual Reviews},
	pages = {165--192},
}

@article{tkacik_natural_2011,
	title = {Natural images from the birthplace of the human eye},
	volume = {6},
	number = {6:e20409},
	journal = {PLoS ONE},
	author = {Tkacik, G. and Garrigan, P. and Ratliff, C. and Milcinski, G. and Klein, J. M. and Sterling, P. and Brainard, D. H. and Balasubramanian, V.},
	year = {2011},
}

@article{mergenthaler_modeling_2007,
	title = {Modeling the control of fixational eye movements with neurophysiological delays},
	volume = {98},
	number = {13},
	journal = {Physical Review Letters},
	author = {Mergenthaler, K. and Engbert, R.},
	year = {2007},
	pages = {138104},
}

@book{cox_multidimensional_2001,
	address = {Boca Raton, FL},
	edition = {2},
	title = {Multidimensional {Scaling}},
	publisher = {Chapman \& Hall/CRC},
	author = {Cox, T. F. and Cox, M. A. A.},
	year = {2001},
}

@book{knoblauch_modeling_2012,
	address = {New York},
	title = {Modeling {Psychophysical} {Data} in {R} ({Use} {R}!)},
	publisher = {Springer},
	author = {Knoblauch, K. and Maloney, L. T.},
	year = {2012},
}

@article{balasubramanian_metabolically_2001,
	title = {Metabolically efficient information processing},
	volume = {13},
	number = {4},
	journal = {Neural Computation},
	author = {Balasubramanian, V. and Kimber, D. and Berry, M. J., 2nd},
	year = {2001},
	pages = {799--815},
}

@article{hattar_melanopsin-containing_2002,
	title = {Melanopsin-containing retinal ganglion cells: {Architecture}, projections, and intrinsic photosensitivity},
	volume = {295},
	number = {5557},
	journal = {Science},
	author = {Hattar, S. and Liao, H. W. and Takao, M. and Berson, D. M. and Yau, K. W.},
	year = {2002},
	pages = {1065--1070},
}

@article{vrhel_measurement_1994,
	title = {Measurement and analysis of object reflectance spectra},
	volume = {19},
	number = {1},
	journal = {Color Research And Application},
	author = {Vrhel, M.J. and Gershon, R. and Iwan, L.S.},
	year = {1994},
	pages = {4--9},
}

@article{priebe_mechanisms_2016,
	title = {Mechanisms of orientation selectivity in the primary visual cortex},
	volume = {2},
	journal = {Annual Review of Vision Science},
	author = {Priebe, Nicholas J},
	year = {2016},
	note = {Publisher: Annual Reviews},
	pages = {85--107},
}

@article{maloney_maximum_2003,
	title = {Maximum likelihood difference scaling},
	volume = {3},
	number = {8},
	journal = {Journal of Vision},
	author = {Maloney, L. T. and Yang, J. N.},
	year = {2003},
	pages = {573--585},
}

@article{marimont_matching_1993,
	title = {Matching color images: {The} effects of axial chromatic aberration},
	volume = {12},
	journal = {J. Opt. Soc. Am. A},
	author = {Marimont, D. and Wandell, B.},
	year = {1993},
}

@misc{wikipedia_contributors_lytro_2021,
	title = {Lytro},
	url = {https://en.wikipedia.org/w/index.php?title=Lytro&oldid=1032099081},
	author = {{Wikipedia contributors}},
	month = jul,
	year = {2021},
	note = {Publication Title: Wikipedia, The Free Encyclopedia},
	annote = {Accessed: 2021-7-8},
	annote = {Accessed: 2021-7-8},
	annote = {Accessed: 2021-7-8},
}

@article{whitehead_macular_2006,
	title = {Macular pigment. {A} review of current knowledge},
	volume = {124},
	number = {7},
	journal = {Archives of Ophthalmology},
	author = {Whitehead, A.J. and Mares, J.A. and Danis, R.P.},
	year = {2006},
	pages = {1038--1045},
}

@misc{ayscough_light_1755,
	title = {Light field image},
	url = {https://commons.wikimedia.org/wiki/File:1755_james_ayscough.jpg},
	author = {Ayscough, James},
	year = {1755},
	note = {Publication Title: Wikipedia},
	annote = {Accessed: 2021-7-24},
	annote = {Accessed: 2021-7-24},
	annote = {Accessed: 2021-7-24},
}

@phdthesis{ng_light_2005,
	type = {{PhD} {Thesis}},
	title = {Light field photography with a hand-held plenoptic camera},
	school = {Stanford university},
	author = {Ng, Ren and Levoy, Marc and Brédif, Mathieu and Duval, Gene and Horowitz, Mark and Hanrahan, Pat},
	month = apr,
	year = {2005},
}

@incollection{packer_light_2003,
	address = {Oxford},
	edition = {2},
	title = {Light, the retinal image, and photoreceptors},
	booktitle = {The {Science} of {Color}},
	publisher = {Optical Society of America; Elsevier Ltd},
	author = {Packer, O. and Williams, D. R.},
	editor = {Shevell, S. K.},
	year = {2003},
	pages = {41--102},
}

@article{pasternak_linking_2020,
	title = {Linking neuronal direction selectivity to perceptual decisions about visual motion},
	volume = {6},
	journal = {Annual Review of Vision Science},
	author = {Pasternak, Tatiana and Tadin, Duje},
	year = {2020},
	note = {Publisher: Annual Reviews},
	pages = {335--362},
}

@patent{mlinar_image_2016,
	title = {Image processing methods for image sensors with phase detection pixels},
	number = {9338380},
	author = {Mlinar, Marko},
	month = may,
	year = {2016},
}

@book{manning_introduction_2008,
	address = {Cambridge},
	title = {Introduction to {Information} {Retrieval}},
	publisher = {Cambridge University Press},
	author = {Manning, C. D. and Raghavean, P. and Schutze, H.},
	year = {2008},
}

@book{scholkopf_learning_2002,
	title = {Learning with kernels: {Support} vector machines, regularization, optimization, and beyond},
	publisher = {MIT press},
	author = {Schölkopf, Bernhard and Smola, Alexander J and Bach, Francis and {others}},
	year = {2002},
}

@misc{canon_usa__inc_introduction_2017,
	title = {Introduction to {Dual} {Pixel} {Autofocus}},
	url = {https://www.usa.canon.com/internet/portal/us/home/learn/education/topics/article/2018/July/Intro-to-Dual-Pixel-Autofocus-(DPAF)/Intro-to-Dual-Pixel-Autofocus-(DPAF)},
	author = {{Canon U.S.A. , Inc}},
	month = apr,
	year = {2017},
	annote = {Accessed: 2021-7-8},
	annote = {Accessed: 2021-7-8},
	annote = {Accessed: 2021-7-8},
}

@article{burge_image-computable_2020,
	title = {Image-{Computable} {Ideal} {Observers} for {Tasks} with {Natural} {Stimuli}},
	volume = {6},
	journal = {Annual Review of Vision Science},
	author = {Burge, J.},
	year = {2020},
	pages = {491--517},
}

@book{cahan_hermann_1993,
	title = {Hermann {Von} {Helmholtz} and the {Foundations} of {Nineteenth}-{Century} {Science}},
	publisher = {University of California Press},
	author = {Cahan, David},
	year = {1993},
}

@book{helmholtz_handbuch_1866,
	address = {Hamburg},
	title = {Handbuch der physiologischen {Optik} {II} (3rd {Edition}, 1911) (pp. 243–244)},
	publisher = {Voss},
	author = {Helmholtz, H.},
	year = {1866},
}

@article{curcio_human_1990,
	title = {Human photoreceptor topography},
	volume = {292},
	number = {4},
	journal = {Journal of Comparative Neurology},
	author = {Curcio, C.A. and Sloan, K.R. and Kalina, R.E. and Hendrickson, A.E.},
	year = {1990},
	pages = {497--523},
}

@article{gamlin_human_2007,
	title = {Human and macaque pupil responses driven by melanopsin-containing retinal ganglion cells},
	volume = {47},
	number = {7},
	journal = {Vision Research},
	author = {Gamlin, P. D. R. and McDougal, D. H. and Pokorny, J. and Smith, V. C. and Yau, K. W. and Dacey, D. M.},
	year = {2007},
	pages = {946--954},
}

@article{williams_foveal_1981,
	title = {Foveal tritanopia},
	volume = {19},
	number = {9},
	journal = {Vision Research},
	author = {Williams, D.R. and MacLeod, D.I.A. and Hayhoe, M.M.},
	year = {1981},
	pages = {1341--1356},
}

@book{cie_fundamental_2007,
	address = {Vienna},
	title = {Fundamental chromaticity diagram with physiological axes – {Parts} 1 and 2. {Technical} {Report} 170-1},
	publisher = {Central Bureau of the Commission Internationale de l' Éclairage},
	author = {{CIE}},
	year = {2007},
}

@book{rasmussen_gaussian_2006,
	address = {Cambridge, MA},
	title = {Gaussian {Processes} for {Machine} {Learning}},
	publisher = {MIT Press},
	author = {Rasmussen, C. E. and Williams, C. K. I.},
	year = {2006},
}

@book{wandell_foundations_1995,
	address = {Sunderland, MA},
	title = {Foundations of {Vision}},
	publisher = {Sinauer},
	author = {Wandell, B. A.},
	year = {1995},
}

@article{laughlin_energy_2001,
	title = {Energy as a constraint on the coding and processing of sensory information},
	volume = {11},
	number = {4},
	journal = {Current Opinion in Neurobiology},
	author = {Laughlin, S. B.},
	year = {2001},
	pages = {475--480},
}

@article{wyszecki_evaluation_1958,
	title = {Evaluation of metameric colors},
	volume = {48},
	number = {7},
	journal = {Journal of the Optical Society of America},
	author = {Wyszecki, G.},
	year = {1958},
	pages = {451--454},
}

@article{maloney_evaluation_1986,
	title = {Evaluation of linear models of surface spectral reflectance with small numbers of parameters},
	volume = {3},
	number = {10},
	journal = {Journal of The Optical Society of America A},
	author = {Maloney, L. T.},
	year = {1986},
	pages = {1673--1683},
}

@article{hecht_energy_1942,
	title = {Energy, quanta and vision},
	volume = {38},
	number = {6},
	journal = {Journal of the Optical Society of America},
	author = {Hecht, S. and Schlaer, S. and Pirenne, M.H.},
	year = {1942},
	pages = {196--208},
}

@article{koch_efficiency_2004,
	title = {Efficiency of information transmission by retinal ganglion cells},
	volume = {14},
	number = {17},
	journal = {Current Biology},
	author = {Koch, K. and McLean, J. and Berry, M. and Sterling, P. and Balasubramanian, V. and Freed, M. A.},
	year = {2004},
	pages = {1523--1530},
}

@book{pratt_digital_1978,
	address = {New York},
	title = {Digital {Image} {Processing}},
	publisher = {John Wiley \& Sons},
	author = {Pratt, W. K.},
	year = {1978},
}

@book{cover_elements_1991,
	address = {New York},
	title = {Elements of {Information} {Theory}},
	publisher = {John Wiley and Sons},
	author = {Cover, T. M. and Thomas, J. A.},
	year = {1991},
}

@article{garrigan_design_2010,
	title = {Design of a trichromatic cone array},
	volume = {6},
	number = {2},
	journal = {PLoS Computational Biology},
	author = {Garrigan, P. and Ratliff, C. P. and Klein, J. M. and Sterling, P. and Brainard, D. H. and Balasubramanian, V.},
	year = {2010},
	pages = {e1000677},
}

@article{li_unconventional_2022,
	title = {Unconventional {Visual} {Sensors} for {Autonomous} {Vehicles}},
	abstract = {Autonomous vehicles rely on perception systems to understand their surroundings for further navigation missions. Cameras are essential for perception systems due to the advantages of object detection and recognition provided by modern computer vision algorithms, comparing to other sensors, such as LiDARs and radars. However, limited by its inherent imaging principle, a standard RGB camera may perform poorly in a variety of adverse scenarios, including but not limited to: low illumination, high contrast, bad weather such as fog/rain/snow, etc. Meanwhile, estimating the 3D information from the 2D image detection is generally more difficult when compared to LiDARs or radars. Several new sensing technologies have emerged in recent years to address the limitations of conventional RGB cameras. In this paper, we review the principles of four novel image sensors: infrared cameras, range-gated cameras, polarization cameras, and event cameras. Their comparative advantages, existing or potential applications, and corresponding data processing algorithms are all presented in a systematic manner. We expect that this study will assist practitioners in the autonomous driving society with new perspectives and insights.},
	author = {Li, You and Moreau, Julien and Ibanez-Guzman, Javier},
	month = may,
	year = {2022},
	note = {\_eprint: 2205.09383},
}

@misc{tapia_lica-ucm_nodate,
	title = {{LICA}-{UCM} lamps spectral database},
	url = {https://eprints.ucm.es/id/eprint/40930/1/LICA_Spectra_database_v2_6.pdf},
	abstract = {Spectra of the lamps that are used for public lighting and ornamental purposes have been obtained with a portable spectrograph around Madrid city. The database is presented in this report along with a description of the procedures.},
	author = {Tapia, Carlos and de Miguel, Alejandro Sánchez and Zamorano, Jaime},
	annote = {Accessed: 2022-6-23},
}

@article{dai_model_nodate,
	title = {Model adaptation with synthetic and real data for semantic dense foggy scene understanding},
	journal = {Proc. Estonian Acad. Sci. Biol. Ecol.},
	author = {{Dai} and {Hecker} and {Van Gool}},
}

@article{yu_bdd100k_2018,
	title = {{BDD100K}: {A} {Diverse} {Driving} {Video} {Database} with {Scalable} {Annotation} {Tooling}},
	abstract = {Datasets drive vision progress and autonomous driving is a critical vision application, yet existing driving datasets are impoverished in terms of visual content. Driving imagery is becoming plentiful, but annotation is slow and expensive, as annotation tools have not kept pace with the flood of data. Our first contribution is the design and implementation of a scalable annotation system that can provide a comprehensive set of image labels for large-scale driving datasets. Our second contribution is a new driving dataset, facilitated by our tooling, which is an order of magnitude larger than previous efforts, and is comprised of over 100K videos with diverse kinds of annotations including image level tagging, object bounding boxes, drivable areas, lane markings, and full-frame instance segmentation. The dataset possesses geographic, environmental, and weather diversity, which is useful for training models so that they are less likely to be surprised by new conditions. The dataset can be requested at http://bdd-data.berkeley.edu.},
	journal = {arXiv [cs.CV]},
	author = {Yu, Fisher and Xian, Wenqi and Chen, Yingying and Liu, Fangchen and Liao, Mike and Madhavan, Vashisht and Darrell, Trevor},
	month = may,
	year = {2018},
	keywords = {NewAccount},
}

@misc{kinzey_investigation_nodate,
	title = {An investigation of {LED} street lighting's impact on sky glow},
	url = {https://www.energy.gov/sites/prod/files/2017/05/f34/2017_led-impact-sky-glow.pdf},
	author = {Kinzey, Bruce and Perrin, Tess and Miller, Naomi J. and Kocifaj, Miroslav and Aubé, Martin and Lamphar, Héctor S.},
	annote = {Accessed: 2022-6-26},
}

@article{kim_convolutional_2017,
	title = {Convolutional {Neural} {Network}-{Based} {Human} {Detection} in {Nighttime} {Images} {Using} {Visible} {Light} {Camera} {Sensors}},
	volume = {17},
	abstract = {Because intelligent surveillance systems have recently undergone rapid growth, research on accurately detecting humans in videos captured at a long distance is growing in importance. The existing research using visible light cameras has mainly focused on methods of human detection for daytime hours when there is outside light, but human detection during nighttime hours when there is no outside light is difficult. Thus, methods that employ additional near-infrared (NIR) illuminators and NIR cameras or thermal cameras have been used. However, in the case of NIR illuminators, there are limitations in terms of the illumination angle and distance. There are also difficulties because the illuminator power must be adaptively adjusted depending on whether the object is close or far away. In the case of thermal cameras, their cost is still high, which makes it difficult to install and use them in a variety of places. Because of this, research has been conducted on nighttime human detection using visible light cameras, but this has focused on objects at a short distance in an indoor environment or the use of video-based methods to capture multiple images and process them, which causes problems related to the increase in the processing time. To resolve these problems, this paper presents a method that uses a single image captured at night on a visible light camera to detect humans in a variety of environments based on a convolutional neural network. Experimental results using a self-constructed Dongguk night-time human detection database (DNHD-DB1) and two open databases (Korea advanced institute of science and technology (KAIST) and computer vision center (CVC) databases), as well as high-accuracy human detection in a variety of environments, show that the method has excellent performance compared to existing methods.},
	language = {en},
	number = {5},
	journal = {Sensors},
	author = {Kim, Jong Hyun and Hong, Hyung Gil and Park, Kang Ryoung},
	month = may,
	year = {2017},
	keywords = {convolutional neural network, intelligent surveillance system, nighttime human detection, visible light image},
}

@article{wu_how_2020,
	title = {How to {Train} {Neural} {Networks} for {Flare} {Removal}},
	journal = {arXiv:2011.12485},
	author = {Wu, Yicheng and He, Qiurui and Xue, Tianfan and Garg, Rahul and Chen, Jiawen and Veeraraghavan, Ashok and Barron, Jonathan T},
	month = nov,
	year = {2020},
	note = {\_eprint: 2011.12485},
}

@inproceedings{chen_learning_2018,
	title = {Learning to see in the dark},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Chen, Chen and Chen, Qifeng and Xu, Jia and Koltun, Vladlen},
	year = {2018},
	pages = {3291--3300},
}

@article{dai_curriculum_2020,
	title = {Curriculum model adaptation with synthetic and real data for semantic foggy scene understanding},
	volume = {128},
	language = {en},
	number = {5},
	journal = {Int. J. Comput. Vis.},
	author = {Dai, Dengxin and Sakaridis, Christos and Hecker, Simon and Van Gool, Luc},
	month = may,
	year = {2020},
	note = {Publisher: Springer Science and Business Media LLC},
	pages = {1182--1204},
}

@misc{noauthor_ieee_2023,
	title = {{IEEE} {Draft} {Standard} for {Automotive} {System} {Image} {Quality}},
	url = {https://www.techstreet.com/ieee/standards/ieee-p2020?gateway_code=ieee&vendor_id=6765&product_id=2505612},
	language = {en},
	year = {2023},
	annote = {Accessed: 2023-2-12},
}

@article{zapata_artificial_2019,
	title = {Artificial {Lighting} at {Night} in {Estuaries}—{Implications} from {Individuals} to {Ecosystems}},
	volume = {42},
	abstract = {Artificial lighting at night (ALAN) produced by urban, industrial, and roadway lighting, as well as other sources, has dramatically increased in recent decades, especially in coastal environments that support dense human populations. Artificial “lightscapes” are characterized by distinct spatial, temporal, and spectral patterns that can alter natural patterns of light and dark with consequences across levels of biological organization. At the individual level, ALAN can elicit a suite of physiological and behavioral responses associated with light-mediated processes such as diel activity patterns and predator-prey interactions. ALAN has also been shown to modify community composition and trophic structure, with implications for ecosystem-level processes including primary productivity, nutrient cycling, and the energetic linkages between aquatic and terrestrial systems. Here, we review the state of the science relative to the impacts of ALAN on estuaries, which is an important step in assessing the long-term sustainability of coastal regions. We first consider how multiple properties of ALAN (e.g., intensity and spectral content) influence the interaction between physiology and behavior of individual estuarine biota (drawing from studies on invertebrates, fishes, and birds). Second, we link individual- to community- and ecosystem-level responses, with a focus on the impacts of ALAN on food webs and implications for estuarine ecosystem functions. Coastal aquatic communities and ecosystems have been identified as a key priority for ALAN research, and a cohesive research framework will be critical for understanding and mitigating ecological consequences.},
	number = {2},
	journal = {Estuaries Coasts},
	author = {Zapata, Martha J and Sullivan, S Mažeika P and Gray, Suzanne M},
	month = mar,
	year = {2019},
	pages = {309--330},
}

@article{wu_immvp_2019,
	title = {{IMMVP}: {An} {Efficient} {Daytime} and {Nighttime} {On}-{Road} {Object} {Detector}},
	url = {http://arxiv.org/abs/1910.06573},
	journal = {arXiv:1910.06573},
	author = {Wu, Cheng-En and Chan, Yi-Ming and Chen, Chien-Hung and Chen, Wen-Cheng and Chen, Chu-Song},
	month = oct,
	year = {2019},
}

@article{choi_use_2021,
	title = {On the use of simulation in robotics: {Opportunities}, challenges, and suggestions for moving forward},
	volume = {118},
	url = {http://dx.doi.org/10.1073/pnas.1907856118},
	language = {en},
	number = {1},
	journal = {Proc. Natl. Acad. Sci. U. S. A.},
	author = {Choi, Heesun and Crump, Cindy and Duriez, Christian and Elmquist, Asher and Hager, Gregory and Han, David and Hearl, Frank and Hodgins, Jessica and Jain, Abhinandan and Leve, Frederick and Li, Chen and Meier, Franziska and Negrut, Dan and Righetti, Ludovic and Rodriguez, Alberto and Tan, Jie and Trinkle, Jeff},
	month = jan,
	year = {2021},
	keywords = {controls, design, machine learning, robotics, simulation},
}

@article{kumar_lidar_2020,
	title = {{LiDAR} and {Camera} {Fusion} {Approach} for {Object} {Distance} {Estimation} in {Self}-{Driving} {Vehicles}},
	volume = {12},
	abstract = {The fusion of light detection and ranging (LiDAR) and camera data in real-time is known to be a crucial process in many applications, such as in autonomous driving, industrial automation, and robotics. Especially in the case of autonomous vehicles, the efficient fusion of data from these two types of sensors is important to enabling the depth of objects as well as the detection of objects at short and long distances. As both the sensors are capable of capturing the different attributes of the environment simultaneously, the integration of those attributes with an efficient fusion approach greatly benefits the reliable and consistent perception of the environment. This paper presents a method to estimate the distance (depth) between a self-driving car and other vehicles, objects, and signboards on its path using the accurate fusion approach. Based on the geometrical transformation and projection, low-level sensor fusion was performed between a camera and LiDAR using a 3D marker. Further, the fusion information is utilized to estimate the distance of objects detected by the RefineDet detector. Finally, the accuracy and performance of the sensor fusion and distance estimation approach were evaluated in terms of quantitative and qualitative analysis by considering real road and simulation environment scenarios. Thus the proposed low-level sensor fusion, based on the computational geometric transformation and projection for object distance estimation proves to be a promising solution for enabling reliable and consistent environment perception ability for autonomous vehicles.},
	language = {en},
	number = {2},
	journal = {Symmetry},
	author = {Kumar, G Ajay and Lee, Jin Hee and Hwang, Jongrak and Park, Jaehyeong and Youn, Sung Hoon and Kwon, Soon},
	month = feb,
	year = {2020},
	note = {Publisher: Multidisciplinary Digital Publishing Institute},
	pages = {324},
}

@misc{the-waymo-team_simulation_nodate,
	title = {Simulation {City}: {Introducing} {Waymo}'s most advanced simulation system yet for autonomous driving},
	url = {https://blog.waymo.com/2021/06/SimulationCity.html},
	author = {{The-Waymo-Team}},
	annote = {Accessed: 2021-12-23},
}

@article{elmquist_sensor_2021,
	title = {A {Sensor} {Simulation} {Framework} for {Training} and {Testing} {Robots} and {Autonomous} {Vehicles}},
	volume = {1},
	url = {https://asmedigitalcollection.asme.org/autonomousvehicles/article-abstract/1/2/021001/1098101},
	language = {en},
	number = {2},
	journal = {Int. J. Veh. Auton. Syst.},
	author = {Elmquist, Asher and Serban, Radu and Negrut, Dan},
	month = feb,
	year = {2021},
	note = {Publisher: American Society of Mechanical Engineers Digital Collection},
	keywords = {Autonomous vehicles, Robots, Sensors, Simulation, Vehicles},
}

@article{dai_flare7k_2022,
	title = {{Flare7K}: {A} {Phenomenological} {Nighttime} {Flare} {Removal} {Dataset}},
	url = {"http://arxiv.org/abs/2210.06570"},
	journal = {arXiv:2210.06570},
	author = {Dai, Yuekun and Li, Chongyi and Zhou, Shangchen and Feng, Ruicheng and Loy, Chen Change},
	month = oct,
	year = {2022},
	note = {\_eprint: 2210.06570},
}

@inproceedings{rong_lgsvl_2020,
	title = {Lgsvl simulator: {A} high fidelity simulator for autonomous driving},
	booktitle = {2020 {IEEE} 23rd {International} {Conference} on {Intelligent} {Transportation} {Systems} ({ITSC})},
	publisher = {IEEE},
	author = {Rong, Guodong and Shin, Byung Hyun and Tabatabaee, Hadi and Lu, Qiang and Lemke, Steve and Možeiko, Mārtiņš and Boise, Eric and Uhm, Geehoon and Gerow, Mark and Mehta, Shalin and {others}},
	year = {2020},
	pages = {1--6},
}

@inproceedings{gokturk_time--flight_2004,
	title = {A {Time}-{Of}-{Flight} {Depth} {Sensor} - {System} {Description}, {Issues} and {Solutions}},
	abstract = {This paper describes a CMOS-based time-of-flight depth sensor and presents some experimental data while addressing various issues arising from its use. Our system is a single-chip solution based on a special CMOS pixel structure that can extract phase information from the received light pulses. The sensor chip integrates a 64x64 pixel array with a high-speed clock generator and ADC. A unique advantage of the chip is that it can be manufactured with an ordinary CMOS process. Compared with other types of depth sensors reported in the literature, our solution offers significant advantages, including superior accuracy, high frame rate, cost effectiveness and a drastic reduction in processing required to construct the depth maps. We explain the factors that determine the resolution of our system, discuss various problems that a time-of-flight depth sensor might face, and propose practical solutions.},
	booktitle = {2004 {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshop}},
	author = {Gokturk, S B and Yalcin, H and Bamji, C},
	month = jun,
	year = {2004},
	keywords = {Application software, Clocks, CMOS process, Computer vision, Data mining, Optical sensors, Phase modulation, Sensor arrays, Sensor phenomena and characterization, Sensor systems},
	pages = {35--35},
}

@inproceedings{kim_15mpixel_2012,
	title = {A 1.{5Mpixel} {RGBZ} {CMOS} image sensor for simultaneous color and range image capture},
	abstract = {A 1.5Mpixel RGBZ image sensor that simultaneously captures color (RGB) and time-of-flight (ToF) range (Z) images is presented. While ToF sensors are well documented, few, if any, monolithic sensors have been reported that capture both range and color. In one sensor, a combined pixel structure captures color and range, but presumably in sequential fields. This approach does not allow simultaneous capturing of range with color, and the pixel performance cannot be optimized for each mode. In thie paper, we introduce a sensor that is designed to capture color and range simultaneously with individually optimized pixels.},
	booktitle = {2012 {IEEE} {International} {Solid}-{State} {Circuits} {Conference}},
	author = {Kim, W and Yibing, W and Ovsiannikov, I and Lee, S and Park, Y and Chung, C and Fossum, E},
	month = feb,
	year = {2012},
	keywords = {Sensors, Arrays, CMOS image sensors, Distance measurement, Image color analysis, Image sensors, monolithic sensors, pixel performance, pixel structure, RGB color, RGBZ image sensor, Three dimensional displays, time-of-flight range images, ToF sensors},
	pages = {392--394},
}

@article{ito_small_2018,
	title = {Small {Imaging} {Depth} {LIDAR} and {DCNN}-{Based} {Localization} for {Automated} {Guided} {Vehicle}},
	volume = {18},
	abstract = {We present our third prototype sensor and a localization method for Automated Guided Vehicles (AGVs), for which small imaging LIght Detection and Ranging (LIDAR) and fusion-based localization are fundamentally important. Our small imaging LIDAR, named the Single-Photon Avalanche Diode (SPAD) LIDAR, uses a time-of-flight method and SPAD arrays. A SPAD is a highly sensitive photodetector capable of detecting at the single-photon level, and the SPAD LIDAR has two SPAD arrays on the same chip for detection of laser light and environmental light. Therefore, the SPAD LIDAR simultaneously outputs range image data and monocular image data with the same coordinate system and does not require external calibration among outputs. As AGVs travel both indoors and outdoors with vibration, this calibration-less structure is particularly useful for AGV applications. We also introduce a fusion-based localization method, named SPAD DCNN, which uses the SPAD LIDAR and employs a Deep Convolutional Neural Network (DCNN). SPAD DCNN can fuse the outputs of the SPAD LIDAR: range image data, monocular image data and peak intensity image data. The SPAD DCNN has two outputs: the regression result of the position of the SPAD LIDAR and the classification result of the existence of a target to be approached. Our third prototype sensor and the localization method are evaluated in an indoor environment by assuming various AGV trajectories. The results show that the sensor and localization method improve the localization accuracy.},
	language = {en},
	number = {1},
	journal = {Sensors},
	author = {Ito, Seigo and Hiratsuka, Shigeyoshi and Ohta, Mitsuhiko and Matsubara, Hiroyuki and Ogawa, Masaru},
	month = jan,
	year = {2018},
	note = {Publisher: mdpi.com},
	keywords = {deep convolutional neural network, deep learning, imaging LIDAR, light detection and ranging, localization, sensor fusion, single-photon avalanche diode},
}

@misc{noauthor_flywheel_2017,
	title = {Flywheel • modern informatics platform for biomedical research \& collaboration},
	url = {https://flywheel.io/},
	abstract = {Flywheel is an informatics platform for biomedical research \& collaboration, with solutions for Imaging Centers, Clinical Research, and Life Sciences.},
	language = {en},
	month = mar,
	year = {2017},
	annote = {Accessed: 2021-1-25},
}

@inproceedings{behrisch_sumosimulation_2011,
	title = {Sumo–simulation of urban mobility},
	volume = {42},
	booktitle = {The {Third} {International} {Conference} on {Advances} in {System} {Simulation} ({SIMUL} 2011), {Barcelona}, {Spain}},
	author = {Behrisch, Michael and Bieker, Laura and Erdmann, Jakob and Krajzewicz, Daniel},
	year = {2011},
	keywords = {NewAccount},
}

@article{sun_spadnet_2020,
	title = {{SPADnet}: deep {RGB}-{SPAD} sensor fusion assisted by monocular depth estimation},
	volume = {28},
	abstract = {Single-photon light detection and ranging (LiDAR) techniques use emerging single-photon detectors (SPADs) to push 3D imaging capabilities to unprecedented ranges. However, it remains challenging to robustly estimate scene depth from the noisy and otherwise corrupted measurements recorded by a SPAD. Here, we propose a deep sensor fusion strategy that combines corrupted SPAD data and a conventional 2D image to estimate the depth of a scene. Our primary contribution is a neural network architecture—SPADnet—that uses a monocular depth estimation algorithm together with a SPAD denoising and sensor fusion strategy. This architecture, together with several techniques in network training, achieves state-of-the-art results for RGB-SPAD fusion with simulated and captured data. Moreover, SPADnet is more computationally efficient than previous RGB-SPAD fusion networks.},
	language = {en},
	number = {10},
	journal = {Opt. Express, OE},
	author = {Sun, Zhanghao and Lindell, David B and Solgaard, Olav and Wetzstein, Gordon},
	month = may,
	year = {2020},
	note = {Publisher: Optical Society of America},
	keywords = {Sensors, Light detection, Neural networks, Reconstruction algorithms, Three dimensional imaging, Three dimensional sensing},
	pages = {14948--14962},
}

@article{geng_structured-light_2011,
	title = {Structured-light {3D} surface imaging: a tutorial},
	volume = {3},
	abstract = {We provide a review of recent advances in 3D surface imaging technologies. We focus particularly on noncontact 3D surface measurement techniques based on structured illumination. The high-speed and high-resolution pattern projection capability offered by the digital light projection technology, together with the recent advances in imaging sensor technologies, may enable new generation systems for 3D surface measurement applications that will provide much better functionality and performance than existing ones in terms of speed, accuracy, resolution, modularization, and ease of use. Performance indexes of 3D imaging system are discussed, and various 3D surface imaging schemes are categorized, illustrated, and compared. Calibration techniques are also discussed, since they play critical roles in achieving the required precision. Numerous applications of 3D surface imaging technologies are discussed with several examples.},
	language = {en},
	number = {2},
	journal = {Adv. Opt. Photon., AOP},
	author = {Geng, Jason},
	month = jun,
	year = {2011},
	note = {Publisher: Optical Society of America},
	keywords = {Three dimensional imaging, Imaging systems, Imaging techniques, Medical imaging, Spatial light modulators, Structured illumination microscopy},
	pages = {128--160},
}

@article{caltagirone_lidarcamera_2019,
	title = {{LIDAR}–camera fusion for road detection using fully convolutional neural networks},
	volume = {111},
	abstract = {In this work, a deep learning approach has been developed to carry out road detection by fusing LIDAR point clouds and camera images. An unstructured and sparse point cloud is first projected onto the camera image plane and then upsampled to obtain a set of dense 2D images encoding spatial information. Several fully convolutional neural networks (FCNs) are then trained to carry out road detection, either by using data from a single sensor, or by using three fusion strategies: early, late, and the newly proposed cross fusion. Whereas in the former two fusion approaches, the integration of multimodal information is carried out at a predefined depth level, the cross fusion FCN is designed to directly learn from data where to integrate information; this is accomplished by using trainable cross connections between the LIDAR and the camera processing branches. To further highlight the benefits of using a multimodal system for road detection, a data set consisting of visually challenging scenes was extracted from driving sequences of the KITTI raw data set. It was then demonstrated that, as expected, a purely camera-based FCN severely underperforms on this data set. A multimodal system, on the other hand, is still able to provide high accuracy. Finally, the proposed cross fusion FCN was evaluated on the KITTI road benchmark where it achieved excellent performance, with a MaxF score of 96.03\%, ranking it among the top-performing approaches.},
	journal = {Rob. Auton. Syst.},
	author = {Caltagirone, Luca and Bellone, Mauro and Svensson, Lennart and Wahde, Mattias},
	month = jan,
	year = {2019},
	keywords = {Deep learning, Intelligent vehicles, Road detection, Sensor fusion},
	pages = {125--131},
}

@article{manufacturer_lidar_nodate,
	title = {{LiDAR} {SPECIFICATION} {COMPARION} {CHART}},
	author = {Manufacturer, Hor Angular and Li, D A R and Automotive, Resolution Ibeo and Range, Features: Long and Tracking/Classification, Embedded Object and Quality, Automotive},
}

@article{manivasagam_lidarsim_2020,
	title = {{LiDARsim}: {Realistic} {LiDAR} {Simulation} by {Leveraging} the {Real} {World}},
	abstract = {We tackle the problem of producing realistic simulations of LiDAR point clouds, the sensor of preference for most self-driving vehicles. We argue that, by leveraging real data, we can simulate the complex world more realistically compared to employing virtual worlds built from CAD/procedural models. Towards this goal, we first build a large catalog of 3D static maps and 3D dynamic objects by driving around several cities with our self-driving fleet. We can then generate scenarios by selecting a scene from our catalog and “virtually” placing the self-driving vehicle (SDV) and a set of dynamic objects from the catalog in plausible locations in the scene. To produce realistic simulations, we develop a novel simulator that captures both the power of physics-based and learning-based simulation. We first utilize ray casting over the 3D scene and then use a deep neural network to produce deviations from the physics-based simulation, producing realistic LiDAR point clouds. We showcase LiDARsim's usefulness for perception algorithms-testing on long-tail events and end-to-end closed-loop evaluation on safety-critical scenarios.},
	author = {Manivasagam, Sivabalan and Wang, Shenlong and Wong, Kelvin and Zeng, Wenyuan and Sazanovich, Mikita and Tan, Shuhan and Yang, Bin and Ma, Wei-Chiu and Urtasun, Raquel},
	month = jun,
	year = {2020},
	note = {\_eprint: 2006.09348},
}

@article{sun_real-time_2020,
	title = {Real-time {Fusion} {Network} for {RGB}-{D} {Semantic} {Segmentation} {Incorporating} {Unexpected} {Obstacle} {Detection} for {Road}-driving {Images}},
	abstract = {Semantic segmentation has made striking progress due to the success of deep convolutional neural networks. Considering the demands of autonomous driving, real-time semantic segmentation has become a research hotspot these years. However, few real-time RGB-D fusion semantic segmentation studies are carried out despite readily accessible depth information nowadays. In this paper, we propose a real-time fusion semantic segmentation network termed RFNet that effectively exploits complementary cross-modal information. Building on an efficient network architecture, RFNet is capable of running swiftly, which satisfies autonomous vehicles applications. Multi-dataset training is leveraged to incorporate unexpected small obstacle detection, enriching the recognizable classes required to face unforeseen hazards in the real world. A comprehensive set of experiments demonstrates the effectiveness of our framework. On Cityscapes, Our method outperforms previous state-of-the-art semantic segmenters, with excellent accuracy and 22Hz inference speed at the full 2048x1024 resolution, outperforming most existing RGB-D networks.},
	author = {Sun, Lei and Yang, Kailun and Hu, Xinxin and Hu, Weijian and Wang, Kaiwei},
	month = feb,
	year = {2020},
	note = {\_eprint: 2002.10570},
}

@article{atapour-abarghouei_complete_2019,
	title = {To complete or to estimate, that is the question: {A} {Multi}-{Task} {Approach} to {Depth} {Completion} and {Monocular} {Depth} {Estimation}},
	abstract = {Robust three-dimensional scene understanding is now an ever-growing area of research highly relevant in many real-world applications such as autonomous driving and robotic navigation. In this paper, we propose a multi-task learning-based model capable of performing two tasks:- sparse depth completion (i.e. generating complete dense scene depth given a sparse depth image as the input) and monocular depth estimation (i.e. predicting scene depth from a single RGB image) via two sub-networks jointly trained end to end using data randomly sampled from a publicly available corpus of synthetic and real-world images. The first sub-network generates a sparse depth image by learning lower level features from the scene and the second predicts a full dense depth image of the entire scene, leading to a better geometric and contextual understanding of the scene and, as a result, superior performance of the approach. The entire model can be used to infer complete scene depth from a single RGB image or the second network can be used alone to perform depth completion given a sparse depth input. Using adversarial training, a robust objective function, a deep architecture relying on skip connections and a blend of synthetic and real-world training data, our approach is capable of producing superior high quality scene depth. Extensive experimental evaluation demonstrates the efficacy of our approach compared to contemporary state-of-the-art techniques across both problem domains.},
	author = {Atapour-Abarghouei, Amir and Breckon, Toby P},
	month = aug,
	year = {2019},
	note = {\_eprint: 1908.05540},
}

@article{griffin_echolocation_1965,
	title = {The echolocation of flying insects by bats},
	abstract = {When bats are hunting insects they adjust the pattern and tempo of their high frequency orientation sounds in a way that seems quite appropriate for active echolocation of small moving targets but distinctly unsuited for the passive detection of insects by listening for their …},
	journal = {Readings in the Psychology of Perception},
	author = {Griffin, Donald R and Webster, Frederic A and Michael, Charles R},
	year = {1965},
	note = {Publisher: Ardent Media},
	pages = {21},
}

@article{gupta_learning_2014,
	title = {Learning {Rich} {Features} from {RGB}-{D} {Images} for {Object} {Detection} and {Segmentation}},
	abstract = {In this paper we study the problem of object detection for RGB-D images using semantically rich image and depth features. We propose a new geocentric embedding for depth images that encodes height above ground and angle with gravity for each pixel in addition to the horizontal disparity. We demonstrate that this geocentric embedding works better than using raw depth images for learning feature representations with convolutional neural networks. Our final object detection system achieves an average precision of 37.3\%, which is a 56\% relative improvement over existing methods. We then focus on the task of instance segmentation where we label pixels belonging to object instances found by our detector. For this task, we propose a decision forest approach that classifies pixels in the detection window as foreground or background using a family of unary and binary tests that query shape and geocentric pose features. Finally, we use the output from our object detectors in an existing superpixel classification framework for semantic scene segmentation and achieve a 24\% relative improvement over current state-of-the-art for the object categories that we study. We believe advances such as those represented in this paper will facilitate the use of perception in fields like robotics.},
	author = {Gupta, Saurabh and Girshick, Ross and Arbeláez, Pablo and Malik, Jitendra},
	month = jul,
	year = {2014},
	note = {\_eprint: 1407.5736},
}

@techreport{daniel_seita_jeff_mahler_mike_danielczuk_matthew_matl_and_ken_goldberg_drilling_nodate,
	title = {Drilling {Down} on {Depth} {Sensing} and {Deep} {Learning}},
	abstract = {The BAIR Blog},
	author = {{Daniel Seita, Jeff Mahler, Mike Danielczuk, Matthew Matl, and Ken Goldberg}},
	note = {Publication Title: Berkeley Artificial Intellgence Research},
	annote = {Accessed: 2020-12-26},
}

@inproceedings{bergman_deep_2020,
	title = {Deep {Adaptive} {LiDAR}: {End}-to-end {Optimization} of {Sampling} and {Depth} {Completion} at {Low} {Sampling} {Rates}},
	abstract = {Current LiDAR systems are limited in their ability to capture dense 3D point clouds. To overcome this challenge, deep learning-based depth completion algorithms have been developed to inpaint missing depth guided by an RGB image. However, these methods fail for low sampling rates. Here, we propose an adaptive sampling scheme for LiDAR systems that demonstrates state-of-the-art performance for depth completion at low sampling rates. Our system is fully differentiable, allowing the sparse depth sampling and the depth inpainting components to be trained end-to-end with an upstream task.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Computational} {Photography} ({ICCP})},
	author = {Bergman, A W and Lindell, D B and Wetzstein, G},
	month = apr,
	year = {2020},
	keywords = {adaptive sampling, adaptive sampling scheme, current LiDAR systems, deep adaptive LiDAR, deep learning-based depth completion algorithms, dense 3D point clouds, depth imaging, depth inpainting components, end-to-end optimization, image colour analysis, image restoration, image sampling, learning (artificial intelligence), LiDAR, low sampling rates, optical images, optical radar, radar imaging, sparse depth sampling},
	pages = {1--11},
}

@book{e_bruce_goldstein_and_james_r_brockmole_sensation_2017,
	title = {Sensation and {Perception} (10th {Edition})},
	publisher = {Cengage Learning},
	author = {{E. Bruce Goldstein and James R. Brockmole}},
	year = {2017},
}

@patent{droz_long_2016,
	title = {Long {Range} {Steerable} {LIDAR} {System}},
	abstract = {Systems and methods are described that relate to a light detection and ranging (LIDAR) device. The LIDAR device includes a fiber laser configured to emit light within a wavelength range, a scanning portion configured to direct the emitted light in a reciprocating manner about a first axis, and a plurality of detectors configured to sense light within the wavelength range. The device additionally includes a controller configured to receive target information, which may be indicative of an object, a position, a location, or an angle range. In response to receiving the target information, the controller may cause the rotational mount to rotate so as to adjust a pointing direction of the LIDAR. The controller is further configured to cause the LIDAR to scan a field-of-view (FOV) of the environment. The controller may determine a three-dimensional (3D) representation of the environment based on data from scanning the FOV.},
	number = {20160291134:A1},
	author = {Droz, Pierre-Yves and Pennecot, Gaetan and Levandowski, Anthony and Ulrich, Drew Eugene and Morriss, Zach and Wachter, Luke and Iordach, Dorel Ionut and McCann, William and Gruver, Daniel and Fidric, Bernard and Lenius, Samuel William},
	month = oct,
	year = {2016},
}

@inproceedings{hazirbas_fusenet_2016,
	title = {{FuseNet}: {Incorporating} {Depth} into {Semantic} {Segmentation} via {Fusion}-{Based} {CNN} {Architecture}},
	abstract = {In this paper we address the problem of semantic labeling of indoor scenes on RGB-D data. With the availability of RGB-D cameras, it is expected that additional depth measurement will improve the accuracy. Here we investigate a solution how to incorporate complementary depth information into a semantic segmentation framework by making use of convolutional neural networks (CNNs). Recently encoder-decoder type fully convolutional CNN architectures have achieved a great success in the field of semantic segmentation. Motivated by this observation we propose an encoder-decoder type network, where the encoder part is composed of two branches of networks that simultaneously extract features from RGB and depth images and fuse depth features into the RGB feature maps as the network goes deeper. Comprehensive experimental evaluations demonstrate that the proposed fusion-based architecture achieves competitive results with the state-of-the-art methods on the challenging SUN RGB-D benchmark obtaining 76.27\% global accuracy , 48.30\% average class accuracy and 37.29\% average intersection-over-union score.},
	author = {Hazırbaş, Caner and Ma, Lingni and Domokos, Csaba and Cremers, Daniel},
	month = nov,
	year = {2016},
}

@article{reith_convolutional_2019,
	title = {A convolutional neural network reaches optimal sensitivity for detecting some, but not all, patterns},
	abstract = {We investigate the performance of a convolutional neural network (CNN) at detecting a signal-known-exactly in Poisson noise. We compare the network performance with that of a Bayesian ideal observer (IO) that has the theoretical optimum in detection performance and a linear support vector machine (SVM). For several types of stimuli, including harmonics, faces, and certain regular patterns, the CNN performance asymptotes at the level of the IO. The SVM detection sensitivity is approximately 3-times lower. For other stimuli, including random patterns and certain cellular automata, the CNN sensitivity is significantly lower than that of the IO and the SVM. Finally, when the signal can appear in one of multiple locations, CNN sensitivity continues to match the ideal sensitivity.},
	author = {Reith, Fabian and Wandell, Brian},
	month = nov,
	year = {2019},
	note = {\_eprint: 1911.05055},
}

@article{feng_deep_2019,
	title = {Deep {Multi}-modal {Object} {Detection} and {Semantic} {Segmentation} for {Autonomous} {Driving}: {Datasets}, {Methods}, and {Challenges}},
	abstract = {Recent advancements in perception for autonomous driving are driven by deep learning. In order to achieve robust and accurate scene understanding, autonomous vehicles are usually equipped with different sensors (e.g. cameras, LiDARs, Radars), and multiple sensing modalities can be fused to exploit their complementary properties. In this context, many methods have been proposed for deep multi-modal perception problems. However, there is no general guideline for network architecture design, and questions of “what to fuse”, “when to fuse”, and “how to fuse” remain open. This review paper attempts to systematically summarize methodologies and discuss challenges for deep multi-modal object detection and semantic segmentation in autonomous driving. To this end, we first provide an overview of on-board sensors on test vehicles, open datasets, and background information for object detection and semantic segmentation in autonomous driving research. We then summarize the fusion methodologies and discuss challenges and open questions. In the appendix, we provide tables that summarize topics and methods. We also provide an interactive online platform to navigate each reference: https://boschresearch.github.io/multimodalperception/.},
	author = {Feng, Di and Haase-Schütz, Christian and Rosenbaum, Lars and Hertlein, Heinz and Glaeser, Claudius and Timm, Fabian and Wiesbeck, Werner and Dietmayer, Klaus},
	month = feb,
	year = {2019},
	note = {\_eprint: 1902.07830},
}

@article{you_pseudo-lidar_2019,
	title = {Pseudo-{LiDAR}++: {Accurate} {Depth} for {3D} {Object} {Detection} in {Autonomous} {Driving}},
	abstract = {Detecting objects such as cars and pedestrians in 3D plays an indispensable role in autonomous driving. Existing approaches largely rely on expensive LiDAR sensors for accurate depth information. While recently pseudo-LiDAR has been introduced as a promising alternative, at a much lower cost based solely on stereo images, there is still a notable performance gap. In this paper we provide substantial advances to the pseudo-LiDAR framework through improvements in stereo depth estimation. Concretely, we adapt the stereo network architecture and loss function to be more aligned with accurate depth estimation of faraway objects — currently the primary weakness of pseudo-LiDAR. Further, we explore the idea to leverage cheaper but extremely sparse LiDAR sensors, which alone provide insufficient information for 3D detection, to de-bias our depth estimation. We propose a depth-propagation algorithm, guided by the initial depth estimates, to diffuse these few exact measurements across the entire depth map. We show on the KITTI object detection benchmark that our combined approach yields substantial improvements in depth estimation and stereo-based 3D object detection — outperforming the previous state-of-the-art detection accuracy for faraway objects by 40\%. Our code is available at https://github.com/mileyan/Pseudo\_Lidar\_V2.},
	author = {You, Yurong and Wang, Yan and Chao, Wei-Lun and Garg, Divyansh and Pleiss, Geoff and Hariharan, Bharath and Campbell, Mark and Weinberger, Kilian Q},
	month = jun,
	year = {2019},
	note = {\_eprint: 1906.06310},
}

@article{gkioxari_mesh_2019,
	title = {Mesh {R}-{CNN}},
	abstract = {Rapid advances in 2D perception have led to systems that accurately detect objects in real-world images. However, these systems make predictions in 2D, ignoring the 3D structure of the world. Concurrently, advances in 3D shape prediction have mostly focused on synthetic benchmarks and isolated objects. We unify advances in these two areas. We propose a system that detects objects in real-world images and produces a triangle mesh giving the full 3D shape of each detected object. Our system, called Mesh R-CNN, augments Mask R-CNN with a mesh prediction branch that outputs meshes with varying topological structure by first predicting coarse voxel representations which are converted to meshes and refined with a graph convolution network operating over the mesh's vertices and edges. We validate our mesh prediction branch on ShapeNet, where we outperform prior work on single-image shape prediction. We then deploy our full Mesh R-CNN system on Pix3D, where we jointly detect objects and predict their 3D shapes.},
	author = {Gkioxari, Georgia and Malik, Jitendra and Johnson, Justin},
	month = jun,
	year = {2019},
	note = {\_eprint: 1906.02739},
}

@article{coates_correction_2002,
	title = {The correction for photon `pile-up' in the measurement of radiative lifetimes},
	volume = {1},
	abstract = {In determinations of radiative lifetimes in which time intervals are measured up to the first detected photon, an error is introduced when subsequent photons are neglected. An accurate correction for this effect is described, having the advantages of general applicability and allowing the use of high photon detection rates. It is also shown that the error introduced by the use of this correction is negligible under all practical experimental conditions.},
	language = {en},
	number = {8},
	journal = {J. Phys. E},
	author = {Coates, P B},
	month = may,
	year = {2002},
	note = {Publisher: IOP Publishing},
	pages = {878},
}

@inproceedings{chen_progressively_2018,
	title = {Progressively complementarity-aware fusion network for {RGB}-{D} salient object detection},
	booktitle = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
	author = {Chen, Hao and Li, Youfu},
	year = {2018},
	pages = {3051--3060},
}

@article{wang_pseudo-lidar_2018,
	title = {Pseudo-{LiDAR} from {Visual} {Depth} {Estimation}: {Bridging} the {Gap} in {3D} {Object} {Detection} for {Autonomous} {Driving}},
	abstract = {3D object detection is an essential task in autonomous driving. Recent techniques excel with highly accurate detection rates, provided the 3D input data is obtained from precise but expensive LiDAR technology. Approaches based on cheaper monocular or stereo imagery data have, until now, resulted in drastically lower accuracies — a gap that is commonly attributed to poor image-based depth estimation. However, in this paper we argue that data representation (rather than its quality) accounts for the majority of the difference. Taking the inner workings of convolutional neural networks into consideration, we propose to convert image-based depth maps to pseudo-LiDAR representations — essentially mimicking LiDAR signal. With this representation we can apply different existing LiDAR-based detection algorithms. On the popular KITTI benchmark, our approach achieves impressive improvements over the existing state-of-the-art in image-based performance — raising the detection accuracy of objects within 30m range from the previous state-of-the-art of 22\% to an unprecedented 74\%. At the time of submission our algorithm holds the highest entry on the KITTI 3D object detection leaderboard for stereo image based approaches.},
	author = {Wang, Yan and Chao, Wei-Lun and Garg, Divyansh and Hariharan, Bharath and Campbell, Mark and Weinberger, Kilian Q},
	month = dec,
	year = {2018},
	note = {\_eprint: 1812.07179},
}

@article{gruber_gated2depth_2019,
	title = {{Gated2Depth}: {Real}-time {Dense} {Lidar} from {Gated} {Images}},
	abstract = {We present an imaging framework which converts three images from a gated camera into high-resolution depth maps with depth accuracy comparable to pulsed lidar measurements. Existing scanning lidar systems achieve low spatial resolution at large ranges due to mechanically-limited angular sampling rates, restricting scene understanding tasks to close-range clusters with dense sampling. Moreover, today's pulsed lidar scanners suffer from high cost, power consumption, large form-factors, and they fail in the presence of strong backscatter. We depart from point scanning and demonstrate that it is possible to turn a low-cost CMOS gated imager into a dense depth camera with at least 80m range - by learning depth from three gated images. The proposed architecture exploits semantic context across gated slices, and is trained on a synthetic discriminator loss without the need of dense depth labels. The proposed replacement for scanning lidar systems is real-time, handles back-scatter and provides dense depth at long ranges. We validate our approach in simulation and on real-world data acquired over 4,000km driving in northern Europe. Data and code are available at https://github.com/gruberto/Gated2Depth.},
	author = {Gruber, Tobias and Julca-Aguilar, Frank and Bijelic, Mario and Ritter, Werner and Dietmayer, Klaus and Heide, Felix},
	month = feb,
	year = {2019},
	note = {\_eprint: 1902.04997},
}

@article{lopez-rodriguez_project_2020,
	title = {Project to {Adapt}: {Domain} {Adaptation} for {Depth} {Completion} from {Noisy} and {Sparse} {Sensor} {Data}},
	abstract = {Depth completion aims to predict a dense depth map from a sparse depth input. The acquisition of dense ground truth annotations for depth completion settings can be difficult and, at the same time, a significant domain gap between real LiDAR measurements and synthetic data has prevented from successful training of models in virtual settings. We propose a domain adaptation approach for sparse-to-dense depth completion that is trained from synthetic data, without annotations in the real domain or additional sensors. Our approach simulates the real sensor noise in an RGB+LiDAR set-up, and consists of three modules: simulating the real LiDAR input in the synthetic domain via projections, filtering the real noisy LiDAR for supervision and adapting the synthetic RGB image using a CycleGAN approach. We extensively evaluate these modules against the state-of-the-art in the KITTI depth completion benchmark, showing significant improvements.},
	author = {Lopez-Rodriguez, Adrian and Busam, Benjamin and Mikolajczyk, Krystian},
	month = aug,
	year = {2020},
	note = {\_eprint: 2008.01034},
}

@article{fang_augmented_2020,
	title = {Augmented lidar simulator for autonomous driving},
	abstract = {In Autonomous Driving (AD), detection and tracking of obstacles on the roads is a critical task. Deep-learning based methods using annotated LiDAR data have been the most widely adopted approach for this. Unfortunately, annotating 3D point cloud is a very challenging, time-and money-consuming task. In this letter, we propose a novel LiDAR simulator that augments real point cloud with synthetic obstacles (eg, vehicles, pedestrians, and other movable objects). Unlike previous simulators that entirely rely on CG (Computer Graphics) …},
	journal = {IEEE Robotics and},
	author = {Fang, J and Zhou, D and Yan, F and Zhao, T and Zhang, F and {others}},
	year = {2020},
	note = {Publisher: ieeexplore.ieee.org},
}

@article{everingham_pascal_2010,
	title = {The pascal visual object classes (voc) challenge},
	abstract = {Abstract The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures …},
	journal = {International journal of},
	author = {Everingham, M and Van Gool, L and Williams, C K I and {others}},
	year = {2010},
	note = {Publisher: Springer},
}

@article{griffin_more_1958,
	title = {More about {Bat} “{Radar}”},
	volume = {199},
	abstract = {A sequel to an earlier article which described the capacity of bats to locate objects by supersonic echoes. This natural sonar is now known to incorporate extraordinary refinements},
	number = {1},
	journal = {Scientific American},
	author = {Griffin, D W},
	year = {1958},
	pages = {40--45},
}

@inproceedings{he_deep_2016,
	title = {Deep residual learning for image recognition},
	booktitle = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2016},
	pages = {770--778},
}

@inproceedings{hoffman_cross-modal_2016,
	title = {Cross-modal adaptation for {RGB}-{D} detection},
	booktitle = {2016 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Hoffman, J and Gupta, S and Leong, J and Guadarrama, S and Darrell, T},
	month = may,
	year = {2016},
	keywords = {convolutional neural network, Robots, image colour analysis, Adaptation models, CNN based object detectors, cross-modal adaptation, Detectors, image fusion, labeled depth images, mid-level fusion, neural nets, object detection, Object detection, Proposals, RGB images, RGB object detector, RGB-D detection, Training, Training data},
	pages = {5032--5039},
}

@article{ophoff_exploring_2019,
	title = {Exploring {RGB}+{Depth} {Fusion} for {Real}-{Time} {Object} {Detection}},
	volume = {19},
	abstract = {In this paper, we investigate whether fusing depth information on top of normal RGB data for camera-based object detection can help to increase the performance of current state-of-the-art single-shot detection networks. Indeed, depth sensing is easily acquired using depth cameras such as a Kinect or stereo setups. We investigate the optimal manner to perform this sensor fusion with a special focus on lightweight single-pass convolutional neural network (CNN) architectures, enabling real-time processing on limited hardware. For this, we implement a network architecture allowing us to parameterize at which network layer both information sources are fused together. We performed exhaustive experiments to determine the optimal fusion point in the network, from which we can conclude that fusing towards the mid to late layers provides the best results. Our best fusion models significantly outperform the baseline RGB network in both accuracy and localization of the detections.},
	language = {en},
	number = {4},
	journal = {Sensors},
	author = {Ophoff, Tanguy and Van Beeck, Kristof and Goedemé, Toon},
	month = feb,
	year = {2019},
	keywords = {Sensor fusion, Object detection, Depth, Neural Networks, RGB, RGBD, Single-shot},
}

@article{yuanzhouhan_cao_exploiting_2017,
	title = {Exploiting {Depth} {From} {Single} {Monocular} {Images} for {Object} {Detection} and {Semantic} {Segmentation}},
	volume = {26},
	abstract = {Augmenting RGB data with measured depth has been shown to improve the performance of a range of tasks in computer vision, including object detection and semantic segmentation. Although depth sensors such as the Microsoft Kinect have facilitated easy acquisition of such depth information, the vast majority of images used in vision tasks do not contain depth information. In this paper, we show that augmenting RGB images with estimated depth can also improve the accuracy of both object detection and semantic segmentation. Specifically, we first exploit the recent success of depth estimation from monocular images and learn a deep depth estimation model. Then, we learn deep depth features from the estimated depth and combine with RGB features for object detection and semantic segmentation. In addition, we propose an RGB-D semantic segmentation method, which applies a multi-task training scheme: semantic label prediction and depth value regression. We test our methods on several data sets and demonstrate that incorporating information from estimated depth improves the performance of object detection and semantic segmentation remarkably.},
	language = {en},
	number = {2},
	journal = {IEEE Trans. Image Process.},
	author = {{Yuanzhouhan Cao} and {Chunhua Shen} and {Heng Tao Shen}},
	month = feb,
	year = {2017},
	pages = {836--846},
}

@article{engbert_microsaccades_2004,
	title = {Microsaccades keep the eyes’ balance during fixation},
	volume = {15},
	number = {6},
	journal = {Psychological Science},
	author = {Engbert, R. and Kliegl, R.},
	year = {2004},
	pages = {431--436},
}

@article{artal_opotical-digital_1989,
	title = {Opotical-digital procedure for the determination of white-light retinal images of a point test},
	volume = {28},
	journal = {Opt. Eng.},
	author = {Artal, P. and Santamaria, J. and Bescos, J.},
	year = {1989},
	pages = {687--690},
}

@book{bracewell_hartley_1986,
	address = {Oxford},
	title = {The {Hartley} {Transform}},
	publisher = {Clarendon Press},
	author = {Bracewell, R. N.},
	year = {1986},
}

@article{spekreijse_mathematical_1993,
	title = {A mathematical description of the foveal visual point spread function with parameters for age, pupil size and pigmentation},
	volume = {33},
	journal = {Vision Res.},
	author = {Spekreijse, J. K. IJspeert T. Van Den Berg H.},
	year = {1993},
	pages = {15--20},
}

@article{hennings_off-axis_1981,
	title = {Off-axis image quality in the human eye},
	volume = {21},
	journal = {Vision Res.},
	author = {Hennings, J. A. M. and Charman, W. N.},
	year = {1981},
	pages = {445--455},
}

@article{curcio_distribution_1991,
	title = {Distribution and {Morphology} of {Human} {Cone} {Photoreceptors} {Stained} with {Anti}-{BLue} {Opsin}},
	volume = {312},
	journal = {J. Comp. Neurology},
	author = {Curcio, C. A. and Allen, K. A. and Sloan, K. R. and Lerea, C. L. and Hurley, J. B. and Klock, I. B. and Milam, A. H.},
	year = {1991},
	pages = {610--624},
	annote = {Blue cone sampling array.},
}

@article{wald_change_1947,
	title = {The {Change} in {Refractive} {Power} of the {Human} {Eye} in {Dim} and {Bright} {Light}},
	volume = {37},
	journal = {J. Opt. Soc. Am.},
	author = {Wald, G. and Griffin, D. R.},
	year = {1947},
	pages = {321--336},
}

@incollection{westheimer_eye_1986,
	address = {New York},
	title = {The eye as an optical instrument},
	booktitle = {Handbook of {Perception}},
	publisher = {Wiley},
	author = {Westheimer, G.},
	editor = {al, J. Thomas et},
	year = {1986},
	pages = {Chapter 4},
}

@article{flamant_etude_1955,
	title = {Etude de la repartition de lumiere dans l'image retinienne d'une fente},
	volume = {34},
	journal = {Rev. Opt. (theor. instrum.)},
	author = {Flamant, F.},
	year = {1955},
	pages = {433--459},
}

@article{bedford_axial_1957,
	title = {Axial {Chromatic} {Aberration} of the {Human} {Eye}},
	volume = {47},
	number = {6},
	journal = {J. Opt. Soc. Am.},
	author = {Bedford, R. E. and Wyszecki, G.},
	year = {1957},
	pages = {564--565},
}

@article{campbell_optical_1966,
	title = {Optical quality of the human eye},
	volume = {186},
	journal = {J. Physiol},
	author = {Campbell, F. W. and Gubisch, R. W.},
	year = {1966},
	pages = {558--578},
}

@inproceedings{dosovitskiy_carla_2017,
	title = {{CARLA}: {An} open urban driving simulator},
	booktitle = {Conference on robot learning},
	publisher = {PMLR},
	author = {Dosovitskiy, Alexey and Ros, German and Codevilla, Felipe and Lopez, Antonio and Koltun, Vladlen},
	year = {2017},
	pages = {1--16},
}

@inproceedings{shah_airsim_2018,
	title = {Airsim: {High}-fidelity visual and physical simulation for autonomous vehicles},
	booktitle = {Field and service robotics},
	publisher = {Springer},
	author = {Shah, Shital and Dey, Debadeepta and Lovett, Chris and Kapoor, Ashish},
	year = {2018},
	pages = {621--635},
}

@article{liu_system_2019,
	title = {A system for generating complex physically accurate sensor images for automotive applications},
	abstract = {We describe an open-source simulator that creates sensor irradiance and sensor images of typical automotive scenes in urban settings. The purpose of the system is to support camera design and testing for automotive applications. The user can specify scene parameters (eg …},
	journal = {Electronic},
	author = {Liu, Z and Shen, M and Zhang, J and Liu, S and Blasinski, H and {others}},
	year = {2019},
	note = {Publisher: ingentaconnect.com},
	keywords = {2019 ICCV (Zhenyi Brian), Automotive, Camera simulation, Image systems simulation, Automobile, CNN, Computer graphics, Image systems engineering, Machine-learning},
}

@inproceedings{karaimer_software_2016,
	title = {A software platform for manipulating the camera imaging pipeline},
	booktitle = {European {Conference} on {Computer} {Vision}},
	publisher = {Springer},
	author = {Karaimer, Hakki Can and Brown, Michael S},
	year = {2016},
	pages = {429--444},
}

@inproceedings{robidoux_end--end_2021,
	title = {End-to-end high dynamic range camera pipeline optimization},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Robidoux, Nicolas and Capel, Luis E Garcia and Seo, Dong-eun and Sharma, Avinash and Ariza, Federico and Heide, Felix},
	year = {2021},
	pages = {6297--6307},
}

@inproceedings{bastos_overview_2021,
	title = {An overview of {LiDAR} requirements and techniques for autonomous driving},
	url = {https://ieeexplore.ieee.org/abstract/document/9435580?casa_token=IiT0nwSSR74AAAAA:SIzdu-GbqWzQIYil66y22KFVDaChmh_-KFrY1sBFvLJmEK54G4DI-rJZmOd5_JvE19C_zxqP},
	language = {en},
	booktitle = {2021 {Telecoms} {Conference} ({Conf}℡{E})},
	publisher = {IEEE},
	author = {Bastos, Daniel and Monteiro, Paulo P and Oliveira, Arnaldo S R and Drummond, Miguel V},
	month = feb,
	year = {2021},
	pages = {1--6},
}

@misc{bitterli_rendering_2016,
	title = {Rendering resources},
	author = {Bitterli, Benedikt},
	year = {2016},
	annote = {https://benedikt-bitterli.me/resources/},
}

@misc{kijima_image_2007,
	title = {Image sensor with improved light sensitivity},
	url = {https://patents.google.com/patent/US20070268533A1/en?oq=20070268533},
	author = {Kijima, Takayuki and Nakamura, Hideo and Compton, John T and Hamilton, John F and DeWeese, Thomas E},
	month = nov,
	year = {2007},
	note = {Issue: 20070268533:A1
Publication Title: US Patent},
}

@misc{bayer_color_1976,
	title = {Color imaging array},
	url = {https://cir.nii.ac.jp/crid/1572261550680217344},
	author = {Bayer, B},
	year = {1976},
	note = {Publication Title: United States Patent, no. 3971065},
}

@inproceedings{ramazzina_gated_2024,
	title = {Gated {Fields}: {Learning} {Scene} {Reconstruction} from {Gated} {Videos}},
	url = {https://openaccess.thecvf.com/content/CVPR2024/papers/Ramazzina_Gated_Fields_Learning_Scene_Reconstruction_from_Gated_Videos_CVPR_2024_paper.pdf},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Ramazzina, Andrea and Walz, Stefanie and Dahal, Pragyan and Bijelic, Mario and Heide, Felix},
	year = {2024},
	pages = {10530--10541},
}

@inproceedings{walia_gated2gated_2022,
	title = {{Gated2Gated}: {Self}-supervised depth estimation from gated images},
	url = {http://openaccess.thecvf.com/content/CVPR2022/html/Walia_Gated2Gated_Self-Supervised_Depth_Estimation_From_Gated_Images_CVPR_2022_paper.html},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Walia, Amanpreet and Walz, Stefanie and Bijelic, Mario and Mannan, Fahim and Julca-Aguilar, Frank and Langer, Michael and Ritter, Werner and Heide, Felix},
	month = jun,
	year = {2022},
	pages = {2811--2821},
}

@incollection{gruber_high-resolution_2021,
	address = {Wiesbaden},
	series = {Proceedings},
	title = {High-{Resolution} {Gated} {Depth} {Estimation} for {Self}-{Driving} {Cars} in {AdverseWeather}: {Von} der {Fahrerassistenz} zum autonomen {Fahren} 6. {Internationale} {ATZ}-{Fachtagung}},
	url = {https://link.springer.com/chapter/10.1007/978-3-658-34752-9_11},
	booktitle = {Automatisiertes {Fahren} 2020},
	publisher = {Springer Fachmedien Wiesbaden},
	author = {Gruber, Tobias and Walz, Stefanie and Ritter, Werner and Dietmayer, Klaus},
	editor = {Bertram, Torsten},
	year = {2021},
	pages = {125--139},
}

@inproceedings{adams_design_2002,
	title = {Design of practical color filter array interpolation algorithms for digital cameras. {Part} 2.},
	url = {http://dx.doi.org/10.1109/icip.1998.723540},
	booktitle = {Proceedings 1998 {International} {Conference} on {Image} {Processing}. {ICIP98} ({Cat}. {No}.{98CB36269})},
	publisher = {IEEE Comput. Soc},
	author = {Adams, J E},
	year = {2002},
}

@misc{hamilton_adaptive_1997,
	title = {Adaptive color plan interpolation in single sensor color electronic camera},
	url = {https://patents.google.com/patent/US5629734A/en},
	author = {Hamilton, Jr, John F and Adams, Jr, James E},
	month = may,
	year = {1997},
	note = {Issue: 5629734
Publication Title: US Patent},
}

@misc{wikipedia_contributors_bayer_2024,
	title = {Bayer filter},
	url = {https://en.wikipedia.org/wiki/Bayer_filter},
	publisher = {Wikimedia Foundation, Inc.},
	author = {{Wikipedia contributors}},
	month = jun,
	year = {2024},
	note = {Publication Title: Wikipedia, The Free Encyclopedia},
}

@misc{canon_canon_nodate,
	title = {{CANON} 19 {Pro} {5G}},
	url = {https://tinyurl.com/mwssb3uv},
	language = {en},
	author = {{Canon}},
	annote = {Accessed: 2024-8-2},
}

@misc{oppo_oppo_nodate,
	title = {{OPPO} {Unveils} {Multiple} {Innovative} {Imaging} {Technologies}},
	url = {https://tinyurl.com/yc8rz4nm},
	language = {en},
	author = {{Oppo}},
	annote = {Accessed: 2024-8-2},
}

@misc{vivo_vivo_nodate,
	title = {Vivo {X80} is the only vivo smartphone with a {Sony} {IMX866} {Sensor}: {The} {World}’s {First} {RGBW} {Bottom} {Sensors}},
	url = {https://tinyurl.com/ys22xkwr},
	language = {en},
	author = {{Vivo}},
	annote = {Accessed: 2024-8-2},
}

@article{menon_color_2011,
	title = {Color image demosaicking: {An} overview},
	volume = {26},
	url = {https://www.sciencedirect.com/science/article/pii/S0923596511000415},
	language = {en},
	number = {8-9},
	journal = {Signal Process. Image Commun.},
	author = {Menon, Daniele and Calvagno, Giancarlo},
	month = oct,
	year = {2011},
	note = {Publisher: Elsevier BV},
	pages = {518--533},
}

@inproceedings{innocent_pixel_2019,
	title = {Pixel with nested photo diodes and 120 {dB} single exposure dynamic range},
	url = {https://imagesensors.org/Past%20Workshops/2019%20Workshop/2019%20Papers/P13.pdf},
	booktitle = {International image sensors workshop},
	author = {Innocent, M and Rodríguez, Ángel D and Guruaribam, Debashree and Rahman, M and Sulfridge, Marc and Borthakur, S and Gravelle, B and Goto, T and Dougherty, Nathan and Desjardin, Bill and Sabo, David and Mlinar, Marko and Geurts, T},
	year = {2019},
	pages = {95--98},
}

@article{iida_30_2023,
	title = {A 3.0 µm pixels and 1.5 µm pixels combined complementary metal-oxide semiconductor image sensor for high dynamic range vision beyond 106 {dB}},
	volume = {23},
	url = {https://www.mdpi.com/1424-8220/23/21/8998},
	language = {en},
	number = {21},
	journal = {Sensors (Basel)},
	author = {Iida, Satoko and Kawamata, Daisuke and Sakano, Yorito and Yamanaka, Takaya and Nabeyoshi, Shohei and Matsuura, Tomohiro and Toshida, Masahiro and Baba, Masahiro and Fujimori, Nobuhiko and Basavalingappa, Adarsh and Han, Sungin and Katayama, Hidetoshi and Azami, Junichiro},
	month = nov,
	year = {2023},
	note = {Publisher: MDPI AG},
	keywords = {automotive, HDR, high definition, LFM, motion artifact, motion blur, quadrate-square pixel, Ta-Kuchi pixel},
	pages = {8998},
}

@incollection{lin_microsoft_2014,
	address = {Cham},
	series = {Lecture notes in computer science},
	title = {Microsoft {COCO}: {Common} objects in context},
	booktitle = {Computer {Vision} – {ECCV} 2014},
	publisher = {Springer International Publishing},
	author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Dollár, Piotr and Zitnick, C Lawrence},
	year = {2014},
	pages = {740--755},
}

@misc{global_canon_nodate,
	title = {Canon develops {CMOS} sensor for monitoring applications with industry-leading dynamic range, automatic exposure optimization function for each sensor area that improves accuracy for recognizing moving subjects},
	url = {https://global.canon/en/news/2023/20230112.html},
	language = {en},
	author = {Global, Canon},
	note = {Publication Title: Canon Global},
	annote = {Accessed: 2024-6-18},
}

@article{lyu_rtmdet_2022,
	title = {{RTMDet}: {An} empirical study of designing real-time object detectors},
	url = {http://arxiv.org/abs/2212.07784},
	journal = {arXiv [cs.CV]},
	author = {Lyu, Chengqi and Zhang, Wenwei and Huang, Haian and Zhou, Yue and Wang, Yudong and Liu, Yanyi and Zhang, Shilong and Chen, Kai},
	month = dec,
	year = {2022},
}

@inproceedings{zamir_restormer_2022,
	title = {Restormer: {Efficient} transformer for high-resolution image restoration},
	booktitle = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition},
	author = {Zamir, Syed Waqas and Arora, Aditya and Khan, Salman and Hayat, Munawar and Khan, Fahad Shahbaz and Yang, Ming-Hsuan},
	year = {2022},
	pages = {5728--5739},
}

@inproceedings{zhang_deep_2022,
	title = {Deep spatial adaptive network for real image demosaicing},
	volume = {36},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Zhang, Tao and Fu, Ying and Li, Cheng},
	year = {2022},
	note = {Issue: 3},
	pages = {3326--3334},
}

@inproceedings{xing_end--end_2021,
	title = {End-to-end learning for joint image demosaicing, denoising and super-resolution},
	booktitle = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition},
	author = {Xing, Wenzhu and Egiazarian, Karen},
	year = {2021},
	pages = {3507--3516},
}

@inproceedings{liu_joint_2020,
	title = {Joint demosaicing and denoising with self guidance},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Liu, Lin and Jia, Xu and Liu, Jianzhuang and Tian, Qi},
	year = {2020},
	pages = {2240--2249},
}

@inproceedings{honda_high_2007,
	title = {High sensitivity color {CMOS} image sensor with {WRGB} color filter array and color separation process using edge detection},
	url = {https://www.academia.edu/download/30168052/068_honda.pdf},
	booktitle = {Proc. {Int}. {Image} {Sens}. {Workshop}},
	author = {Honda, H and Iida, Y and Egawa, Y},
	year = {2007},
	pages = {263--266},
}

@misc{corporation_high_nodate,
	title = {High {Dynamic} {Range} ({HDR}) {Technology} for {Mobile}},
	url = {https://www.sony-semicon.com/en/technology/mobile/hdr.html},
	language = {en},
	author = {Corporation, Sony},
	note = {Publication Title: Sony Semiconductor Solutions Group},
	annote = {Accessed: 2024-6-22},
}

@misc{dipert_split_nodate,
	title = {Split pixel technology},
	url = {https://www.edge-ai-vision.com/2023/08/everything-you-need-to-know-about-split-pixel-hdr-technology/},
	author = {Dipert, Brian},
	annote = {Accessed: 2024-5-11},
}

@misc{kumar_split_nodate,
	title = {Split pixel explanation},
	url = {https://www.e-consystems.com/blog/camera/technology/everything-you-need-to-know-about-split-pixel-hdr-technology/},
	author = {Kumar, Prabu},
	annote = {Accessed: 2024-6-22},
}

@misc{labs_imx490_2022,
	title = {{IMX490}: {On}-{Sensor} {HDR} and {Flicker} {Mitigation} for {High} {Contrast} {Scenes}},
	url = {https://thinklucid.com/tech-briefs/sony-imx490-hdr-sensor-and-flicker-mitigation/},
	language = {en},
	publisher = {LUCID Vision Labs},
	author = {Labs, Lucid Vision},
	month = jan,
	year = {2022},
	note = {Publication Title: LUCID Vision Labs - Modern Machine Vision Cameras},
	annote = {Accessed: 2024-6-22},
}

@inproceedings{solhusvik_1280x960_2019,
	title = {A 1280x960 2.8μm {HDR} {CIS} with {DCG} and {Split}-{Pixel} {Combined}},
	url = {https://www.imagesensors.org/Past%20Workshops/2019%20Workshop/2019%20Papers/R32.pdf},
	booktitle = {International {Image} {Sensors} {Workshop}},
	author = {Solhusvik, Johannes and Willassen, Trygve and Mikkelsen, Sindre and Wilhelmsen, Mathias and Manabe, Sohei and Mao, Duli and He, Zhaoyu and Mabuchi, Keiji and Hasegawa, Takuma},
	year = {2019},
}

@misc{willassen_1280x1080_2015,
	title = {A 1280x1080 μm {Split}-diode {Pixel} {HDR} {Sensor} in 110nm {BSI} {CMOS} {Process}},
	url = {https://www.imagesensors.org/Past%20Workshops/2015%20Workshop/2015%20Papers/Sessions/Session_13/13-01_Willassen.pdf},
	author = {Willassen, Trygve and Solhusvik, Johannes and Johansson, Robert and Yaghmai, Sohrab and Rhodes, Howard and Manabe, Sohei and Mao, Duli and Lin, Zhiqiang and Yang, Dajiang and Cellek, Orkun and Webster, Eric and Ma, Siguang and Zhang, Bowei},
	year = {2015},
	annote = {Accessed: 2023-11-21},
}

@article{sakaridis_guided_2019,
	title = {Guided curriculum model adaptation and uncertainty-aware evaluation for semantic nighttime image segmentation},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/papers/Sakaridis_Guided_Curriculum_Model_Adaptation_and_Uncertainty-Aware_Evaluation_for_Semantic_Nighttime_ICCV_2019_paper.pdf},
	journal = {ICCV},
	author = {Sakaridis, Christos and Dai, Dengxin and Gool, L},
	month = jan,
	year = {2019},
	pages = {7373--7382},
}

@article{dai_flare7k_2023,
	title = {{Flare7K}++: {Mixing} {Synthetic} and {Real} {Datasets} for {Nighttime} {Flare} {Removal} and {Beyond}},
	url = {http://arxiv.org/abs/2306.04236},
	journal = {arXiv [cs.CV]},
	author = {Dai, Yuekun and Li, Chongyi and Zhou, Shangchen and Feng, Ruicheng and Luo, Yihang and Loy, Chen Change},
	month = jun,
	year = {2023},
}

@article{wang_select_2023,
	title = {Select {Informative} {Samples} for {Night}-{Time} {Vehicle} {Detection} {Benchmark} in {Urban} {Scenes}},
	volume = {15},
	url = {https://www.mdpi.com/2072-4292/15/17/4310},
	language = {en},
	number = {17},
	journal = {Remote Sensing},
	author = {Wang, Xiao and Tu, Xingyue and Al-Hassani, Baraa and Lin, Chia-Wen and Xu, Xin},
	month = aug,
	year = {2023},
	note = {Publisher: Multidisciplinary Digital Publishing Institute},
	pages = {4310},
}

@inproceedings{parmar_interleaved_2009,
	title = {Interleaved imaging: an imaging system design inspired by rod-cone vision},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/7250/725008/Interleaved-imaging--an-imaging-system-design-inspired-by-rod/10.1117/12.806367.short},
	booktitle = {Digital {Photography} {V}},
	publisher = {SPIE},
	author = {Parmar, Manu and Wandell, Brian A},
	editor = {Rodricks, Brian G and Süsstrunk, Sabine E},
	month = jan,
	year = {2009},
}

@misc{wikipedia_contributors_multi-exposure_2024,
	title = {Multi-exposure {HDR} capture},
	url = {https://en.wikipedia.org/w/index.php?title=Multi-exposure_HDR_capture&oldid=1215265940},
	author = {{Wikipedia contributors}},
	month = mar,
	year = {2024},
	note = {Publication Title: Wikipedia, The Free Encyclopedia},
	annote = {Accessed: NA-NA-NA},
}

@article{nayar_computational_2006,
	title = {Computational {Cameras}: {Redefining} the {Image}},
	volume = {39},
	url = {http://dx.doi.org/10.1109/MC.2006.258},
	number = {8},
	journal = {Computer},
	author = {Nayar, S K},
	month = aug,
	year = {2006},
	note = {Publisher: IEEE},
	pages = {30--38},
}

@inproceedings{nayar_high_2000,
	title = {High dynamic range imaging: spatially varying pixel exposures},
	volume = {1},
	url = {http://dx.doi.org/10.1109/CVPR.2000.855857},
	booktitle = {Proceedings {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}. {CVPR} 2000 ({Cat}. {No}.{PR00662})},
	publisher = {IEEE},
	author = {Nayar, S K and Mitsunaga, T},
	year = {2000},
	pages = {472--479 vol.1},
}

@article{yang_mipi_2022,
	title = {{MIPI} 2022 {Challenge} on {RGBW} {Sensor} {Re}-mosaic: {Dataset} and {Report}},
	url = {https://github.com/mipi-},
	journal = {arXiv [cs.CV]},
	author = {Yang, Qingyu and Yang, Guang and Jiang, Jun and Li, Chongyi and Feng, Ruicheng and Zhou, Shangchen and Sun, Wenxiu and Zhu, Qingpeng and Loy, Chen Change and Gu, Jinwei},
	month = sep,
	year = {2022},
}

@book{ward_high_2005,
	title = {High dynamic range imaging},
	url = {http://www.anyhere.com/gward/papers/cic01.pdf},
	publisher = {Morgan Kaufmann},
	author = {Ward, G and Pattanaik, S and Debevec, P E},
	year = {2005},
}

@article{banterle_advanced_2011,
	title = {Advanced high dynamic range imaging: {Theory} and practice},
	url = {https://www.taylorfrancis.com/books/mono/10.1201/9781315119526/advanced-high-dynamic-range-imaging-alan-chalmers-kurt-debattista-francesco-banterle-alessandro-artusi},
	author = {Banterle, F and Artusi, A and Debattista, Kurt and Chalmers, A},
	month = feb,
	year = {2011},
	note = {Publisher: taylorfrancis.com},
}

@article{el_gamal_cmos_2005,
	title = {{CMOS} image sensors},
	volume = {21},
	url = {http://dx.doi.org/10.1109/MCD.2005.1438751},
	number = {3},
	journal = {IEEE Circuits Devices Mag.},
	author = {El Gamal, A and Eltoukhy, H},
	month = may,
	year = {2005},
	keywords = {Sensor arrays, CMOS image sensors, Image sensors, Circuits, CMOS technology, Color, Image converters, Optical arrays, Optical imaging, Pixel},
	pages = {6--20},
}

@article{wandell_common_2002,
	title = {Common principles of image acquisition systems and biological vision},
	volume = {90},
	url = {http://dx.doi.org/10.1109/5.982401},
	number = {1},
	journal = {Proc. IEEE},
	author = {Wandell, B A and El Gamal, A and Girod, B},
	month = jan,
	year = {2002},
	note = {Publisher: IEEE},
	pages = {5--17},
}

@inproceedings{wandell_multiple_1999,
	title = {Multiple {Capture} {Single} {Image} {Architecture} with a {CMOS} {Sensor}},
	url = {https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=8d79544d693d8af1cfb7037ca05b9cfd2e7e1aab},
	booktitle = {Proc. {Chiba} {Conf}. {Multispectral} {Imaging}},
	author = {Wandell, B A and Catrysse, P and DiCarlo, Jeffrey M and Yang, D and El Gamal, A},
	year = {1999},
	note = {Place: Chiba, Japan},
	pages = {1--7},
}

@article{tan_night-time_2021,
	title = {Night-{Time} {Scene} {Parsing} {With} a {Large} {Real} {Dataset}},
	volume = {30},
	url = {http://dx.doi.org/10.1109/TIP.2021.3122004},
	language = {en},
	journal = {IEEE Trans. Image Process.},
	author = {Tan, Xin and Xu, Ke and Cao, Ying and Zhang, Yiheng and Ma, Lizhuang and Lau, Rynson W H},
	month = nov,
	year = {2021},
	pages = {9085--9098},
}

@inproceedings{van_bastelaer_glare_2023,
	title = {Glare {Removal} for {Astronomical} {Images} with {High} {Local} {Dynamic} {Range}},
	url = {http://dx.doi.org/10.1109/ICCP56744.2023.10233804},
	booktitle = {2023 {IEEE} {International} {Conference} on {Computational} {Photography} ({ICCP})},
	publisher = {IEEE},
	author = {Van Bastelaer, Max-Olivier and Kremer, Heiner and Volchkov, Valentin and Passy, Jean-Claude and Schölkopf, Bernhard},
	month = jul,
	year = {2023},
	pages = {1--11},
}

@book{pattanaik_high_2014,
	edition = {2},
	series = {Morgan {Kaufmann} {Series} in {Computer} {Graphics}]},
	title = {High dynamic range imaging: {Acquisition}, display, and image-based lighting},
	url = {http://www.anyhere.com/gward/papers/cic01.pdf},
	publisher = {Morgan Kaufmann},
	author = {Pattanaik, Sumanta and Reinhard, Erik and Heidrich, Wolfgang and Debevec, Paul and Ward, Greg and Myszkowski, Karol},
	month = may,
	year = {2014},
}

@article{hasinoff_burst_2016,
	title = {Burst photography for high dynamic range and low-light imaging on mobile cameras},
	volume = {35},
	url = {https://doi.org/10.1145/2980179.2980254},
	number = {6},
	journal = {ACM Trans. Graph.},
	author = {Hasinoff, Samuel W and Sharlet, Dillon and Geiss, Ryan and Adams, Andrew and Barron, Jonathan T and Kainz, Florian and Chen, Jiawen and Levoy, Marc},
	month = dec,
	year = {2016},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {computational photography, high dynamic range},
	pages = {1--12},
}

@article{zhou_improving_2023,
	title = {Improving lens flare removal with general-purpose pipeline and multiple light sources recovery},
	url = {http://openaccess.thecvf.com/content/ICCV2023/html/Zhou_Improving_Lens_Flare_Removal_with_General-Purpose_Pipeline_and_Multiple_Light_ICCV_2023_paper.html},
	journal = {ICCV},
	author = {Zhou, Yuyan and Liang, Dong and Chen, Songcan and Huang, Shengjian and Yang, Shuo and Li, Chongyi},
	month = aug,
	year = {2023},
	pages = {12923--12933},
}

@misc{li_compressing_2005,
	title = {Compressing and companding high dynamic range images with subband architectures},
	url = {http://dx.doi.org/10.1145/1186822.1073271},
	author = {Li, Yuanzhen and Sharan, Lavanya and Adelson, Edward H},
	year = {2005},
	note = {Publication Title: ACM SIGGRAPH 2005 Papers},
}

@article{jiang_enlightengan_2021,
	title = {{EnlightenGAN}: {Deep} {Light} {Enhancement} {Without} {Paired} {Supervision}},
	volume = {30},
	url = {http://dx.doi.org/10.1109/TIP.2021.3051462},
	language = {en},
	journal = {IEEE Trans. Image Process.},
	author = {Jiang, Yifan and Gong, Xinyu and Liu, Ding and Cheng, Yu and Fang, Chen and Shen, Xiaohui and Yang, Jianchao and Zhou, Pan and Wang, Zhangyang},
	month = jan,
	year = {2021},
	pages = {2340--2349},
}

@inproceedings{liu_single-image_2020,
	title = {Single-image {HDR} reconstruction by learning to reverse the camera pipeline},
	url = {http://openaccess.thecvf.com/content_CVPR_2020/html/Liu_Single-Image_HDR_Reconstruction_by_Learning_to_Reverse_the_Camera_Pipeline_CVPR_2020_paper.html},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Liu, Yu-Lun and Lai, Wei-Sheng and Chen, Yu-Sheng and Kao, Yi-Lung and Yang, Ming-Hsuan and Chuang, Yung-Yu and Huang, Jia-Bin},
	year = {2020},
	pages = {1651--1660},
}

@misc{xu_cdada_2021,
	title = {{CDAda}: {A} {Curriculum} {Domain} {Adaptation} for {Nighttime} {Semantic} {Segmentation}},
	url = {http://dx.doi.org/10.1109/iccvw54120.2021.00331},
	author = {Xu, Qi and Ma, Yinan and Wu, Jing and Long, Chengnian and Huang, Xiaolin},
	year = {2021},
	note = {Publication Title: 2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)},
}

@inproceedings{zhu_unpaired_2017,
	title = {Unpaired image-to-image translation using cycle-consistent adversarial networks},
	url = {http://openaccess.thecvf.com/content_iccv_2017/html/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.html},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A},
	month = oct,
	year = {2017},
	note = {Place: Venice},
	pages = {2223--2232},
}

@article{lin_gan-based_2021,
	title = {{GAN}-{Based} {Day}-to-{Night} {Image} {Style} {Transfer} for {Nighttime} {Vehicle} {Detection}},
	volume = {22},
	url = {http://dx.doi.org/10.1109/TITS.2019.2961679},
	number = {2},
	journal = {IEEE Trans. Intell. Transp. Syst.},
	author = {Lin, Che-Tsung and Huang, Sheng-Wei and Wu, Yen-Yi and Lai, Shang-Hong},
	month = feb,
	year = {2021},
	keywords = {Detectors, Object detection, Training, domain adaptation, Feature extraction, Gallium nitride, generative adversarial network, Image segmentation, image-to-image translation, semantic segmentation, Vehicle detection},
	pages = {951--963},
}

@misc{li_learning_2021,
	title = {Learning to {Enhance} {Low}-{Light} {Image} via {Zero}-{Reference} {Deep} {Curve} {Estimation}},
	url = {http://dx.doi.org/10.1109/tpami.2021.3063604},
	author = {Li, Chongyi and Guo, Chunle and Chen, Change Loy},
	year = {2021},
	note = {Pages: 1–1
Publication Title: IEEE Transactions on Pattern Analysis and Machine Intelligence},
}

@inproceedings{ronneberger_u-net_2015,
	title = {U-net: {Convolutional} networks for biomedical image segmentation},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention}–{MICCAI} 2015: 18th {International} {Conference}, {Munich}, {Germany}, {October} 5-9, 2015, {Proceedings}, {Part} {III} 18},
	publisher = {Springer},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	year = {2015},
	pages = {234--241},
}

@article{liu_using_2023,
	title = {Using simulation to quantify the performance of automotive perception systems},
	url = {http://arxiv.org/abs/2303.00983},
	author = {Liu, Zhenyi and Shah, Devesh and Rahimpour, Alireza and Upadhyay, Devesh and Farrell, Joyce and Wandell, Brian A},
	month = mar,
	year = {2023},
	note = {\_eprint: 2303.00983},
}

@misc{noauthor_dense_2022,
	title = {{DENSE} datasets - {Ulm} university},
	url = {https://www.uni-ulm.de/en/in/driveu/projects/dense-datasets/},
	language = {en},
	month = mar,
	year = {2022},
	annote = {Accessed: 2022-6-22},
}

@inproceedings{punnappurath_day--night_2022,
	title = {Day-to-night image synthesis for training nighttime neural {ISPs}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/papers/Punnappurath_Day-to-Night_Image_Synthesis_for_Training_Nighttime_Neural_ISPs_CVPR_2022_paper.pdf},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Punnappurath, Abhijith and Abuolaim, Abdullah and Abdelhamed, Abdelrahman and Levinshtein, Alex and Brown, Michael S},
	month = jun,
	year = {2022},
	note = {Place: New Orleans, LA, USA},
}

@inproceedings{sharma_nighttime_2021,
	title = {Nighttime {Visibility} {Enhancement} by {Increasing} the {Dynamic} {Range} and {Suppression} of {Light} {Effects}},
	url = {http://dx.doi.org/10.1109/CVPR46437.2021.01180},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Sharma, Aashish and Tan, Robby T},
	month = jun,
	year = {2021},
	keywords = {Computer vision, Cameras, Dynamic range, Hafnium, Low-frequency noise, Pattern recognition, Semisupervised learning},
	pages = {11972--11981},
}

@article{weng_all--one_2020,
	title = {All-in-one drive: {A} large-scale comprehensive perception dataset with high-density long-range point clouds},
	url = {https://www.researchgate.net/profile/Xinshuo-Weng/publication/347112693_All-In-One_Drive_A_Large-Scale_Comprehensive_Perception_Dataset_with_High-Density_Long-Range_Point_Clouds/links/5fd8156492851c13fe8925e8/All-In-One-Drive-A-Large-Scale-Comprehensive-Perception-Dataset-with-High-Density-Long-Range-Point-Clouds.pdf},
	journal = {arXiv},
	author = {Weng, Xinshuo and Man, Yunze and Cheng, Dazhi and Park, Jinhyung and O'Toole, Matthew and Kitani, Kris and Wang, Jianren and Held, David},
	year = {2020},
}

@inproceedings{sun_see_2019,
	title = {See clearer at night: towards robust nighttime semantic segmentation through day-night image conversion},
	volume = {11169},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11169/111690A/See-clearer-at-night--towards-robust-nighttime-semantic-segmentation/10.1117/12.2532477.short},
	language = {en},
	booktitle = {Artificial {Intelligence} and {Machine} {Learning} in {Defense} {Applications}},
	publisher = {SPIE},
	author = {Sun, Lei and Wang, Kaiwei and Yang, Kailun and Xiang, Kaite},
	month = sep,
	year = {2019},
	keywords = {Convolutional Neural Networks, Generative Adversarial Networks, Intelligent Vehicles, Semantic Segmentation},
	pages = {77--89},
}

@inproceedings{dai_dark_2018,
	title = {Dark {Model} {Adaptation}: {Semantic} {Image} {Segmentation} from {Daytime} to {Nighttime}},
	url = {http://dx.doi.org/10.1109/ITSC.2018.8569387},
	booktitle = {2018 21st {International} {Conference} on {Intelligent} {Transportation} {Systems} ({ITSC})},
	author = {Dai, Dengxin and Van Gool, Luc},
	month = nov,
	year = {2018},
	keywords = {Adaptation models, Image segmentation, Cameras, Mathematical model, Meteorology, Roads, Semantics},
	pages = {3819--3824},
}

@misc{wu_one-stage_2023,
	title = {A {One}-{Stage} {Domain} {Adaptation} {Network} {With} {Image} {Alignment} for {Unsupervised} {Nighttime} {Semantic} {Segmentation}},
	url = {http://dx.doi.org/10.1109/tpami.2021.3138829},
	author = {Wu, Xinyi and Wu, Zhenyao and Ju, Lili and Wang, Song},
	year = {2023},
	note = {Issue: 1
Pages: 58–72
Publication Title: IEEE Transactions on Pattern Analysis and Machine Intelligence
Volume: 45},
}

@inproceedings{qiao_light_2021,
	title = {Light source guided single-image flare removal from unpaired data},
	url = {http://openaccess.thecvf.com/content/ICCV2021/html/Qiao_Light_Source_Guided_Single-Image_Flare_Removal_From_Unpaired_Data_ICCV_2021_paper.html},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Qiao, Xiaotian and Hancke, Gerhard P and Lau, Rynson W H},
	month = oct,
	year = {2021},
	note = {Place: Montreal, QC, Canada},
	pages = {4177--4185},
}

@inproceedings{sakaridis_acdc_2021,
	title = {{ACDC}: {The} adverse conditions dataset with correspondences for semantic driving scene understanding},
	url = {https://openaccess.thecvf.com/content/ICCV2021/papers/Sakaridis_ACDC_The_Adverse_Conditions_Dataset_With_Correspondences_for_Semantic_Driving_ICCV_2021_paper.pdf},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Sakaridis, Christos and Dai, Dengxin and Van Gool, Luc},
	month = oct,
	year = {2021},
	note = {Place: Montreal, QC, Canada},
}

@article{talvala_veiling_2007,
	title = {Veiling glare in high dynamic range imaging},
	volume = {26},
	url = {https://doi.org/10.1145/1276377.1276424},
	number = {3},
	journal = {ACM Trans. Graph.},
	author = {Talvala, Eino-Ville and Adams, Andrew and Horowitz, Mark and Levoy, Marc},
	month = jul,
	year = {2007},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {computational photography, global illumination, HDR imaging, structured occlusion mask, veiling glare},
	pages = {37--es},
}

@misc{sakaridis_map-guided_2022,
	title = {Map-{Guided} {Curriculum} {Domain} {Adaptation} and {Uncertainty}-{Aware} {Evaluation} for {Semantic} {Nighttime} {Image} {Segmentation}},
	url = {http://dx.doi.org/10.1109/tpami.2020.3045882},
	author = {Sakaridis, Christos and Dai, Dengxin and Van Gool, Luc},
	year = {2022},
	note = {Issue: 6
Pages: 3139–3153
Publication Title: IEEE Transactions on Pattern Analysis and Machine Intelligence
Volume: 44},
}

@article{sun_shift_2022,
	title = {{SHIFT}: {A} {Synthetic} {Driving} {Dataset} for {Continuous} {Multi}-{Task} {Domain} {Adaptation}},
	url = {http://arxiv.org/abs/2206.08367},
	author = {Sun, Tao and Segu, Mattia and Postels, Janis and Wang, Yuxuan and Van Gool, Luc and Schiele, Bernt and Tombari, Federico and Yu, Fisher},
	month = jun,
	year = {2022},
	note = {\_eprint: 2206.08367},
}

@misc{vistalabstanfordedu_isethdrsensor_2024,
	title = {{ISETHdrsensor}},
	url = {https://github.com/ISET/isethdrsensor},
	author = {{Vistalab.stanford.edu}},
	year = {2024},
}

@misc{vistalabstanfordedu_isetcam_2022,
	title = {{ISETCam}},
	url = {https://github.com/ISET/isetcam},
	author = {{Vistalab.stanford.edu}},
	year = {2022},
}

@misc{vistalabstanfordedu_iset3d_2022,
	title = {{ISET3d}},
	url = {https://github.com/ISET/iset3d},
	author = {{Vistalab.stanford.edu}},
	year = {2022},
}

@misc{bui_deep_2020,
	title = {Deep {Learning} {Based} {Semantic} {Segmentation} for {Nighttime} {Image}},
	url = {http://dx.doi.org/10.1109/gtsd50082.2020.9303171},
	author = {Bui, Hien T T and Le, Duy H and Nguyen, Thu T A and Pham, Tuan V},
	year = {2020},
	note = {Publication Title: 2020 5th International Conference on Green Technology and Sustainable Development (GTSD)},
}

@misc{noauthor_specular_nodate,
	title = {Specular {Reflection} and {Transmission}},
	url = {https://www.pbr-book.org/3ed-2018/Reflection_Models/Specular_Reflection_and_Transmission},
	language = {en},
	annote = {Accessed: 2022-8-5},
}

@misc{noauthor_isetcornellbox_nodate,
	title = {{ISETcornellbox}},
	url = {https://github.com/ISET/isetcornellbox},
}

@inproceedings{gaidon_virtualworlds_2016,
	title = {{VirtualWorlds} as {Proxy} for {Multi}-object {Tracking} {Analysis}},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Gaidon, Adrien and Wang, Qiao and Cabon, Yohann and Vig, Eleonora},
	month = jun,
	year = {2016},
	keywords = {Computer vision, Training, Cameras, Benchmark testing, Cloning, Computational modeling, Three-dimensional displays},
	pages = {4340--4349},
}

@misc{pharr_pbrt_nodate,
	title = {{PBRT}: {Version} 4},
	url = {https://github.com/mmp/pbrt-v4},
	author = {Pharr, Matt and Jakob, Wenzel and Humphreys, Greg},
	annote = {Accessed: 2022-05-05},
}

@article{hossain_autonomous-driving_2019,
	title = {Autonomous-{Driving} {Vehicle} {Learning} {Environments} using {Unity} {Real}-time {Engine} and {End}-to-{End} {CNN} {Approach}},
	volume = {14},
	language = {en},
	number = {2},
	journal = {The Journal of Korea Robotics Society},
	author = {Hossain, Sabir and Lee, Deok-Jin},
	year = {2019},
	note = {Publisher: Korea Robotics Society},
	keywords = {Artificial Intelligence, Autonomous Shuttle Vehicle, Behavior Learning, Virtual Environment},
	pages = {122--130},
}

@misc{zhang_physically-based_nodate,
	title = {Physically-based rendering for indoor scene understanding using convolutional neural networks. {CoRR} abs/1612.07429 (2016)},
	author = {Zhang, Y and Song, S and Yumer, E and Savva, M and Lee, J and Jin, H and Funkhouser, TA},
}

@inproceedings{son_simulation-based_2019,
	title = {Simulation-based testing framework for autonomous driving development},
	volume = {1},
	booktitle = {2019 {IEEE} {International} {Conference} on {Mechatronics} ({ICM})},
	publisher = {IEEE},
	author = {Son, Tong Duy and Bhave, Ajinkya and Van der Auweraer, Herman},
	year = {2019},
	pages = {576--583},
}

@inproceedings{ros_synthia_2016,
	title = {The {SYNTHIA} {Dataset}: {A} {Large} {Collection} of {Synthetic} {Images} for {Semantic} {Segmentation} of {Urban} {Scenes}},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Ros, German and Sellart, Laura and Materzynska, Joanna and Vazquez, David and Lopez, Antonio M},
	month = jun,
	year = {2016},
	keywords = {Computer vision, Training, Image segmentation, Semantics, Context, Urban areas, Visualization},
	pages = {3234--3243},
}

@inproceedings{hossain_caias_2019,
	title = {{CAIAS} {Simulator}: {Self}-driving {Vehicle} {Simulator} for {AI} {Research}},
	booktitle = {Intelligent {Computing} \& {Optimization}},
	publisher = {Springer International Publishing},
	author = {Hossain, Sabir and Fayjie, Abdur R and Doukhi, Oualid and Lee, Deok-Jin},
	year = {2019},
	pages = {187--195},
}

@misc{noauthor_dirsigtextsuperscripttm_nodate,
	title = {{DIRSIG}{\textbackslash}{textsuperscriptTM} = {Digital} {Imaging} and {Remote} {Sensing} {Image} {Generation}},
	url = {http://dirsig.cis.rit.edu/},
	annote = {Accessed: 2022-05-05},
}

@article{tsirikoglou_procedural_2017,
	title = {Procedural modeling and physically based rendering for synthetic data generation in automotive applications},
	journal = {arXiv preprint arXiv:1710.06270},
	author = {Tsirikoglou, Apostolia and Kronander, Joel and Wrenninge, Magnus and Unger, Jonas},
	year = {2017},
}

@article{huck_image_1985,
	title = {Image gathering and processing: information and fidelity},
	volume = {2},
	number = {10},
	journal = {JOSA A},
	author = {Huck, Friedrich O and Fales, Carl L and Halyo, Nesim and Samms, Richard W and Stacy, Kathryn},
	year = {1985},
	note = {Publisher: Optical Society of America},
	pages = {1644--1666},
}

@article{cathey_image_1984,
	title = {Image gathering and processing for enhanced resolution},
	volume = {1},
	number = {3},
	journal = {JOSA A},
	author = {Cathey, W Thomas and Frieden, B Roy and Rhodes, William T and Rushforth, Craig K},
	year = {1984},
	note = {Publisher: Optical Society of America},
	pages = {241--250},
}

@article{huck_image_1976,
	title = {Image quality prediction: {An} aid to the {Viking} {Lander} imaging investigation on {Mars}},
	volume = {15},
	number = {7},
	journal = {Applied optics},
	author = {Huck, FO and Wall, SD},
	year = {1976},
	note = {Publisher: Optical Society of America},
	pages = {1748--1766},
}

@article{zhang_study_2007,
	title = {Study of the performance of a novel 1 mm resolution dual-panel {PET} camera design dedicated to breast cancer imaging using {Monte} {Carlo} simulation},
	volume = {34},
	language = {en},
	number = {2},
	journal = {Med. Phys.},
	author = {Zhang, Jin and Olcott, Peter D and Chinn, Garry and Foudray, Angela M K and Levine, Craig S},
	month = feb,
	year = {2007},
	note = {Publisher: Wiley},
	pages = {689--702},
}

@article{etxebeste_ccmod_2020,
	title = {{CCMod}: a {GATE} module for {Compton} camera imaging simulation},
	volume = {65},
	language = {en},
	number = {5},
	journal = {Phys. Med. Biol.},
	author = {Etxebeste, A and Dauvergne, D and Fontana, M and Létang, J M and Llosá, G and Munoz, E and Oliver, J F and Testa, É and Sarrut, D},
	month = feb,
	year = {2020},
	note = {Publisher: iopscience.iop.org},
	pages = {055004},
}

@article{kang_test_2019,
	title = {Test your self-driving algorithm: {An} overview of publicly available driving datasets and virtual testing environments},
	volume = {4},
	number = {2},
	journal = {IEEE Transactions on Intelligent Vehicles},
	author = {Kang, Yue and Yin, Hang and Berger, Christian},
	year = {2019},
	note = {Publisher: IEEE},
	pages = {171--185},
}

@article{booth_design_1977,
	title = {Design considerations for digital image processing systems},
	volume = {10},
	number = {8},
	journal = {Computer},
	author = {Booth, John M and Schroeder, John B},
	year = {1977},
	note = {Publisher: IEEE},
	pages = {15--20},
}

@article{huck_spectrophotometric_1977,
	title = {Spectrophotometric and color estimates of the {Viking} lander sites},
	volume = {82},
	number = {28},
	journal = {Journal of Geophysical Research},
	author = {Huck, Friedrich O and Jobson, Daniel J and Park, Stephen K and Wall, Stephen D and Arvidson, Raymond E and Patterson, William R and Benton, William D},
	year = {1977},
	note = {Publisher: Wiley Online Library},
	pages = {4401--4411},
}

@inproceedings{lin_photo-realistic_2007,
	title = {Photo-{Realistic} {Depth}-of-{Field} {Effects} {Synthesis} {Based} on {Real} {Camera} {Parameters}},
	booktitle = {Advances in {Visual} {Computing}},
	publisher = {Springer Berlin Heidelberg},
	author = {Lin, Huei-Yung and Gu, Kai-Da},
	year = {2007},
	pages = {298--309},
}

@inproceedings{hempe_combining_2014,
	title = {Combining realistic virtual environments with real-time sensor simulations for close-to-reality testing and development in {eRobotics} applications},
	booktitle = {2014 {IEEE} {International} {Symposium} on {Robotics} and {Manufacturing} {Automation} ({ROMA})},
	publisher = {ieeexplore.ieee.org},
	author = {Hempe, Nico and Rossmann, Jürgen},
	month = dec,
	year = {2014},
	keywords = {Integrated optics, Noise, Optical buffering, Optical distortion, Optical filters, Robot sensing systems},
	pages = {1--6},
}

@inproceedings{giesler_using_2002,
	title = {Using a panoramic camera for {3D} head tracking in an {AR} environment},
	abstract = {… camera that is affixed to the AR glasses and tracks artificial environmental features. This approach, using only a single camera , … for the simulation system and one for the actual camera . …},
	booktitle = {Proc. of {Conf}. {IEEE} {Machine} {Vision} and {Mechatronics} in {Practice} ({M2VIP})},
	publisher = {researchgate.net},
	author = {Giesler, Björn and Salb, Tobias and Weyrich, T and Dillmann, R},
	year = {2002},
	pages = {1--11},
}

@patent{groh_method_2018,
	title = {Method and system for virtual sensor data generation with depth ground truth annotation},
	number = {10096158},
	author = {Groh, Alexander and Micks, Ashley Elizabeth and Murali, Vidya Nariyambut},
	month = oct,
	year = {2018},
}

@article{zhang_simulation_2020,
	title = {Simulation of a machine vision system for reflective surface defect inspection based on ray tracing},
	volume = {59},
	abstract = {A complete simulation of a machine vision system aimed at defect inspection on a reflective surface is proposed by ray tracing. The simulated scene is composed of the camera model, surface reflectance property, and light intensity distribution along with their corresponding object geometries. A virtual reflective plane geometry with scratches of various directions and pits of various sizes is built as the sample. Its realistic image is obtained by Monte Carlo ray tracing. Compared to the pinhole camera model, the camera model with a finite aperture emits more rays to deliver physical imaging. The bidirectional reflectance distribution function is applied to describe the surface reflectance property. The illustrated machine vision system captures a number of images while translating the light tubes. Then the image sequence obtained by experiment or simulation is fused to generate a well-contrasted synthetic image for defect detection. A flexible fusion method based on differential images is introduced to enhance the defect contrast on a uniform flawless background. To improve detection efficiency, defect contrast of synthetic images obtained by various fusion methods is evaluated. Influence of total image number, light tube width, and fusion interval is further discussed to optimize the inspection process. Experiments on car painted surfaces have shown that the simulated parameters can instruct the setup of the optical system and detect surface defects efficiently. The proposed simulation is capable of saving great effort in carrying out experimental trials and making improvements on reflective surface defect inspection.},
	language = {en},
	number = {8},
	journal = {Appl. Opt.},
	author = {Zhang, Pengfei and Cao, Pin and Yang, Yongying and Guo, Pan and Chen, Shiwei and Zhang, Danhui},
	month = mar,
	year = {2020},
	pages = {2656--2666},
}

@article{goossens_ray-transfer_2022,
	title = {Ray-transfer functions for camera simulation of {3D} scenes with hidden lens design},
	volume = {30},
	number = {13},
	journal = {Optics Express},
	author = {Goossens, Thomas and Lyu, Zheng and Ko, Jamyuen and Wan, Gordon C and Farrell, Joyce and Wandell, Brian},
	year = {2022},
	note = {Publisher: Optica Publishing Group},
	pages = {24031--24047},
}

@patent{connell_simulating_2020,
	title = {Simulating lenses},
	number = {10713836},
	author = {Connell, Trebor Lee},
	month = jul,
	year = {2020},
}

@patent{narayanswamy_simulating_2018,
	title = {Simulating multi-camera imaging systems},
	number = {10119809},
	author = {Narayanswamy, Ramkumar and Grover, Ginni and Nalla, Ram C},
	month = nov,
	year = {2018},
}

@article{elmquist_modeling_2021,
	title = {Modeling {Cameras} for {Autonomous} {Vehicle} and {Robot} {Simulation}: {An} {Overview}},
	volume = {21},
	number = {22},
	journal = {IEEE Sens. J.},
	author = {Elmquist, Asher and Negrut, Dan},
	month = nov,
	year = {2021},
	keywords = {robotics, Robots, Sensors, Simulation, Training, automotive, Cameras, Context modeling, imaging sensors, Pipelines, Robot vision systems},
	pages = {25547--25560},
}

@article{oconnor_model-based_2015,
	title = {Model-based defect detection on structured surfaces having optically unresolved features},
	volume = {54},
	abstract = {In this paper, we demonstrate, both numerically and experimentally, a method for the detection of defects on structured surfaces having optically unresolved features. The method makes use of synthetic reference data generated by an observational model that is able to simulate the response of the selected optical inspection system to the ideal structure, thereby providing an ideal measure of deviation from nominal geometry. The method addresses the high dynamic range challenge faced in highly parallel manufacturing by enabling the use of low resolution, wide field of view optical systems for defect detection on surfaces containing small features over large regions.},
	language = {en},
	number = {30},
	journal = {Appl. Opt.},
	author = {O'Connor, Daniel and Henning, Andrew J and Sherlock, Ben and Leach, Richard K and Coupland, Jeremy and Giusca, Claudiu L},
	month = oct,
	year = {2015},
	note = {Publisher: osapublishing.org},
	pages = {8872--8877},
}

@misc{noauthor_stanford_nodate,
	title = {The {Stanford} {3D} {Scanning} {Repository}},
	url = {http://graphics.stanford.edu/data/3Dscanrep/},
	annote = {Accessed: 2022-1-10},
}

@book{janesick_photon_2007,
	title = {Photon transfer},
	publisher = {SPIE Bellingham},
	author = {Janesick, James R},
	year = {2007},
}

@inproceedings{gruyer_modeling_2012,
	title = {Modeling and validation of a new generic virtual optical sensor for {ADAS} prototyping},
	abstract = {In the early design stages of embedded applications, it becomes necessary to have a very realistic simulation environment dedicated to the prototyping and to the evaluation of these Advanced Driving Assistance Systems (ADAS). This Numerical simulation stage is gradually becoming a strong advantage in active safety. The use of realistic numerical models enabling to substitute real data by simulated data is primordial. For such virtual platform it is mandatory to provide physics-driven road environments, virtual embedded sensors, and physics-based vehicle models. In this publication, a generic solution for cameras modelling is presented. The use of this optical sensor simulation can easily and efficiently replace real camera test campaigns. This optical sensor is very important due to the great number of applications and algorithms based on it. The presented model involves a filter mechanism in order to reproduce, in the most realistic way, the behaviour of optical sensors. The main filters used in ADAS developments will be presented. Moreover, an optical analysis of these virtual sensors has been achieved allowing the confrontation between real and simulated results. An optical platform has been developed to characterize and validate any camera, permitting to measure their performances. By comparing real and simulated sensors with this platform, this paper demonstrates this virtual platform (Pro-SiVIC™) accurately reproduces real optical sensors' behaviour.},
	booktitle = {2012 {IEEE} {Intelligent} {Vehicles} {Symposium}},
	author = {Gruyer, D and Grapinet, M and De Souza, P},
	month = jun,
	year = {2012},
	keywords = {Optical sensors, Cameras, Computational modeling, Optical distortion, Optical filters, Rendering (computer graphics)},
	pages = {969--974},
}

@article{grapinet_characterization_2013,
	title = {Characterization and simulation of optical sensors},
	volume = {60},
	abstract = {Numerical simulation is gradually becoming an advantage in active safety. This is why the development of realistic numerical models enabling to substitute real truth by simulated truth is primordial. In order to provide an accurate and cost effective solution to simulate real optical sensor behavior, the software Pro-SiVIC™ has been developed. Simulations with the software Pro-SiVIC™ can replace real tests with optical sensors and hence allow substantial cost and time savings during the development of solutions for driver assistance systems. An optical platform has been developed by IFSTTAR (French Institute of Science and Technology for Transport, Development and Networks) to characterize and validate any existing camera, in order to measure their characteristics as distortion, vignetting, focal length, etc. By comparing real and simulated sensors with this platform, this paper demonstrates that Pro-SiVIC™ accurately reproduces real sensors' behavior.},
	journal = {Accid. Anal. Prev.},
	author = {Grapinet, M and De Souza, Ph and Smal, J-C and Blosseville, J-M},
	month = nov,
	year = {2013},
	keywords = {Characterization platform, Driving assistance system, Numerical simulation, Optical sensor, Pro-SiVIC},
	pages = {344--352},
}

@misc{european_machine_vision_association_standard_nodate,
	title = {Standard for {Characterization} of {Image} {Sensors} and {Cameras}},
	author = {{European Machine Vision Association}},
	note = {Published: Standard for Characterization of Image Sensors and Cameras},
	annote = {Accessed: 2022-1-7},
}

@misc{svs-vistek_gmbh_relative_nodate,
	title = {Relative illumination and microlenses},
	url = {https://www.svs-vistek.com/en/news/svs-news-article.php?p=shading-with-cmos-cameras},
	abstract = {Sensor based shading with latest CMOS sensors like Sony IMX342 or Canon 120MXSM},
	author = {{SVS-Vistek GmbH}},
	annote = {Accessed: 2022-1-6},
}

@inproceedings{iv_color_2014,
	title = {The color of water: using underwater photography to estimatewater quality},
	volume = {9023},
	url = {https://doi.org/10.1117/12.2043132},
	doi = {10.1117/12.2043132},
	booktitle = {Digital {Photography} {X}},
	publisher = {SPIE},
	author = {IV, John Breneman and Blasinski, Henryk and Farrell, Joyce},
	editor = {Sampat, Nitin and Tezaur, Radka and Battiato, Sebastiano and Fowler, Boyd A.},
	year = {2014},
	note = {Backup Publisher: International Society for Optics and Photonics},
	keywords = {Underwater Color Correction},
	pages = {224 -- 234},
}

@inproceedings{tian_automatically_2015,
	title = {Automatically designing an image processing pipeline for a five-band camera prototype using the local, linear, learned ({L3}) method},
	volume = {9404},
	url = {https://doi.org/10.1117/12.2083435},
	doi = {10.1117/12.2083435},
	booktitle = {Digital {Photography} {XI}},
	publisher = {SPIE},
	author = {Tian, Qiyuan and Blasinski, Henryk and Lansel, Steven and Jiang, Haomiao and Fukunishi, Munenori and Farrell, Joyce E. and Wandell, Brian A.},
	editor = {Sampat, Nitin and Tezaur, Radka and Wüller, Dietmar},
	year = {2015},
	note = {Backup Publisher: International Society for Optics and Photonics},
	keywords = {camera calibration and simulation, graphics processing units, image processing pipeline, local linear learned (L3 ), parallel computing},
	pages = {18 -- 23},
}

@misc{noauthor_luxcorerender_nodate,
	title = {{LuxCoreRender}},
	url = {https://luxcorerender.org/},
}

@inproceedings{blasinski_three_2016,
	title = {A {Three} {Parameter} {Underwater} {Image} {Formation} {Model}},
	booktitle = {Digital {Photography} and {Mobile} {Imaging}},
	author = {Blasinski, Henryk and Farrell, Joyce E.},
	year = {2016},
}

@inproceedings{blasinski_model_2014,
	title = {A model for estimating spectral properties of water from {RGB} images},
	doi = {10.1109/ICIP.2014.7025122},
	booktitle = {2014 {IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	author = {Blasinski, Henryk and Breneman, John and Farrell, Joyce},
	year = {2014},
	pages = {610--614},
}

@inproceedings{tian_automating_2014,
	title = {Automating the design of image processing pipelines for novel color filter arrays: local, linear, learned ({L3}) method},
	volume = {9023},
	url = {https://doi.org/10.1117/12.2042565},
	doi = {10.1117/12.2042565},
	booktitle = {Digital {Photography} {X}},
	publisher = {SPIE},
	author = {Tian, Qiyuan and Lansel, Steven and Farrell, Joyce E. and Wandell, Brian A.},
	editor = {Sampat, Nitin and Tezaur, Radka and Battiato, Sebastiano and Fowler, Boyd A.},
	year = {2014},
	note = {Backup Publisher: International Society for Optics and Photonics},
	keywords = {Color filter array, Image processing, Image processing pipeline, Local linear learned (L3), Low light imaging},
	pages = {189 -- 196},
}

@inproceedings{lansel_local_2011,
	title = {Local {Linear} {Learned} {Image} {Processing} {Pipeline}},
	url = {http://www.osapublishing.org/abstract.cfm?URI=ISA-2011-IMC3},
	doi = {10.1364/ISA.2011.IMC3},
	abstract = {The local linear learned (L3) algorithm is presented that simultaneously performs the demosaicking, denoising, and color transform calculations of an image processing pipeline for a digital camera with any color filter array.},
	booktitle = {Imaging and {Applied} {Optics}},
	publisher = {Optical Society of America},
	author = {Lansel, Steven and Wandell, Brian A.},
	year = {2011},
	note = {Journal Abbreviation: Imaging and Applied Optics},
	keywords = {Cameras, Image processing, Color spaces, Digital image processing, Image processing algorithms, Linear filtering, Multispectral imaging},
	pages = {IMC3},
}

@misc{noauthor_iso_nodate,
	title = {{ISO} 12233:2017 {Photography} — {Electronic} still picture imaging — {Resolution} and spatial frequency responses},
}

@inproceedings{tewari_state_2020,
	title = {State of the art on neural rendering},
	volume = {39},
	booktitle = {Computer {Graphics} {Forum}},
	publisher = {Wiley Online Library},
	author = {Tewari, Ayush and Fried, Ohad and Thies, Justus and Sitzmann, Vincent and Lombardi, Stephen and Sunkavalli, Kalyan and Martin-Brualla, Ricardo and Simon, Tomas and Saragih, Jason and Nießner, Matthias and {others}},
	year = {2020},
	note = {Issue: 2},
	pages = {701--727},
}

@article{kato_differentiable_2020,
	title = {Differentiable rendering: {A} survey},
	journal = {arXiv preprint arXiv:2006.12057},
	author = {Kato, Hiroharu and Beker, Deniz and Morariu, Mihai and Ando, Takahiro and Matsuoka, Toru and Kehl, Wadim and Gaidon, Adrien},
	year = {2020},
}

@article{han_image-based_2019,
	title = {Image-based {3D} object reconstruction: {State}-of-the-art and trends in the deep learning era},
	volume = {43},
	number = {5},
	journal = {IEEE transactions on pattern analysis and machine intelligence},
	author = {Han, Xian-Feng and Laga, Hamid and Bennamoun, Mohammed},
	year = {2019},
	note = {Publisher: IEEE},
	pages = {1578--1604},
}

@inproceedings{guarnera_brdf_2016,
	title = {{BRDF} representation and acquisition},
	volume = {35},
	booktitle = {Computer {Graphics} {Forum}},
	publisher = {Wiley Online Library},
	author = {Guarnera, Darya and Guarnera, Giuseppe Claudio and Ghosh, Abhijeet and Denk, Cornelia and Glencross, Mashhuda},
	year = {2016},
	note = {Issue: 2},
	pages = {625--650},
}

@misc{noauthor_roadrunner_2023,
	title = {Roadrunner},
	url = {https://www.mathworks.com/products/roadrunner.html},
	abstract = {Design 3D scenes for automated driving simulation},
	language = {en},
	month = mar,
	year = {2023},
	annote = {Accessed: 2023-2-28},
}

@misc{noauthor_synthetic_2024,
	title = {Synthetic data for perception {AI}},
	url = {https://anyverse.ai/},
	language = {en},
	publisher = {Anyerse},
	month = feb,
	year = {2024},
	annote = {Accessed: 2024-8-2},
}

@article{georgiev_laboratory-based_2008,
	title = {Laboratory-based bidirectional reflectance distribution functions of radiometric tarps},
	volume = {47},
	number = {18},
	journal = {Applied Optics},
	author = {Georgiev, Georgi T and Butler, James J},
	year = {2008},
	note = {Publisher: Optical Society of America},
	pages = {3313--3323},
}

@misc{noauthor_luciddrive_2023,
	title = {{LucidDrive}},
	url = {https://www.synopsys.com/optical-solutions/lucidshape/luciddrive.html},
	abstract = {As a standalone night driving simulation tool, LucidDrive displays and evaluates beam patterns of vehicle headlamps under conditions that are as realistic as possible.},
	language = {en},
	month = mar,
	year = {2023},
	annote = {Accessed: 2023-2-28},
}

@misc{kamel_validating_2021,
	title = {Validating {NVIDIA} {DRIVE} {Sim} {Camera} {Models}},
	url = {https://developer.nvidia.com/blog/validating-drive-sim-camera-models/},
	abstract = {This post covers the validation of camera models in DRIVE Sim, assessing the performance elements, from rendering of the world scene to protocol simulation.},
	author = {Kamel, Carène and Reid, Ashley and Plepp, Fabian and Kamel, View All Posts by and Reid, View All Posts by and Plepp, View All Posts by},
	month = dec,
	year = {2021},
	annote = {Accessed: 2021-12-23},
}

@article{white_validation_1996,
	title = {Validation of {Rochester} {Institute} of {Technology}'s ({RIT}'s) {Digital} {Image} and {Remote} {Sensing} {Generation} ({DIRSIG}) {Model}, reflective region},
	url = {http://scholarworks.rit.edu/cgi/viewcontent.cgiarticle=4004&context=theses},
	author = {White, R},
	year = {1996},
	note = {Publisher: scholarworks.rit.edu},
}

@misc{noauthor_verification_nodate,
	title = {Verification and {Validation} {Studies}},
	url = {https://dirsig.cis.rit.edu/docs/new/validation.html},
	annote = {Accessed: 2021-12-21},
}

@inproceedings{costantini_virtual_2004,
	title = {Virtual sensor design},
	volume = {5301},
	abstract = {We present a virtual digital camera sensor, whose aim is to simulate a real (physical) image capturing sensor. To accomplish this task, the virtual sensor operates in two steps. First, it accepts a physical description of a given scene and simulates the entire process of photon sensing and charge generation in the sensor device. This process is affected by noise, mostly photon noise. Second, it adds to the image the noise that results from the electronic circuitry. We present a model for the different sources of noise relative to each sensor-based image formation step, and use measurements of real digital camera images to validate the model.},
	language = {en},
	booktitle = {Sensors and {Camera} {Systems} for {Scientific}, {Industrial}, and {Digital} {Photography} {Applications} {V}},
	publisher = {SPIE},
	author = {Costantini, Roberto and Susstrunk, Sabine},
	month = jun,
	year = {2004},
	keywords = {CCD, CMOS, digital cameras, noise},
	pages = {408--419},
}

@misc{gisgeography_13_2014,
	title = {13 {Open} {Source} {Remote} {Sensing} {Software} {Packages}},
	url = {https://gisgeography.com/open-source-remote-sensing-software-packages/},
	abstract = {There is an abundance of choice for open source remote sensing software. This list of 10 free applications describes what each one brings to the table.},
	language = {en},
	author = {{GISGeography}},
	month = jul,
	year = {2014},
	annote = {Accessed: 2021-12-21},
}

@article{toadere_simulating_2013,
	title = {Simulating the functionality of a digital camera pipeline},
	volume = {52},
	abstract = {The goal of this paper is to simulate the functionality of a digital camera system. The simulations cover the conversion from light to numerical signal, color processing, and rendering. A spectral image processing algorithm is used to simulate the radiometric properties of a digital camera. In the algorithm, we take into consideration the spectral image and the transmittances of the light source, lenses, filters, and the quantum efficiency of a complementary metal-oxide semiconductor (CMOS) image sensor. The optical part is characterized by a multiple convolution between the different point spread functions optical components such as the Cooke triplet, the aperture, the light fall off, and the optical part of the CMOS sensor. The electrical part consists of the Bayer sampling, interpolation, dynamic range, and analog to digital conversion. The reconstruction of the noisy blurred image is performed by blending different light exposed images in order to reduce the noise. Then, the image is filtered, deconvoluted, and sharpened to eliminate the noise and blur. Next, we have the color processing and rendering blocks interpolation, white balancing, color correction, conversion from XYZ color space to LAB color space, and, then, into the RGB color space, the color saturation and contrast.},
	language = {en},
	number = {10},
	journal = {Organ. Ethic.},
	author = {Toadere, Florin},
	month = jun,
	year = {2013},
	note = {Publisher: SPIE},
	keywords = {Image sensors, Image processing, CMOS sensors, Device simulation, Digital cameras, Digital imaging, Image filtering, Image quality, Photons, Point spread functions},
	pages = {102005},
}

@article{konnik_high-level_2014,
	title = {High-level numerical simulations of noise in {CCD} and {CMOS} photosensors: review and tutorial},
	url = {http://arxiv.org/abs/1412.4031},
	journal = {arXiv [astro-ph.IM]},
	author = {Konnik, Mikhail and Welsh, James},
	month = dec,
	year = {2014},
}

@article{lyu_simulations_2021,
	title = {Simulations of fluorescence imaging in the oral cavity},
	volume = {12},
	abstract = {We describe an end-to-end image systems simulation that models a device capable of measuring fluorescence in the oral cavity. Our software includes a 3D model of the oral cavity and excitation-emission matrices of endogenous fluorophores that predict the spectral radiance of oral mucosal tissue. The predicted radiance is transformed by a model of the optics and image sensor to generate expected sensor image values. We compare simulated and real camera data from tongues in healthy individuals and show that the camera sensor chromaticity values can be used to quantify the fluorescence from porphyrins relative to the bulk fluorescence from multiple fluorophores (elastin, NADH, FAD, and collagen). Validation of the simulations supports the use of soft-prototyping in guiding system design for fluorescence imaging.},
	language = {en},
	number = {7},
	journal = {Biomed. Opt. Express},
	author = {Lyu, Zheng and Jiang, Haomiao and Xiao, Feng and Rong, Jian and Zhang, Tingcheng and Wandell, Brian and Farrell, Joyce},
	month = jul,
	year = {2021},
	note = {Publisher: osapublishing.org},
	pages = {4276--4292},
}

@article{borner_sensor_2001,
	title = {{SENSOR}: a tool for the simulation of hyperspectral remote sensing systems},
	volume = {55},
	abstract = {The consistent end-to-end simulation of airborne and spaceborne earth remote sensing systems is an important task, and sometimes the only way for the adaptation and optimisation of a sensor and its observation conditions, the choice and test of algorithms for data processing, error estimation and the evaluation of the capabilities of the whole sensor system. The presented software simulator SENSOR (Software Environment for the Simulation of Optical Remote sensing systems) includes a full model of the sensor hardware, the observed scene, and the atmosphere in between. The simulator consists of three parts. The first part describes the geometrical relations between scene, sun, and the remote sensing system using a ray-tracing algorithm. The second part of the simulation environment considers the radiometry. It calculates the at-sensor radiance using a pre-calculated multidimensional lookup-table taking the atmospheric influence on the radiation into account. The third part consists of an optical and an electronic sensor model for the generation of digital images. Using SENSOR for an optimisation requires the additional application of task-specific data processing algorithms. The principle of the end-to-end-simulation approach is explained, all relevant concepts of SENSOR are discussed, and first examples of its use are given. The verification of SENSOR is demonstrated. This work is closely related to the Airborne PRISM Experiment (APEX), an airborne imaging spectrometer funded by the European Space Agency.},
	number = {5},
	journal = {ISPRS J. Photogramm. Remote Sens.},
	author = {Börner, Anko and Wiest, Lorenz and Keller, Peter and Reulke, Ralf and Richter, Rolf and Schaepman, Michael and Schläpfer, Daniel},
	month = mar,
	year = {2001},
	keywords = {Simulation, APEX, Hyperspectral, Optimisation, Sensor},
	pages = {299--312},
}

@inproceedings{peterson_surface_2004,
	title = {Surface and buried landmine scene generation and validation using the digital imaging and remote sensing image generation model},
	volume = {5546},
	abstract = {Detection and neutralization of surface-laid and buried landmines has been a slow and dangerous endeavor for military forces and humanitarian organizations throughout the world. In an effort to make the process faster and safer, scientists have begun to exploit the ever-evolving passive electro-optical realm, both from a broadband perspective and a multi or hyperspectral perspective. Carried with this exploitation is the development of mine detection algorithms that take advantage of spectral features exhibited by mine targets, only available in a multi or hyperspectral data set. Difficulty in algorithm development arises from a lack of robust data, which is needed to appropriately test the validity of an algorithm's results. This paper discusses the development of synthetic data using the Digital Imaging and Remote Sensing Image Generation (DIRSIG) model. A synthetic landmine scene has been modeled after data collected at a US Army arid testing site by the University of Hawaii's Airborne Hyperspectral Imager (AHI). The synthetic data has been created and validated to represent the surrogate minefield thermally, spatially, spectrally, and temporally over the 7.9 to 11.5 micron region using 70 bands of data. Validation of the scene has been accomplished by direct comparison to the AHI truth data using qualitative band to band visual analysis, Rank Order Correlation comparison, Principle Components dimensionality analysis, and an evaluation of the R(x) algorithm's performance. This paper discusses landmine detection phenomenology, describes the steps taken to build the scene, modeling methods utilized to overcome input parameter limitations, and compares the synthetic scene to truth data.},
	language = {en},
	booktitle = {Imaging {Spectrometry} {X}},
	publisher = {SPIE},
	author = {Peterson, Erin D and Brown, Scott D and Hattenberger, Timothy J and Schott, John R},
	month = oct,
	year = {2004},
	keywords = {DIRSIG, hyperspectral image simulation, long wave infrared, mine detection, reststrahlen},
	pages = {312--323},
}

@phdthesis{white_validation_1996-1,
	type = {{PhD} {Thesis}},
	title = {Validation of {Rochester} {Institute} of {Technology}'s ({RIT}'s) {Digital} {Image} and {Remote} {Sensing} {Generation} ({DIRSIG}) {Model}, reflective region},
	abstract = {The performance of RIT's Digital Imaging and Remote Sensing Image Generation (DIRSIG) model is validated. The model is robust enough to treat solar, atmospheric, target/background, and sensor interactions. It operates over the 0.28 - 28 micrometers (Ultraviolet- Long Wavelength Infra-Red) spectral region. However, this study focuses only on the 0.4 - 1.0 micrometer (reflective) region. To validate the model, reference (actual) imagery from an airborne frame sensor is compared to synthetic imagery of the same scene. This study also evaluates DIRSIG's treatment of reflectivity and recommends improvements.},
	school = {Rochester Institute of Technology},
	author = {White, Russell},
	year = {1996},
}

@misc{wikipedia_zernike_nodate,
	title = {Zernike polynomials — {Wikipedia}, {The} {Free} {Encyclopedia}},
	url = {https://en.wikipedia.org/wiki/Zernike_polynomials},
	author = {{Wikipedia}},
	annote = {[Online; accessed 09-May-2021]},
}

@article{chen_digital_2009,
	title = {Digital {Camera} {Imaging} {System} {Simulation}},
	volume = {56},
	abstract = {A digital camera is a complex system including a lens, a sensor (physics and circuits), and a digital image processor, where each component is a sophisticated system on its own. Since prototyping a digital camera is very expensive, it is highly desirable to have the capability to explore the system design tradeoffs and preview the system output ahead of time. An empirical digital imaging system simulation that aims to achieve such a goal is presented. It traces the photons reflected by the objects in a scene through the optics and color filter array, converts photons into electrons with consideration of noise introduced by the system, quantizes the accumulated voltage to digital counts by an analog-to-digital converter, and generates a Bayer raw image just as a real camera does. The simulated images are validated against real system outputs and show a close resemblance to the images captured under similar condition at all illumination levels.},
	number = {11},
	journal = {IEEE Trans. Electron Devices},
	author = {Chen, J and Venkataraman, K and Bakin, D and Rodricks, B and Gravelle, R and Rao, P and Ni, Y},
	month = nov,
	year = {2009},
	keywords = {Image color analysis, image colour analysis, Camera simulation, Pixel, Cameras, Noise, analog-to-digital converter, analogue-digital conversion, Bayer raw image, color filter array, Crosstalk, digital camera imaging system, digital image processor, filtering theory, Hyperspectral scene, image sensors, imaging sensor, lens characterization, Lenses, modulation transfer function (MTF), pixel characterization, pixel noise, point spread function (PSF)},
	pages = {2496--2505},
}

@article{goral_modeling_1984,
	title = {Modeling the interaction of light between diffuse surfaces},
	volume = {18},
	abstract = {A method is described which models the interaction of light between diffusely reflecting surfaces. Current light reflection models used in computer graphics do not account for the object-to-object reflection between diffuse surfaces, and thus incorrectly compute the global illumination effects. The new procedure, based on methods used in thermal engineering, includes the effects of diffuse light sources of finite area, as well as the “color-bleeding” effects which are caused by the diffuse reflections. A simple environment is used to illustrate these simulated effects and is presented with photographs of a physical model. The procedure is applicable to environments composed of ideal diffuse reflectors and can account for direct illumination from a variety of light sources. The resultant surface intensities are independent of observer position, and thus environments can be preprocessed for dynamic sequences.},
	number = {3},
	journal = {SIGGRAPH Comput. Graph.},
	author = {Goral, Cindy M and Torrance, Kenneth E and Greenberg, Donald P and Battaile, Bennett},
	month = jan,
	year = {1984},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Diffuse reflections, Form factors, Light reflection models, Radiosity, Shading},
	pages = {213--222},
}

@article{farrell_soft-prototyping_2020,
	title = {Soft-prototyping imaging systems for oral cancer screening},
	volume = {2020},
	abstract = {We are using image systems simulation technology to design a digital camera for measuring fluorescent signals; a first application is oral cancer screening. We validate the simulations by creating a camera model that accurately predicts measured RGB values for any spectral radiance. Then we use the excitationemission spectra for different biological fluorophores to predict measurements of fluorescence of oral mucosal tissue under several different illuminations. The simulations and measurements are useful for (a) designing cameras that measure tissue fluorescence and (b) clarifying which fluorophores may be diagnostic in identifying precancerous tissue.},
	number = {7},
	journal = {Electronic Imaging},
	author = {Farrell, Joyce and Lyu, Zheng and Liu, Zhenyi and Blasinski, Henryk and Xu, Zhihao and Rong, Jian and Xiao, Feng and Wandell, Brian},
	year = {2020},
	keywords = {Image Systems Simulation, Oral Cancer Screening, Tissue Fluorescence},
	pages = {212--1--212--7},
}

@inproceedings{blasinski_underwater_2017,
	title = {Underwater {Image} {Systems} {Simulation}},
	abstract = {We use modern computer graphics tools such as ray-tracing, digital camera simulation tools, and a physically accurate model of seawater constituents to simulate how light is captured by the imaging sensor in a digital camera placed in underwater ocean environments. Our water model includes parameters for the type and amount of phytoplankton, the concentration of chlorophyll, the amount of dissolved organic matter (CDOM) and the concentration of detritus (non-algal particles, NAP). We show that by adjusting the model parameters, we can predict real sensor data captured by a digital camera at a fixed distance and depth from a reference target with known spectral reflectance. We also provide an open-source implementation of all our tools and simulations.},
	language = {en},
	booktitle = {Imaging and {Applied} {Optics} 2017 ({3D}, {AIO}, {COSI}, {IS}, {MATH}, {pcAOP})},
	publisher = {Optical Society of America},
	author = {Blasinski, Henryk and Lian, Trisha and Farrell, Joyce},
	month = jun,
	year = {2017},
	note = {Place: San Francisco, California United States},
	keywords = {Digital imaging, Absorption coefficient, Beam scattering, Light propagation, Light scattering, Underwater imaging},
}

@article{cohen_efficient_1986,
	title = {An {Efficient} {Radiosity} {Approach} for {Realistic} {Image} {Synthesis}},
	volume = {6},
	abstract = {The radiosity method models the interaction of light between diffusely reflecting surfaces and accurately predicts the global illumination effects. Procedures are now available to simulate complex environments including occluded and textured surfaces. For accurate rendering, the environment must be discretized into a fine mesh, particularly in areas of high intensity gradients. The interdependence between surfaces implies solution techniques which are computationally intractable. This article describes new procedures to predict the global illumination function without excessive computational expense. Statistics indicate the enormous potential of this approach for realistic image synthesis, particularly for dynamic images of static environments.},
	number = {3},
	journal = {IEEE Comput. Graph. Appl.},
	author = {Cohen, Michael F and Greenberg, Donald P and Immel, David S and Brock, Philip J},
	month = mar,
	year = {1986},
	keywords = {Computer graphics, Equations, Image generation, Layout, Lighting, Mirrors, Optical reflection, Reflectivity, Statistics, Surface texture},
	pages = {26--35},
}

@inproceedings{farrell_simulation_2003,
	title = {A simulation tool for evaluating digital camera image quality},
	volume = {5294},
	abstract = {The Image Systems Evaluation Toolkit (ISET) is an integrated suite of software routines that simulate the capture and processing of visual scenes. ISET includes a graphical user interface (GUI) for users to control the physical characteristics of the scene and many parameters of the optics, sensor electronics and image processing-pipeline. ISET also includes color tools and metrics based on international standards (chromaticity coordinates, CIELAB and others) that assist the engineer in evaluating the color accuracy and quality of the rendered image.},
	booktitle = {Image {Quality} and {System} {Performance}},
	publisher = {International Society for Optics and Photonics},
	author = {Farrell, Joyce E and Xiao, Feng and Catrysse, Peter Bert and Wandell, Brian A},
	month = dec,
	year = {2003},
	keywords = {Camera simulation, image processing pipeline, Digital camera simulation, image quality evaluation, virtual camera},
	pages = {124--131},
}

@inproceedings{liu_soft_2019,
	title = {Soft {Prototyping} {Camera} {Designs} for {Car} {Detection} {Based} on a {Convolutional} {Neural} {Network}},
	booktitle = {Proceedings of the {IEEE} {International} {Conference} on {Computer} {Vision} {Workshops}},
	author = {Liu, Zhenyi and Lian, Trisha and Farrell, Joyce and Wandell, Brian},
	year = {2019},
	keywords = {Automotive},
}

@article{farrell_digital_2012,
	title = {Digital camera simulation},
	volume = {51},
	abstract = {We describe a simulation of the complete image processing pipeline of a digital camera, beginning with a radiometric description of the scene captured by the camera and ending with a radiometric description of the image rendered on a display. We show that there is a good correspondence between measured and simulated sensor performance. Through the use of simulation, we can quantify the effects of individual digital camera components on system performance and image quality. This computational approach can be helpful for both camera design and image quality assessment.},
	language = {en},
	number = {4},
	journal = {Appl. Opt.},
	author = {Farrell, Joyce E and Catrysse, Peter B and Wandell, Brian A},
	month = feb,
	year = {2012},
	keywords = {2019 ICCV (Zhenyi Brian), Camera simulation},
	pages = {A80--90},
}

@inproceedings{sumanta_n_pattanaik_james_a_ferwerda_kenneth_e_torrance_and_donald_greenberg_validation_1997,
	title = {Validation of {Global} {Illumination} {Simulations} through {CCD} {Camera} {Measurements}},
	booktitle = {Color and {Imaging} {Conference} 1997},
	author = {{Sumanta N. Pattanaik, James A. Ferwerda, Kenneth E. Torrance, and Donald Greenberg}},
	year = {1997},
	pages = {250--53},
}

@article{liu_isetauto_2021,
	title = {{ISETAuto}: {Detecting} {Vehicles} {With} {Depth} and {Radiance} {Information}},
	volume = {9},
	journal = {IEEE Access},
	author = {Liu, Zhenyi and Farrell, Joyce and Wandell, Brian A},
	year = {2021},
	keywords = {Sensors, Sensor fusion, LiDAR, Object detection, Training, Cameras, Three-dimensional displays, Automotive engineering, Autonomous driving, Camera, Convolutional neural network, Laser radar, Spatial resolution},
	pages = {41799--41808},
}

@article{meyer_experimental_1986,
	title = {An experimental evaluation of computer graphics imagery},
	volume = {5},
	number = {1},
	journal = {ACM Trans. Graph.},
	author = {Meyer, Gary W and Rushmeier, Holly E and Cohen, Michael F and Greenberg, Donald P and Torrance, Kenneth E},
	month = jan,
	year = {1986},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	pages = {30--50},
}

@article{lian_image_2018,
	title = {Image {Systems} {Simulation} for 360 deg {Camera} {Rigs}},
	volume = {2018},
	number = {5},
	journal = {Electronic Imaging},
	author = {Lian, Trisha and Farrell, Joyce and Wandell, Brian},
	month = jan,
	year = {2018},
	keywords = {2019 ICCV (Zhenyi Brian)},
	pages = {353--1--353--5},
}

@inproceedings{schraml_physically_2019,
	title = {Physically based synthetic image generation for machine learning: a review of pertinent literature},
	volume = {11144},
	booktitle = {Photonics and {Education} in {Measurement} {Science} 2019},
	publisher = {International Society for Optics and Photonics},
	author = {Schraml, Dominik},
	year = {2019},
	pages = {111440J},
}

@inproceedings{farrell_sensor_2008,
	title = {Sensor calibration and simulation},
	volume = {6817},
	abstract = {We describe a method for simulating the output of an image sensor to a broad array of test targets. The method uses a modest set of sensor calibration measurements to define the sensor parameters; these parameters are used by an integrated suite of Matlab software routines that simulate the sensor and create output images. We compare the simulations of specific targets to measured data for several different imaging sensors with very different imaging properties. The simulation captures the essential features of the images created by these different sensors. Finally, we show that by specifying the sensor properties the simulations can predict sensor performance to natural scenes that are difficult to measure with a laboratory apparatus, such as natural scenes with high dynamic range or low light levels.},
	booktitle = {Digital {Photography} {IV}},
	publisher = {International Society for Optics and Photonics},
	author = {Farrell, Joyce and Okincha, Michael and Parmar, Manu},
	month = mar,
	year = {2008},
	keywords = {simulation, characterization, Digital camera calibration},
	pages = {68170R},
}

@misc{noauthor_opencamera_nodate,
	title = {{OpenCamera}},
	url = {https://opencamera.org.uk/},
}

@article{lyu_validation_2021,
	title = {Validation of image systems simulation technology using a cornell box},
	journal = {arXiv preprint arXiv:2105.04106},
	author = {Lyu, Zheng and Kripakaran, Krithin and Furth, Max and Tang, Eric and Wandell, Brian and Farrell, Joyce},
	year = {2021},
}

@article{nimier-david_mitsuba_2019,
	title = {Mitsuba 2: {A} retargetable forward and inverse renderer},
	volume = {38},
	number = {6},
	journal = {ACM Transactions on Graphics (TOG)},
	author = {Nimier-David, Merlin and Vicini, Delio and Zeltner, Tizian and Jakob, Wenzel},
	year = {2019},
	note = {Publisher: ACM New York, NY, USA},
	pages = {1--17},
}

@article{burgess_rtx_2020,
	title = {Rtx on—the nvidia turing gpu},
	volume = {40},
	number = {2},
	journal = {IEEE Micro},
	author = {Burgess, John},
	year = {2020},
	note = {Publisher: IEEE},
	pages = {36--44},
}

@inproceedings{selgrad_real-time_2015,
	title = {Real-time depth of field using multi-layer filtering},
	booktitle = {Proceedings of the 19th {Symposium} on {Interactive} {3D} {Graphics} and {Games}},
	author = {Selgrad, Kai and Reintges, Christian and Penk, Dominik and Wagner, Pascal and Stamminger, Marc},
	year = {2015},
	pages = {121--127},
}

@inproceedings{hullin_polynomial_2012,
	title = {Polynomial {Optics}: {A} construction kit for efficient ray-tracing of lens systems},
	volume = {31},
	booktitle = {Computer {Graphics} {Forum}},
	publisher = {Wiley Online Library},
	author = {Hullin, Matthias B and Hanika, Johannes and Heidrich, Wolfgang},
	year = {2012},
	note = {Issue: 4},
	pages = {1375--1383},
}

@article{fiete_modeling_2014,
	title = {Modeling the optical transfer function in the imaging chain},
	volume = {53},
	number = {8},
	journal = {Optical Engineering},
	author = {Fiete, Robert D and Paul, Bradley D},
	year = {2014},
	note = {Publisher: International Society for Optics and Photonics},
	pages = {083103},
}

@inproceedings{toadere_process_2012,
	title = {Process simulation in digital camera system},
	volume = {8436},
	booktitle = {Optics, {Photonics}, and {Digital} {Technologies} for {Multimedia} {Applications} {II}},
	publisher = {International Society for Optics and Photonics},
	author = {Toadere, Florin},
	year = {2012},
	pages = {843618},
}

@article{klein_simulating_2009,
	title = {Simulating low-cost cameras for augmented reality compositing},
	volume = {16},
	number = {3},
	journal = {IEEE transactions on visualization and computer graphics},
	author = {Klein, Georg and Murray, David W},
	year = {2009},
	note = {Publisher: IEEE},
	pages = {369--380},
}

@article{florin_simulation_2009,
	title = {Simulation the functionality of a web cam image capture system},
	volume = {8},
	number = {10},
	journal = {WSEAS Transactions on Circuits and Systems},
	author = {Florin, Toadere and Mastorakis, Nikos E},
	year = {2009},
	note = {Publisher: World Scientific and Engineering Academy and Society (WSEAS) Stevens Point …},
	pages = {811--821},
}

@inproceedings{chen_virtual_2008,
	title = {A virtual simulation system of {TDI} line scan camera},
	booktitle = {2008 {IEEE}/{ASME} {International} {Conference} on {Advanced} {Intelligent} {Mechatronics}},
	publisher = {IEEE},
	author = {Chen, Xiaoli and Yin, Chengliang and Feng, Yong},
	year = {2008},
	pages = {138--144},
}

@inproceedings{schrade_sparse_2016,
	title = {Sparse high-degree polynomials for wide-angle lenses},
	volume = {35},
	booktitle = {Computer {Graphics} {Forum}},
	publisher = {Wiley Online Library},
	author = {Schrade, Emanuel and Hanika, Johannes and Dachsbacher, Carsten},
	year = {2016},
	note = {Issue: 4},
	pages = {89--97},
}

@inproceedings{hanika_efficient_2014,
	title = {Efficient {Monte} {Carlo} rendering with realistic lenses},
	volume = {33},
	booktitle = {Computer {Graphics} {Forum}},
	publisher = {Wiley Online Library},
	author = {Hanika, Johannes and Dachsbacher, Carsten},
	year = {2014},
	note = {Issue: 2},
	pages = {323--332},
}

@inproceedings{zheng_neurolens_2017,
	title = {{NeuroLens}: {Data}-{Driven} {Camera} {Lens} {Simulation} {Using} {Neural} {Networks}},
	volume = {36},
	booktitle = {Computer {Graphics} {Forum}},
	publisher = {Wiley Online Library},
	author = {Zheng, Quan and Zheng, Changwen},
	year = {2017},
	note = {Issue: 8},
	pages = {390--401},
}

@article{zheng_adaptive_2017,
	title = {Adaptive sparse polynomial regression for camera lens simulation},
	volume = {33},
	number = {6},
	journal = {The Visual Computer},
	author = {Zheng, Quan and Zheng, Changwen},
	year = {2017},
	note = {Publisher: Springer},
	pages = {715--724},
}

@article{kim_estimation_2021,
	title = {Estimation of any fields of lens {PSFs} for image simulation},
	volume = {2021},
	number = {7},
	journal = {Electronic Imaging},
	author = {Kim, Sangmin and Kim, Daekwan and Chung, Kilwoo and Yim, JoonSeo},
	year = {2021},
	note = {Publisher: Society for Imaging Science and Technology},
	pages = {72--1},
}

@book{keshmirian_physically-based_2008,
	title = {A physically-based approach for lens flare simulation},
	publisher = {University of California, San Diego},
	author = {Keshmirian, Arash},
	year = {2008},
}

@article{lian_image_2018-1,
	title = {Image systems simulation for 360 camera rigs},
	volume = {2018},
	number = {5},
	journal = {Electronic Imaging},
	author = {Lian, Trisha and Farrell, Joyce and Wandell, Brian},
	year = {2018},
	note = {Publisher: Society for Imaging Science and Technology},
	pages = {353--1},
}

@inproceedings{florin_simulation_2009-1,
	title = {Simulation the optical part of an image capture system},
	volume = {7297},
	booktitle = {Advanced {Topics} in {Optoelectronics}, {Microelectronics}, and {Nanotechnologies} {IV}},
	publisher = {International Society for Optics and Photonics},
	author = {Florin, Toadere},
	year = {2009},
	pages = {729719},
}

@article{mout_ray-based_2018,
	title = {Ray-based method for simulating cascaded diffraction in high-numerical-aperture systems},
	volume = {35},
	number = {8},
	journal = {JOSA A},
	author = {Mout, Marco and Flesch, Andreas and Wick, Michael and Bociort, Florian and Petschulat, Joerg and Urbach, Paul},
	year = {2018},
	note = {Publisher: Optical Society of America},
	pages = {1356--1367},
}

@inproceedings{lodhi_modeling_2015,
	title = {Modeling and performance analysis of an imaging system},
	booktitle = {2015 {IEEE} {International} {Conference} on {Research} in {Computational} {Intelligence} and {Communication} {Networks} ({ICRCICN})},
	publisher = {IEEE},
	author = {Lodhi, Vaibhav and Kumar, Abhay and Chakravarty, Debashish},
	year = {2015},
	pages = {329--334},
}

@article{tantalo_modeling_1996,
	title = {Modeling the {MTF} and noise characteristics of an image chain for a synthetic image generation system},
	author = {Tantalo, Frank},
	year = {1996},
}

@inproceedings{subbarao_model_1991,
	title = {Model for image sensing and digitization in machine vision},
	volume = {1385},
	booktitle = {Optics, {Illumination}, and {Image} {Sensing} for {Machine} {Vision} {V}},
	publisher = {International Society for Optics and Photonics},
	author = {Subbarao, Murali and Nikzad, Arman},
	year = {1991},
	pages = {70--84},
}

@inproceedings{garnier_infrared_1999,
	title = {Infrared sensor modeling for realistic thermal image synthesis},
	volume = {6},
	booktitle = {1999 {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}. {Proceedings}. {ICASSP99} ({Cat}. {No}. {99CH36258})},
	publisher = {IEEE},
	author = {Garnier, Christelle and Collorec, René and Flifla, Jihed and Mouclier, Christophe and Rousee, F},
	year = {1999},
	pages = {3513--3516},
}

@article{schott_advanced_1999,
	title = {An advanced synthetic image generation model and its application to multi/hyperspectral algorithm development},
	volume = {25},
	number = {2},
	journal = {Canadian Journal of Remote Sensing},
	author = {Schott, John R and Brown, Scott D and Raqueno, Rolando V and Gross, Harry N and Robinson, Gary},
	year = {1999},
	note = {Publisher: Taylor \& Francis},
	pages = {99--111},
}

@inproceedings{short_image_1999,
	title = {Image capture simulation using an accurate and realistic lens model},
	volume = {3650},
	booktitle = {Sensors, {Cameras}, and {Applications} for {Digital} {Photography}},
	publisher = {International Society for Optics and Photonics},
	author = {Short, Ralph C and Williams, Donald and Jones, Andrew EW},
	year = {1999},
	pages = {138--148},
}

@inproceedings{maeda_integrating_2005,
	title = {Integrating lens design with digital camera simulation},
	volume = {5678},
	booktitle = {Digital {Photography}},
	publisher = {International Society for Optics and Photonics},
	author = {Maeda, Patrick Y and Catrysse, Peter B and Wandell, Brian A},
	year = {2005},
	pages = {48--58},
}

@article{subbarao_image_1994,
	title = {Image sensing model and computer simulation for {CCD} camera systems},
	volume = {7},
	number = {4},
	journal = {Machine Vision and Applications},
	author = {Subbarao, Murali and Lu, Ming-Chin},
	year = {1994},
	note = {Publisher: Springer},
	pages = {277--289},
}

@phdthesis{lu_computer_1993,
	type = {{PhD} {Thesis}},
	title = {Computer modeling and simulation techniques for computer vision problems},
	school = {State University of New York at Stony Brook},
	author = {Lu, Ming-Chin},
	year = {1993},
}

@inproceedings{subbarao_computer_1993,
	title = {Computer modeling and simulation of camera defocus},
	volume = {1822},
	booktitle = {Optics, {Illumination}, and {Image} {Sensing} for {Machine} {Vision} {VII}},
	publisher = {International Society for Optics and Photonics},
	author = {Subbarao, Murali and Lu, MingChin},
	year = {1993},
	pages = {110--120},
}

@inproceedings{garnier_general_1998,
	title = {General framework for infrared sensor modeling},
	volume = {3377},
	booktitle = {Infrared {Imaging} {Systems}: {Design}, {Analysis}, {Modeling}, and {Testing} {IX}},
	publisher = {International Society for Optics and Photonics},
	author = {Garnier, Christelle and Collorec, Rene and Flifla, Jihed and Rousee, Frank},
	year = {1998},
	pages = {59--70},
}

@book{shirley_realistic_2008,
	title = {Realistic ray tracing},
	publisher = {AK Peters, Ltd.},
	author = {Shirley, Peter and Morley, R Keith},
	year = {2008},
}

@article{potmesil_lens_1981,
	title = {A lens and aperture camera model for synthetic image generation},
	volume = {15},
	number = {3},
	journal = {ACM SIGGRAPH Computer Graphics},
	author = {Potmesil, Michael and Chakravarty, Indranil},
	year = {1981},
	note = {Publisher: ACM New York, NY, USA},
	pages = {297--305},
}

@inproceedings{philion_learning_2020,
	title = {Learning to evaluate perception models using planner-centric metrics},
	abstract = {Variants of accuracy and precision are the gold-standard by which the computer vision community measures progress of perception algorithms. One reason for the ubiquity of these metrics is that they are largely task-agnostic; we in general seek to detect zero false negatives or positives. The downside of these metrics is that, at worst, they penalize all incorrect detections equally without conditioning on the task or scene, and at best, heuristics need to be chosen to ensure that different mistakes count differently. In this paper, we propose a principled metric for 3D object detection specifically for the task of self-driving. The core idea behind our metric is to isolate the task of object detection and measure the impact the produced detections would induce on the downstream task of driving. Without hand-designing it to, we find that our metric penalizes many of the mistakes that other metrics penalize by design. In addition, our metric downweighs detections based on additional factors such as distance from a detection to the ego car and the speed of the detection in intuitive ways that other detection metrics do not. For human evaluation, we generate scenes in which standard metrics and our metric disagree and find that humans side with our metric 79\% of the time. Our project page including an evaluation server can be found at https://nv-tlabs.github.io/detection-relevance.},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Philion, Jonah and Kar, Amlan and Fidler, Sanja},
	month = jun,
	year = {2020},
	note = {Place: Seattle, WA, USA},
}

@article{di_rainy_2021,
	title = {Rainy {Night} {Scene} {Understanding} {With} {Near} {Scene} {Semantic} {Adaptation}},
	volume = {22},
	abstract = {Deep networks have been used for semantic segmentation tasks on scenes of outdoor environments with increasing popularity. However, the majority of existing work centers on daytime scenes with favorable illumination and weather conditions, and relies on supervision with pixel-level annotations. This paper seeks to address the problem of semantic segmentation for rainy, night-time scenes without using pixel-level annotations. We introduce a near scene semantic approach that uses images of daytime scenes as a bridge for transferring knowledge from pre-trained segmentation models to rainy night images. Specifically, we first present near scene oriented Representation Adaptation (RA) to reduce the domain shift on the representation level. Next, we adapt the segmentation model from the daytime scenario, under varying weather conditions, to the rainy night scenario by using near scene oriented Segmentation Space Adaptation (SSA). Consequently, this further reduces the impact of the domain shift on the segmentation space level. For evaluation, we created a new dataset containing 7000 distinct daytime-night-time image pairs of near scenes obtained by a webcam, and 5266 daytime-rainy night image pairs collected by a car-mounted camera. In addition, we carefully annotated 226 rainy night images with classes defined in Cityscapes. The experimental results clearly demonstrate the advantage of the proposed algorithm.},
	number = {3},
	journal = {IEEE Trans. Intell. Transp. Syst.},
	author = {Di, Shuai and Feng, Qi and Li, Chun-Guang and Zhang, Mei and Zhang, Honggang and Elezovikj, Semir and Tan, Chiu C and Ling, Haibin},
	month = mar,
	year = {2021},
	keywords = {Adaptation models, Training, domain adaptation, Image segmentation, semantic segmentation, Meteorology, Semantics, Lighting, Bridges, road scene, Traffic scene understanding, vehicle environment perception},
	pages = {1594--1602},
}

@article{sakaridis_semantic_2018,
	title = {Semantic foggy scene understanding with synthetic data},
	volume = {126},
	language = {en},
	number = {9},
	journal = {Int. J. Comput. Vis.},
	author = {Sakaridis, Christos and Dai, Dengxin and Van Gool, Luc},
	month = sep,
	year = {2018},
	note = {Publisher: Springer Science and Business Media LLC},
	pages = {973--992},
}

@inproceedings{kolb_realistic_1995,
	title = {A realistic camera model for computer graphics},
	booktitle = {Proceedings of the 22nd annual conference on computer graphics and interactive techniques},
	author = {Kolb, Craig and Mitchell, Don and Hanrahan, Pat},
	year = {1995},
	pages = {317--324},
}

@article{chen_lens_1987,
	title = {Lens effect on synthetic image generation based on light particle theory},
	volume = {3},
	number = {3},
	journal = {The Visual Computer},
	author = {Chen, Yong C},
	year = {1987},
	note = {Publisher: Springer},
	pages = {125--136},
}

@article{lyu_validation_2022,
	title = {Validation of {Physics}-{Based} {Image} {Systems} {Simulation} {With} 3-{D} {Scenes}},
	volume = {22},
	abstract = {Image systems simulation software can accelerate innovation by reducing many of the time-consuming and expensive steps in designing, building, and evaluating image systems. To realize this potential, it is necessary to build trust in physics-based end-to-end image systems simulations. Toward this goal, we describe and experimentally validate an end-to-end physics-based image systems simulation of a digital camera. The simulation models the spectral radiance of 3-D scenes, the formation of the spectral irradiance by multielement optics, and the conversion of the irradiance to digital values (DVs) by the image sensor. We quantify the accuracy of the simulation by comparing real and simulated images of a precisely constructed, 3-D high-dynamic range (HDR) test scene.},
	number = {20},
	journal = {IEEE Sens. J.},
	author = {Lyu, Zheng and Goossens, Thomas and Wandell, Brian A and Farrell, Joyce},
	month = oct,
	year = {2022},
	keywords = {Computer graphics, Cameras, Computational modeling, Lenses, end-to-end simulation, image systems, Imaging, optics, Optics, physically based ray tracing, sensor, Software, Systems simulation},
	pages = {19400--19410},
}

@phdthesis{cech_day_2021,
	type = {{PhD} {Thesis}},
	title = {Day to {Night} {Image} {Style} {Transfer} with {Light} {Control}},
	school = {Czech Technical University in Prague},
	author = {Čech, B Josef and Hurych, David},
	month = may,
	year = {2021},
}

@inproceedings{lin_microsoft_2014-1,
	address = {Cham},
	title = {Microsoft {COCO}: {Common} {Objects} in {Context}},
	isbn = {978-3-319-10602-1},
	shorttitle = {Microsoft {COCO}},
	doi = {10.1007/978-3-319-10602-1_48},
	abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2014},
	publisher = {Springer International Publishing},
	author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Dollár, Piotr and Zitnick, C. Lawrence},
	editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
	year = {2014},
	note = {Changing to force duplicate update.},
	keywords = {Common Object, Object Category, Object Detection, Object Instance, Scene Understanding},
	pages = {740--755},
	file = {Full Text PDF:/Users/wandell/Zotero/storage/6NLESKYE/Lin et al. - 2014 - Microsoft COCO Common Objects in Context.pdf:application/pdf},
}

@article{noauthor_notitle_nodate,
}

@article{grassmann_zur_1853,
	title = {Zur {Theorie} der {Farbenmischung}},
	volume = {165},
	journal = {Annalen der Physik und Chemie},
	author = {Grassmann, H.},
	year = {1853},
	pages = {69--84},
}

@article{maxwell_xviiiexperiments_1857,
	title = {{XVIII}.—{Experiments} on {Colour}, as perceived by the {Eye}, with {Remarks} on {Colour}-{Blindness}},
	volume = {21},
	number = {2},
	journal = {Earth Environ. Sci. Trans. R. Soc. Edinb.},
	author = {Maxwell, James Clerk},
	year = {1857},
	note = {Publisher: Royal Society of Edinburgh Scotland Foundation},
	pages = {275--298},
}

@article{polans_wide-field_2015,
	title = {Wide-field optical model of the human eye with asymmetrically tilted and decentered lens that reproduces measured ocular aberrations},
	volume = {2},
	number = {2},
	journal = {Optica},
	author = {Polans, James and Jaeken, Bart and McNabb, Ryan P and Artal, Pablo and Izatt, Joseph A},
	year = {2015},
	pages = {124--134},
}

@misc{quoteresearch_when_nodate,
	title = {When {You} {Come} to a {Fork} in the {Road}, {Take} {It}},
	url = {https://quoteinvestigator.com/2013/07/25/fork-road/},
	author = {{quoteresearch}},
	note = {Publication Title: Yogiisms},
	annote = {Accessed: 2021-7-22},
	annote = {Accessed: 2021-7-22},
	annote = {Accessed: 2021-7-22},
}

@book{cornsweet_visual_1970,
	address = {New York},
	title = {Visual {Perception}},
	publisher = {Academic Press},
	author = {Cornsweet, T. N.},
	year = {1970},
}

@book{graham_visual_1989,
	title = {Visual {Pattern} {Analyzers}},
	publisher = {Oxford University Press},
	author = {Graham, Norma},
	month = sep,
	year = {1989},
}

@incollection{pugh_vision_1988,
	address = {New York},
	title = {Vision: {Physics} and {Retinal} {Physiology}},
	volume = {1},
	booktitle = {Stevens' {Handbook} of {Experimental} {Psychology}, {Second} {Edition}},
	publisher = {Wiley},
	author = {Pugh, E.N., Jr.},
	editor = {Atkinson, R.C. and Herrnstein, R.J. and Lindsey, G. and Luce, R.D.},
	year = {1988},
	pages = {75--163},
}

@article{pelli_uncertainty_1985,
	title = {Uncertainty explains many aspects of visual contrast detection and discrimination},
	volume = {2},
	number = {9},
	journal = {Journal of the Optical Society of America A},
	author = {Pelli, D.G.},
	year = {1985},
	pages = {1508--1532},
}

@article{thibos_theory_1990,
	title = {Theory and measurement of ocular chromatic aberration},
	volume = {30},
	number = {1},
	journal = {Vision Research},
	author = {Thibos, L N and Bradley, A and Still, D L and Zhang, X and Howarth, P A},
	year = {1990},
	pages = {33--49},
}

@techreport{levin_understanding_2008,
	type = {Report},
	title = {Understanding camera trade-offs through a {Bayesian} analysis of light field projections},
	institution = {MIT},
	author = {Levin, A. and Durand, F. and Freeman, W. T.},
	year = {2008},
}

@article{brainard_trichromatic_2008,
	title = {Trichromatic reconstruction from the interleaved cone mosaic: {Bayesian} model and the color appearance of small spots},
	volume = {8},
	number = {5},
	journal = {Journal of Vision},
	author = {Brainard, D. H. and Williams, D. R. and Hofer, H.},
	year = {2008},
	pages = {15 1--23},
}

@article{stockman_spectral_1999,
	title = {The spectral sensitivity of the human short-wavelength cones},
	volume = {39},
	number = {17},
	journal = {Vision Research},
	author = {Stockman, A. and Sharpe, L. T. and Fach, C. C.},
	year = {1999},
	pages = {2901--2927},
}

@book{hunt_reproduction_2004,
	address = {Chichester, England},
	edition = {6},
	title = {The {Reproduction} of {Colour}},
	publisher = {John Wiley \& Sons},
	author = {Hunt, R. W. G.},
	year = {2004},
}

@incollection{adelson_plenoptic_1991,
	address = {Cambridge},
	title = {The plenoptic function and the elements of early vision},
	booktitle = {Computational {Models} of {Visual} {Processing}},
	publisher = {MIT Press},
	author = {Adelson, E. H. and Bergen, J.R.},
	editor = {Landy, M.S. and Movshon, J.A.},
	year = {1991},
	pages = {3--20},
}

@book{da_vinci_notebooks_1970,
	title = {The {Notebooks} of {Leonardo} {Da} {Vinci}},
	volume = {1},
	publisher = {Dover, New York},
	author = {Da Vinci, Leonardo},
	editor = {Richter, Jean Paul},
	month = jan,
	year = {1970},
}

@article{snodderly_macular_1984,
	title = {The macular pigment. {II}. {Spatial} distribution in primate retinas},
	volume = {25},
	journal = {Investigative Ophthalmology and Visual Science},
	author = {Snodderly, D.M. and Auran, J.D. and Delori, F.C.},
	year = {1984},
	pages = {674--685},
}

@article{banks_physical_1987,
	title = {The physical limits of grating visibility},
	volume = {27},
	number = {11},
	journal = {Vision Research},
	author = {Banks, M. S and Geisler, W. S. and Bennett, P. J.},
	year = {1987},
	pages = {1915--1924},
}

@article{cumming_physiology_2001,
	title = {The physiology of stereopsis},
	volume = {24},
	number = {1},
	journal = {Annual Review of Neuroscience},
	author = {Cumming, Bruce G and DeAngelis, Gregory C},
	year = {2001},
	note = {Publisher: Annual Reviews 4139 El Camino Way, PO Box 10139, Palo Alto, CA 94303-0139, USA},
	pages = {203--238},
}

@article{gershun_light_1939,
	title = {The {Light} {Field}},
	volume = {18},
	number = {1–4},
	journal = {Journal of Mathematical Physics},
	author = {Gershun, A},
	year = {1939},
	pages = {51--151},
}

@article{shapley_importance_1986,
	title = {The importance of contrast for the activity of single neurons, the {VEP} and perception},
	volume = {26},
	number = {1},
	journal = {Vision Research},
	author = {Shapley, R. M.},
	year = {1986},
	pages = {45--62},
}

@book{bracewell_fourier_1978,
	title = {The {Fourier} {Transform} and its {Applications}},
	publisher = {McGraw-Hill},
	author = {Bracewell, R.N.},
	year = {1978},
}

@article{stiles_luminous_1933,
	title = {The luminous efficiency of rays entering the eye pupil at different points},
	volume = {112},
	number = {778},
	journal = {Proceedings of the Royal Society of London. Series B},
	author = {Stiles, W.S. and Crawford, B.H.},
	year = {1933},
	pages = {428--450},
}

@article{solomon_machinery_2007,
	title = {The machinery of colour vision},
	volume = {8},
	number = {4},
	journal = {Nature Reviews Neuroscience},
	author = {Solomon, S.G. and Lennie, P.},
	year = {2007},
	pages = {276--286},
}

@incollection{williams_cost_1991,
	address = {New York},
	title = {The cost of trichromacy for spatial vision},
	booktitle = {From {Pigments} to {Perception}},
	publisher = {Plenum Press},
	author = {Williams, D. R. and Sekiguchi, N. and Haake, W. and Brainard, D. H. and Packer, O.},
	year = {1991},
	pages = {11--22},
}

@article{ruderman_statistics_1998,
	title = {Statistics of cone responses to natural images: {Implications} for visual coding},
	volume = {15},
	number = {8},
	journal = {Journal of the Optical Society of America A},
	author = {Ruderman, D. L. and Cronin, T. W. and Chiao, C. C.},
	year = {1998},
	pages = {2036--2045},
}

@book{rodieck_first_1998,
	address = {Sunderland, Mass.},
	title = {The {First} {Steps} in {Seeing}},
	publisher = {Sinauer},
	author = {Rodieck, R.W.},
	year = {1998},
}

@article{stiles_directional_1939,
	title = {The directional sensitivity of the retina and the spectral sensitivity of the rods and cones},
	volume = {127},
	number = {846},
	journal = {Proceedings of the Royal Society of London. Series B},
	author = {Stiles, W.S.},
	year = {1939},
	pages = {64--105},
}

@incollection{yellott_beginnings_1984,
	address = {New York},
	title = {The beginnings of visual perception: the retinal image and its initial encoding},
	volume = {III},
	booktitle = {Handbook of {Physiology}: {The} {Nervous} {System}},
	publisher = {Easton},
	author = {Yellott, J. I. and Wandell, B. A. and Cornsweet, T. N.},
	editor = {Darien-Smith, I.},
	year = {1984},
	pages = {257--316},
}

@article{thibos_statistical_2002,
	title = {Statistical variation of aberration structure and image quality in a normal population of healthy eyes},
	volume = {19},
	number = {12},
	journal = {Journal of the Optical Society of America A},
	author = {Thibos, L. N. and Hong, X. and Bradley, A. and Cheng, X.},
	year = {2002},
	pages = {2329--2348},
}

@book{berger_statistical_1985,
	address = {New York},
	title = {Statistical {Decision} {Theory} and {Bayesian} {Analysis}},
	publisher = {Springer-Verlag},
	author = {Berger, T. O.},
	year = {1985},
}

@incollection{simoncelli_statistical_2005,
	title = {Statistical modeling of photographic images},
	booktitle = {Handbook of {Image} and {Video} {Processing}},
	publisher = {Academic Press},
	author = {Simoncelli, E. P.},
	editor = {Bovik, A.},
	year = {2005},
	pages = {431--441},
}

@article{stockman_spectral_2000,
	title = {Spectral sensitivities of the middle- and long-wavelength sensitive cones derived from measurements in observers of known genotype},
	volume = {40},
	number = {13},
	journal = {Vision Research},
	author = {Stockman, A. and Sharpe, L.T.},
	year = {2000},
	pages = {1711--1737},
}

@misc{branwyn_sky_2016,
	title = {Sky {Angles}},
	url = {https://makezine.com/2016/09/16/measuring-tip-ruler/sky_angles/},
	language = {en},
	author = {Branwyn, Gareth},
	month = sep,
	year = {2016},
	annote = {Accessed: 2021-8-8},
	annote = {Accessed: 2021-8-8},
	annote = {Accessed: 2021-8-8},
}

@article{adelson_single_1992,
	title = {Single lens stereo with a plenoptic camera},
	volume = {14},
	number = {2},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Adelson, E H and Wang, J Y A},
	month = feb,
	year = {1992},
	note = {Publisher: Institute of Electrical and Electronics Engineers (IEEE)},
	pages = {99--106},
}

@article{shapley_spatial_1985,
	title = {Spatial frequency analysis in the visual system},
	volume = {8},
	number = {1},
	journal = {Annual Review of Neuroscience},
	author = {Shapley, Robert and Lennie, Peter},
	year = {1985},
	note = {Publisher: Annual Reviews 4139 El Camino Way, PO Box 10139, Palo Alto, CA 94303-0139, USA},
	pages = {547--581},
}

@article{baylor_responses_1979,
	title = {Responses of retinal rods to single photons},
	volume = {288},
	number = {Mar},
	journal = {Journal of Physiology},
	author = {Baylor, D. A. and Lamb, T. D. and Yau, K. W.},
	year = {1979},
	pages = {613--634},
}

@article{geisler_sequential_1989,
	title = {Sequential ideal-observer analysis of visual discriminations},
	volume = {96},
	number = {2},
	journal = {Psychological Review},
	author = {Geisler, W.S.},
	year = {1989},
	pages = {267--314},
}

@article{field_relations_1987,
	title = {Relations between the statistics of natural images and the response properties of cortical cells},
	volume = {4},
	number = {12},
	journal = {Journal of the Optical Society of America A},
	author = {Field, D. J.},
	year = {1987},
	pages = {2379--2394},
}

@article{wald_realtime_2003,
	title = {Realtime {Ray} {Tracing} and its use for {Interactive} {Global} {Illumination}},
	journal = {Eurographics, State of the Art Reports},
	author = {Wald, I and Purcell, T J and Schmittler, J and Benthin, C and {others}},
	year = {2003},
	note = {Publisher: Citeseer},
}

@article{lian_ray_2019,
	title = {Ray tracing {3D} spectral scenes through human optics models},
	volume = {19},
	number = {12},
	journal = {Journal of Vision},
	author = {Lian, T. and MacKenzie, K. J. and Brainard, D. H. and Cottaris, N. P. and Wandell, B. A.},
	year = {2019},
	pages = {23},
}

@article{watson_quest_2017,
	title = {{QUEST}+: {A} general multidimensional {Bayesian} adaptive psychometric method},
	volume = {17},
	number = {3},
	journal = {Journal of Vision},
	author = {Watson, A. B.},
	year = {2017},
	pages = {10},
}

@article{geisler_psychometric_2018,
	title = {Psychometric functions of uncertain template matching observers},
	volume = {18},
	number = {2},
	journal = {Journal of Vision},
	author = {Geisler, W. S.},
	year = {2018},
	pages = {1},
}

@article{burnham_prediction_1957,
	title = {Prediction of color appearance with different adaptation illuminations},
	volume = {47},
	number = {1},
	journal = {Journal of the Optical Society of America},
	author = {Burnham, R.W. and Evans, R.M. and Newhall, S.M.},
	year = {1957},
	pages = {35--42},
}

@article{watson_quest_1983,
	title = {{QUEST}: {A} {Bayesian} adaptive psychometric method},
	volume = {33},
	number = {2},
	journal = {Perception and Psychophysics},
	author = {Watson, A.B. and Pelli, D.G.},
	year = {1983},
	pages = {113--120},
}

@article{de_valois_psychophysical_1974,
	title = {Psychophysical studies of monkey vision–{III}. {Spatial} luminance contrast sensitivity tests of macaque and human observers},
	volume = {14},
	number = {1},
	journal = {Vision Research},
	author = {De Valois, Russell L and Morgan, Herman and Snodderly, D Ma},
	year = {1974},
	note = {Publisher: Elsevier},
	pages = {75--81},
}

@book{pharr_physically_2016,
	title = {Physically {Based} {Rendering}: {From} {Theory} to {Implementation}},
	language = {en},
	publisher = {Morgan Kaufmann},
	author = {Pharr, Matt and Jakob, Wenzel and Humphreys, Greg},
	month = sep,
	year = {2016},
	keywords = {NewAccount, ray-tracing},
}

@article{rushton_pigments_1972,
	title = {Pigments and signals in colour vision},
	volume = {220},
	number = {3},
	journal = {Journal of Physiology},
	author = {Rushton, W.A.H.},
	year = {1972},
	pages = {1P--31P},
}

@article{geisler_physical_1984,
	title = {Physical limits of acuity and hyperacuity},
	volume = {1},
	number = {7},
	journal = {Journal of the Optical Society of America A},
	author = {Geisler, W. S.},
	year = {1984},
	pages = {775--782},
}

@article{cohen_dependency_1964,
	title = {Dependency of the spectral reflectance curves of the {Munsell} color chips},
	volume = {1},
	journal = {Psychonomic Science},
	author = {Cohen, J.},
	year = {1964},
	pages = {369--370},
}

@article{serre_deep_2019,
	title = {Deep learning: {The} good, the bad, and the ugly},
	volume = {5},
	journal = {Annual Review of Vision Science},
	author = {Serre, Thomas},
	year = {2019},
	note = {Publisher: Annual Reviews},
	pages = {399--426},
}

@book{wyszecki_color_1982,
	address = {New York},
	edition = {2nd},
	title = {Color {Science}: {Concepts} and {Methods}, {Quantitative} {Data} and {Formulae}},
	publisher = {John Wiley \& Sons},
	author = {Wyszecki, G. and Stiles, W.S.},
	year = {1982},
}

@incollection{hofer_color_2014,
	address = {Cambridge, MA},
	title = {Color vision and the retinal mosaic},
	booktitle = {The {New} {Visual} {Neurosciences}},
	publisher = {MIT Press},
	author = {Hofer, H.J. and Williams, D.R.},
	editor = {Chalupa, L. M. and Werner, J. S.},
	year = {2014},
	pages = {469--483},
}

@incollection{brainard_colorimetry_2010,
	address = {New York},
	title = {Colorimetry},
	booktitle = {The {Optical} {Society} of {America} {Handbook} of {Optics}, 3rd edition, {Volume} {III}: {Vision} and {Vision} {Optics}},
	publisher = {McGraw Hill},
	author = {Brainard, D. H. and Stockman, A.},
	editor = {Bass, M. and DeCusatis, C. and Enoch, J. and Lakshminarayanan, V. and Li, G. and Macdonald, C. and Mahajan, V. and van Stryland, E.},
	year = {2010},
	pages = {10.1--10.56},
}

@techreport{cie_colorimetry_1986,
	address = {Vienna},
	type = {Report},
	title = {Colorimetry, second edition},
	number = {15.2},
	institution = {Bureau Central de la CIE},
	author = {{CIE}},
	year = {1986},
}

@article{burton_color_1987,
	title = {Color and spatial structure in natural images},
	volume = {26},
	number = {1},
	journal = {Applied Optics},
	author = {Burton, G. J. and Moorehead, I. R.},
	year = {1987},
	pages = {157--170},
}

@article{brainard_color_2015,
	title = {Color and the cone mosaic},
	volume = {1},
	journal = {Annual Review of Vision Science},
	author = {Brainard, D. H.},
	year = {2015},
	pages = {519--546},
}

@article{shevell_color_2017,
	title = {Color opponency: {Tutorial}},
	volume = {34},
	number = {7},
	journal = {Journal of the Optical Society of America A},
	author = {Shevell, S. K. and Martin, P. R.},
	year = {2017},
	pages = {1099--1108},
}

@inproceedings{wald_applying_2006,
	title = {Applying {Ray} {Tracing} for {Virtual} {Reality} and {Industrial} {Design}},
	booktitle = {2006 {IEEE} {Symposium} on {Interactive} {Ray} {Tracing}},
	publisher = {ieeexplore.ieee.org},
	author = {Wald, Ingo and Dietrich, Andreas and Benthin, Carsten and Efremov, Alexander and Dahmen, Tim and Gunther, Johannes and Havran, Vlastimil and Seidel, Hans-Peter and Slusallek, Philipp},
	month = sep,
	year = {2006},
	pages = {177--185},
}

@incollection{von_kries_chromatic_1902,
	address = {1970, Cambridge, MA},
	title = {Chromatic adaptation},
	booktitle = {Sources of {Color} {Vision}},
	publisher = {MIT Press},
	author = {von Kries, J.},
	year = {1902},
	pages = {109--119},
}

@book{lee_bayesian_1989,
	address = {London},
	title = {Bayesian {Statistics}},
	publisher = {Oxford University Press},
	author = {Lee, P. M.},
	year = {1989},
}

@article{brainard_asymmetric_1992,
	title = {Asymmetric color-matching: {How} color appearance depends on the illuminant},
	volume = {9},
	number = {9},
	journal = {Journal of the Optical Society of America A},
	author = {Brainard, D. H. and Wandell, B. A.},
	year = {1992},
	pages = {1433--1448},
}

@article{zhang_image_2021,
	title = {An image reconstruction framework for characterizing early vision},
	doi = {10.1101/2021.06.02.446829},
	journal = {bioRxiv},
	author = {Zhang, L. and Cottaris, N. P. and Brainard, D. H.},
	year = {2021},
}

@techreport{brainard_ideal_1995,
	title = {An ideal observer for appearance: {Reconstruction} from samples},
	number = {95-1},
	institution = {UCSB Vision Labs Technical Report},
	author = {Brainard, D. H.},
	year = {1995},
}

@article{burns_psychophysical_1987,
	title = {A psychophysical technique for measuring cone photopigment bleaching},
	volume = {28},
	number = {4},
	journal = {Investigative Ophthalmology \& Visual Science},
	author = {Burns, S A and Elsner, A E and Lobes, Jr, L A and Doft, B H},
	month = apr,
	year = {1987},
	note = {Publisher: iovs.arvojournals.org},
	pages = {711--717},
}

@article{cottaris_computational_2020,
	title = {A computational observer model of spatial contrast sensitivity: {Effects} of photocurrent encoding, fixational eye movements, and inference engine},
	volume = {20},
	number = {7},
	journal = {Journal of Vision},
	author = {Cottaris, N. P. and Wandell, B. A. and Rieke, F. and Brainard, D. H.},
	year = {2020},
	pages = {17},
}

@article{engbert_microsaccades_2004-1,
	title = {Microsaccades keep the eyes' balance during fixation},
	volume = {15},
	number = {6},
	journal = {Psychological Science},
	author = {Engbert, R. and Kliegl, R.},
	year = {2004},
	pages = {431--436},
}

@article{krantz_color_1975,
	title = {Color measurement and color theory: {I}. {Representation} theorem for {Grassmann} structures},
	volume = {12},
	url = {https://www.sciencedirect.com/science/article/pii/0022249675900267},
	number = {3},
	journal = {J. Math. Psychol.},
	author = {Krantz, David H},
	month = aug,
	year = {1975},
	pages = {283--303},
}

@article{webster_factors_1988,
	title = {Factors underlying individual differences in the color matches of normal observers},
	volume = {5},
	number = {10},
	journal = {Journal of the Optical Society of America A},
	author = {Webster, M. A. and MacLeod, D. I. A.},
	year = {1988},
	pages = {1722--1735},
}

@article{stockman_formulae_2023,
	title = {Formulae for generating standard and individual human cone spectral sensitivities},
	volume = {48},
	number = {6},
	journal = {Color Research \& Application},
	author = {Stockman, A. and Rider, A. T.},
	year = {2023},
	pages = {818--840},
}

@article{asano_individual_2016,
	title = {Individual {Colorimetric} {Observer} {Model}},
	volume = {11},
	doi = {10.1371/journal.pone.0145671},
	number = {2},
	journal = {PLoS ONE},
	author = {Asano, Y. and Fairchild, M. D. and Blonde, L.},
	year = {2016},
	pages = {e0145671},
}

@article{judd_relation_1964,
	title = {Relation between normal trichromatic vision and dichromatic vision representing a reduced form of normal vision},
	volume = {1},
	number = {3},
	journal = {Acta Chromatica},
	author = {Judd, Deane B.},
	year = {1964},
	pages = {524--527},
	annote = {Available in NBS Special Publication Series 300, 1972, Precision Measurement and Calibration: Colorimetry, Volume 9, 147-150.},
}

@incollection{moreira_colorimetry_2017,
	title = {Colorimetry and {Dichromatic} {Vision}},
	booktitle = {Colorimetry and {Image} {Processing}},
	publisher = {InTechOpen},
	author = {Moreira, Humberto and Álvaro, Leticia and Melnikova, Anna and Lillo, Julio},
	editor = {Travieso-Gonzalez, Carlos M.},
	year = {2017},
	doi = {10.5772/intechopen.71563},
}

@article{stockman_spectral_1993,
	title = {Spectral sensitivities of the human cones},
	volume = {10},
	url = {http://dx.doi.org/10.1364/josaa.10.002491},
	language = {en},
	number = {12},
	journal = {J. Opt. Soc. Am. A Opt. Image Sci. Vis.},
	author = {Stockman, A and MacLeod, D I and Johnson, N E},
	month = dec,
	year = {1993},
	pages = {2491--2521},
}

@techreport{cie_fundamental_2007-1,
	address = {Vienna},
	title = {Fundamental chromaticity diagram with physiological axes – {Parts} 1 and 2},
	number = {170-1},
	institution = {Central Bureau of the Commission Internationale de l'Éclairage},
	author = {{CIE}},
	year = {2007},
}

@article{nuberg_issledovanie_1955,
	title = {Issledovanie cvetovogo zrenija dikhromatov},
	journal = {Trudy Gosudarstvennogo Opticheskogo Instituta},
	author = {Nuberg, N D and Yustova, E N},
	year = {1955},
}

@article{smith_spectral_1972,
	title = {Spectral sensitivity of color-blind observers and the cone photopigments},
	volume = {12},
	url = {http://dx.doi.org/10.1016/0042-6989(72)90058-2},
	language = {en},
	number = {12},
	journal = {Vision Res.},
	author = {Smith, V C and Pokorny, J},
	month = dec,
	year = {1972},
	pages = {2059--2071},
}

@misc{stockman_colour_2001,
	title = {Colour and {Vision} {Research} {Laboratory}},
	url = {http://www.cvrl.org/},
	author = {Stockman, Andrew},
	year = {2001},
	note = {Publication Title: CVRL},
	annote = {Accessed: 2019-2-19},
}

@article{afifi_sensor-independent_2019,
	title = {Sensor-independent illumination estimation for {DNN} models},
	journal = {arXiv preprint arXiv:1912.06888},
	author = {Afifi, Mahmoud and Brown, Michael S},
	year = {2019},
}

@book{strang_introduction_2022,
	title = {Introduction to linear algebra},
	url = {https://epubs.siam.org/doi/pdf/10.1137/1.9781733146678.fm},
	publisher = {SIAM},
	author = {Strang, Gilbert},
	year = {2022},
}

@article{vos_improved_1990,
	title = {Improved color fundamentals offer a new view on photometric additivity},
	volume = {30},
	url = {http://dx.doi.org/10.1016/0042-6989(90)90059-t},
	language = {en},
	number = {6},
	journal = {Vision Res.},
	author = {Vos, J J and Estévez, O and Walraven, P L},
	year = {1990},
	pages = {937--943},
}

@incollection{brown_color_2023,
	title = {Color {Processing} for {Digital} {Cameras}},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/9781119827214.ch5},
	booktitle = {Fundamentals and {Applications} of {Colour} {Engineering}},
	publisher = {Wiley},
	author = {Brown, Michael S.},
	editor = {Green, Phil},
	month = oct,
	year = {2023},
	pages = {81--98},
}

@article{delbracio_mobile_2021,
	title = {Mobile {Computational} {Photography}: {A} {Tour}},
	volume = {7},
	url = {http://dx.doi.org/10.1146/annurev-vision-093019-115521},
	language = {en},
	journal = {Annu Rev Vis Sci},
	author = {Delbracio, Mauricio and Kelly, Damien and Brown, Michael S and Milanfar, Peyman},
	month = sep,
	year = {2021},
	keywords = {computational photography, burst processing, camera pipeline, low-light imaging, mobile camera, noise reduction, super-resolution, zoom},
	pages = {571--604},
}

@article{pokorny_fifty_2020,
	title = {Fifty {Years} {Exploring} the {Visual} {System}},
	volume = {6},
	url = {http://dx.doi.org/10.1146/annurev-vision-121219-081824},
	language = {en},
	journal = {Annu Rev Vis Sci},
	author = {Pokorny, Joel and Smith, Vivianne C},
	month = sep,
	year = {2020},
	keywords = {autobiography, color vision, melanopsin, photopigments, retinal physiology, visual pathways},
	pages = {1--23},
}

@article{vos_derivation_1971,
	title = {On the derivation of the foveal receptor primaries},
	volume = {11},
	url = {http://dx.doi.org/10.1016/0042-6989(71)90003-4},
	language = {en},
	number = {8},
	journal = {Vision Res.},
	author = {Vos, J J and Walraven, P L},
	month = aug,
	year = {1971},
	pages = {799--818},
}

@article{bouma_mathematical_1942,
	title = {Mathematical relationship between the colour vision systems of trichromats and dichromats},
	volume = {9},
	url = {https://www.sciencedirect.com/science/article/pii/S0031891442800543},
	number = {8},
	journal = {Physica},
	author = {Bouma, P J},
	month = sep,
	year = {1942},
	pages = {773--784},
}

@article{luther_grundlagen_1928,
	title = {Grundlagen der {Farbmessung}},
	volume = {9},
	number = {5},
	journal = {Zeitschrift für technische Physik},
	author = {Luther, Robert},
	year = {1928},
	pages = {207--222},
}

@article{howard_helmholtzhering_1999,
	title = {The {Helmholtz}–{Hering} debate in retrospect},
	volume = {28},
	url = {https://journals.sagepub.com/doi/pdf/10.1068/p2805ed},
	language = {en},
	number = {5},
	journal = {Perception},
	author = {Howard, I P},
	year = {1999},
	note = {Publisher: SAGE Publications},
	pages = {543--549},
}

@book{brindley_physiology_1970,
	title = {Physiology of the {Retina} and {Visual} {Pathway}},
	url = {https://play.google.com/store/books/details?id=Ee1EAAAAIAAJ},
	language = {en},
	publisher = {Williams \& Wilkins},
	author = {Brindley, Giles Skey},
	year = {1970},
}

@incollection{pitt_characteristics_1935,
	title = {Characteristics of {Dichromatic} {Vision}},
	url = {https://www.cabdirect.org/cabdirect/abstract/19352701859},
	number = {14},
	booktitle = {Reports of the {Committee} upon the {Physiology} of {Vision}},
	publisher = {Medical Research Council},
	author = {Pitt, F.H.G.},
	year = {1935},
	pages = {1--55},
}

@article{wright_characteristics_1952,
	title = {The characteristics of tritanopia},
	volume = {42},
	url = {http://dx.doi.org/10.1364/josa.42.000509},
	language = {en},
	number = {8},
	journal = {J. Opt. Soc. Am.},
	author = {Wright, W D},
	month = aug,
	year = {1952},
	keywords = {COLOR VISION},
	pages = {509--521},
}

@book{wright_colour_1938,
	title = {Colour {Vision}. {Research} on {Normal} and {Defective} {Colour} {Vision}},
	url = {https://play.google.com/store/books/details?id=UTxjXwAACAAJ},
	language = {en},
	publisher = {C.V. Mosby},
	author = {Wright, W D},
	year = {1938},
}

@incollection{mollon_origins_2003,
	title = {The {Origins} of {Modern} {Color} {Science}},
	booktitle = {The {Science} of {Color}},
	publisher = {Elsevier},
	author = {Mollon, J D},
	editor = {Shevell, Steven K},
	year = {2003},
	pages = {1--39},
}

@article{wandell_visual_2022,
	title = {Visual encoding: {Principles} and software},
	volume = {273},
	url = {http://dx.doi.org/10.1016/bs.pbr.2022.04.006},
	language = {en},
	number = {1},
	journal = {Prog. Brain Res.},
	author = {Wandell, Brian A and Brainard, David H and Cottaris, Nicolas P},
	month = jun,
	year = {2022},
	note = {Publisher: Elsevier B.V.},
	keywords = {Computer graphics, Optics, Cone fundamentals, Cone mosaic, Display, ipRGC, Irradiance, Lens, Photoreceptor, Pigment density, Point spread function, Radiance, Retina},
	pages = {199--229},
}

@article{judd_standard_1945,
	title = {Standard response functions for protanopic and deuteranopic vision},
	volume = {35},
	url = {https://opg.optica.org/abstract.cfm?uri=josa-35-3-199},
	language = {en},
	number = {3},
	journal = {J. Opt. Soc. Am.},
	author = {Judd, Deane B},
	month = mar,
	year = {1945},
	note = {Publisher: The Optical Society},
	keywords = {Color vision, Crystalline lens, Field size, Optic nerve, Standard observers, Visual system},
	pages = {199},
}

@article{wright_colour-vision_1935,
	title = {The colour-vision characteristics of two trichromats},
	volume = {47},
	url = {https://iopscience.iop.org/article/10.1088/0959-5309/47/2/302/meta},
	language = {en},
	number = {2},
	journal = {Proc. Phys. Soc. London},
	author = {Wright, W D and Pitt, F H G},
	month = mar,
	year = {1935},
	note = {Publisher: IOP Publishing},
	pages = {205},
}

@incollection{turner_origins_1996,
	title = {The origins of colorimetry: {What} did {Helmholtz} and {Maxwell} learn from {Grassmann}?},
	url = {https://link.springer.com/chapter/10.1007/978-94-015-8753-2_8},
	booktitle = {Hermann {Günther} {Graßmann} (1809–1877): {Visionary} {Mathematician}, {Scientist} and {Neohumanist} {Scholar}},
	publisher = {Springer},
	author = {Turner, R Steven},
	year = {1996},
	pages = {71--86},
}

@misc{normann_james_2023,
	title = {James {Clerk} {Maxwell} {Produces} the {First} {Color} {Photograph}},
	url = {https://www.historyofinformation.com/detail.php?id=3666},
	language = {en},
	author = {Normann, J.},
	year = {2023},
	annote = {Accessed: 2023-8-15},
}

@article{brewster_iv_1834,
	title = {{IV}. {On} a {New} {Analysis} of {Solar} {Light}, indicating three {Primary} {Colours}, forming {Coincident} {Spectra} of equal length},
	volume = {12},
	url = {https://www.cambridge.org/core/journals/earth-and-environmental-science-transactions-of-royal-society-of-edinburgh/article/iv-on-a-new-analysis-of-solar-light-indicating-three-primary-colours-forming-coincident-spectra-of-equal-length/9CA6593A5F2116180FC4D80D2ED00D6B},
	number = {1},
	journal = {Earth Environ. Sci. Trans. R. Soc. Edinb.},
	author = {Brewster, David},
	month = jan,
	year = {1834},
	note = {Publisher: Royal Society of Edinburgh Scotland Foundation},
	pages = {123--136},
}

@article{judd_maxwell_1961,
	title = {Maxwell and {Modern} {Colorimetry}},
	volume = {9},
	url = {https://doi.org/10.1080/00223638.1961.11736821},
	number = {6},
	journal = {The Journal of Photographic Science},
	author = {Judd, D B},
	month = nov,
	year = {1961},
	note = {Publisher: Taylor \& Francis},
	pages = {341--352},
}

@article{smith_spectral_1975,
	title = {Spectral sensitivity of the foveal cone photopigments between 400 and 500 nm},
	volume = {15},
	url = {http://dx.doi.org/10.1016/0042-6989(75)90203-5},
	language = {en},
	number = {2},
	journal = {Vision Res.},
	author = {Smith, V C and Pokorny, J},
	month = feb,
	year = {1975},
	note = {Publisher: Elsevier},
	pages = {161--171},
}

@misc{heesen_young-helmholtz-maxwell_2015,
	title = {The {Young}-({Helmholtz})-{Maxwell} {Theory} of {Color} {Vision}},
	url = {http://philsci-archive.pitt.edu/11279/},
	author = {Heesen, Remco},
	month = jan,
	year = {2015},
	keywords = {Hermann von Helmholtz, History of color mixing, History of physiology, James Clerk Maxwell, Priority},
}

@article{stockman_cone_2019,
	title = {Cone fundamentals and {CIE} standards},
	volume = {30},
	url = {https://www.sciencedirect.com/science/article/pii/S2352154619300221},
	journal = {Current Opinion in Behavioral Sciences},
	author = {Stockman, Andrew},
	month = dec,
	year = {2019},
	pages = {87--93},
}

@article{baylor_photoreceptor_1987,
	title = {Photoreceptor signals and vision. {Proctor} lecture},
	volume = {28},
	url = {https://www.ncbi.nlm.nih.gov/pubmed/3026986},
	language = {en},
	number = {1},
	journal = {Invest. Ophthalmol. Vis. Sci.},
	author = {Baylor, D A},
	month = jan,
	year = {1987},
	pages = {34--49},
}

@article{grassmann_xxxvii_1854,
	title = {{XXXVII}. {On} the theory of compound colours},
	volume = {7},
	url = {https://www.tandfonline.com/doi/pdf/10.1080/14786445408647464},
	number = {45},
	journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
	author = {{Grassmann}},
	year = {1854},
	note = {Publisher: Taylor \& Francis},
	pages = {254--264},
}

@book{hering_grundzuge_1964,
	address = {Cambridge, MA},
	title = {Grundzüge der {Lehre} vom {Lichtsinn} (1874) ({Outlines} of a theory of the light sense, translated by {Hurvich} and {Jameson})},
	publisher = {Harvard University Press},
	author = {Hering, E.},
	translator = {Hurvich, Leo M. and Jameson, Dorothea},
	year = {1964},
}

@article{judd_fundamental_1966,
	title = {Fundamental studies of color vision from 1860 to 1960},
	volume = {55},
	url = {http://dx.doi.org/10.1073/pnas.55.6.1313},
	language = {en},
	number = {6},
	journal = {Proc. Natl. Acad. Sci. U. S. A.},
	author = {Judd, D B},
	month = jun,
	year = {1966},
	pages = {1313--1330},
}

@article{maxwell_theory_1993,
	title = {On the theory of compound colours, and the relations of the colours of the spectrum},
	volume = {18},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/col.5080180411?casa_token=SwuswNp7dxQAAAAA:T-GCEGPpelKj8wW44tCxLhRp8-_JoQP1ycC0Zo6XAPB7kpxmYNXuAgOxO7rkh2F87FuX8P1pRVmnO5s},
	number = {4},
	journal = {Color Res. Appl.},
	author = {Maxwell, James Clerk and Zaidi, Qasim},
	year = {1993},
	note = {Publisher: Wiley Online Library},
	pages = {270--287},
}

@article{maxwell_diagram_1857,
	title = {The diagram of colors},
	volume = {21},
	journal = {Trans. R. Soc. Edinburgh},
	author = {Maxwell, James Clerk},
	year = {1857},
	pages = {275--298},
}

@article{logvinenko_derivation_1998,
	title = {On derivation of spectral sensitivities of the human cones from trichromatic colour matching functions},
	volume = {38},
	url = {http://dx.doi.org/10.1016/s0042-6989(98)00029-7},
	language = {en},
	number = {21},
	journal = {Vision Res.},
	author = {Logvinenko, A D},
	month = nov,
	year = {1998},
	pages = {3207--3211},
}

@article{maxwell_iv_1860,
	title = {{IV}. {On} the theory of compound colours, and the relations of the colours of the spectrum},
	volume = {150},
	url = {https://doi.org/10.1098/rstl.1860.0005},
	language = {en},
	number = {0},
	journal = {Philos. Trans. R. Soc. Lond.},
	author = {Maxwell, J C},
	month = dec,
	year = {1860},
	note = {Publisher: The Royal Society},
	pages = {57--84},
}

@book{hering_outlines_1964,
	address = {Cambridge, MA, US},
	title = {Outlines of a theory of the light sense},
	publisher = {Harvard University Press},
	author = {Hering, Ewald},
	year = {1964},
	annote = {translated by Hurvich, Leo and Jameson, Dorothea},
}

@article{helmholtz_lxxxi_1852,
	title = {{LXXXI}. {On} the theory of compound colours},
	volume = {4},
	url = {https://doi.org/10.1080/14786445208647175},
	number = {28},
	journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
	author = {Helmholtz, H},
	month = jan,
	year = {1852},
	note = {Publisher: Taylor \& Francis},
	pages = {519--534},
}

@article{konig_grundempfindungen_1886,
	title = {Die {Grundempfindungen} und ihre {Intensitäts}-{Vertheilung} im {Spectrum} ({Fundamental} sensations and their intensity distribution in the spectrum)},
	journal = {Sitzungsberichte der Akademie der Wissenschaften},
	author = {Konig, A and {Dieterici, C (translated by Kuehni, Rolf G)}},
	month = jul,
	year = {1886},
	pages = {805--829},
}

@article{blasinski_optimizing_2018,
	title = {Optimizing image acquisition systems for autonomous driving},
	volume = {2018},
	number = {5},
	journal = {Electronic Imaging},
	author = {Blasinski, Henryk and Farrell, Joyce and Lian, Trisha and Liu, Zhenyi and Wandell, Brian},
	year = {2018},
	note = {Publisher: Society for Imaging Science and Technology},
	keywords = {AUTONOMOUS VEHICLES, IMAGE SENSORS, IMAGE SYSTEMS SIMULATION, MACHINE LEARNING},
	pages = {161--1},
}

@article{schutera_night--day_2021,
	title = {Night-to-{Day}: {Online} {Image}-to-{Image} {Translation} for {Object} {Detection} {Within} {Autonomous} {Driving} by {Night}},
	volume = {6},
	abstract = {Object detectors are central to autonomous driving and are widely used in driver assistance systems. Object detectors are trained on a finite amount of data within a specific domain, hampering detection performance when applying object detectors to samples from other domains during inference, an effect known as domain gap. Domain gap is a concern for data-driven applications, evoking repetitive retraining of networks when the applications unfold into other domains. With object detectors that have been trained on day images only, a domain gap can be observed in object detection by night. Training object detectors on night images is critical because of the enormous effort required to generate an adequate amount of diversely labeled data, and existing data sets often tend to overfit specific domain characteristics. For the first time, this work proposes adapting domains by online image-to-image translation to expand an object detector's domain of operation. The domain gap is decreased without additional labeling effort and without having to retrain the object detector while unfolding into the target domain. The approach follows the concept of domain adaptation, shifting the target domain samples into the domain knownto the object detector (source domain). Firstly, the UNIT network is trained for domain adaptation and subsequently cast into an online domain adaptation module, which narrows down the domain gap. Domain adaptation capabilities are evaluated qualitatively by displaying translated samples and visualizing the domain shift through the 2D tSNE algorithm. We quantitatively benchmark the domain adaptation's influence on a state-of-the-art object detector, and on a retrained object detector, for mean average precision, mean recall, and the resulting F1-score. Our approach achieves an F1 score improvement of 5.27 \% within object detection by night when applying online domain adaptation. The evaluation is executed on the BDD100K benchmark data set.},
	number = {3},
	journal = {IEEE Transactions on Intelligent Vehicles},
	author = {Schutera, Mark and Hussein, Mostafa and Abhau, Jochen and Mikut, Ralf and Reischl, Markus},
	month = sep,
	year = {2021},
	keywords = {Autonomous vehicles, Neural networks, Detectors, object detection, Object detection, Training, domain adaptation, Gallium nitride, image-to-image translation, BDD100 K, Generators, UNIT architecture},
	pages = {480--489},
}

@article{liu_neural_2020,
	title = {Neural {Network} {Generalization}: {The} impact of camera parameters},
	abstract = {We quantify the generalization of a convolutional neural network (CNN) trained to identify cars. First, we perform a series of experiments to train the network using one image dataset- either synthetic or from a camera-and then test on a different image dataset. We show that …},
	journal = {IEEE Access},
	author = {Liu, Z and Lian, T and Farrell, J and Wandell, B A},
	year = {2020},
	note = {Publisher: ieeexplore.ieee.org},
	keywords = {convolutional neural network, Imaging systems, physically based ray tracing, autonomous driving, camera design, network generalization},
}

@article{jiang_learning_2017,
	title = {Learning the image processing pipeline},
	volume = {26},
	number = {10},
	journal = {IEEE Transactions on Image Processing},
	author = {Jiang, Haomiao and Tian, Qiyuan and Farrell, Joyce and Wandell, Brian A},
	year = {2017},
	note = {Publisher: IEEE},
	pages = {5032--5042},
}

@inproceedings{blasinski_computational_2017,
	title = {Computational multispectral flash},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computational} {Photography} ({ICCP})},
	publisher = {IEEE},
	author = {Blasinski, Henryk and Farrell, Joyce},
	year = {2017},
	keywords = {Image color analysis, Cameras, Lighting, Estimation, Narrowband, Photonics, Power distribution},
	pages = {1--10},
}

@article{cottaris_computational_2019,
	title = {A computational observer model of spatial contrast sensitivity: {Effects} of wavefront-based optics, cone mosaic structure, and inference engine},
	volume = {19},
	number = {4},
	journal = {Journal of Vision},
	author = {Cottaris, N. P. and Jiang, H. and Ding, X. and Wandell, B. A. and Brainard, D. H.},
	year = {2019},
	pages = {8},
}

@misc{noauthor_tests_nodate,
	title = {Tests: {Images} – {Image} {Systems} {Engineering}},
	shorttitle = {Tests},
	url = {https://foundationsofimagesystems.vista.su.domains/blog/image-tests/},
	language = {en},
	urldate = {2024-09-30},
	file = {Snapshot:/Users/wandell/Zotero/storage/DPL6B4RC/image-tests.html:text/html},
}

@article{noauthor_notitle_nodate-1,
}
