@inproceedings{adamsDesignPracticalColor2002,
  title = {Design of Practical Color Filter Array Interpolation Algorithms for Digital Cameras. {{Part}} 2.},
  booktitle = {Proceedings 1998 {{International Conference}} on {{Image Processing}}. {{ICIP98}} ({{Cat}}. {{No}}.{{98CB36269}})},
  author = {Adams, J E},
  year = {2002},
  publisher = {IEEE Comput. Soc}
}

@incollection{adelsonPlenopticFunctionElements1991,
  title = {The Plenoptic Function and the Elements of Early Vision},
  booktitle = {Computational {{Models}} of {{Visual Processing}}},
  author = {Adelson, E. H. and Bergen, J.R.},
  editor = {Landy, M.S. and Movshon, J.A.},
  year = {1991},
  pages = {3--20},
  publisher = {MIT Press},
  address = {Cambridge}
}

@article{adelsonSingleLensStereo1992,
  title = {Single Lens Stereo with a Plenoptic Camera},
  author = {Adelson, E H and Wang, J Y A},
  year = {1992},
  month = feb,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {14},
  number = {2},
  pages = {99--106},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}}
}

@article{afifiSensorindependentIlluminationEstimation2019,
  title = {Sensor-Independent Illumination Estimation for {{DNN}} Models},
  author = {Afifi, Mahmoud and Brown, Michael S},
  year = {2019},
  journal = {arXiv preprint arXiv:1912.06888},
  eprint = {1912.06888},
  archiveprefix = {arXiv}
}

@article{artalOpoticaldigitalProcedureDetermination1989,
  title = {Opotical-Digital Procedure for the Determination of White-Light Retinal Images of a Point Test},
  author = {Artal, P. and Santamaria, J. and Bescos, J.},
  year = {1989},
  journal = {Opt. Eng.},
  volume = {28},
  pages = {687--690}
}

@article{asanoIndividualColorimetricObserver2016,
  title = {Individual {{Colorimetric Observer Model}}},
  author = {Asano, Y. and Fairchild, M. D. and Blonde, L.},
  year = {2016},
  journal = {PLoS ONE},
  volume = {11},
  number = {2},
  pages = {e0145671},
  doi = {10.1371/journal.pone.0145671}
}

@article{atapour-abarghoueiCompleteEstimateThat2019,
  title = {To Complete or to Estimate, That Is the Question: {{A Multi-Task Approach}} to {{Depth Completion}} and {{Monocular Depth Estimation}}},
  author = {{Atapour-Abarghouei}, Amir and Breckon, Toby P},
  year = {2019},
  month = aug,
  abstract = {Robust three-dimensional scene understanding is now an ever-growing area of research highly relevant in many real-world applications such as autonomous driving and robotic navigation. In this paper, we propose a multi-task learning-based model capable of performing two tasks:- sparse depth completion (i.e. generating complete dense scene depth given a sparse depth image as the input) and monocular depth estimation (i.e. predicting scene depth from a single RGB image) via two sub-networks jointly trained end to end using data randomly sampled from a publicly available corpus of synthetic and real-world images. The first sub-network generates a sparse depth image by learning lower level features from the scene and the second predicts a full dense depth image of the entire scene, leading to a better geometric and contextual understanding of the scene and, as a result, superior performance of the approach. The entire model can be used to infer complete scene depth from a single RGB image or the second network can be used alone to perform depth completion given a sparse depth input. Using adversarial training, a robust objective function, a deep architecture relying on skip connections and a blend of synthetic and real-world training data, our approach is capable of producing superior high quality scene depth. Extensive experimental evaluation demonstrates the efficacy of our approach compared to contemporary state-of-the-art techniques across both problem domains.}
}

@misc{ayscough-image-wikipedia,
  title = {Da-{{Vinci-image}}},
  author = {Ayscough, James},
  year = {1755},
  howpublished = {https://commons.wikimedia.org/wiki/File:1755\_james\_ayscough.jpg}
}

@misc{ayscoughLightFieldImage1755,
  title = {Light Field Image},
  author = {Ayscough, James},
  year = {1755},
  journal = {Wikipedia}
}

@book{ayscoughShortAccountEye1755,
  title = {{A short account of the eye and nature of vision. Chiefly designed to illustrate the use and advantage of spectacles. ... By James Ayscough, optician. ... - The fourth edition.  1755}},
  author = {Ayscough, James},
  year = {1755},
  urldate = {2024-10-25},
  abstract = {A short account of the eye and nature of vision. Chiefly designed to illustrate the use and advantage of spectacles. ... By James Ayscough, optician. ... - The fourth edition. 1755.. Digitized from IA40316215-78. Previous issue: bim\_eighteenth-century\_dissertatio-medica-inaug\_elston-john\_1788.},
  collaborator = {{Internet Archive}},
  langid = {Middle English},
  keywords = {Language & Literature}
}

@article{balasubramanianMetabolicallyEfficientInformation2001,
  title = {Metabolically Efficient Information Processing},
  author = {Balasubramanian, V. and Kimber, D. and Berry, M. J., 2nd},
  year = {2001},
  journal = {Neural Computation},
  volume = {13},
  number = {4},
  pages = {799--815}
}

@article{banksPhysicalLimitsGrating1987,
  title = {The Physical Limits of Grating Visibility},
  author = {Banks, M. S and Geisler, W. S. and Bennett, P. J.},
  year = {1987},
  journal = {Vision Research},
  volume = {27},
  number = {11},
  pages = {1915--1924}
}

@article{banterleAdvancedHighDynamic2011,
  title = {Advanced High Dynamic Range Imaging: {{Theory}} and Practice},
  author = {Banterle, F and Artusi, A and Debattista, Kurt and Chalmers, A},
  year = {2011},
  month = feb,
  publisher = {taylorfrancis.com}
}

@inproceedings{bastosOverviewLiDARRequirements2021,
  title = {An Overview of {{LiDAR}} Requirements and Techniques for Autonomous Driving},
  booktitle = {2021 {{Telecoms Conference}} ({{Conf}}TEL{{E}})},
  author = {Bastos, Daniel and Monteiro, Paulo P and Oliveira, Arnaldo S R and Drummond, Miguel V},
  year = {2021},
  month = feb,
  pages = {1--6},
  publisher = {IEEE},
  langid = {english}
}

@misc{bayerColorImagingArray1976,
  title = {Color Imaging Array},
  author = {Bayer, B},
  year = {1976},
  journal = {United States Patent, no. 3971065}
}

@article{baylorPhotoreceptorSignalsVision1987,
  title = {Photoreceptor Signals and Vision. {{Proctor}} Lecture},
  author = {Baylor, D A},
  year = {1987},
  month = jan,
  journal = {Invest. Ophthalmol. Vis. Sci.},
  volume = {28},
  number = {1},
  eprint = {3026986},
  eprinttype = {pubmed},
  pages = {34--49},
  langid = {english}
}

@article{baylorResponsesRetinalRods1979,
  title = {Responses of Retinal Rods to Single Photons},
  author = {Baylor, D. A. and Lamb, T. D. and Yau, K. W.},
  year = {1979},
  journal = {Journal of Physiology},
  volume = {288},
  number = {Mar},
  pages = {613--634}
}

@article{bedfordAxialChromaticAberration1957,
  title = {Axial {{Chromatic Aberration}} of the {{Human Eye}}},
  author = {Bedford, R. E. and Wyszecki, G.},
  year = {1957},
  journal = {J. Opt. Soc. Am.},
  volume = {47},
  number = {6},
  pages = {564--565}
}

@inproceedings{behrischSumoSimulationUrban2011,
  title = {Sumo--Simulation of Urban Mobility},
  booktitle = {The {{Third International Conference}} on {{Advances}} in {{System Simulation}} ({{SIMUL}} 2011), {{Barcelona}}, {{Spain}}},
  author = {Behrisch, Michael and Bieker, Laura and Erdmann, Jakob and Krajzewicz, Daniel},
  year = {2011},
  volume = {42},
  keywords = {NewAccount}
}

@book{bergerStatisticalDecisionTheory1985,
  title = {Statistical {{Decision Theory}} and {{Bayesian Analysis}}},
  author = {Berger, T. O.},
  year = {1985},
  publisher = {Springer-Verlag},
  address = {New York}
}

@inproceedings{bergmanDeepAdaptiveLiDAR2020,
  title = {Deep {{Adaptive LiDAR}}: {{End-to-end Optimization}} of {{Sampling}} and {{Depth Completion}} at {{Low Sampling Rates}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Computational Photography}} ({{ICCP}})},
  author = {Bergman, A W and Lindell, D B and Wetzstein, G},
  year = {2020},
  month = apr,
  pages = {1--11},
  abstract = {Current LiDAR systems are limited in their ability to capture dense 3D point clouds. To overcome this challenge, deep learning-based depth completion algorithms have been developed to inpaint missing depth guided by an RGB image. However, these methods fail for low sampling rates. Here, we propose an adaptive sampling scheme for LiDAR systems that demonstrates state-of-the-art performance for depth completion at low sampling rates. Our system is fully differentiable, allowing the sparse depth sampling and the depth inpainting components to be trained end-to-end with an upstream task.},
  keywords = {adaptive sampling,adaptive sampling scheme,current LiDAR systems,deep adaptive LiDAR,deep learning-based depth completion algorithms,dense 3D point clouds,depth imaging,depth inpainting components,end-to-end optimization,image colour analysis,image restoration,image sampling,learning (artificial intelligence),LiDAR,low sampling rates,optical images,optical radar,radar imaging,sparse depth sampling}
}

@book{bishopPatternRecognitionMachine2006,
  title = {Pattern {{Recognition}} and {{Machine Learning}}},
  author = {Bishop, C. M.},
  year = {2006},
  publisher = {Springer Science + Business Media LLC},
  address = {New York}
}

@misc{bitterliRenderingResources2016,
  title = {Rendering Resources},
  author = {Bitterli, Benedikt},
  year = {2016}
}

@inproceedings{blasinskiComputationalMultispectralFlash2017,
  title = {Computational Multispectral Flash},
  booktitle = {2017 {{IEEE International Conference}} on {{Computational Photography}} ({{ICCP}})},
  author = {Blasinski, Henryk and Farrell, Joyce},
  year = {2017},
  pages = {1--10},
  publisher = {IEEE},
  keywords = {Cameras,Estimation,Image color analysis,Lighting,Narrowband,Photonics,Power distribution}
}

@inproceedings{blasinskiModelEstimatingSpectral2014,
  title = {A Model for Estimating Spectral Properties of Water from {{RGB}} Images},
  booktitle = {2014 {{IEEE International Conference}} on {{Image Processing}} ({{ICIP}})},
  author = {Blasinski, Henryk and Breneman, John and Farrell, Joyce},
  year = {2014},
  pages = {610--614},
  doi = {10.1109/ICIP.2014.7025122}
}

@article{blasinskiOptimizingImageAcquisition2018,
  title = {Optimizing Image Acquisition Systems for Autonomous Driving},
  author = {Blasinski, Henryk and Farrell, Joyce and Lian, Trisha and Liu, Zhenyi and Wandell, Brian},
  year = {2018},
  journal = {Electronic Imaging},
  volume = {2018},
  number = {5},
  pages = {161--1},
  publisher = {{Society for Imaging Science and Technology}},
  keywords = {AUTONOMOUS VEHICLES,IMAGE SENSORS,IMAGE SYSTEMS SIMULATION,MACHINE LEARNING}
}

@inproceedings{blasinskiThreeParameterUnderwater2016,
  title = {A {{Three Parameter Underwater Image Formation Model}}},
  booktitle = {Digital {{Photography}} and {{Mobile Imaging}}},
  author = {Blasinski, Henryk and Farrell, Joyce E.},
  year = {2016}
}

@inproceedings{blasinskiUnderwaterImageSystems2017,
  title = {Underwater {{Image Systems Simulation}}},
  booktitle = {Imaging and {{Applied Optics}} 2017 ({{3D}}, {{AIO}}, {{COSI}}, {{IS}}, {{MATH}}, {{pcAOP}})},
  author = {Blasinski, Henryk and Lian, Trisha and Farrell, Joyce},
  year = {2017},
  month = jun,
  publisher = {Optical Society of America},
  address = {San Francisco, California United States},
  abstract = {We use modern computer graphics tools such as ray-tracing, digital camera simulation tools, and a physically accurate model of seawater constituents to simulate how light is captured by the imaging sensor in a digital camera placed in underwater ocean environments. Our water model includes parameters for the type and amount of phytoplankton, the concentration of chlorophyll, the amount of dissolved organic matter (CDOM) and the concentration of detritus (non-algal particles, NAP). We show that by adjusting the model parameters, we can predict real sensor data captured by a digital camera at a fixed distance and depth from a reference target with known spectral reflectance. We also provide an open-source implementation of all our tools and simulations.},
  langid = {english},
  keywords = {Absorption coefficient,Beam scattering,Digital imaging,Light propagation,Light scattering,Underwater imaging}
}

@article{boothDesignConsiderationsDigital1977,
  title = {Design Considerations for Digital Image Processing Systems},
  author = {Booth, John M and Schroeder, John B},
  year = {1977},
  journal = {Computer},
  volume = {10},
  number = {8},
  pages = {15--20},
  publisher = {IEEE}
}

@article{bornerSENSORToolSimulation2001,
  title = {{{SENSOR}}: A Tool for the Simulation of Hyperspectral Remote Sensing Systems},
  author = {B{\"o}rner, Anko and Wiest, Lorenz and Keller, Peter and Reulke, Ralf and Richter, Rolf and Schaepman, Michael and Schl{\"a}pfer, Daniel},
  year = {2001},
  month = mar,
  journal = {ISPRS J. Photogramm. Remote Sens.},
  volume = {55},
  number = {5},
  pages = {299--312},
  abstract = {The consistent end-to-end simulation of airborne and spaceborne earth remote sensing systems is an important task, and sometimes the only way for the adaptation and optimisation of a sensor and its observation conditions, the choice and test of algorithms for data processing, error estimation and the evaluation of the capabilities of the whole sensor system. The presented software simulator SENSOR (Software Environment for the Simulation of Optical Remote sensing systems) includes a full model of the sensor hardware, the observed scene, and the atmosphere in between. The simulator consists of three parts. The first part describes the geometrical relations between scene, sun, and the remote sensing system using a ray-tracing algorithm. The second part of the simulation environment considers the radiometry. It calculates the at-sensor radiance using a pre-calculated multidimensional lookup-table taking the atmospheric influence on the radiation into account. The third part consists of an optical and an electronic sensor model for the generation of digital images. Using SENSOR for an optimisation requires the additional application of task-specific data processing algorithms. The principle of the end-to-end-simulation approach is explained, all relevant concepts of SENSOR are discussed, and first examples of its use are given. The verification of SENSOR is demonstrated. This work is closely related to the Airborne PRISM Experiment (APEX), an airborne imaging spectrometer funded by the European Space Agency.},
  keywords = {APEX,Hyperspectral,Optimisation,Sensor,Simulation}
}

@article{boumaMathematicalRelationshipColour1942,
  title = {Mathematical Relationship between the Colour Vision Systems of Trichromats and Dichromats},
  author = {Bouma, P J},
  year = {1942},
  month = sep,
  journal = {Physica},
  volume = {9},
  number = {8},
  pages = {773--784}
}

@book{bracewellFourierTransformIts1978,
  title = {The {{Fourier Transform}} and Its {{Applications}}},
  author = {Bracewell, R.N.},
  year = {1978},
  publisher = {McGraw-Hill}
}

@book{bracewellHartleyTransform1986,
  title = {The {{Hartley Transform}}},
  author = {Bracewell, R. N.},
  year = {1986},
  publisher = {Clarendon Press},
  address = {Oxford}
}

@article{brainardAsymmetricColormatchingHow1992,
  title = {Asymmetric Color-Matching: {{How}} Color Appearance Depends on the Illuminant},
  author = {Brainard, D. H. and Wandell, B. A.},
  year = {1992},
  journal = {Journal of the Optical Society of America A},
  volume = {9},
  number = {9},
  pages = {1433--1448}
}

@article{brainardColorConeMosaic2015,
  title = {Color and the Cone Mosaic},
  author = {Brainard, D. H.},
  year = {2015},
  journal = {Annual Review of Vision Science},
  volume = {1},
  pages = {519--546}
}

@incollection{brainardColorimetry2010,
  title = {Colorimetry},
  booktitle = {The {{Optical Society}} of {{America Handbook}} of {{Optics}}, 3rd Edition, {{Volume III}}: {{Vision}} and {{Vision Optics}}},
  author = {Brainard, D. H. and Stockman, A.},
  editor = {Bass, M. and DeCusatis, C. and Enoch, J. and Lakshminarayanan, V. and Li, G. and Macdonald, C. and Mahajan, V. and {van Stryland}, E.},
  year = {2010},
  pages = {10.1--10.56},
  publisher = {McGraw Hill},
  address = {New York}
}

@techreport{brainardIdealObserverAppearance1995,
  title = {An Ideal Observer for Appearance: {{Reconstruction}} from Samples},
  author = {Brainard, D. H.},
  year = {1995},
  number = {95-1},
  institution = {UCSB Vision Labs Technical Report}
}

@article{brainardTrichromaticReconstructionInterleaved2008,
  title = {Trichromatic Reconstruction from the Interleaved Cone Mosaic: {{Bayesian}} Model and the Color Appearance of Small Spots},
  author = {Brainard, D. H. and Williams, D. R. and Hofer, H.},
  year = {2008},
  journal = {Journal of Vision},
  volume = {8},
  number = {5},
  pages = {15 1--23}
}

@misc{branwynSkyAngles2016,
  title = {Sky {{Angles}}},
  author = {Branwyn, Gareth},
  year = {2016},
  month = sep,
  langid = {english}
}

@article{brewsterIVNewAnalysis1834,
  title = {{{IV}}. {{On}} a {{New Analysis}} of {{Solar Light}}, Indicating Three {{Primary Colours}}, Forming {{Coincident Spectra}} of Equal Length},
  author = {Brewster, David},
  year = {1834},
  month = jan,
  journal = {Earth Environ. Sci. Trans. R. Soc. Edinb.},
  volume = {12},
  number = {1},
  pages = {123--136},
  publisher = {Royal Society of Edinburgh Scotland Foundation}
}

@book{brindleyPhysiologyRetinaVisual1970,
  title = {Physiology of the {{Retina}} and {{Visual Pathway}}},
  author = {Brindley, Giles Skey},
  year = {1970},
  publisher = {Williams \& Wilkins},
  langid = {english}
}

@incollection{brownColorProcessingDigital2023,
  title = {Color {{Processing}} for {{Digital Cameras}}},
  booktitle = {Fundamentals and {{Applications}} of {{Colour Engineering}}},
  author = {Brown, Michael S.},
  editor = {Green, Phil},
  year = {2023},
  month = oct,
  pages = {81--98},
  publisher = {Wiley}
}

@misc{buiDeepLearningBased2020,
  title = {Deep {{Learning Based Semantic Segmentation}} for {{Nighttime Image}}},
  author = {Bui, Hien T T and Le, Duy H and Nguyen, Thu T A and Pham, Tuan V},
  year = {2020},
  journal = {2020 5th International Conference on Green Technology and Sustainable Development (GTSD)}
}

@article{burgeImageComputableIdealObservers2020,
  title = {Image-{{Computable Ideal Observers}} for {{Tasks}} with {{Natural Stimuli}}},
  author = {Burge, J.},
  year = {2020},
  journal = {Annual Review of Vision Science},
  volume = {6},
  pages = {491--517}
}

@article{burgessRtxNvidiaTuring2020,
  title = {Rtx on---the Nvidia Turing Gpu},
  author = {Burgess, John},
  year = {2020},
  journal = {IEEE Micro},
  volume = {40},
  number = {2},
  pages = {36--44},
  publisher = {IEEE}
}

@article{burnhamPredictionColorAppearance1957,
  title = {Prediction of Color Appearance with Different Adaptation Illuminations},
  author = {Burnham, R.W. and Evans, R.M. and Newhall, S.M.},
  year = {1957},
  journal = {Journal of the Optical Society of America},
  volume = {47},
  number = {1},
  pages = {35--42}
}

@article{burnsPsychophysicalTechniqueMeasuring1987,
  title = {A Psychophysical Technique for Measuring Cone Photopigment Bleaching},
  author = {Burns, S A and Elsner, A E and Lobes, Jr, L A and Doft, B H},
  year = {1987},
  month = apr,
  journal = {Investigative Ophthalmology \& Visual Science},
  volume = {28},
  number = {4},
  pages = {711--717},
  publisher = {iovs.arvojournals.org}
}

@article{burtonColorSpatialStructure1987,
  title = {Color and Spatial Structure in Natural Images},
  author = {Burton, G. J. and Moorehead, I. R.},
  year = {1987},
  journal = {Applied Optics},
  volume = {26},
  number = {1},
  pages = {157--170}
}

@book{cahanHermannHelmholtzFoundations1993,
  title = {Hermann {{Von Helmholtz}} and the {{Foundations}} of {{Nineteenth-Century Science}}},
  author = {Cahan, David},
  year = {1993},
  publisher = {University of California Press}
}

@article{caltagironeLIDARCameraFusion2019,
  title = {{{LIDAR}}--Camera Fusion for Road Detection Using Fully Convolutional Neural Networks},
  author = {Caltagirone, Luca and Bellone, Mauro and Svensson, Lennart and Wahde, Mattias},
  year = {2019},
  month = jan,
  journal = {Rob. Auton. Syst.},
  volume = {111},
  pages = {125--131},
  abstract = {In this work, a deep learning approach has been developed to carry out road detection by fusing LIDAR point clouds and camera images. An unstructured and sparse point cloud is first projected onto the camera image plane and then upsampled to obtain a set of dense 2D images encoding spatial information. Several fully convolutional neural networks (FCNs) are then trained to carry out road detection, either by using data from a single sensor, or by using three fusion strategies: early, late, and the newly proposed cross fusion. Whereas in the former two fusion approaches, the integration of multimodal information is carried out at a predefined depth level, the cross fusion FCN is designed to directly learn from data where to integrate information; this is accomplished by using trainable cross connections between the LIDAR and the camera processing branches. To further highlight the benefits of using a multimodal system for road detection, a data set consisting of visually challenging scenes was extracted from driving sequences of the KITTI raw data set. It was then demonstrated that, as expected, a purely camera-based FCN severely underperforms on this data set. A multimodal system, on the other hand, is still able to provide high accuracy. Finally, the proposed cross fusion FCN was evaluated on the KITTI road benchmark where it achieved excellent performance, with a MaxF score of 96.03\%, ranking it among the top-performing approaches.},
  keywords = {Deep learning,Intelligent vehicles,Road detection,Sensor fusion}
}

@article{campbellOpticalQualityHuman1966,
  title = {Optical Quality of the Human Eye},
  author = {Campbell, F. W. and Gubisch, R. W.},
  year = {1966},
  journal = {J. Physiol},
  volume = {186},
  pages = {558--578}
}

@misc{canonCANON19Pro,
  title = {{{CANON}} 19 {{Pro 5G}}},
  author = {{Canon}},
  langid = {english}
}

@misc{canonu.s.a.incIntroductionDualPixel2017,
  title = {Introduction to {{Dual Pixel Autofocus}}},
  author = {{Canon U.S.A. , Inc}},
  year = {2017},
  month = apr
}

@article{catheyImageGatheringProcessing1984,
  title = {Image Gathering and Processing for Enhanced Resolution},
  author = {Cathey, W Thomas and Frieden, B Roy and Rhodes, William T and Rushforth, Craig K},
  year = {1984},
  journal = {JOSA A},
  volume = {1},
  number = {3},
  pages = {241--250},
  publisher = {Optical Society of America}
}

@phdthesis{cechDayNightImage2021,
  title = {Day to {{Night Image Style Transfer}} with {{Light Control}}},
  author = {{\v C}ech, B Josef and Hurych, David},
  year = {2021},
  month = may,
  school = {Czech Technical University in Prague}
}

@article{chenDigitalCameraImaging2009,
  title = {Digital {{Camera Imaging System Simulation}}},
  author = {Chen, J and Venkataraman, K and Bakin, D and Rodricks, B and Gravelle, R and Rao, P and Ni, Y},
  year = {2009},
  month = nov,
  journal = {IEEE Trans. Electron Devices},
  volume = {56},
  number = {11},
  pages = {2496--2505},
  abstract = {A digital camera is a complex system including a lens, a sensor (physics and circuits), and a digital image processor, where each component is a sophisticated system on its own. Since prototyping a digital camera is very expensive, it is highly desirable to have the capability to explore the system design tradeoffs and preview the system output ahead of time. An empirical digital imaging system simulation that aims to achieve such a goal is presented. It traces the photons reflected by the objects in a scene through the optics and color filter array, converts photons into electrons with consideration of noise introduced by the system, quantizes the accumulated voltage to digital counts by an analog-to-digital converter, and generates a Bayer raw image just as a real camera does. The simulated images are validated against real system outputs and show a close resemblance to the images captured under similar condition at all illumination levels.},
  keywords = {analog-to-digital converter,analogue-digital conversion,Bayer raw image,Camera simulation,Cameras,color filter array,Crosstalk,digital camera imaging system,digital image processor,filtering theory,Hyperspectral scene,Image color analysis,image colour analysis,image sensors,imaging sensor,lens characterization,Lenses,modulation transfer function (MTF),Noise,Pixel,pixel characterization,pixel noise,point spread function (PSF)}
}

@inproceedings{chenLearningSeeDark2018,
  title = {Learning to See in the Dark},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Chen, Chen and Chen, Qifeng and Xu, Jia and Koltun, Vladlen},
  year = {2018},
  pages = {3291--3300}
}

@article{chenLensEffectSynthetic1987,
  title = {Lens Effect on Synthetic Image Generation Based on Light Particle Theory},
  author = {Chen, Yong C},
  year = {1987},
  journal = {The Visual Computer},
  volume = {3},
  number = {3},
  pages = {125--136},
  publisher = {Springer}
}

@inproceedings{chenProgressivelyComplementarityawareFusion2018,
  title = {Progressively Complementarity-Aware Fusion Network for {{RGB-D}} Salient Object Detection},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Chen, Hao and Li, Youfu},
  year = {2018},
  pages = {3051--3060}
}

@inproceedings{chenVirtualSimulationSystem2008,
  title = {A Virtual Simulation System of {{TDI}} Line Scan Camera},
  booktitle = {2008 {{IEEE}}/{{ASME International Conference}} on {{Advanced Intelligent Mechatronics}}},
  author = {Chen, Xiaoli and Yin, Chengliang and Feng, Yong},
  year = {2008},
  pages = {138--144},
  publisher = {IEEE}
}

@article{choiUseSimulationRobotics2021,
  title = {On the Use of Simulation in Robotics: {{Opportunities}}, Challenges, and Suggestions for Moving Forward},
  author = {Choi, Heesun and Crump, Cindy and Duriez, Christian and Elmquist, Asher and Hager, Gregory and Han, David and Hearl, Frank and Hodgins, Jessica and Jain, Abhinandan and Leve, Frederick and Li, Chen and Meier, Franziska and Negrut, Dan and Righetti, Ludovic and Rodriguez, Alberto and Tan, Jie and Trinkle, Jeff},
  year = {2021},
  month = jan,
  journal = {Proc. Natl. Acad. Sci. U. S. A.},
  volume = {118},
  number = {1},
  langid = {english},
  keywords = {controls,design,machine learning,robotics,simulation}
}

@techreport{cieColorimetrySecondEdition1986,
  type = {Report},
  title = {Colorimetry, Second Edition},
  author = {{CIE}},
  year = {1986},
  number = {15.2},
  address = {Vienna},
  institution = {Bureau Central de la CIE}
}

@book{cieFundamentalChromaticityDiagram2007,
  title = {Fundamental Chromaticity Diagram with Physiological Axes -- {{Parts}} 1 and 2. {{Technical Report}} 170-1},
  author = {{CIE}},
  year = {2007},
  publisher = {Central Bureau of the Commission Internationale de l' {\'E}clairage},
  address = {Vienna}
}

@techreport{cieFundamentalChromaticityDiagram2007a,
  title = {Fundamental Chromaticity Diagram with Physiological Axes -- {{Parts}} 1 and 2},
  author = {{CIE}},
  year = {2007},
  number = {170-1},
  address = {Vienna},
  institution = {Central Bureau of the Commission Internationale de l'{\'E}clairage}
}

@article{coatesCorrectionPhotonPilemeasurement2002,
  title = {The Correction for Photon `pile-up' in the Measurement of Radiative Lifetimes},
  author = {Coates, P B},
  year = {2002},
  month = may,
  journal = {J. Phys. E},
  volume = {1},
  number = {8},
  pages = {878},
  publisher = {IOP Publishing},
  abstract = {In determinations of radiative lifetimes in which time intervals are measured up to the first detected photon, an error is introduced when subsequent photons are neglected. An accurate correction for this effect is described, having the advantages of general applicability and allowing the use of high photon detection rates. It is also shown that the error introduced by the use of this correction is negligible under all practical experimental conditions.},
  langid = {english}
}

@article{cohenDependencySpectralReflectance1964,
  title = {Dependency of the Spectral Reflectance Curves of the {{Munsell}} Color Chips},
  author = {Cohen, J.},
  year = {1964},
  journal = {Psychonomic Science},
  volume = {1},
  pages = {369--370}
}

@article{cohenEfficientRadiosityApproach1986,
  title = {An {{Efficient Radiosity Approach}} for {{Realistic Image Synthesis}}},
  author = {Cohen, Michael F and Greenberg, Donald P and Immel, David S and Brock, Philip J},
  year = {1986},
  month = mar,
  journal = {IEEE Comput. Graph. Appl.},
  volume = {6},
  number = {3},
  pages = {26--35},
  abstract = {The radiosity method models the interaction of light between diffusely reflecting surfaces and accurately predicts the global illumination effects. Procedures are now available to simulate complex environments including occluded and textured surfaces. For accurate rendering, the environment must be discretized into a fine mesh, particularly in areas of high intensity gradients. The interdependence between surfaces implies solution techniques which are computationally intractable. This article describes new procedures to predict the global illumination function without excessive computational expense. Statistics indicate the enormous potential of this approach for realistic image synthesis, particularly for dynamic images of static environments.},
  keywords = {Computer graphics,Equations,Image generation,Layout,Lighting,Mirrors,Optical reflection,Reflectivity,Statistics,Surface texture}
}

@patent{connellSimulatingLenses2020,
  title = {Simulating Lenses},
  author = {Connell, Trebor Lee},
  year = {2020},
  month = jul,
  number = {10713836}
}

@book{cornsweetVisualPerception1970,
  title = {Visual {{Perception}}},
  author = {Cornsweet, T. N.},
  year = {1970},
  publisher = {Academic Press},
  address = {New York}
}

@misc{corporationHighDynamicRange,
  title = {High {{Dynamic Range}} ({{HDR}}) {{Technology}} for {{Mobile}}},
  author = {Corporation, Sony},
  journal = {Sony Semiconductor Solutions Group},
  langid = {english}
}

@inproceedings{costantiniVirtualSensorDesign2004,
  title = {Virtual Sensor Design},
  booktitle = {Sensors and {{Camera Systems}} for {{Scientific}}, {{Industrial}}, and {{Digital Photography Applications V}}},
  author = {Costantini, Roberto and Susstrunk, Sabine},
  year = {2004},
  month = jun,
  volume = {5301},
  pages = {408--419},
  publisher = {SPIE},
  abstract = {We present a virtual digital camera sensor, whose aim is to simulate a real (physical) image capturing sensor. To accomplish this task, the virtual sensor operates in two steps. First, it accepts a physical description of a given scene and simulates the entire process of photon sensing and charge generation in the sensor device. This process is affected by noise, mostly photon noise. Second, it adds to the image the noise that results from the electronic circuitry. We present a model for the different sources of noise relative to each sensor-based image formation step, and use measurements of real digital camera images to validate the model.},
  langid = {english},
  keywords = {CCD,CMOS,digital cameras,noise}
}

@article{cottarisComputationalObserverModel2019,
  title = {A Computational Observer Model of Spatial Contrast Sensitivity: {{Effects}} of Wavefront-Based Optics, Cone Mosaic Structure, and Inference Engine},
  author = {Cottaris, N. P. and Jiang, H. and Ding, X. and Wandell, B. A. and Brainard, D. H.},
  year = {2019},
  journal = {Journal of Vision},
  volume = {19},
  number = {4},
  pages = {8}
}

@article{cottarisComputationalObserverModel2020,
  title = {A Computational Observer Model of Spatial Contrast Sensitivity: {{Effects}} of Photocurrent Encoding, Fixational Eye Movements, and Inference Engine},
  author = {Cottaris, N. P. and Wandell, B. A. and Rieke, F. and Brainard, D. H.},
  year = {2020},
  journal = {Journal of Vision},
  volume = {20},
  number = {7},
  pages = {17}
}

@book{coverElementsInformationTheory1991,
  title = {Elements of {{Information Theory}}},
  author = {Cover, T. M. and Thomas, J. A.},
  year = {1991},
  publisher = {{John Wiley and Sons}},
  address = {New York}
}

@book{coxMultidimensionalScaling2001,
  title = {Multidimensional {{Scaling}}},
  author = {Cox, T. F. and Cox, M. A. A.},
  year = {2001},
  edition = {2},
  publisher = {Chapman \& Hall/CRC},
  address = {Boca Raton, FL}
}

@article{coxNeuralNetworksNeuroscienceinspired2014,
  title = {Neural Networks and Neuroscience-Inspired Computer Vision},
  author = {Cox, David Daniel and Dean, Thomas},
  year = {2014},
  month = sep,
  journal = {Current Biology},
  volume = {24},
  number = {18},
  pages = {R921--R929}
}

@article{cummingPhysiologyStereopsis2001,
  title = {The Physiology of Stereopsis},
  author = {Cumming, Bruce G and DeAngelis, Gregory C},
  year = {2001},
  journal = {Annual Review of Neuroscience},
  volume = {24},
  number = {1},
  pages = {203--238},
  publisher = {Annual Reviews 4139 El Camino Way, PO Box 10139, Palo Alto, CA 94303-0139, USA}
}

@article{curcioDistributionMorphologyHuman1991,
  title = {Distribution and {{Morphology}} of {{Human Cone Photoreceptors Stained}} with {{Anti-BLue Opsin}}},
  author = {Curcio, C. A. and Allen, K. A. and Sloan, K. R. and Lerea, C. L. and Hurley, J. B. and Klock, I. B. and Milam, A. H.},
  year = {1991},
  journal = {J. Comp. Neurology},
  volume = {312},
  pages = {610--624}
}

@article{curcioHumanPhotoreceptorTopography1990,
  title = {Human Photoreceptor Topography},
  author = {Curcio, C.A. and Sloan, K.R. and Kalina, R.E. and Hendrickson, A.E.},
  year = {1990},
  journal = {Journal of Comparative Neurology},
  volume = {292},
  number = {4},
  pages = {497--523}
}

@article{daiCurriculumModelAdaptation2020,
  title = {Curriculum Model Adaptation with Synthetic and Real Data for Semantic Foggy Scene Understanding},
  author = {Dai, Dengxin and Sakaridis, Christos and Hecker, Simon and Van Gool, Luc},
  year = {2020},
  month = may,
  journal = {Int. J. Comput. Vis.},
  volume = {128},
  number = {5},
  pages = {1182--1204},
  publisher = {{Springer Science and Business Media LLC}},
  langid = {english}
}

@inproceedings{daiDarkModelAdaptation2018,
  title = {Dark {{Model Adaptation}}: {{Semantic Image Segmentation}} from {{Daytime}} to {{Nighttime}}},
  booktitle = {2018 21st {{International Conference}} on {{Intelligent Transportation Systems}} ({{ITSC}})},
  author = {Dai, Dengxin and Van Gool, Luc},
  year = {2018},
  month = nov,
  pages = {3819--3824},
  keywords = {Adaptation models,Cameras,Image segmentation,Mathematical model,Meteorology,Roads,Semantics}
}

@article{daiFlare7KMixingSynthetic2023,
  title = {{{Flare7K}}++: {{Mixing Synthetic}} and {{Real Datasets}} for {{Nighttime Flare Removal}} and {{Beyond}}},
  author = {Dai, Yuekun and Li, Chongyi and Zhou, Shangchen and Feng, Ruicheng and Luo, Yihang and Loy, Chen Change},
  year = {2023},
  month = jun,
  journal = {arXiv [cs.CV]}
}

@article{daiFlare7KPhenomenologicalNighttime2022,
  title = {{{Flare7K}}: {{A Phenomenological Nighttime Flare Removal Dataset}}},
  author = {Dai, Yuekun and Li, Chongyi and Zhou, Shangchen and Feng, Ruicheng and Loy, Chen Change},
  year = {2022},
  month = oct,
  journal = {arXiv:2210.06570},
  eprint = {2210.06570},
  archiveprefix = {arXiv}
}

@article{daiModelAdaptationSynthetic,
  title = {Model Adaptation with Synthetic and Real Data for Semantic Dense Foggy Scene Understanding},
  author = {{Dai} and {Hecker} and {Van Gool}},
  journal = {Proc. Estonian Acad. Sci. Biol. Ecol.}
}

@techreport{danielseitajeffmahlermikedanielczukmatthewmatlandkengoldbergDrillingDepthSensing,
  title = {Drilling {{Down}} on {{Depth Sensing}} and {{Deep Learning}}},
  author = {{Daniel Seita, Jeff Mahler, Mike Danielczuk, Matthew Matl, and Ken Goldberg}},
  journal = {Berkeley Artificial Intellgence Research},
  abstract = {The BAIR Blog}
}

@book{davinciNotebooksLeonardoVinci1970,
  title = {The {{Notebooks}} of {{Leonardo Da Vinci}}},
  author = {Da Vinci, Leonardo},
  editor = {Richter, Jean Paul},
  year = {1970},
  month = jan,
  volume = {1},
  publisher = {Dover, New York}
}

@article{delbracioMobileComputationalPhotography2021,
  title = {Mobile {{Computational Photography}}: {{A Tour}}},
  author = {Delbracio, Mauricio and Kelly, Damien and Brown, Michael S and Milanfar, Peyman},
  year = {2021},
  month = sep,
  journal = {Annu Rev Vis Sci},
  volume = {7},
  pages = {571--604},
  langid = {english},
  keywords = {burst processing,camera pipeline,computational photography,low-light imaging,mobile camera,noise reduction,super-resolution,zoom}
}

@misc{DENSEDatasetsUlm2022,
  title = {{{DENSE}} Datasets - {{Ulm}} University},
  year = {2022},
  month = mar,
  langid = {english}
}

@article{devaloisPsychophysicalStudiesMonkey1974,
  title = {Psychophysical Studies of Monkey Vision--{{III}}. {{Spatial}} Luminance Contrast Sensitivity Tests of Macaque and Human Observers},
  author = {De Valois, Russell L and Morgan, Herman and Snodderly, D Ma},
  year = {1974},
  journal = {Vision Research},
  volume = {14},
  number = {1},
  pages = {75--81},
  publisher = {Elsevier}
}

@misc{dipertSplitPixelTechnology,
  title = {Split Pixel Technology},
  author = {Dipert, Brian}
}

@article{diRainyNightScene2021,
  title = {Rainy {{Night Scene Understanding With Near Scene Semantic Adaptation}}},
  author = {Di, Shuai and Feng, Qi and Li, Chun-Guang and Zhang, Mei and Zhang, Honggang and Elezovikj, Semir and Tan, Chiu C and Ling, Haibin},
  year = {2021},
  month = mar,
  journal = {IEEE Trans. Intell. Transp. Syst.},
  volume = {22},
  number = {3},
  pages = {1594--1602},
  abstract = {Deep networks have been used for semantic segmentation tasks on scenes of outdoor environments with increasing popularity. However, the majority of existing work centers on daytime scenes with favorable illumination and weather conditions, and relies on supervision with pixel-level annotations. This paper seeks to address the problem of semantic segmentation for rainy, night-time scenes without using pixel-level annotations. We introduce a near scene semantic approach that uses images of daytime scenes as a bridge for transferring knowledge from pre-trained segmentation models to rainy night images. Specifically, we first present near scene oriented Representation Adaptation (RA) to reduce the domain shift on the representation level. Next, we adapt the segmentation model from the daytime scenario, under varying weather conditions, to the rainy night scenario by using near scene oriented Segmentation Space Adaptation (SSA). Consequently, this further reduces the impact of the domain shift on the segmentation space level. For evaluation, we created a new dataset containing 7000 distinct daytime-night-time image pairs of near scenes obtained by a webcam, and 5266 daytime-rainy night image pairs collected by a car-mounted camera. In addition, we carefully annotated 226 rainy night images with classes defined in Cityscapes. The experimental results clearly demonstrate the advantage of the proposed algorithm.},
  keywords = {Adaptation models,Bridges,domain adaptation,Image segmentation,Lighting,Meteorology,road scene,semantic segmentation,Semantics,Traffic scene understanding,Training,vehicle environment perception}
}

@misc{DIRSIGtextbackslashtextsuperscriptTMDigitalImaging,
  title = {{{DIRSIG}}{\textbackslash}{{textbackslashtextsuperscriptTM}} = {{Digital Imaging}} and {{Remote Sensing Image Generation}}}
}

@misc{DIRSIGtextsuperscriptTMDigitalImaging,
  title = {{{DIRSIG}}{\textbackslash}{{textsuperscriptTM}} = {{Digital Imaging}} and {{Remote Sensing Image Generation}}}
}

@inproceedings{dosovitskiyCARLAOpenUrban2017,
  title = {{{CARLA}}: {{An}} Open Urban Driving Simulator},
  booktitle = {Conference on Robot Learning},
  author = {Dosovitskiy, Alexey and Ros, German and Codevilla, Felipe and Lopez, Antonio and Koltun, Vladlen},
  year = {2017},
  pages = {1--16},
  publisher = {PMLR}
}

@patent{drozLongRangeSteerable2016,
  title = {Long {{Range Steerable LIDAR System}}},
  author = {Droz, Pierre-Yves and Pennecot, Gaetan and Levandowski, Anthony and Ulrich, Drew Eugene and Morriss, Zach and Wachter, Luke and Iordach, Dorel Ionut and McCann, William and Gruver, Daniel and Fidric, Bernard and Lenius, Samuel William},
  year = {2016},
  month = oct,
  number = {20160291134:A1},
  abstract = {Systems and methods are described that relate to a light detection and ranging (LIDAR) device. The LIDAR device includes a fiber laser configured to emit light within a wavelength range, a scanning portion configured to direct the emitted light in a reciprocating manner about a first axis, and a plurality of detectors configured to sense light within the wavelength range. The device additionally includes a controller configured to receive target information, which may be indicative of an object, a position, a location, or an angle range. In response to receiving the target information, the controller may cause the rotational mount to rotate so as to adjust a pointing direction of the LIDAR. The controller is further configured to cause the LIDAR to scan a field-of-view (FOV) of the environment. The controller may determine a three-dimensional (3D) representation of the environment based on data from scanning the FOV.}
}

@book{dudaPatternClassificationScene2001,
  title = {Pattern Classification and Scene Analysis (2nd {{Edition}})},
  author = {Duda, R.O. and Hart, P.E. and Stork, D.G.},
  year = {2001},
  publisher = {John Wiley \& Sons},
  address = {New York}
}

@book{e.brucegoldsteinandjamesr.brockmoleSensationPerception10th2017,
  title = {Sensation and {{Perception}} (10th {{Edition}})},
  author = {{E. Bruce Goldstein and James R. Brockmole}},
  year = {2017},
  publisher = {Cengage Learning}
}

@article{elgamalCMOSImageSensors2005,
  title = {{{CMOS}} Image Sensors},
  author = {El Gamal, A and Eltoukhy, H},
  year = {2005},
  month = may,
  journal = {IEEE Circuits Devices Mag.},
  volume = {21},
  number = {3},
  pages = {6--20},
  keywords = {Circuits,CMOS image sensors,CMOS technology,Color,Image converters,Image sensors,Optical arrays,Optical imaging,Pixel,Sensor arrays}
}

@article{elmquistModelingCamerasAutonomous2021,
  title = {Modeling {{Cameras}} for {{Autonomous Vehicle}} and {{Robot Simulation}}: {{An Overview}}},
  author = {Elmquist, Asher and Negrut, Dan},
  year = {2021},
  month = nov,
  journal = {IEEE Sens. J.},
  volume = {21},
  number = {22},
  pages = {25547--25560},
  keywords = {automotive,Cameras,Context modeling,imaging sensors,Pipelines,Robot vision systems,robotics,Robots,Sensors,Simulation,Training}
}

@article{elmquistSensorSimulationFramework2021,
  title = {A {{Sensor Simulation Framework}} for {{Training}} and {{Testing Robots}} and {{Autonomous Vehicles}}},
  author = {Elmquist, Asher and Serban, Radu and Negrut, Dan},
  year = {2021},
  month = feb,
  journal = {Int. J. Veh. Auton. Syst.},
  volume = {1},
  number = {2},
  publisher = {American Society of Mechanical Engineers Digital Collection},
  langid = {english},
  keywords = {Autonomous vehicles,Robots,Sensors,Simulation,Vehicles}
}

@article{engbertMicrosaccadesKeepEyes2004,
  title = {Microsaccades Keep the Eyes' Balance during Fixation},
  author = {Engbert, R. and Kliegl, R.},
  year = {2004},
  journal = {Psychological Science},
  volume = {15},
  number = {6},
  pages = {431--436}
}

@article{engbertMicrosaccadesKeepEyes2004a,
  title = {Microsaccades Keep the Eyes' Balance during Fixation},
  author = {Engbert, R. and Kliegl, R.},
  year = {2004},
  journal = {Psychological Science},
  volume = {15},
  number = {6},
  pages = {431--436}
}

@article{etxebesteCCModGATEModule2020,
  title = {{{CCMod}}: A {{GATE}} Module for {{Compton}} Camera Imaging Simulation},
  author = {Etxebeste, A and Dauvergne, D and Fontana, M and L{\'e}tang, J M and Llos{\'a}, G and Munoz, E and Oliver, J F and Testa, {\'E} and Sarrut, D},
  year = {2020},
  month = feb,
  journal = {Phys. Med. Biol.},
  volume = {65},
  number = {5},
  pages = {055004},
  publisher = {iopscience.iop.org},
  langid = {english}
}

@misc{europeanmachinevisionassociationStandardCharacterizationImage,
  title = {Standard for {{Characterization}} of {{Image Sensors}} and {{Cameras}}},
  author = {{European Machine Vision Association}},
  annotation = {Published: Standard for Characterization of Image Sensors and Cameras}
}

@article{everinghamPascalVisualObject2010,
  title = {The Pascal Visual Object Classes (Voc) Challenge},
  author = {Everingham, M and Van Gool, L and Williams, C K I and others},
  year = {2010},
  journal = {International journal of},
  publisher = {Springer},
  abstract = {Abstract The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures {\dots}}
}

@article{fangAugmentedLidarSimulator2020,
  title = {Augmented Lidar Simulator for Autonomous Driving},
  author = {Fang, J and Zhou, D and Yan, F and Zhao, T and Zhang, F and others},
  year = {2020},
  journal = {IEEE Robotics and},
  publisher = {ieeexplore.ieee.org},
  abstract = {In Autonomous Driving (AD), detection and tracking of obstacles on the roads is a critical task. Deep-learning based methods using annotated LiDAR data have been the most widely adopted approach for this. Unfortunately, annotating 3D point cloud is a very challenging, time-and money-consuming task. In this letter, we propose a novel LiDAR simulator that augments real point cloud with synthetic obstacles (eg, vehicles, pedestrians, and other movable objects). Unlike previous simulators that entirely rely on CG (Computer Graphics) {\dots}}
}

@article{farrellDigitalCameraSimulation2012,
  title = {Digital Camera Simulation},
  author = {Farrell, Joyce E and Catrysse, Peter B and Wandell, Brian A},
  year = {2012},
  month = feb,
  journal = {Appl. Opt.},
  volume = {51},
  number = {4},
  pages = {A80--90},
  abstract = {We describe a simulation of the complete image processing pipeline of a digital camera, beginning with a radiometric description of the scene captured by the camera and ending with a radiometric description of the image rendered on a display. We show that there is a good correspondence between measured and simulated sensor performance. Through the use of simulation, we can quantify the effects of individual digital camera components on system performance and image quality. This computational approach can be helpful for both camera design and image quality assessment.},
  langid = {english},
  keywords = {2019 ICCV (Zhenyi Brian),Camera simulation}
}

@inproceedings{farrellSensorCalibrationSimulation2008,
  title = {Sensor Calibration and Simulation},
  booktitle = {Digital {{Photography IV}}},
  author = {Farrell, Joyce and Okincha, Michael and Parmar, Manu},
  year = {2008},
  month = mar,
  volume = {6817},
  pages = {68170R},
  publisher = {{International Society for Optics and Photonics}},
  abstract = {We describe a method for simulating the output of an image sensor to a broad array of test targets. The method uses a modest set of sensor calibration measurements to define the sensor parameters; these parameters are used by an integrated suite of Matlab software routines that simulate the sensor and create output images. We compare the simulations of specific targets to measured data for several different imaging sensors with very different imaging properties. The simulation captures the essential features of the images created by these different sensors. Finally, we show that by specifying the sensor properties the simulations can predict sensor performance to natural scenes that are difficult to measure with a laboratory apparatus, such as natural scenes with high dynamic range or low light levels.},
  keywords = {characterization,Digital camera calibration,simulation}
}

@inproceedings{farrellSimulationToolEvaluating2003,
  title = {A Simulation Tool for Evaluating Digital Camera Image Quality},
  booktitle = {Image {{Quality}} and {{System Performance}}},
  author = {Farrell, Joyce E and Xiao, Feng and Catrysse, Peter Bert and Wandell, Brian A},
  year = {2003},
  month = dec,
  volume = {5294},
  pages = {124--131},
  publisher = {{International Society for Optics and Photonics}},
  abstract = {The Image Systems Evaluation Toolkit (ISET) is an integrated suite of software routines that simulate the capture and processing of visual scenes. ISET includes a graphical user interface (GUI) for users to control the physical characteristics of the scene and many parameters of the optics, sensor electronics and image processing-pipeline. ISET also includes color tools and metrics based on international standards (chromaticity coordinates, CIELAB and others) that assist the engineer in evaluating the color accuracy and quality of the rendered image.},
  keywords = {Camera simulation,Digital camera simulation,image processing pipeline,image quality evaluation,virtual camera}
}

@article{farrellSoftprototypingImagingSystems2020,
  title = {Soft-Prototyping Imaging Systems for Oral Cancer Screening},
  author = {Farrell, Joyce and Lyu, Zheng and Liu, Zhenyi and Blasinski, Henryk and Xu, Zhihao and Rong, Jian and Xiao, Feng and Wandell, Brian},
  year = {2020},
  journal = {Electronic Imaging},
  volume = {2020},
  number = {7},
  pages = {212--1--212--7},
  abstract = {We are using image systems simulation technology to design a digital camera for measuring fluorescent signals; a first application is oral cancer screening. We validate the simulations by creating a camera model that accurately predicts measured RGB values for any spectral radiance. Then we use the excitationemission spectra for different biological fluorophores to predict measurements of fluorescence of oral mucosal tissue under several different illuminations. The simulations and measurements are useful for (a) designing cameras that measure tissue fluorescence and (b) clarifying which fluorophores may be diagnostic in identifying precancerous tissue.},
  keywords = {Image Systems Simulation,Oral Cancer Screening,Tissue Fluorescence}
}

@article{fengDeepMultimodalObject2019,
  title = {Deep {{Multi-modal Object Detection}} and {{Semantic Segmentation}} for {{Autonomous Driving}}: {{Datasets}}, {{Methods}}, and {{Challenges}}},
  author = {Feng, Di and {Haase-Sch{\"u}tz}, Christian and Rosenbaum, Lars and Hertlein, Heinz and Glaeser, Claudius and Timm, Fabian and Wiesbeck, Werner and Dietmayer, Klaus},
  year = {2019},
  month = feb,
  abstract = {Recent advancements in perception for autonomous driving are driven by deep learning. In order to achieve robust and accurate scene understanding, autonomous vehicles are usually equipped with different sensors (e.g. cameras, LiDARs, Radars), and multiple sensing modalities can be fused to exploit their complementary properties. In this context, many methods have been proposed for deep multi-modal perception problems. However, there is no general guideline for network architecture design, and questions of ``what to fuse'', ``when to fuse'', and ``how to fuse'' remain open. This review paper attempts to systematically summarize methodologies and discuss challenges for deep multi-modal object detection and semantic segmentation in autonomous driving. To this end, we first provide an overview of on-board sensors on test vehicles, open datasets, and background information for object detection and semantic segmentation in autonomous driving research. We then summarize the fusion methodologies and discuss challenges and open questions. In the appendix, we provide tables that summarize topics and methods. We also provide an interactive online platform to navigate each reference: https://boschresearch.github.io/multimodalperception/.}
}

@article{fieldRelationsStatisticsNatural1987,
  title = {Relations between the Statistics of Natural Images and the Response Properties of Cortical Cells},
  author = {Field, D. J.},
  year = {1987},
  journal = {Journal of the Optical Society of America A},
  volume = {4},
  number = {12},
  pages = {2379--2394}
}

@article{fieteModelingOpticalTransfer2014,
  title = {Modeling the Optical Transfer Function in the Imaging Chain},
  author = {Fiete, Robert D and Paul, Bradley D},
  year = {2014},
  journal = {Optical Engineering},
  volume = {53},
  number = {8},
  pages = {083103},
  publisher = {{International Society for Optics and Photonics}}
}

@article{flamantEtudeRepartitionLumiere1955,
  title = {Etude de La Repartition de Lumiere Dans l'image Retinienne d'une Fente},
  author = {Flamant, F.},
  year = {1955},
  journal = {Rev. Opt. (theor. instrum.)},
  volume = {34},
  pages = {433--459}
}

@article{florinSimulationFunctionalityWeb2009,
  title = {Simulation the Functionality of a Web Cam Image Capture System},
  author = {Florin, Toadere and Mastorakis, Nikos E},
  year = {2009},
  journal = {WSEAS Transactions on Circuits and Systems},
  volume = {8},
  number = {10},
  pages = {811--821},
  publisher = {{World Scientific and Engineering Academy and Society (WSEAS) Stevens Point {\dots}}}
}

@inproceedings{florinSimulationOpticalPart2009,
  title = {Simulation the Optical Part of an Image Capture System},
  booktitle = {Advanced {{Topics}} in {{Optoelectronics}}, {{Microelectronics}}, and {{Nanotechnologies IV}}},
  author = {Florin, Toadere},
  year = {2009},
  volume = {7297},
  pages = {729719},
  publisher = {{International Society for Optics and Photonics}}
}

@misc{FlywheelModernInformatics2017,
  title = {Flywheel {$\bullet$} Modern Informatics Platform for Biomedical Research \& Collaboration},
  year = {2017},
  month = mar,
  abstract = {Flywheel is an informatics platform for biomedical research \& collaboration, with solutions for Imaging Centers, Clinical Research, and Life Sciences.},
  langid = {english}
}

@inproceedings{gaidonVirtualWorldsProxyMultiobject2016,
  title = {{{VirtualWorlds}} as {{Proxy}} for {{Multi-object Tracking Analysis}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Gaidon, Adrien and Wang, Qiao and Cabon, Yohann and Vig, Eleonora},
  year = {2016},
  month = jun,
  pages = {4340--4349},
  keywords = {Benchmark testing,Cameras,Cloning,Computational modeling,Computer vision,Three-dimensional displays,Training}
}

@article{gamlinHumanMacaquePupil2007,
  title = {Human and Macaque Pupil Responses Driven by Melanopsin-Containing Retinal Ganglion Cells},
  author = {Gamlin, P. D. R. and McDougal, D. H. and Pokorny, J. and Smith, V. C. and Yau, K. W. and Dacey, D. M.},
  year = {2007},
  journal = {Vision Research},
  volume = {47},
  number = {7},
  pages = {946--954}
}

@inproceedings{garnierGeneralFrameworkInfrared1998,
  title = {General Framework for Infrared Sensor Modeling},
  booktitle = {Infrared {{Imaging Systems}}: {{Design}}, {{Analysis}}, {{Modeling}}, and {{Testing IX}}},
  author = {Garnier, Christelle and Collorec, Rene and Flifla, Jihed and Rousee, Frank},
  year = {1998},
  volume = {3377},
  pages = {59--70},
  publisher = {{International Society for Optics and Photonics}}
}

@inproceedings{garnierInfraredSensorModeling1999,
  title = {Infrared Sensor Modeling for Realistic Thermal Image Synthesis},
  booktitle = {1999 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}. {{Proceedings}}. {{ICASSP99}} ({{Cat}}. {{No}}. {{99CH36258}})},
  author = {Garnier, Christelle and Collorec, Ren{\'e} and Flifla, Jihed and Mouclier, Christophe and Rousee, F},
  year = {1999},
  volume = {6},
  pages = {3513--3516},
  publisher = {IEEE}
}

@article{garriganDesignTrichromaticCone2010,
  title = {Design of a Trichromatic Cone Array},
  author = {Garrigan, P. and Ratliff, C. P. and Klein, J. M. and Sterling, P. and Brainard, D. H. and Balasubramanian, V.},
  year = {2010},
  journal = {PLoS Computational Biology},
  volume = {6},
  number = {2},
  pages = {e1000677}
}

@article{geislerPhysicalLimitsAcuity1984,
  title = {Physical Limits of Acuity and Hyperacuity},
  author = {Geisler, W. S.},
  year = {1984},
  journal = {Journal of the Optical Society of America A},
  volume = {1},
  number = {7},
  pages = {775--782}
}

@article{geislerPsychometricFunctionsUncertain2018,
  title = {Psychometric Functions of Uncertain Template Matching Observers},
  author = {Geisler, W. S.},
  year = {2018},
  journal = {Journal of Vision},
  volume = {18},
  number = {2},
  pages = {1}
}

@article{geislerSequentialIdealobserverAnalysis1989,
  title = {Sequential Ideal-Observer Analysis of Visual Discriminations},
  author = {Geisler, W.S.},
  year = {1989},
  journal = {Psychological Review},
  volume = {96},
  number = {2},
  pages = {267--314}
}

@article{gengStructuredlight3DSurface2011,
  title = {Structured-Light {{3D}} Surface Imaging: A Tutorial},
  author = {Geng, Jason},
  year = {2011},
  month = jun,
  journal = {Adv. Opt. Photon., AOP},
  volume = {3},
  number = {2},
  pages = {128--160},
  publisher = {Optical Society of America},
  abstract = {We provide a review of recent advances in 3D surface imaging technologies. We focus particularly on noncontact 3D surface measurement techniques based on structured illumination. The high-speed and high-resolution pattern projection capability offered by the digital light projection technology, together with the recent advances in imaging sensor technologies, may enable new generation systems for 3D surface measurement applications that will provide much better functionality and performance than existing ones in terms of speed, accuracy, resolution, modularization, and ease of use. Performance indexes of 3D imaging system are discussed, and various 3D surface imaging schemes are categorized, illustrated, and compared. Calibration techniques are also discussed, since they play critical roles in achieving the required precision. Numerous applications of 3D surface imaging technologies are discussed with several examples.},
  langid = {english},
  keywords = {Imaging systems,Imaging techniques,Medical imaging,Spatial light modulators,Structured illumination microscopy,Three dimensional imaging}
}

@article{georgievLaboratorybasedBidirectionalReflectance2008,
  title = {Laboratory-Based Bidirectional Reflectance Distribution Functions of Radiometric Tarps},
  author = {Georgiev, Georgi T and Butler, James J},
  year = {2008},
  journal = {Applied Optics},
  volume = {47},
  number = {18},
  pages = {3313--3323},
  publisher = {Optical Society of America}
}

@article{gershunLightField1939,
  title = {The {{Light Field}}},
  author = {Gershun, A},
  year = {1939},
  journal = {Journal of Mathematical Physics},
  volume = {18},
  number = {1--4},
  pages = {51--151}
}

@inproceedings{gieslerUsingPanoramicCamera2002,
  title = {Using a Panoramic Camera for {{3D}} Head Tracking in an {{AR}} Environment},
  booktitle = {Proc. of {{Conf}}. {{IEEE Machine Vision}} and {{Mechatronics}} in {{Practice}} ({{M2VIP}})},
  author = {Giesler, Bj{\"o}rn and Salb, Tobias and Weyrich, T and Dillmann, R},
  year = {2002},
  pages = {1--11},
  publisher = {researchgate.net},
  abstract = {{\dots} camera that is affixed to the AR glasses and tracks artificial environmental features. This approach, using only a single camera , {\dots} for the simulation system and one for the actual camera . {\dots}}
}

@misc{gisgeography13OpenSource2014,
  title = {13 {{Open Source Remote Sensing Software Packages}}},
  author = {{GISGeography}},
  year = {2014},
  month = jul,
  abstract = {There is an abundance of choice for open source remote sensing software. This list of 10 free applications describes what each one brings to the table.},
  howpublished = {https://gisgeography.com/open-source-remote-sensing-software-packages/},
  langid = {english}
}

@misc{gisgeography13OpenSource2014a,
  title = {13 {{Open Source Remote Sensing Software Packages}}},
  author = {{GISGeography}},
  year = {2014},
  month = jul,
  abstract = {There is an abundance of choice for open source remote sensing software. This list of 10 free applications describes what each one brings to the table.},
  langid = {english}
}

@article{gkioxariMeshRCNN2019,
  title = {Mesh {{R-CNN}}},
  author = {Gkioxari, Georgia and Malik, Jitendra and Johnson, Justin},
  year = {2019},
  month = jun,
  abstract = {Rapid advances in 2D perception have led to systems that accurately detect objects in real-world images. However, these systems make predictions in 2D, ignoring the 3D structure of the world. Concurrently, advances in 3D shape prediction have mostly focused on synthetic benchmarks and isolated objects. We unify advances in these two areas. We propose a system that detects objects in real-world images and produces a triangle mesh giving the full 3D shape of each detected object. Our system, called Mesh R-CNN, augments Mask R-CNN with a mesh prediction branch that outputs meshes with varying topological structure by first predicting coarse voxel representations which are converted to meshes and refined with a graph convolution network operating over the mesh's vertices and edges. We validate our mesh prediction branch on ShapeNet, where we outperform prior work on single-image shape prediction. We then deploy our full Mesh R-CNN system on Pix3D, where we jointly detect objects and predict their 3D shapes.}
}

@misc{globalCanonDevelopsCMOS,
  title = {Canon Develops {{CMOS}} Sensor for Monitoring Applications with Industry-Leading Dynamic Range, Automatic Exposure Optimization Function for Each Sensor Area That Improves Accuracy for Recognizing Moving Subjects},
  author = {Global, Canon},
  journal = {Canon Global},
  langid = {english}
}

@inproceedings{gokturkTimeFlightDepthSensor2004,
  title = {A {{Time-Of-Flight Depth Sensor}} - {{System Description}}, {{Issues}} and {{Solutions}}},
  booktitle = {2004 {{Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshop}}},
  author = {Gokturk, S B and Yalcin, H and Bamji, C},
  year = {2004},
  month = jun,
  pages = {35--35},
  abstract = {This paper describes a CMOS-based time-of-flight depth sensor and presents some experimental data while addressing various issues arising from its use. Our system is a single-chip solution based on a special CMOS pixel structure that can extract phase information from the received light pulses. The sensor chip integrates a 64x64 pixel array with a high-speed clock generator and ADC. A unique advantage of the chip is that it can be manufactured with an ordinary CMOS process. Compared with other types of depth sensors reported in the literature, our solution offers significant advantages, including superior accuracy, high frame rate, cost effectiveness and a drastic reduction in processing required to construct the depth maps. We explain the factors that determine the resolution of our system, discuss various problems that a time-of-flight depth sensor might face, and propose practical solutions.},
  keywords = {Application software,Clocks,CMOS process,Computer vision,Data mining,Optical sensors,Phase modulation,Sensor arrays,Sensor phenomena and characterization,Sensor systems}
}

@article{goossensRaytransferFunctionsCamera2022,
  title = {Ray-Transfer Functions for Camera Simulation of {{3D}} Scenes with Hidden Lens Design},
  author = {Goossens, Thomas and Lyu, Zheng and Ko, Jamyuen and Wan, Gordon C and Farrell, Joyce and Wandell, Brian},
  year = {2022},
  journal = {Optics Express},
  volume = {30},
  number = {13},
  pages = {24031--24047},
  publisher = {Optica Publishing Group}
}

@article{goralModelingInteractionLight1984,
  title = {Modeling the Interaction of Light between Diffuse Surfaces},
  author = {Goral, Cindy M and Torrance, Kenneth E and Greenberg, Donald P and Battaile, Bennett},
  year = {1984},
  month = jan,
  journal = {SIGGRAPH Comput. Graph.},
  volume = {18},
  number = {3},
  pages = {213--222},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  abstract = {A method is described which models the interaction of light between diffusely reflecting surfaces. Current light reflection models used in computer graphics do not account for the object-to-object reflection between diffuse surfaces, and thus incorrectly compute the global illumination effects. The new procedure, based on methods used in thermal engineering, includes the effects of diffuse light sources of finite area, as well as the ``color-bleeding'' effects which are caused by the diffuse reflections. A simple environment is used to illustrate these simulated effects and is presented with photographs of a physical model. The procedure is applicable to environments composed of ideal diffuse reflectors and can account for direct illumination from a variety of light sources. The resultant surface intensities are independent of observer position, and thus environments can be preprocessed for dynamic sequences.},
  keywords = {Diffuse reflections,Form factors,Light reflection models,Radiosity,Shading}
}

@book{grahamVisualPatternAnalyzers1989,
  title = {Visual {{Pattern Analyzers}}},
  author = {Graham, Norma},
  year = {1989},
  month = sep,
  publisher = {Oxford University Press}
}

@article{grapinetCharacterizationSimulationOptical2013,
  title = {Characterization and Simulation of Optical Sensors},
  author = {Grapinet, M and De Souza, Ph and Smal, J-C and Blosseville, J-M},
  year = {2013},
  month = nov,
  journal = {Accid. Anal. Prev.},
  volume = {60},
  pages = {344--352},
  abstract = {Numerical simulation is gradually becoming an advantage in active safety. This is why the development of realistic numerical models enabling to substitute real truth by simulated truth is primordial. In order to provide an accurate and cost effective solution to simulate real optical sensor behavior, the software Pro-SiVIC™ has been developed. Simulations with the software Pro-SiVIC™ can replace real tests with optical sensors and hence allow substantial cost and time savings during the development of solutions for driver assistance systems. An optical platform has been developed by IFSTTAR (French Institute of Science and Technology for Transport, Development and Networks) to characterize and validate any existing camera, in order to measure their characteristics as distortion, vignetting, focal length, etc. By comparing real and simulated sensors with this platform, this paper demonstrates that Pro-SiVIC™ accurately reproduces real sensors' behavior.},
  keywords = {Characterization platform,Driving assistance system,Numerical simulation,Optical sensor,Pro-SiVIC}
}

@article{grassmannXXXVIITheoryCompound1854,
  title = {{{XXXVII}}. {{On}} the Theory of Compound Colours},
  author = {{Grassmann}},
  year = {1854},
  journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
  volume = {7},
  number = {45},
  pages = {254--264},
  publisher = {Taylor \& Francis}
}

@article{grassmannZurTheorieFarbenmischung1853,
  title = {Zur {{Theorie}} Der {{Farbenmischung}}},
  author = {Grassmann, H.},
  year = {1853},
  journal = {Annalen der Physik und Chemie},
  volume = {165},
  pages = {69--84}
}

@article{griffinEcholocationFlyingInsects1965,
  title = {The Echolocation of Flying Insects by Bats},
  author = {Griffin, Donald R and Webster, Frederic A and Michael, Charles R},
  year = {1965},
  journal = {Readings in the Psychology of Perception},
  pages = {21},
  publisher = {Ardent Media},
  abstract = {When bats are hunting insects they adjust the pattern and tempo of their high frequency orientation sounds in a way that seems quite appropriate for active echolocation of small moving targets but distinctly unsuited for the passive detection of insects by listening for their {\dots}}
}

@article{griffinMoreBatRadar1958,
  title = {More about {{Bat}} ``{{Radar}}''},
  author = {Griffin, D W},
  year = {1958},
  journal = {Scientific American},
  volume = {199},
  number = {1},
  pages = {40--45},
  abstract = {A sequel to an earlier article which described the capacity of bats to locate objects by supersonic echoes. This natural sonar is now known to incorporate extraordinary refinements}
}

@patent{grohMethodSystemVirtual2018,
  title = {Method and System for Virtual Sensor Data Generation with Depth Ground Truth Annotation},
  author = {Groh, Alexander and Micks, Ashley Elizabeth and Murali, Vidya Nariyambut},
  year = {2018},
  month = oct,
  number = {10096158}
}

@article{gruberGated2DepthRealtimeDense2019,
  title = {{{Gated2Depth}}: {{Real-time Dense Lidar}} from {{Gated Images}}},
  author = {Gruber, Tobias and {Julca-Aguilar}, Frank and Bijelic, Mario and Ritter, Werner and Dietmayer, Klaus and Heide, Felix},
  year = {2019},
  month = feb,
  abstract = {We present an imaging framework which converts three images from a gated camera into high-resolution depth maps with depth accuracy comparable to pulsed lidar measurements. Existing scanning lidar systems achieve low spatial resolution at large ranges due to mechanically-limited angular sampling rates, restricting scene understanding tasks to close-range clusters with dense sampling. Moreover, today's pulsed lidar scanners suffer from high cost, power consumption, large form-factors, and they fail in the presence of strong backscatter. We depart from point scanning and demonstrate that it is possible to turn a low-cost CMOS gated imager into a dense depth camera with at least 80m range - by learning depth from three gated images. The proposed architecture exploits semantic context across gated slices, and is trained on a synthetic discriminator loss without the need of dense depth labels. The proposed replacement for scanning lidar systems is real-time, handles back-scatter and provides dense depth at long ranges. We validate our approach in simulation and on real-world data acquired over 4,000km driving in northern Europe. Data and code are available at https://github.com/gruberto/Gated2Depth.}
}

@incollection{gruberHighResolutionGatedDepth2021,
  title = {High-{{Resolution Gated Depth Estimation}} for {{Self-Driving Cars}} in {{AdverseWeather}}: {{Von}} Der {{Fahrerassistenz}} Zum Autonomen {{Fahren}} 6. {{Internationale ATZ-Fachtagung}}},
  booktitle = {Automatisiertes {{Fahren}} 2020},
  author = {Gruber, Tobias and Walz, Stefanie and Ritter, Werner and Dietmayer, Klaus},
  editor = {Bertram, Torsten},
  year = {2021},
  series = {Proceedings},
  pages = {125--139},
  publisher = {Springer Fachmedien Wiesbaden},
  address = {Wiesbaden}
}

@inproceedings{gruyerModelingValidationNew2012,
  title = {Modeling and Validation of a New Generic Virtual Optical Sensor for {{ADAS}} Prototyping},
  booktitle = {2012 {{IEEE Intelligent Vehicles Symposium}}},
  author = {Gruyer, D and Grapinet, M and De Souza, P},
  year = {2012},
  month = jun,
  pages = {969--974},
  abstract = {In the early design stages of embedded applications, it becomes necessary to have a very realistic simulation environment dedicated to the prototyping and to the evaluation of these Advanced Driving Assistance Systems (ADAS). This Numerical simulation stage is gradually becoming a strong advantage in active safety. The use of realistic numerical models enabling to substitute real data by simulated data is primordial. For such virtual platform it is mandatory to provide physics-driven road environments, virtual embedded sensors, and physics-based vehicle models. In this publication, a generic solution for cameras modelling is presented. The use of this optical sensor simulation can easily and efficiently replace real camera test campaigns. This optical sensor is very important due to the great number of applications and algorithms based on it. The presented model involves a filter mechanism in order to reproduce, in the most realistic way, the behaviour of optical sensors. The main filters used in ADAS developments will be presented. Moreover, an optical analysis of these virtual sensors has been achieved allowing the confrontation between real and simulated results. An optical platform has been developed to characterize and validate any camera, permitting to measure their performances. By comparing real and simulated sensors with this platform, this paper demonstrates this virtual platform (Pro-SiVIC™) accurately reproduces real optical sensors' behaviour.},
  keywords = {Cameras,Computational modeling,Optical distortion,Optical filters,Optical sensors,Rendering (computer graphics)}
}

@inproceedings{guarneraBRDFRepresentationAcquisition2016,
  title = {{{BRDF}} Representation and Acquisition},
  booktitle = {Computer {{Graphics Forum}}},
  author = {Guarnera, Darya and Guarnera, Giuseppe Claudio and Ghosh, Abhijeet and Denk, Cornelia and Glencross, Mashhuda},
  year = {2016},
  volume = {35},
  pages = {625--650},
  publisher = {Wiley Online Library}
}

@article{guptaLearningRichFeatures2014,
  title = {Learning {{Rich Features}} from {{RGB-D Images}} for {{Object Detection}} and {{Segmentation}}},
  author = {Gupta, Saurabh and Girshick, Ross and Arbel{\'a}ez, Pablo and Malik, Jitendra},
  year = {2014},
  month = jul,
  abstract = {In this paper we study the problem of object detection for RGB-D images using semantically rich image and depth features. We propose a new geocentric embedding for depth images that encodes height above ground and angle with gravity for each pixel in addition to the horizontal disparity. We demonstrate that this geocentric embedding works better than using raw depth images for learning feature representations with convolutional neural networks. Our final object detection system achieves an average precision of 37.3\%, which is a 56\% relative improvement over existing methods. We then focus on the task of instance segmentation where we label pixels belonging to object instances found by our detector. For this task, we propose a decision forest approach that classifies pixels in the detection window as foreground or background using a family of unary and binary tests that query shape and geocentric pose features. Finally, we use the output from our object detectors in an existing superpixel classification framework for semantic scene segmentation and achieve a 24\% relative improvement over current state-of-the-art for the object categories that we study. We believe advances such as those represented in this paper will facilitate the use of perception in fields like robotics.}
}

@misc{hamiltonAdaptiveColorPlan1997,
  title = {Adaptive Color Plan Interpolation in Single Sensor Color Electronic Camera},
  author = {Hamilton, Jr, John F and Adams, Jr, James E},
  year = {1997},
  month = may,
  journal = {US Patent},
  number = {5629734}
}

@inproceedings{hanikaEfficientMonteCarlo2014,
  title = {Efficient {{Monte Carlo}} Rendering with Realistic Lenses},
  booktitle = {Computer {{Graphics Forum}}},
  author = {Hanika, Johannes and Dachsbacher, Carsten},
  year = {2014},
  volume = {33},
  pages = {323--332},
  publisher = {Wiley Online Library}
}

@article{hanImagebased3DObject2019,
  title = {Image-Based {{3D}} Object Reconstruction: {{State-of-the-art}} and Trends in the Deep Learning Era},
  author = {Han, Xian-Feng and Laga, Hamid and Bennamoun, Mohammed},
  year = {2019},
  journal = {IEEE transactions on pattern analysis and machine intelligence},
  volume = {43},
  number = {5},
  pages = {1578--1604},
  publisher = {IEEE}
}

@article{hasinoffBurstPhotographyHigh2016,
  title = {Burst Photography for High Dynamic Range and Low-Light Imaging on Mobile Cameras},
  author = {Hasinoff, Samuel W and Sharlet, Dillon and Geiss, Ryan and Adams, Andrew and Barron, Jonathan T and Kainz, Florian and Chen, Jiawen and Levoy, Marc},
  year = {2016},
  month = dec,
  journal = {ACM Trans. Graph.},
  volume = {35},
  number = {6},
  pages = {1--12},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  keywords = {computational photography,high dynamic range}
}

@article{hattarMelanopsincontainingRetinalGanglion2002,
  title = {Melanopsin-Containing Retinal Ganglion Cells: {{Architecture}}, Projections, and Intrinsic Photosensitivity},
  author = {Hattar, S. and Liao, H. W. and Takao, M. and Berson, D. M. and Yau, K. W.},
  year = {2002},
  journal = {Science},
  volume = {295},
  number = {5557},
  pages = {1065--1070}
}

@inproceedings{hazirbasFuseNetIncorporatingDepth2016,
  title = {{{FuseNet}}: {{Incorporating Depth}} into {{Semantic Segmentation}} via {{Fusion-Based CNN Architecture}}},
  author = {Haz{\i}rba{\c s}, Caner and Ma, Lingni and Domokos, Csaba and Cremers, Daniel},
  year = {2016},
  month = nov,
  abstract = {In this paper we address the problem of semantic labeling of indoor scenes on RGB-D data. With the availability of RGB-D cameras, it is expected that additional depth measurement will improve the accuracy. Here we investigate a solution how to incorporate complementary depth information into a semantic segmentation framework by making use of convolutional neural networks (CNNs). Recently encoder-decoder type fully convolutional CNN architectures have achieved a great success in the field of semantic segmentation. Motivated by this observation we propose an encoder-decoder type network, where the encoder part is composed of two branches of networks that simultaneously extract features from RGB and depth images and fuse depth features into the RGB feature maps as the network goes deeper. Comprehensive experimental evaluations demonstrate that the proposed fusion-based architecture achieves competitive results with the state-of-the-art methods on the challenging SUN RGB-D benchmark obtaining 76.27\% global accuracy , 48.30\% average class accuracy and 37.29\% average intersection-over-union score.}
}

@article{hechtEnergyQuantaVision1942,
  title = {Energy, Quanta and Vision},
  author = {Hecht, S. and Schlaer, S. and Pirenne, M.H.},
  year = {1942},
  journal = {Journal of the Optical Society of America},
  volume = {38},
  number = {6},
  pages = {196--208}
}

@book{hechtOptics2017,
  title = {Optics},
  author = {Hecht, E.},
  year = {2017},
  edition = {5th},
  publisher = {Pearson},
  address = {Boston}
}

@inproceedings{heDeepResidualLearning2016,
  title = {Deep Residual Learning for Image Recognition},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2016},
  pages = {770--778}
}

@misc{heesenYoungHelmholtzMaxwellTheoryColor2015,
  title = {The {{Young-}}({{Helmholtz}})-{{Maxwell Theory}} of {{Color Vision}}},
  author = {Heesen, Remco},
  year = {2015},
  month = jan,
  keywords = {Hermann von Helmholtz,History of color mixing,History of physiology,James Clerk Maxwell,Priority}
}

@book{helmholtzHandbuchPhysiologischenOptik1866,
  title = {Handbuch Der Physiologischen {{Optik II}} (3rd {{Edition}}, 1911) (Pp. 243--244)},
  author = {Helmholtz, H.},
  year = {1866},
  publisher = {Voss},
  address = {Hamburg}
}

@article{helmholtzLXXXITheoryCompound1852,
  title = {{{LXXXI}}. {{On}} the Theory of Compound Colours},
  author = {Helmholtz, H},
  year = {1852},
  month = jan,
  journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
  volume = {4},
  number = {28},
  pages = {519--534},
  publisher = {Taylor \& Francis}
}

@book{helmholtzPhysiologicalOptics1896,
  title = {Physiological {{Optics}}},
  author = {Helmholtz, H.},
  year = {1896},
  publisher = {Dover Publications, Inc.},
  address = {New York}
}

@inproceedings{hempeCombiningRealisticVirtual2014,
  title = {Combining Realistic Virtual Environments with Real-Time Sensor Simulations for Close-to-Reality Testing and Development in {{eRobotics}} Applications},
  booktitle = {2014 {{IEEE International Symposium}} on {{Robotics}} and {{Manufacturing Automation}} ({{ROMA}})},
  author = {Hempe, Nico and Rossmann, J{\"u}rgen},
  year = {2014},
  month = dec,
  pages = {1--6},
  publisher = {ieeexplore.ieee.org},
  keywords = {Integrated optics,Noise,Optical buffering,Optical distortion,Optical filters,Robot sensing systems}
}

@article{henningsAxisImageQuality1981,
  title = {Off-Axis Image Quality in the Human Eye},
  author = {Hennings, J. A. M. and Charman, W. N.},
  year = {1981},
  journal = {Vision Res.},
  volume = {21},
  pages = {445--455}
}

@book{heringGrundzugeLehreVom1964,
  title = {Grundz{\"u}ge Der {{Lehre}} Vom {{Lichtsinn}} (1874) ({{Outlines}} of a Theory of the Light Sense, Translated by {{Hurvich}} and {{Jameson}})},
  author = {Hering, E.},
  translator = {Hurvich, Leo M. and Jameson, Dorothea},
  year = {1964},
  publisher = {Harvard University Press},
  address = {Cambridge, MA}
}

@book{heringOutlinesTheoryLight1964,
  title = {Outlines of a Theory of the Light Sense},
  author = {Hering, Ewald},
  year = {1964},
  publisher = {Harvard University Press},
  address = {Cambridge, MA, US}
}

@incollection{hoferColorVisionRetinal2014,
  title = {Color Vision and the Retinal Mosaic},
  booktitle = {The {{New Visual Neurosciences}}},
  author = {Hofer, H.J. and Williams, D.R.},
  editor = {Chalupa, L. M. and Werner, J. S.},
  year = {2014},
  pages = {469--483},
  publisher = {MIT Press},
  address = {Cambridge, MA}
}

@inproceedings{hoffmanCrossmodalAdaptationRGBdetection2016,
  title = {Cross-Modal Adaptation for {{RGB-D}} Detection},
  booktitle = {2016 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Hoffman, J and Gupta, S and Leong, J and Guadarrama, S and Darrell, T},
  year = {2016},
  month = may,
  pages = {5032--5039},
  keywords = {Adaptation models,CNN based object detectors,convolutional neural network,cross-modal adaptation,Detectors,image colour analysis,image fusion,labeled depth images,mid-level fusion,neural nets,object detection,Object detection,Proposals,RGB images,RGB object detector,RGB-D detection,Robots,Training,Training data}
}

@inproceedings{hondaHighSensitivityColor2007,
  title = {High Sensitivity Color {{CMOS}} Image Sensor with {{WRGB}} Color Filter Array and Color Separation Process Using Edge Detection},
  booktitle = {Proc. {{Int}}. {{Image Sens}}. {{Workshop}}},
  author = {Honda, H and Iida, Y and Egawa, Y},
  year = {2007},
  pages = {263--266}
}

@article{hossainAutonomousDrivingVehicleLearning2019,
  title = {Autonomous-{{Driving Vehicle Learning Environments}} Using {{Unity Real-time Engine}} and {{End-to-End CNN Approach}}},
  author = {Hossain, Sabir and Lee, Deok-Jin},
  year = {2019},
  journal = {The Journal of Korea Robotics Society},
  volume = {14},
  number = {2},
  pages = {122--130},
  publisher = {Korea Robotics Society},
  langid = {english},
  keywords = {Artificial Intelligence,Autonomous Shuttle Vehicle,Behavior Learning,Virtual Environment}
}

@inproceedings{hossainCAIASSimulatorSelfdriving2019,
  title = {{{CAIAS Simulator}}: {{Self-driving Vehicle Simulator}} for {{AI Research}}},
  booktitle = {Intelligent {{Computing}} \& {{Optimization}}},
  author = {Hossain, Sabir and Fayjie, Abdur R and Doukhi, Oualid and Lee, Deok-Jin},
  year = {2019},
  pages = {187--195},
  publisher = {Springer International Publishing}
}

@article{howardHelmholtzHeringDebate1999,
  title = {The {{Helmholtz}}--{{Hering}} Debate in Retrospect},
  author = {Howard, I P},
  year = {1999},
  journal = {Perception},
  volume = {28},
  number = {5},
  pages = {543--549},
  publisher = {SAGE Publications},
  langid = {english}
}

@article{huckImageGatheringProcessing1985,
  title = {Image Gathering and Processing: Information and Fidelity},
  author = {Huck, Friedrich O and Fales, Carl L and Halyo, Nesim and Samms, Richard W and Stacy, Kathryn},
  year = {1985},
  journal = {JOSA A},
  volume = {2},
  number = {10},
  pages = {1644--1666},
  publisher = {Optical Society of America}
}

@article{huckImageQualityPrediction1976,
  title = {Image Quality Prediction: {{An}} Aid to the {{Viking Lander}} Imaging Investigation on {{Mars}}},
  author = {Huck, {\relax FO} and Wall, {\relax SD}},
  year = {1976},
  journal = {Applied optics},
  volume = {15},
  number = {7},
  pages = {1748--1766},
  publisher = {Optical Society of America}
}

@article{huckSpectrophotometricColorEstimates1977,
  title = {Spectrophotometric and Color Estimates of the {{Viking}} Lander Sites},
  author = {Huck, Friedrich O and Jobson, Daniel J and Park, Stephen K and Wall, Stephen D and Arvidson, Raymond E and Patterson, William R and Benton, William D},
  year = {1977},
  journal = {Journal of Geophysical Research},
  volume = {82},
  number = {28},
  pages = {4401--4411},
  publisher = {Wiley Online Library}
}

@inproceedings{hullinPolynomialOpticsConstruction2012,
  title = {Polynomial {{Optics}}: {{A}} Construction Kit for Efficient Ray-Tracing of Lens Systems},
  booktitle = {Computer {{Graphics Forum}}},
  author = {Hullin, Matthias B and Hanika, Johannes and Heidrich, Wolfgang},
  year = {2012},
  volume = {31},
  pages = {1375--1383},
  publisher = {Wiley Online Library}
}

@book{huntReproductionColour2004,
  title = {The {{Reproduction}} of {{Colour}}},
  author = {Hunt, R. W. G.},
  year = {2004},
  edition = {6},
  publisher = {John Wiley \& Sons},
  address = {Chichester, England}
}

@misc{IEEEDraftStandard2023,
  title = {{{IEEE Draft Standard}} for {{Automotive System Image Quality}}},
  year = {2023},
  langid = {english}
}

@article{iida30UmPixels2023,
  title = {A 3.0 {$M$}m Pixels and 1.5 {$M$}m Pixels Combined Complementary Metal-Oxide Semiconductor Image Sensor for High Dynamic Range Vision beyond 106 {{dB}}},
  author = {Iida, Satoko and Kawamata, Daisuke and Sakano, Yorito and Yamanaka, Takaya and Nabeyoshi, Shohei and Matsuura, Tomohiro and Toshida, Masahiro and Baba, Masahiro and Fujimori, Nobuhiko and Basavalingappa, Adarsh and Han, Sungin and Katayama, Hidetoshi and Azami, Junichiro},
  year = {2023},
  month = nov,
  journal = {Sensors (Basel)},
  volume = {23},
  number = {21},
  pages = {8998},
  publisher = {MDPI AG},
  langid = {english},
  keywords = {automotive,HDR,high definition,LFM,motion artifact,motion blur,quadrate-square pixel,Ta-Kuchi pixel}
}

@inproceedings{innocentPixelNestedPhoto2019,
  title = {Pixel with Nested Photo Diodes and 120 {{dB}} Single Exposure Dynamic Range},
  booktitle = {International Image Sensors Workshop},
  author = {Innocent, M and Rodr{\'i}guez, {\'A}ngel D and Guruaribam, Debashree and Rahman, M and Sulfridge, Marc and Borthakur, S and Gravelle, B and Goto, T and Dougherty, Nathan and Desjardin, Bill and Sabo, David and Mlinar, Marko and Geurts, T},
  year = {2019},
  pages = {95--98}
}

@misc{ISETcornellbox,
  title = {{{ISETcornellbox}}}
}

@misc{ISO122332017,
  title = {{{ISO}} 12233:2017 {{Photography}} --- {{Electronic}} Still Picture Imaging --- {{Resolution}} and Spatial Frequency Responses}
}

@article{itoSmallImagingDepth2018,
  title = {Small {{Imaging Depth LIDAR}} and {{DCNN-Based Localization}} for {{Automated Guided Vehicle}}},
  author = {Ito, Seigo and Hiratsuka, Shigeyoshi and Ohta, Mitsuhiko and Matsubara, Hiroyuki and Ogawa, Masaru},
  year = {2018},
  month = jan,
  journal = {Sensors},
  volume = {18},
  number = {1},
  publisher = {mdpi.com},
  abstract = {We present our third prototype sensor and a localization method for Automated Guided Vehicles (AGVs), for which small imaging LIght Detection and Ranging (LIDAR) and fusion-based localization are fundamentally important. Our small imaging LIDAR, named the Single-Photon Avalanche Diode (SPAD) LIDAR, uses a time-of-flight method and SPAD arrays. A SPAD is a highly sensitive photodetector capable of detecting at the single-photon level, and the SPAD LIDAR has two SPAD arrays on the same chip for detection of laser light and environmental light. Therefore, the SPAD LIDAR simultaneously outputs range image data and monocular image data with the same coordinate system and does not require external calibration among outputs. As AGVs travel both indoors and outdoors with vibration, this calibration-less structure is particularly useful for AGV applications. We also introduce a fusion-based localization method, named SPAD DCNN, which uses the SPAD LIDAR and employs a Deep Convolutional Neural Network (DCNN). SPAD DCNN can fuse the outputs of the SPAD LIDAR: range image data, monocular image data and peak intensity image data. The SPAD DCNN has two outputs: the regression result of the position of the SPAD LIDAR and the classification result of the existence of a target to be approached. Our third prototype sensor and the localization method are evaluated in an indoor environment by assuming various AGV trajectories. The results show that the sensor and localization method improve the localization accuracy.},
  langid = {english},
  keywords = {deep convolutional neural network,deep learning,imaging LIDAR,light detection and ranging,localization,sensor fusion,single-photon avalanche diode}
}

@inproceedings{ivColorWaterUsing2014,
  title = {The Color of Water: Using Underwater Photography to Estimatewater Quality},
  booktitle = {Digital {{Photography X}}},
  author = {IV, John Breneman and Blasinski, Henryk and Farrell, Joyce},
  editor = {Sampat, Nitin and Tezaur, Radka and Battiato, Sebastiano and Fowler, Boyd A.},
  year = {2014},
  volume = {9023},
  pages = {224--234},
  publisher = {SPIE},
  doi = {10.1117/12.2043132},
  keywords = {Underwater Color Correction},
  annotation = {Backup Publisher: International Society for Optics and Photonics}
}

@article{jaekenOpticalQualityEmmetropic2012,
  title = {Optical Quality of Emmetropic and Myopic Eyes in the Periphery Measured with High-Angular Resolution},
  author = {Jaeken, Bart and Artal, Pablo},
  year = {2012},
  journal = {Investigative Ophthalmology and Visual Science},
  volume = {53},
  number = {7},
  pages = {3405--3413}
}

@book{janesickPhotonTransfer2007,
  title = {Photon Transfer},
  author = {Janesick, James R},
  year = {2007},
  publisher = {SPIE Bellingham}
}

@article{jiangEnlightenGANDeepLight2021,
  title = {{{EnlightenGAN}}: {{Deep Light Enhancement Without Paired Supervision}}},
  author = {Jiang, Yifan and Gong, Xinyu and Liu, Ding and Cheng, Yu and Fang, Chen and Shen, Xiaohui and Yang, Jianchao and Zhou, Pan and Wang, Zhangyang},
  year = {2021},
  month = jan,
  journal = {IEEE Trans. Image Process.},
  volume = {30},
  pages = {2340--2349},
  langid = {english}
}

@article{jiangLearningImageProcessing2017,
  title = {Learning the Image Processing Pipeline},
  author = {Jiang, Haomiao and Tian, Qiyuan and Farrell, Joyce and Wandell, Brian A},
  year = {2017},
  journal = {IEEE Transactions on Image Processing},
  volume = {26},
  number = {10},
  pages = {5032--5042},
  publisher = {IEEE}
}

@article{juddFundamentalStudiesColor1966,
  title = {Fundamental Studies of Color Vision from 1860 to 1960},
  author = {Judd, D B},
  year = {1966},
  month = jun,
  journal = {Proc. Natl. Acad. Sci. U. S. A.},
  volume = {55},
  number = {6},
  pages = {1313--1330},
  langid = {english}
}

@article{juddMaxwellModernColorimetry1961,
  title = {Maxwell and {{Modern Colorimetry}}},
  author = {Judd, D B},
  year = {1961},
  month = nov,
  journal = {The Journal of Photographic Science},
  volume = {9},
  number = {6},
  pages = {341--352},
  publisher = {Taylor \& Francis}
}

@article{juddRelationNormalTrichromatic1964,
  title = {Relation between Normal Trichromatic Vision and Dichromatic Vision Representing a Reduced Form of Normal Vision},
  author = {Judd, Deane B.},
  year = {1964},
  journal = {Acta Chromatica},
  volume = {1},
  number = {3},
  pages = {524--527}
}

@article{juddStandardResponseFunctions1945,
  title = {Standard Response Functions for Protanopic and Deuteranopic Vision},
  author = {Judd, Deane B},
  year = {1945},
  month = mar,
  journal = {J. Opt. Soc. Am.},
  volume = {35},
  number = {3},
  pages = {199},
  publisher = {The Optical Society},
  langid = {english},
  keywords = {Color vision,Crystalline lens,Field size,Optic nerve,Standard observers,Visual system}
}

@misc{kamelValidatingNVIDIADRIVE2021,
  title = {Validating {{NVIDIA DRIVE Sim Camera Models}}},
  author = {Kamel, Car{\`e}ne and Reid, Ashley and Plepp, Fabian and by Kamel, View All Posts and by Reid, View All Posts and by Plepp, View All Posts},
  year = {2021},
  month = dec,
  abstract = {This post covers the validation of camera models in DRIVE Sim, assessing the performance elements, from rendering of the world scene to protocol simulation.}
}

@article{kangTestYourSelfdriving2019,
  title = {Test Your Self-Driving Algorithm: {{An}} Overview of Publicly Available Driving Datasets and Virtual Testing Environments},
  author = {Kang, Yue and Yin, Hang and Berger, Christian},
  year = {2019},
  journal = {IEEE Transactions on Intelligent Vehicles},
  volume = {4},
  number = {2},
  pages = {171--185},
  publisher = {IEEE}
}

@inproceedings{karaimerSoftwarePlatformManipulating2016,
  title = {A Software Platform for Manipulating the Camera Imaging Pipeline},
  booktitle = {European {{Conference}} on {{Computer Vision}}},
  author = {Karaimer, Hakki Can and Brown, Michael S},
  year = {2016},
  pages = {429--444},
  publisher = {Springer}
}

@article{katoDifferentiableRenderingSurvey2020,
  title = {Differentiable Rendering: {{A}} Survey},
  author = {Kato, Hiroharu and Beker, Deniz and Morariu, Mihai and Ando, Takahiro and Matsuoka, Toru and Kehl, Wadim and Gaidon, Adrien},
  year = {2020},
  journal = {arXiv preprint arXiv:2006.12057},
  eprint = {2006.12057},
  archiveprefix = {arXiv}
}

@book{keshmirianPhysicallybasedApproachLens2008,
  title = {A Physically-Based Approach for Lens Flare Simulation},
  author = {Keshmirian, Arash},
  year = {2008},
  publisher = {University of California, San Diego}
}

@misc{kijimaImageSensorImproved2007,
  title = {Image Sensor with Improved Light Sensitivity},
  author = {Kijima, Takayuki and Nakamura, Hideo and Compton, John T and Hamilton, John F and DeWeese, Thomas E},
  year = {2007},
  month = nov,
  journal = {US Patent},
  number = {20070268533:A1}
}

@inproceedings{kim15MpixelRGBZCMOS2012,
  title = {A 1.{{5Mpixel RGBZ CMOS}} Image Sensor for Simultaneous Color and Range Image Capture},
  booktitle = {2012 {{IEEE International Solid-State Circuits Conference}}},
  author = {Kim, W and Yibing, W and Ovsiannikov, I and Lee, S and Park, Y and Chung, C and Fossum, E},
  year = {2012},
  month = feb,
  pages = {392--394},
  abstract = {A 1.5Mpixel RGBZ image sensor that simultaneously captures color (RGB) and time-of-flight (ToF) range (Z) images is presented. While ToF sensors are well documented, few, if any, monolithic sensors have been reported that capture both range and color. In one sensor, a combined pixel structure captures color and range, but presumably in sequential fields. This approach does not allow simultaneous capturing of range with color, and the pixel performance cannot be optimized for each mode. In thie paper, we introduce a sensor that is designed to capture color and range simultaneously with individually optimized pixels.},
  keywords = {Arrays,CMOS image sensors,Distance measurement,Image color analysis,Image sensors,monolithic sensors,pixel performance,pixel structure,RGB color,RGBZ image sensor,Sensors,Three dimensional displays,time-of-flight range images,ToF sensors}
}

@article{kimConvolutionalNeuralNetworkBased2017,
  title = {Convolutional {{Neural Network-Based Human Detection}} in {{Nighttime Images Using Visible Light Camera Sensors}}},
  author = {Kim, Jong Hyun and Hong, Hyung Gil and Park, Kang Ryoung},
  year = {2017},
  month = may,
  journal = {Sensors},
  volume = {17},
  number = {5},
  abstract = {Because intelligent surveillance systems have recently undergone rapid growth, research on accurately detecting humans in videos captured at a long distance is growing in importance. The existing research using visible light cameras has mainly focused on methods of human detection for daytime hours when there is outside light, but human detection during nighttime hours when there is no outside light is difficult. Thus, methods that employ additional near-infrared (NIR) illuminators and NIR cameras or thermal cameras have been used. However, in the case of NIR illuminators, there are limitations in terms of the illumination angle and distance. There are also difficulties because the illuminator power must be adaptively adjusted depending on whether the object is close or far away. In the case of thermal cameras, their cost is still high, which makes it difficult to install and use them in a variety of places. Because of this, research has been conducted on nighttime human detection using visible light cameras, but this has focused on objects at a short distance in an indoor environment or the use of video-based methods to capture multiple images and process them, which causes problems related to the increase in the processing time. To resolve these problems, this paper presents a method that uses a single image captured at night on a visible light camera to detect humans in a variety of environments based on a convolutional neural network. Experimental results using a self-constructed Dongguk night-time human detection database (DNHD-DB1) and two open databases (Korea advanced institute of science and technology (KAIST) and computer vision center (CVC) databases), as well as high-accuracy human detection in a variety of environments, show that the method has excellent performance compared to existing methods.},
  langid = {english},
  keywords = {convolutional neural network,intelligent surveillance system,nighttime human detection,visible light image}
}

@article{kimEstimationAnyFields2021,
  title = {Estimation of Any Fields of Lens {{PSFs}} for Image Simulation},
  author = {Kim, Sangmin and Kim, Daekwan and Chung, Kilwoo and Yim, JoonSeo},
  year = {2021},
  journal = {Electronic Imaging},
  volume = {2021},
  number = {7},
  pages = {72--1},
  publisher = {{Society for Imaging Science and Technology}}
}

@misc{kinzeyInvestigationLEDStreet,
  title = {An Investigation of {{LED}} Street Lighting's Impact on Sky Glow},
  author = {Kinzey, Bruce and Perrin, Tess and Miller, Naomi J. and Kocifaj, Miroslav and Aub{\'e}, Martin and Lamphar, H{\'e}ctor S.}
}

@article{kleinSimulatingLowcostCameras2009,
  title = {Simulating Low-Cost Cameras for Augmented Reality Compositing},
  author = {Klein, Georg and Murray, David W},
  year = {2009},
  journal = {IEEE transactions on visualization and computer graphics},
  volume = {16},
  number = {3},
  pages = {369--380},
  publisher = {IEEE}
}

@book{knillPerceptionBayesianInference1996,
  title = {Perception as {{Bayesian Inference}}},
  author = {Knill, D. C. and Richards, W.},
  year = {1996},
  publisher = {Cambridge University Press},
  address = {Cambridge}
}

@book{knoblauchModelingPsychophysicalData2012,
  title = {Modeling {{Psychophysical Data}} in {{R}} ({{Use R}}!)},
  author = {Knoblauch, K. and Maloney, L. T.},
  year = {2012},
  publisher = {Springer},
  address = {New York}
}

@article{kochEfficiencyInformationTransmission2004,
  title = {Efficiency of Information Transmission by Retinal Ganglion Cells},
  author = {Koch, K. and McLean, J. and Berry, M. and Sterling, P. and Balasubramanian, V. and Freed, M. A.},
  year = {2004},
  journal = {Current Biology},
  volume = {14},
  number = {17},
  pages = {1523--1530}
}

@inproceedings{kolbRealisticCameraModel1995,
  title = {A Realistic Camera Model for Computer Graphics},
  booktitle = {Proceedings of the 22nd Annual Conference on Computer Graphics and Interactive Techniques},
  author = {Kolb, Craig and Mitchell, Don and Hanrahan, Pat},
  year = {1995},
  pages = {317--324}
}

@article{konigGrundempfindungenUndIhre1886,
  title = {Die {{Grundempfindungen}} Und Ihre {{Intensit{\"a}ts-Vertheilung}} Im {{Spectrum}} ({{Fundamental}} Sensations and Their Intensity Distribution in the Spectrum)},
  author = {Konig, A and {Dieterici, C (translated by Kuehni, Rolf G)}},
  year = {1886},
  month = jul,
  journal = {Sitzungsberichte der Akademie der Wissenschaften},
  pages = {805--829}
}

@article{konnikHighlevelNumericalSimulations2014,
  title = {High-Level Numerical Simulations of Noise in {{CCD}} and {{CMOS}} Photosensors: Review and Tutorial},
  author = {Konnik, Mikhail and Welsh, James},
  year = {2014},
  month = dec,
  journal = {arXiv [astro-ph.IM]}
}

@article{krantzColorMeasurementColor1975,
  title = {Color Measurement and Color Theory: {{I}}. {{Representation}} Theorem for {{Grassmann}} Structures},
  author = {Krantz, David H},
  year = {1975},
  month = aug,
  journal = {J. Math. Psychol.},
  volume = {12},
  number = {3},
  pages = {283--303}
}

@article{kumarLiDARCameraFusion2020,
  title = {{{LiDAR}} and {{Camera Fusion Approach}} for {{Object Distance Estimation}} in {{Self-Driving Vehicles}}},
  author = {Kumar, G Ajay and Lee, Jin Hee and Hwang, Jongrak and Park, Jaehyeong and Youn, Sung Hoon and Kwon, Soon},
  year = {2020},
  month = feb,
  journal = {Symmetry},
  volume = {12},
  number = {2},
  pages = {324},
  publisher = {Multidisciplinary Digital Publishing Institute},
  abstract = {The fusion of light detection and ranging (LiDAR) and camera data in real-time is known to be a crucial process in many applications, such as in autonomous driving, industrial automation, and robotics. Especially in the case of autonomous vehicles, the efficient fusion of data from these two types of sensors is important to enabling the depth of objects as well as the detection of objects at short and long distances. As both the sensors are capable of capturing the different attributes of the environment simultaneously, the integration of those attributes with an efficient fusion approach greatly benefits the reliable and consistent perception of the environment. This paper presents a method to estimate the distance (depth) between a self-driving car and other vehicles, objects, and signboards on its path using the accurate fusion approach. Based on the geometrical transformation and projection, low-level sensor fusion was performed between a camera and LiDAR using a 3D marker. Further, the fusion information is utilized to estimate the distance of objects detected by the RefineDet detector. Finally, the accuracy and performance of the sensor fusion and distance estimation approach were evaluated in terms of quantitative and qualitative analysis by considering real road and simulation environment scenarios. Thus the proposed low-level sensor fusion, based on the computational geometric transformation and projection for object distance estimation proves to be a promising solution for enabling reliable and consistent environment perception ability for autonomous vehicles.},
  langid = {english}
}

@misc{kumarSplitPixelExplanation,
  title = {Split Pixel Explanation},
  author = {Kumar, Prabu}
}

@misc{labsIMX490SensorHDR2022,
  title = {{{IMX490}}: {{On-Sensor HDR}} and {{Flicker Mitigation}} for {{High Contrast Scenes}}},
  author = {Labs, Lucid Vision},
  year = {2022},
  month = jan,
  journal = {LUCID Vision Labs - Modern Machine Vision Cameras},
  publisher = {LUCID Vision Labs},
  langid = {english}
}

@inproceedings{lanselLocalLinearLearned2011,
  title = {Local {{Linear Learned Image Processing Pipeline}}},
  booktitle = {Imaging and {{Applied Optics}}},
  author = {Lansel, Steven and Wandell, Brian A.},
  year = {2011},
  pages = {IMC3},
  publisher = {Optical Society of America},
  doi = {10.1364/ISA.2011.IMC3},
  abstract = {The local linear learned (L3) algorithm is presented that simultaneously performs the demosaicking, denoising, and color transform calculations of an image processing pipeline for a digital camera with any color filter array.},
  keywords = {Cameras,Color spaces,Digital image processing,Image processing,Image processing algorithms,Linear filtering,Multispectral imaging}
}

@article{laughlinEnergyConstraintCoding2001,
  title = {Energy as a Constraint on the Coding and Processing of Sensory Information},
  author = {Laughlin, S. B.},
  year = {2001},
  journal = {Current Opinion in Neurobiology},
  volume = {11},
  number = {4},
  pages = {475--480}
}

@book{leeBayesianStatistics1989,
  title = {Bayesian {{Statistics}}},
  author = {Lee, P. M.},
  year = {1989},
  publisher = {Oxford University Press},
  address = {London}
}

@techreport{levinUnderstandingCameraTradeoffs2008,
  type = {Report},
  title = {Understanding Camera Trade-Offs through a {{Bayesian}} Analysis of Light Field Projections},
  author = {Levin, A. and Durand, F. and Freeman, W. T.},
  year = {2008},
  institution = {MIT}
}

@article{lianImageSystemsSimulation2018,
  title = {Image Systems Simulation for 360 Camera Rigs},
  author = {Lian, Trisha and Farrell, Joyce and Wandell, Brian},
  year = {2018},
  journal = {Electronic Imaging},
  volume = {2018},
  number = {5},
  pages = {353--1},
  publisher = {{Society for Imaging Science and Technology}}
}

@article{lianImageSystemsSimulation2018a,
  title = {Image {{Systems Simulation}} for 360 Deg {{Camera Rigs}}},
  author = {Lian, Trisha and Farrell, Joyce and Wandell, Brian},
  year = {2018},
  month = jan,
  journal = {Electronic Imaging},
  volume = {2018},
  number = {5},
  pages = {353--1--353--5},
  keywords = {2019 ICCV (Zhenyi Brian)}
}

@article{lianRayTracing3D2019,
  title = {Ray Tracing {{3D}} Spectral Scenes through Human Optics Models},
  author = {Lian, T. and MacKenzie, K. J. and Brainard, D. H. and Cottaris, N. P. and Wandell, B. A.},
  year = {2019},
  journal = {Journal of Vision},
  volume = {19},
  number = {12},
  pages = {23}
}

@misc{liCompressingCompandingHigh2005,
  title = {Compressing and Companding High Dynamic Range Images with Subband Architectures},
  author = {Li, Yuanzhen and Sharan, Lavanya and Adelson, Edward H},
  year = {2005},
  journal = {ACM SIGGRAPH 2005 Papers}
}

@misc{liLearningEnhanceLowLight2021,
  title = {Learning to {{Enhance Low-Light Image}} via {{Zero-Reference Deep Curve Estimation}}},
  author = {Li, Chongyi and Guo, Chunle and Chen, Change Loy},
  year = {2021},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  pages = {1--1}
}

@article{linGANBasedDayNightImage2021,
  title = {{{GAN-Based Day-to-Night Image Style Transfer}} for {{Nighttime Vehicle Detection}}},
  author = {Lin, Che-Tsung and Huang, Sheng-Wei and Wu, Yen-Yi and Lai, Shang-Hong},
  year = {2021},
  month = feb,
  journal = {IEEE Trans. Intell. Transp. Syst.},
  volume = {22},
  number = {2},
  pages = {951--963},
  keywords = {Detectors,domain adaptation,Feature extraction,Gallium nitride,generative adversarial network,Image segmentation,image-to-image translation,Object detection,semantic segmentation,Training,Vehicle detection}
}

@incollection{linMicrosoftCOCOCommon2014,
  title = {Microsoft {{COCO}}: {{Common}} Objects in Context},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2014},
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  year = {2014},
  series = {Lecture Notes in Computer Science},
  pages = {740--755},
  publisher = {Springer International Publishing},
  address = {Cham}
}

@inproceedings{linMicrosoftCOCOCommon2014a,
  title = {Microsoft {{COCO}}: {{Common Objects}} in {{Context}}},
  shorttitle = {Microsoft {{COCO}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2014},
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C. Lawrence},
  editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  year = {2014},
  pages = {740--755},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-10602-1_48},
  abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
  isbn = {978-3-319-10602-1},
  langid = {english},
  keywords = {Common Object,Object Category,Object Detection,Object Instance,Scene Understanding},
  annotation = {Changing to force duplicate update.},
  file = {/Users/wandell/Zotero/storage/6NLESKYE/Lin et al. - 2014 - Microsoft COCO Common Objects in Context.pdf}
}

@inproceedings{linMicrosoftCOCOCommon2014b,
  title = {Microsoft {{COCO}}: {{Common Objects}} in {{Context}}},
  shorttitle = {Microsoft {{COCO}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2014},
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C. Lawrence},
  editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  year = {2014},
  pages = {740--755},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-10602-1_48},
  abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
  isbn = {978-3-319-10602-1},
  langid = {english},
  keywords = {Common Object,Object Category,Object Detection,Object Instance,Scene Understanding},
  file = {/Users/wandell/Zotero/storage/GWUQ22YS/Lin et al. - 2014 - Microsoft COCO Common Objects in Context.pdf}
}

@inproceedings{linPhotoRealisticDepthFieldEffects2007,
  title = {Photo-{{Realistic Depth-of-Field Effects Synthesis Based}} on {{Real Camera Parameters}}},
  booktitle = {Advances in {{Visual Computing}}},
  author = {Lin, Huei-Yung and Gu, Kai-Da},
  year = {2007},
  pages = {298--309},
  publisher = {Springer Berlin Heidelberg}
}

@article{liuISETAutoDetectingVehicles2021,
  title = {{{ISETAuto}}: {{Detecting Vehicles With Depth}} and {{Radiance Information}}},
  author = {Liu, Zhenyi and Farrell, Joyce and Wandell, Brian A},
  year = {2021},
  journal = {IEEE Access},
  volume = {9},
  pages = {41799--41808},
  keywords = {Automotive engineering,Autonomous driving,Camera,Cameras,Convolutional neural network,Laser radar,LiDAR,Object detection,Sensor fusion,Sensors,Spatial resolution,Three-dimensional displays,Training}
}

@inproceedings{liuJointDemosaicingDenoising2020,
  title = {Joint Demosaicing and Denoising with Self Guidance},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Liu, Lin and Jia, Xu and Liu, Jianzhuang and Tian, Qi},
  year = {2020},
  pages = {2240--2249}
}

@article{liUnconventionalVisualSensors2022,
  title = {Unconventional {{Visual Sensors}} for {{Autonomous Vehicles}}},
  author = {Li, You and Moreau, Julien and {Ibanez-Guzman}, Javier},
  year = {2022},
  month = may,
  abstract = {Autonomous vehicles rely on perception systems to understand their surroundings for further navigation missions. Cameras are essential for perception systems due to the advantages of object detection and recognition provided by modern computer vision algorithms, comparing to other sensors, such as LiDARs and radars. However, limited by its inherent imaging principle, a standard RGB camera may perform poorly in a variety of adverse scenarios, including but not limited to: low illumination, high contrast, bad weather such as fog/rain/snow, etc. Meanwhile, estimating the 3D information from the 2D image detection is generally more difficult when compared to LiDARs or radars. Several new sensing technologies have emerged in recent years to address the limitations of conventional RGB cameras. In this paper, we review the principles of four novel image sensors: infrared cameras, range-gated cameras, polarization cameras, and event cameras. Their comparative advantages, existing or potential applications, and corresponding data processing algorithms are all presented in a systematic manner. We expect that this study will assist practitioners in the autonomous driving society with new perspectives and insights.}
}

@article{liuNeuralNetworkGeneralization2020,
  title = {Neural {{Network Generalization}}: {{The}} Impact of Camera Parameters},
  author = {Liu, Z and Lian, T and Farrell, J and Wandell, B A},
  year = {2020},
  journal = {IEEE Access},
  publisher = {ieeexplore.ieee.org},
  abstract = {We quantify the generalization of a convolutional neural network (CNN) trained to identify cars. First, we perform a series of experiments to train the network using one image dataset- either synthetic or from a camera-and then test on a different image dataset. We show that {\dots}},
  keywords = {autonomous driving,camera design,convolutional neural network,Imaging systems,network generalization,physically based ray tracing}
}

@inproceedings{liuSingleimageHDRReconstruction2020,
  title = {Single-Image {{HDR}} Reconstruction by Learning to Reverse the Camera Pipeline},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Liu, Yu-Lun and Lai, Wei-Sheng and Chen, Yu-Sheng and Kao, Yi-Lung and Yang, Ming-Hsuan and Chuang, Yung-Yu and Huang, Jia-Bin},
  year = {2020},
  pages = {1651--1660}
}

@inproceedings{liuSoftPrototypingCamera2019,
  title = {Soft {{Prototyping Camera Designs}} for {{Car Detection Based}} on a {{Convolutional Neural Network}}},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision Workshops}}},
  author = {Liu, Zhenyi and Lian, Trisha and Farrell, Joyce and Wandell, Brian},
  year = {2019},
  keywords = {Automotive}
}

@article{liuSystemGeneratingComplex2019,
  title = {A System for Generating Complex Physically Accurate Sensor Images for Automotive Applications},
  author = {Liu, Z and Shen, M and Zhang, J and Liu, S and Blasinski, H and others},
  year = {2019},
  journal = {Electronic},
  publisher = {ingentaconnect.com},
  abstract = {We describe an open-source simulator that creates sensor irradiance and sensor images of typical automotive scenes in urban settings. The purpose of the system is to support camera design and testing for automotive applications. The user can specify scene parameters (eg {\dots}},
  keywords = {2019 ICCV (Zhenyi Brian),Automobile,Automotive,Camera simulation,CNN,Computer graphics,Image systems engineering,Image systems simulation,Machine-learning}
}

@article{liuUsingSimulationQuantify2023,
  title = {Using Simulation to Quantify the Performance of Automotive Perception Systems},
  author = {Liu, Zhenyi and Shah, Devesh and Rahimpour, Alireza and Upadhyay, Devesh and Farrell, Joyce and Wandell, Brian A},
  year = {2023},
  month = mar
}

@inproceedings{lodhiModelingPerformanceAnalysis2015,
  title = {Modeling and Performance Analysis of an Imaging System},
  booktitle = {2015 {{IEEE International Conference}} on {{Research}} in {{Computational Intelligence}} and {{Communication Networks}} ({{ICRCICN}})},
  author = {Lodhi, Vaibhav and Kumar, Abhay and Chakravarty, Debashish},
  year = {2015},
  pages = {329--334},
  publisher = {IEEE}
}

@article{logvinenkoDerivationSpectralSensitivities1998,
  title = {On Derivation of Spectral Sensitivities of the Human Cones from Trichromatic Colour Matching Functions},
  author = {Logvinenko, A D},
  year = {1998},
  month = nov,
  journal = {Vision Res.},
  volume = {38},
  number = {21},
  pages = {3207--3211},
  langid = {english}
}

@article{lopez-rodriguezProjectAdaptDomain2020,
  title = {Project to {{Adapt}}: {{Domain Adaptation}} for {{Depth Completion}} from {{Noisy}} and {{Sparse Sensor Data}}},
  author = {{Lopez-Rodriguez}, Adrian and Busam, Benjamin and Mikolajczyk, Krystian},
  year = {2020},
  month = aug,
  abstract = {Depth completion aims to predict a dense depth map from a sparse depth input. The acquisition of dense ground truth annotations for depth completion settings can be difficult and, at the same time, a significant domain gap between real LiDAR measurements and synthetic data has prevented from successful training of models in virtual settings. We propose a domain adaptation approach for sparse-to-dense depth completion that is trained from synthetic data, without annotations in the real domain or additional sensors. Our approach simulates the real sensor noise in an RGB+LiDAR set-up, and consists of three modules: simulating the real LiDAR input in the synthetic domain via projections, filtering the real noisy LiDAR for supervision and adapting the synthetic RGB image using a CycleGAN approach. We extensively evaluate these modules against the state-of-the-art in the KITTI depth completion benchmark, showing significant improvements.}
}

@misc{LucidDrive2023,
  title = {{{LucidDrive}}},
  year = {2023},
  month = mar,
  abstract = {As a standalone night driving simulation tool, LucidDrive displays and evaluates beam patterns of vehicle headlamps under conditions that are as realistic as possible.},
  langid = {english}
}

@phdthesis{luComputerModelingSimulation1993,
  title = {Computer Modeling and Simulation Techniques for Computer Vision Problems},
  author = {Lu, Ming-Chin},
  year = {1993},
  school = {State University of New York at Stony Brook}
}

@article{lutherGrundlagenFarbmessung1928,
  title = {Grundlagen Der {{Farbmessung}}},
  author = {Luther, Robert},
  year = {1928},
  journal = {Zeitschrift f{\"u}r technische Physik},
  volume = {9},
  number = {5},
  pages = {207--222}
}

@misc{LuxCoreRender,
  title = {{{LuxCoreRender}}}
}

@article{lyuRTMDetEmpiricalStudy2022,
  title = {{{RTMDet}}: {{An}} Empirical Study of Designing Real-Time Object Detectors},
  author = {Lyu, Chengqi and Zhang, Wenwei and Huang, Haian and Zhou, Yue and Wang, Yudong and Liu, Yanyi and Zhang, Shilong and Chen, Kai},
  year = {2022},
  month = dec,
  journal = {arXiv [cs.CV]}
}

@article{lyuSimulationsFluorescenceImaging2021,
  title = {Simulations of Fluorescence Imaging in the Oral Cavity},
  author = {Lyu, Zheng and Jiang, Haomiao and Xiao, Feng and Rong, Jian and Zhang, Tingcheng and Wandell, Brian and Farrell, Joyce},
  year = {2021},
  month = jul,
  journal = {Biomed. Opt. Express},
  volume = {12},
  number = {7},
  pages = {4276--4292},
  publisher = {osapublishing.org},
  abstract = {We describe an end-to-end image systems simulation that models a device capable of measuring fluorescence in the oral cavity. Our software includes a 3D model of the oral cavity and excitation-emission matrices of endogenous fluorophores that predict the spectral radiance of oral mucosal tissue. The predicted radiance is transformed by a model of the optics and image sensor to generate expected sensor image values. We compare simulated and real camera data from tongues in healthy individuals and show that the camera sensor chromaticity values can be used to quantify the fluorescence from porphyrins relative to the bulk fluorescence from multiple fluorophores (elastin, NADH, FAD, and collagen). Validation of the simulations supports the use of soft-prototyping in guiding system design for fluorescence imaging.},
  langid = {english}
}

@article{lyuValidationImageSystems2021,
  title = {Validation of Image Systems Simulation Technology Using a Cornell Box},
  author = {Lyu, Zheng and Kripakaran, Krithin and Furth, Max and Tang, Eric and Wandell, Brian and Farrell, Joyce},
  year = {2021},
  journal = {arXiv preprint arXiv:2105.04106},
  eprint = {2105.04106},
  archiveprefix = {arXiv}
}

@article{lyuValidationPhysicsBasedImage2022,
  title = {Validation of {{Physics-Based Image Systems Simulation With}} 3-{{D Scenes}}},
  author = {Lyu, Zheng and Goossens, Thomas and Wandell, Brian A and Farrell, Joyce},
  year = {2022},
  month = oct,
  journal = {IEEE Sens. J.},
  volume = {22},
  number = {20},
  pages = {19400--19410},
  abstract = {Image systems simulation software can accelerate innovation by reducing many of the time-consuming and expensive steps in designing, building, and evaluating image systems. To realize this potential, it is necessary to build trust in physics-based end-to-end image systems simulations. Toward this goal, we describe and experimentally validate an end-to-end physics-based image systems simulation of a digital camera. The simulation models the spectral radiance of 3-D scenes, the formation of the spectral irradiance by multielement optics, and the conversion of the irradiance to digital values (DVs) by the image sensor. We quantify the accuracy of the simulation by comparing real and simulated images of a precisely constructed, 3-D high-dynamic range (HDR) test scene.},
  keywords = {Cameras,Computational modeling,Computer graphics,end-to-end simulation,image systems,Imaging,Lenses,optics,Optics,physically based ray tracing,sensor,Software,Systems simulation}
}

@inproceedings{maedaIntegratingLensDesign2005,
  title = {Integrating Lens Design with Digital Camera Simulation},
  booktitle = {Digital {{Photography}}},
  author = {Maeda, Patrick Y and Catrysse, Peter B and Wandell, Brian A},
  year = {2005},
  volume = {5678},
  pages = {48--58},
  publisher = {{International Society for Optics and Photonics}}
}

@article{maloneyEvaluationLinearModels1986,
  title = {Evaluation of Linear Models of Surface Spectral Reflectance with Small Numbers of Parameters},
  author = {Maloney, L. T.},
  year = {1986},
  journal = {Journal of The Optical Society of America A},
  volume = {3},
  number = {10},
  pages = {1673--1683}
}

@article{maloneyMaximumLikelihoodDifference2003,
  title = {Maximum Likelihood Difference Scaling},
  author = {Maloney, L. T. and Yang, J. N.},
  year = {2003},
  journal = {Journal of Vision},
  volume = {3},
  number = {8},
  pages = {573--585}
}

@article{manivasagamLiDARsimRealisticLiDAR2020,
  title = {{{LiDARsim}}: {{Realistic LiDAR Simulation}} by {{Leveraging}} the {{Real World}}},
  author = {Manivasagam, Sivabalan and Wang, Shenlong and Wong, Kelvin and Zeng, Wenyuan and Sazanovich, Mikita and Tan, Shuhan and Yang, Bin and Ma, Wei-Chiu and Urtasun, Raquel},
  year = {2020},
  month = jun,
  abstract = {We tackle the problem of producing realistic simulations of LiDAR point clouds, the sensor of preference for most self-driving vehicles. We argue that, by leveraging real data, we can simulate the complex world more realistically compared to employing virtual worlds built from CAD/procedural models. Towards this goal, we first build a large catalog of 3D static maps and 3D dynamic objects by driving around several cities with our self-driving fleet. We can then generate scenarios by selecting a scene from our catalog and ``virtually'' placing the self-driving vehicle (SDV) and a set of dynamic objects from the catalog in plausible locations in the scene. To produce realistic simulations, we develop a novel simulator that captures both the power of physics-based and learning-based simulation. We first utilize ray casting over the 3D scene and then use a deep neural network to produce deviations from the physics-based simulation, producing realistic LiDAR point clouds. We showcase LiDARsim's usefulness for perception algorithms-testing on long-tail events and end-to-end closed-loop evaluation on safety-critical scenarios.}
}

@book{manningIntroductionInformationRetrieval2008,
  title = {Introduction to {{Information Retrieval}}},
  author = {Manning, C. D. and Raghavean, P. and Schutze, H.},
  year = {2008},
  publisher = {Cambridge University Press},
  address = {Cambridge}
}

@article{manningOptimalDesignPhotoreceptor2009,
  title = {Optimal Design of Photoreceptor Mosaics: {{Why}} We Do Not See Color at Night},
  author = {Manning, J. R. and Brainard, D. H.},
  year = {2009},
  journal = {Visual Neuroscience},
  volume = {26},
  number = {1},
  pages = {5--19}
}

@article{manufacturerLiDARSPECIFICATIONCOMPARION,
  title = {{{LiDAR SPECIFICATION COMPARION CHART}}},
  author = {Manufacturer, Hor Angular and Li, D A R and Automotive, Resolution Ibeo and Range, Features: Long and Tracking/Classification, Embedded Object and Quality, Automotive}
}

@article{marimontMatchingColorImages1993,
  title = {Matching Color Images: {{The}} Effects of Axial Chromatic Aberration},
  author = {Marimont, D. and Wandell, B.},
  year = {1993},
  journal = {J. Opt. Soc. Am. A},
  volume = {12}
}

@article{maxwellDiagramColors1857,
  title = {The Diagram of Colors},
  author = {Maxwell, James Clerk},
  year = {1857},
  journal = {Trans. R. Soc. Edinburgh},
  volume = {21},
  pages = {275--298}
}

@article{maxwellIVTheoryCompound1860,
  title = {{{IV}}. {{On}} the Theory of Compound Colours, and the Relations of the Colours of the Spectrum},
  author = {Maxwell, J C},
  year = {1860},
  month = dec,
  journal = {Philos. Trans. R. Soc. Lond.},
  volume = {150},
  number = {0},
  pages = {57--84},
  publisher = {The Royal Society},
  langid = {english}
}

@article{maxwellTheoryCompoundColours1860,
  title = {On the Theory of Compound Colours and the Relations of the Colours of the Spectrum},
  author = {Maxwell, J.C.},
  year = {1860},
  journal = {Philosophical Transactions of the Royal Society of London},
  volume = {150},
  pages = {57--84}
}

@article{maxwellTheoryCompoundColours1993,
  title = {On the Theory of Compound Colours, and the Relations of the Colours of the Spectrum},
  author = {Maxwell, James Clerk and Zaidi, Qasim},
  year = {1993},
  journal = {Color Res. Appl.},
  volume = {18},
  number = {4},
  pages = {270--287},
  publisher = {Wiley Online Library}
}

@article{maxwellXVIIIExperimentsColourPerceived1857,
  title = {{{XVIII}}.---{{Experiments}} on {{Colour}}, as Perceived by the {{Eye}}, with {{Remarks}} on {{Colour-Blindness}}},
  author = {Maxwell, James Clerk},
  year = {1857},
  journal = {Earth Environ. Sci. Trans. R. Soc. Edinb.},
  volume = {21},
  number = {2},
  pages = {275--298},
  publisher = {Royal Society of Edinburgh Scotland Foundation}
}

@article{menonColorImageDemosaicking2011,
  title = {Color Image Demosaicking: {{An}} Overview},
  author = {Menon, Daniele and Calvagno, Giancarlo},
  year = {2011},
  month = oct,
  journal = {Signal Process. Image Commun.},
  volume = {26},
  number = {8-9},
  pages = {518--533},
  publisher = {Elsevier BV},
  langid = {english}
}

@article{mergenthalerModelingControlFixational2007,
  title = {Modeling the Control of Fixational Eye Movements with Neurophysiological Delays},
  author = {Mergenthaler, K. and Engbert, R.},
  year = {2007},
  journal = {Physical Review Letters},
  volume = {98},
  number = {13},
  pages = {138104}
}

@article{meyerExperimentalEvaluationComputer1986,
  title = {An Experimental Evaluation of Computer Graphics Imagery},
  author = {Meyer, Gary W and Rushmeier, Holly E and Cohen, Michael F and Greenberg, Donald P and Torrance, Kenneth E},
  year = {1986},
  month = jan,
  journal = {ACM Trans. Graph.},
  volume = {5},
  number = {1},
  pages = {30--50},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA}
}

@patent{mlinarImageProcessingMethods2016,
  title = {Image Processing Methods for Image Sensors with Phase Detection Pixels},
  author = {Mlinar, Marko},
  year = {2016},
  month = may,
  number = {9338380}
}

@incollection{mollonOriginsModernColor2003,
  title = {The {{Origins}} of {{Modern Color Science}}},
  booktitle = {The {{Science}} of {{Color}}},
  author = {Mollon, J D},
  editor = {Shevell, Steven K},
  year = {2003},
  pages = {1--39},
  publisher = {Elsevier}
}

@incollection{moreiraColorimetryDichromaticVision2017,
  title = {Colorimetry and {{Dichromatic Vision}}},
  booktitle = {Colorimetry and {{Image Processing}}},
  author = {Moreira, Humberto and {\'A}lvaro, Leticia and Melnikova, Anna and Lillo, Julio},
  editor = {{Travieso-Gonzalez}, Carlos M.},
  year = {2017},
  publisher = {InTechOpen},
  doi = {10.5772/intechopen.71563}
}

@article{moutRaybasedMethodSimulating2018,
  title = {Ray-Based Method for Simulating Cascaded Diffraction in High-Numerical-Aperture Systems},
  author = {Mout, Marco and Flesch, Andreas and Wick, Michael and Bociort, Florian and Petschulat, Joerg and Urbach, Paul},
  year = {2018},
  journal = {JOSA A},
  volume = {35},
  number = {8},
  pages = {1356--1367},
  publisher = {Optical Society of America}
}

@patent{narayanswamySimulatingMulticameraImaging2018,
  title = {Simulating Multi-Camera Imaging Systems},
  author = {Narayanswamy, Ramkumar and Grover, Ginni and Nalla, Ram C},
  year = {2018},
  month = nov,
  number = {10119809}
}

@article{nayarComputationalCamerasRedefining2006,
  title = {Computational {{Cameras}}: {{Redefining}} the {{Image}}},
  author = {Nayar, S K},
  year = {2006},
  month = aug,
  journal = {Computer},
  volume = {39},
  number = {8},
  pages = {30--38},
  publisher = {IEEE}
}

@inproceedings{nayarHighDynamicRange2000,
  title = {High Dynamic Range Imaging: Spatially Varying Pixel Exposures},
  booktitle = {Proceedings {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}. {{CVPR}} 2000 ({{Cat}}. {{No}}.{{PR00662}})},
  author = {Nayar, S K and Mitsunaga, T},
  year = {2000},
  volume = {1},
  pages = {472--479 vol.1},
  publisher = {IEEE}
}

@phdthesis{ngLightFieldPhotography2005,
  title = {Light Field Photography with a Hand-Held Plenoptic Camera},
  author = {Ng, Ren and Levoy, Marc and Br{\'e}dif, Mathieu and Duval, Gene and Horowitz, Mark and Hanrahan, Pat},
  year = {2005},
  month = apr,
  school = {Stanford university}
}

@article{nimier-davidMitsuba2Retargetable2019,
  title = {Mitsuba 2: {{A}} Retargetable Forward and Inverse Renderer},
  author = {{Nimier-David}, Merlin and Vicini, Delio and Zeltner, Tizian and Jakob, Wenzel},
  year = {2019},
  journal = {ACM Transactions on Graphics (TOG)},
  volume = {38},
  number = {6},
  pages = {1--17},
  publisher = {ACM New York, NY, USA}
}

@misc{normannJamesClerkMaxwell2023,
  title = {James {{Clerk Maxwell Produces}} the {{First Color Photograph}}},
  author = {Normann, J.},
  year = {2023},
  langid = {english}
}

@article{nubergIssledovanieCvetovogoZrenija1955,
  title = {Issledovanie Cvetovogo Zrenija Dikhromatov},
  author = {Nuberg, N D and Yustova, E N},
  year = {1955},
  journal = {Trudy Gosudarstvennogo Opticheskogo Instituta}
}

@article{oconnorModelbasedDefectDetection2015,
  title = {Model-Based Defect Detection on Structured Surfaces Having Optically Unresolved Features},
  author = {O'Connor, Daniel and Henning, Andrew J and Sherlock, Ben and Leach, Richard K and Coupland, Jeremy and Giusca, Claudiu L},
  year = {2015},
  month = oct,
  journal = {Appl. Opt.},
  volume = {54},
  number = {30},
  pages = {8872--8877},
  publisher = {osapublishing.org},
  abstract = {In this paper, we demonstrate, both numerically and experimentally, a method for the detection of defects on structured surfaces having optically unresolved features. The method makes use of synthetic reference data generated by an observational model that is able to simulate the response of the selected optical inspection system to the ideal structure, thereby providing an ideal measure of deviation from nominal geometry. The method addresses the high dynamic range challenge faced in highly parallel manufacturing by enabling the use of low resolution, wide field of view optical systems for defect detection on surfaces containing small features over large regions.},
  langid = {english}
}

@misc{OpenCamera,
  title = {{{OpenCamera}}}
}

@article{ophoffExploringRGB+DepthFusion2019,
  title = {Exploring {{RGB}}+{{Depth Fusion}} for {{Real-Time Object Detection}}},
  author = {Ophoff, Tanguy and Van Beeck, Kristof and Goedem{\'e}, Toon},
  year = {2019},
  month = feb,
  journal = {Sensors},
  volume = {19},
  number = {4},
  abstract = {In this paper, we investigate whether fusing depth information on top of normal RGB data for camera-based object detection can help to increase the performance of current state-of-the-art single-shot detection networks. Indeed, depth sensing is easily acquired using depth cameras such as a Kinect or stereo setups. We investigate the optimal manner to perform this sensor fusion with a special focus on lightweight single-pass convolutional neural network (CNN) architectures, enabling real-time processing on limited hardware. For this, we implement a network architecture allowing us to parameterize at which network layer both information sources are fused together. We performed exhaustive experiments to determine the optimal fusion point in the network, from which we can conclude that fusing towards the mid to late layers provides the best results. Our best fusion models significantly outperform the baseline RGB network in both accuracy and localization of the detections.},
  langid = {english},
  keywords = {Depth,Neural Networks,Object detection,RGB,RGBD,Sensor fusion,Single-shot}
}

@misc{oppoOPPOUnveilsMultiple,
  title = {{{OPPO Unveils Multiple Innovative Imaging Technologies}}},
  author = {{Oppo}},
  langid = {english}
}

@incollection{packerLightRetinalImage2003,
  title = {Light, the Retinal Image, and Photoreceptors},
  booktitle = {The {{Science}} of {{Color}}},
  author = {Packer, O. and Williams, D. R.},
  editor = {Shevell, S. K.},
  year = {2003},
  edition = {2},
  pages = {41--102},
  publisher = {Optical Society of America; Elsevier Ltd},
  address = {Oxford}
}

@inproceedings{parmarInterleavedImagingImaging2009,
  title = {Interleaved Imaging: An Imaging System Design Inspired by Rod-Cone Vision},
  booktitle = {Digital {{Photography V}}},
  author = {Parmar, Manu and Wandell, Brian A},
  editor = {Rodricks, Brian G and S{\"u}sstrunk, Sabine E},
  year = {2009},
  month = jan,
  publisher = {SPIE}
}

@article{pasternakLinkingNeuronalDirection2020,
  title = {Linking Neuronal Direction Selectivity to Perceptual Decisions about Visual Motion},
  author = {Pasternak, Tatiana and Tadin, Duje},
  year = {2020},
  journal = {Annual Review of Vision Science},
  volume = {6},
  pages = {335--362},
  publisher = {Annual Reviews}
}

@book{pattanaikHighDynamicRange2014,
  title = {High Dynamic Range Imaging: {{Acquisition}}, Display, and Image-Based Lighting},
  author = {Pattanaik, Sumanta and Reinhard, Erik and Heidrich, Wolfgang and Debevec, Paul and Ward, Greg and Myszkowski, Karol},
  year = {2014},
  month = may,
  series = {Morgan {{Kaufmann Series}} in {{Computer Graphics}}]},
  edition = {2},
  publisher = {Morgan Kaufmann}
}

@article{pelliUncertaintyExplainsMany1985,
  title = {Uncertainty Explains Many Aspects of Visual Contrast Detection and Discrimination},
  author = {Pelli, D.G.},
  year = {1985},
  journal = {Journal of the Optical Society of America A},
  volume = {2},
  number = {9},
  pages = {1508--1532}
}

@inproceedings{petersonSurfaceBuriedLandmine2004,
  title = {Surface and Buried Landmine Scene Generation and Validation Using the Digital Imaging and Remote Sensing Image Generation Model},
  booktitle = {Imaging {{Spectrometry X}}},
  author = {Peterson, Erin D and Brown, Scott D and Hattenberger, Timothy J and Schott, John R},
  year = {2004},
  month = oct,
  volume = {5546},
  pages = {312--323},
  publisher = {SPIE},
  abstract = {Detection and neutralization of surface-laid and buried landmines has been a slow and dangerous endeavor for military forces and humanitarian organizations throughout the world. In an effort to make the process faster and safer, scientists have begun to exploit the ever-evolving passive electro-optical realm, both from a broadband perspective and a multi or hyperspectral perspective. Carried with this exploitation is the development of mine detection algorithms that take advantage of spectral features exhibited by mine targets, only available in a multi or hyperspectral data set. Difficulty in algorithm development arises from a lack of robust data, which is needed to appropriately test the validity of an algorithm's results. This paper discusses the development of synthetic data using the Digital Imaging and Remote Sensing Image Generation (DIRSIG) model. A synthetic landmine scene has been modeled after data collected at a US Army arid testing site by the University of Hawaii's Airborne Hyperspectral Imager (AHI). The synthetic data has been created and validated to represent the surrogate minefield thermally, spatially, spectrally, and temporally over the 7.9 to 11.5 micron region using 70 bands of data. Validation of the scene has been accomplished by direct comparison to the AHI truth data using qualitative band to band visual analysis, Rank Order Correlation comparison, Principle Components dimensionality analysis, and an evaluation of the R(x) algorithm's performance. This paper discusses landmine detection phenomenology, describes the steps taken to build the scene, modeling methods utilized to overcome input parameter limitations, and compares the synthetic scene to truth data.},
  langid = {english},
  keywords = {DIRSIG,hyperspectral image simulation,long wave infrared,mine detection,reststrahlen}
}

@misc{pharrPBRTVersion4,
  title = {{{PBRT}}: {{Version}} 4},
  author = {Pharr, Matt and Jakob, Wenzel and Humphreys, Greg}
}

@misc{pharrPBRTVersion4a,
  title = {{{PBRT}}: {{Version}} 4},
  author = {Pharr, Matt and Jakob, Wenzel and Humphreys, Greg},
  howpublished = {https://github.com/mmp/pbrt-v4}
}

@book{pharrPhysicallyBasedRendering2016,
  title = {Physically {{Based Rendering}}: {{From Theory}} to {{Implementation}}},
  author = {Pharr, Matt and Jakob, Wenzel and Humphreys, Greg},
  year = {2016},
  month = sep,
  publisher = {Morgan Kaufmann},
  langid = {english},
  keywords = {NewAccount,ray-tracing}
}

@inproceedings{philionLearningEvaluatePerception2020,
  title = {Learning to Evaluate Perception Models Using Planner-Centric Metrics},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Philion, Jonah and Kar, Amlan and Fidler, Sanja},
  year = {2020},
  month = jun,
  publisher = {IEEE},
  address = {Seattle, WA, USA},
  abstract = {Variants of accuracy and precision are the gold-standard by which the computer vision community measures progress of perception algorithms. One reason for the ubiquity of these metrics is that they are largely task-agnostic; we in general seek to detect zero false negatives or positives. The downside of these metrics is that, at worst, they penalize all incorrect detections equally without conditioning on the task or scene, and at best, heuristics need to be chosen to ensure that different mistakes count differently. In this paper, we propose a principled metric for 3D object detection specifically for the task of self-driving. The core idea behind our metric is to isolate the task of object detection and measure the impact the produced detections would induce on the downstream task of driving. Without hand-designing it to, we find that our metric penalizes many of the mistakes that other metrics penalize by design. In addition, our metric downweighs detections based on additional factors such as distance from a detection to the ego car and the speed of the detection in intuitive ways that other detection metrics do not. For human evaluation, we generate scenes in which standard metrics and our metric disagree and find that humans side with our metric 79\% of the time. Our project page including an evaluation server can be found at https://nv-tlabs.github.io/detection-relevance.}
}

@incollection{pittCharacteristicsDichromaticVision1935,
  title = {Characteristics of {{Dichromatic Vision}}},
  booktitle = {Reports of the {{Committee}} upon the {{Physiology}} of {{Vision}}},
  author = {Pitt, F.H.G.},
  year = {1935},
  number = {14},
  pages = {1--55},
  publisher = {Medical Research Council}
}

@article{pokornyFiftyYearsExploring2020,
  title = {Fifty {{Years Exploring}} the {{Visual System}}},
  author = {Pokorny, Joel and Smith, Vivianne C},
  year = {2020},
  month = sep,
  journal = {Annu Rev Vis Sci},
  volume = {6},
  pages = {1--23},
  langid = {english},
  keywords = {autobiography,color vision,melanopsin,photopigments,retinal physiology,visual pathways}
}

@article{polansWidefieldOpticalModel2015,
  title = {Wide-Field Optical Model of the Human Eye with Asymmetrically Tilted and Decentered Lens That Reproduces Measured Ocular Aberrations},
  author = {Polans, James and Jaeken, Bart and McNabb, Ryan P and Artal, Pablo and Izatt, Joseph A},
  year = {2015},
  journal = {Optica},
  volume = {2},
  number = {2},
  pages = {124--134}
}

@article{potmesilLensApertureCamera1981,
  title = {A Lens and Aperture Camera Model for Synthetic Image Generation},
  author = {Potmesil, Michael and Chakravarty, Indranil},
  year = {1981},
  journal = {ACM SIGGRAPH Computer Graphics},
  volume = {15},
  number = {3},
  pages = {297--305},
  publisher = {ACM New York, NY, USA}
}

@book{prattDigitalImageProcessing1978,
  title = {Digital {{Image Processing}}},
  author = {Pratt, W. K.},
  year = {1978},
  publisher = {John Wiley \& Sons},
  address = {New York}
}

@article{priebeMechanismsOrientationSelectivity2016,
  title = {Mechanisms of Orientation Selectivity in the Primary Visual Cortex},
  author = {Priebe, Nicholas J},
  year = {2016},
  journal = {Annual Review of Vision Science},
  volume = {2},
  pages = {85--107},
  publisher = {Annual Reviews}
}

@incollection{pughVisionPhysicsRetinal1988,
  title = {Vision: {{Physics}} and {{Retinal Physiology}}},
  booktitle = {Stevens' {{Handbook}} of {{Experimental Psychology}}, {{Second Edition}}},
  author = {Pugh, E.N., {\relax Jr}.},
  editor = {Atkinson, R.C. and Herrnstein, R.J. and Lindsey, G. and Luce, R.D.},
  year = {1988},
  volume = {1},
  pages = {75--163},
  publisher = {Wiley},
  address = {New York}
}

@inproceedings{punnappurathDaynightImageSynthesis2022,
  title = {Day-to-Night Image Synthesis for Training Nighttime Neural {{ISPs}}},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Punnappurath, Abhijith and Abuolaim, Abdullah and Abdelhamed, Abdelrahman and Levinshtein, Alex and Brown, Michael S},
  year = {2022},
  month = jun,
  publisher = {IEEE},
  address = {New Orleans, LA, USA}
}

@inproceedings{qiaoLightSourceGuided2021,
  title = {Light Source Guided Single-Image Flare Removal from Unpaired Data},
  booktitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Qiao, Xiaotian and Hancke, Gerhard P and Lau, Rynson W H},
  year = {2021},
  month = oct,
  pages = {4177--4185},
  publisher = {IEEE},
  address = {Montreal, QC, Canada}
}

@misc{quoteresearchWhenYouCome,
  title = {When {{You Come}} to a {{Fork}} in the {{Road}}, {{Take It}}},
  author = {{quoteresearch}},
  journal = {Yogiisms}
}

@inproceedings{ramazzinaGatedFieldsLearning2024,
  title = {Gated {{Fields}}: {{Learning Scene Reconstruction}} from {{Gated Videos}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Ramazzina, Andrea and Walz, Stefanie and Dahal, Pragyan and Bijelic, Mario and Heide, Felix},
  year = {2024},
  pages = {10530--10541}
}

@book{rasmussenGaussianProcessesMachine2006,
  title = {Gaussian {{Processes}} for {{Machine Learning}}},
  author = {Rasmussen, C. E. and Williams, C. K. I.},
  year = {2006},
  publisher = {MIT Press},
  address = {Cambridge, MA}
}

@article{reithConvolutionalNeuralNetwork2019,
  title = {A Convolutional Neural Network Reaches Optimal Sensitivity for Detecting Some, but Not All, Patterns},
  author = {Reith, Fabian and Wandell, Brian},
  year = {2019},
  month = nov,
  abstract = {We investigate the performance of a convolutional neural network (CNN) at detecting a signal-known-exactly in Poisson noise. We compare the network performance with that of a Bayesian ideal observer (IO) that has the theoretical optimum in detection performance and a linear support vector machine (SVM). For several types of stimuli, including harmonics, faces, and certain regular patterns, the CNN performance asymptotes at the level of the IO. The SVM detection sensitivity is approximately 3-times lower. For other stimuli, including random patterns and certain cellular automata, the CNN sensitivity is significantly lower than that of the IO and the SVM. Finally, when the signal can appear in one of multiple locations, CNN sensitivity continues to match the ideal sensitivity.}
}

@misc{Roadrunner2023,
  title = {Roadrunner},
  year = {2023},
  month = mar,
  abstract = {Design 3D scenes for automated driving simulation},
  langid = {english}
}

@inproceedings{robidouxEndendHighDynamic2021,
  title = {End-to-End High Dynamic Range Camera Pipeline Optimization},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Robidoux, Nicolas and Capel, Luis E Garcia and Seo, Dong-eun and Sharma, Avinash and Ariza, Federico and Heide, Felix},
  year = {2021},
  pages = {6297--6307}
}

@book{rodieckFirstStepsSeeing1998,
  title = {The {{First Steps}} in {{Seeing}}},
  author = {Rodieck, R.W.},
  year = {1998},
  publisher = {Sinauer},
  address = {Sunderland, Mass.}
}

@inproceedings{rongLgsvlSimulatorHigh2020,
  title = {Lgsvl Simulator: {{A}} High Fidelity Simulator for Autonomous Driving},
  booktitle = {2020 {{IEEE}} 23rd {{International Conference}} on {{Intelligent Transportation Systems}} ({{ITSC}})},
  author = {Rong, Guodong and Shin, Byung Hyun and Tabatabaee, Hadi and Lu, Qiang and Lemke, Steve and Mo{\v z}eiko, M{\=a}rti{\c n}{\v s} and Boise, Eric and Uhm, Geehoon and Gerow, Mark and Mehta, Shalin and others},
  year = {2020},
  pages = {1--6},
  publisher = {IEEE}
}

@inproceedings{ronnebergerUnetConvolutionalNetworks2015,
  title = {U-Net: {{Convolutional}} Networks for Biomedical Image Segmentation},
  booktitle = {Medical {{Image Computing}} and {{Computer-Assisted Intervention}}--{{MICCAI}} 2015: 18th {{International Conference}}, {{Munich}}, {{Germany}}, {{October}} 5-9, 2015, {{Proceedings}}, {{Part III}} 18},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  year = {2015},
  pages = {234--241},
  publisher = {Springer}
}

@inproceedings{rosSYNTHIADatasetLarge2016,
  title = {The {{SYNTHIA Dataset}}: {{A Large Collection}} of {{Synthetic Images}} for {{Semantic Segmentation}} of {{Urban Scenes}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Ros, German and Sellart, Laura and Materzynska, Joanna and Vazquez, David and Lopez, Antonio M},
  year = {2016},
  month = jun,
  pages = {3234--3243},
  keywords = {Computer vision,Context,Image segmentation,Semantics,Training,Urban areas,Visualization}
}

@article{rudermanStatisticsConeResponses1998,
  title = {Statistics of Cone Responses to Natural Images: {{Implications}} for Visual Coding},
  author = {Ruderman, D. L. and Cronin, T. W. and Chiao, C. C.},
  year = {1998},
  journal = {Journal of the Optical Society of America A},
  volume = {15},
  number = {8},
  pages = {2036--2045}
}

@article{rushtonPigmentsSignalsColour1972,
  title = {Pigments and Signals in Colour Vision},
  author = {Rushton, W.A.H.},
  year = {1972},
  journal = {Journal of Physiology},
  volume = {220},
  number = {3},
  pages = {1P--31P}
}

@inproceedings{sakaridisACDCAdverseConditions2021,
  title = {{{ACDC}}: {{The}} Adverse Conditions Dataset with Correspondences for Semantic Driving Scene Understanding},
  booktitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Sakaridis, Christos and Dai, Dengxin and Van Gool, Luc},
  year = {2021},
  month = oct,
  publisher = {IEEE},
  address = {Montreal, QC, Canada}
}

@article{sakaridisGuidedCurriculumModel2019,
  title = {Guided Curriculum Model Adaptation and Uncertainty-Aware Evaluation for Semantic Nighttime Image Segmentation},
  author = {Sakaridis, Christos and Dai, Dengxin and Gool, L},
  year = {2019},
  month = jan,
  journal = {ICCV},
  pages = {7373--7382}
}

@misc{sakaridisMapGuidedCurriculumDomain2022,
  title = {Map-{{Guided Curriculum Domain Adaptation}} and {{Uncertainty-Aware Evaluation}} for {{Semantic Nighttime Image Segmentation}}},
  author = {Sakaridis, Christos and Dai, Dengxin and Van Gool, Luc},
  year = {2022},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {44},
  number = {6},
  pages = {3139--3153}
}

@article{sakaridisSemanticFoggyScene2018,
  title = {Semantic Foggy Scene Understanding with Synthetic Data},
  author = {Sakaridis, Christos and Dai, Dengxin and Van Gool, Luc},
  year = {2018},
  month = sep,
  journal = {Int. J. Comput. Vis.},
  volume = {126},
  number = {9},
  pages = {973--992},
  publisher = {{Springer Science and Business Media LLC}},
  langid = {english}
}

@book{scholkopfLearningKernelsSupport2002,
  title = {Learning with Kernels: {{Support}} Vector Machines, Regularization, Optimization, and Beyond},
  author = {Sch{\"o}lkopf, Bernhard and Smola, Alexander J and Bach, Francis and others},
  year = {2002},
  publisher = {MIT press}
}

@article{schottAdvancedSyntheticImage1999,
  title = {An Advanced Synthetic Image Generation Model and Its Application to Multi/Hyperspectral Algorithm Development},
  author = {Schott, John R and Brown, Scott D and Raqueno, Rolando V and Gross, Harry N and Robinson, Gary},
  year = {1999},
  journal = {Canadian Journal of Remote Sensing},
  volume = {25},
  number = {2},
  pages = {99--111},
  publisher = {Taylor \& Francis}
}

@inproceedings{schradeSparseHighdegreePolynomials2016,
  title = {Sparse High-Degree Polynomials for Wide-Angle Lenses},
  booktitle = {Computer {{Graphics Forum}}},
  author = {Schrade, Emanuel and Hanika, Johannes and Dachsbacher, Carsten},
  year = {2016},
  volume = {35},
  pages = {89--97},
  publisher = {Wiley Online Library}
}

@inproceedings{schramlPhysicallyBasedSynthetic2019,
  title = {Physically Based Synthetic Image Generation for Machine Learning: A Review of Pertinent Literature},
  booktitle = {Photonics and {{Education}} in {{Measurement Science}} 2019},
  author = {Schraml, Dominik},
  year = {2019},
  volume = {11144},
  pages = {111440J},
  publisher = {{International Society for Optics and Photonics}}
}

@article{schuteraNightDayOnlineImageImage2021,
  title = {Night-to-{{Day}}: {{Online Image-to-Image Translation}} for {{Object Detection Within Autonomous Driving}} by {{Night}}},
  author = {Schutera, Mark and Hussein, Mostafa and Abhau, Jochen and Mikut, Ralf and Reischl, Markus},
  year = {2021},
  month = sep,
  journal = {IEEE Transactions on Intelligent Vehicles},
  volume = {6},
  number = {3},
  pages = {480--489},
  abstract = {Object detectors are central to autonomous driving and are widely used in driver assistance systems. Object detectors are trained on a finite amount of data within a specific domain, hampering detection performance when applying object detectors to samples from other domains during inference, an effect known as domain gap. Domain gap is a concern for data-driven applications, evoking repetitive retraining of networks when the applications unfold into other domains. With object detectors that have been trained on day images only, a domain gap can be observed in object detection by night. Training object detectors on night images is critical because of the enormous effort required to generate an adequate amount of diversely labeled data, and existing data sets often tend to overfit specific domain characteristics. For the first time, this work proposes adapting domains by online image-to-image translation to expand an object detector's domain of operation. The domain gap is decreased without additional labeling effort and without having to retrain the object detector while unfolding into the target domain. The approach follows the concept of domain adaptation, shifting the target domain samples into the domain knownto the object detector (source domain). Firstly, the UNIT network is trained for domain adaptation and subsequently cast into an online domain adaptation module, which narrows down the domain gap. Domain adaptation capabilities are evaluated qualitatively by displaying translated samples and visualizing the domain shift through the 2D tSNE algorithm. We quantitatively benchmark the domain adaptation's influence on a state-of-the-art object detector, and on a retrained object detector, for mean average precision, mean recall, and the resulting F1-score. Our approach achieves an F1 score improvement of 5.27 \% within object detection by night when applying online domain adaptation. The evaluation is executed on the BDD100K benchmark data set.},
  keywords = {Autonomous vehicles,BDD100 K,Detectors,domain adaptation,Gallium nitride,Generators,image-to-image translation,Neural networks,object detection,Object detection,Training,UNIT architecture}
}

@inproceedings{selgradRealtimeDepthField2015,
  title = {Real-Time Depth of Field Using Multi-Layer Filtering},
  booktitle = {Proceedings of the 19th {{Symposium}} on {{Interactive 3D Graphics}} and {{Games}}},
  author = {Selgrad, Kai and Reintges, Christian and Penk, Dominik and Wagner, Pascal and Stamminger, Marc},
  year = {2015},
  pages = {121--127}
}

@article{serreDeepLearningGood2019,
  title = {Deep Learning: {{The}} Good, the Bad, and the Ugly},
  author = {Serre, Thomas},
  year = {2019},
  journal = {Annual Review of Vision Science},
  volume = {5},
  pages = {399--426},
  publisher = {Annual Reviews}
}

@inproceedings{shahAirsimHighfidelityVisual2018,
  title = {Airsim: {{High-fidelity}} Visual and Physical Simulation for Autonomous Vehicles},
  booktitle = {Field and Service Robotics},
  author = {Shah, Shital and Dey, Debadeepta and Lovett, Chris and Kapoor, Ashish},
  year = {2018},
  pages = {621--635},
  publisher = {Springer}
}

@article{shapleyImportanceContrastActivity1986,
  title = {The Importance of Contrast for the Activity of Single Neurons, the {{VEP}} and Perception},
  author = {Shapley, R. M.},
  year = {1986},
  journal = {Vision Research},
  volume = {26},
  number = {1},
  pages = {45--62}
}

@article{shapleySpatialFrequencyAnalysis1985,
  title = {Spatial Frequency Analysis in the Visual System},
  author = {Shapley, Robert and Lennie, Peter},
  year = {1985},
  journal = {Annual Review of Neuroscience},
  volume = {8},
  number = {1},
  pages = {547--581},
  publisher = {Annual Reviews 4139 El Camino Way, PO Box 10139, Palo Alto, CA 94303-0139, USA}
}

@inproceedings{sharmaNighttimeVisibilityEnhancement2021,
  title = {Nighttime {{Visibility Enhancement}} by {{Increasing}} the {{Dynamic Range}} and {{Suppression}} of {{Light Effects}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Sharma, Aashish and Tan, Robby T},
  year = {2021},
  month = jun,
  pages = {11972--11981},
  keywords = {Cameras,Computer vision,Dynamic range,Hafnium,Low-frequency noise,Pattern recognition,Semisupervised learning}
}

@article{shevellColorOpponencyTutorial2017,
  title = {Color Opponency: {{Tutorial}}},
  author = {Shevell, S. K. and Martin, P. R.},
  year = {2017},
  journal = {Journal of the Optical Society of America A},
  volume = {34},
  number = {7},
  pages = {1099--1108}
}

@book{shirleyRealisticRayTracing2008,
  title = {Realistic Ray Tracing},
  author = {Shirley, Peter and Morley, R Keith},
  year = {2008},
  publisher = {AK Peters, Ltd.}
}

@inproceedings{shortImageCaptureSimulation1999,
  title = {Image Capture Simulation Using an Accurate and Realistic Lens Model},
  booktitle = {Sensors, {{Cameras}}, and {{Applications}} for {{Digital Photography}}},
  author = {Short, Ralph C and Williams, Donald and Jones, Andrew EW},
  year = {1999},
  volume = {3650},
  pages = {138--148},
  publisher = {{International Society for Optics and Photonics}}
}

@incollection{simoncelliStatisticalModelingPhotographic2005,
  title = {Statistical Modeling of Photographic Images},
  booktitle = {Handbook of {{Image}} and {{Video Processing}}},
  author = {Simoncelli, E. P.},
  editor = {Bovik, A.},
  year = {2005},
  pages = {431--441},
  publisher = {Academic Press}
}

@article{smithSpectralSensitivityColorblind1972,
  title = {Spectral Sensitivity of Color-Blind Observers and the Cone Photopigments},
  author = {Smith, V C and Pokorny, J},
  year = {1972},
  month = dec,
  journal = {Vision Res.},
  volume = {12},
  number = {12},
  pages = {2059--2071},
  langid = {english}
}

@article{smithSpectralSensitivityFoveal1975,
  title = {Spectral Sensitivity of the Foveal Cone Photopigments between 400 and 500 Nm},
  author = {Smith, V C and Pokorny, J},
  year = {1975},
  month = feb,
  journal = {Vision Res.},
  volume = {15},
  number = {2},
  pages = {161--171},
  publisher = {Elsevier},
  langid = {english}
}

@article{snodderlyMacularPigmentII1984,
  title = {The Macular Pigment. {{II}}. {{Spatial}} Distribution in Primate Retinas},
  author = {Snodderly, D.M. and Auran, J.D. and Delori, F.C.},
  year = {1984},
  journal = {Investigative Ophthalmology and Visual Science},
  volume = {25},
  pages = {674--685}
}

@inproceedings{solhusvik1280x96028mmHDR2019,
  title = {A 1280x960 2.8{$\mu$}m {{HDR CIS}} with {{DCG}} and {{Split-Pixel Combined}}},
  booktitle = {International {{Image Sensors Workshop}}},
  author = {Solhusvik, Johannes and Willassen, Trygve and Mikkelsen, Sindre and Wilhelmsen, Mathias and Manabe, Sohei and Mao, Duli and He, Zhaoyu and Mabuchi, Keiji and Hasegawa, Takuma},
  year = {2019}
}

@article{solomonMachineryColourVision2007,
  title = {The Machinery of Colour Vision},
  author = {Solomon, S.G. and Lennie, P.},
  year = {2007},
  journal = {Nature Reviews Neuroscience},
  volume = {8},
  number = {4},
  pages = {276--286}
}

@inproceedings{sonSimulationbasedTestingFramework2019,
  title = {Simulation-Based Testing Framework for Autonomous Driving Development},
  booktitle = {2019 {{IEEE International Conference}} on {{Mechatronics}} ({{ICM}})},
  author = {Son, Tong Duy and Bhave, Ajinkya and {Van der Auweraer}, Herman},
  year = {2019},
  volume = {1},
  pages = {576--583},
  publisher = {IEEE}
}

@misc{SpecularReflectionTransmission,
  title = {Specular {{Reflection}} and {{Transmission}}},
  langid = {english}
}

@article{spekreijseMathematicalDescriptionFoveal1993,
  title = {A Mathematical Description of the Foveal Visual Point Spread Function with Parameters for Age, Pupil Size and Pigmentation},
  author = {Spekreijse, J. K. IJspeert T. Van Den Berg H.},
  year = {1993},
  journal = {Vision Res.},
  volume = {33},
  pages = {15--20}
}

@misc{Stanford3DScanning,
  title = {The {{Stanford 3D Scanning Repository}}}
}

@article{stilesDirectionalSensitivityRetina1939,
  title = {The Directional Sensitivity of the Retina and the Spectral Sensitivity of the Rods and Cones},
  author = {Stiles, W.S.},
  year = {1939},
  journal = {Proceedings of the Royal Society of London. Series B},
  volume = {127},
  number = {846},
  pages = {64--105}
}

@article{stilesLuminousEfficiencyRays1933,
  title = {The Luminous Efficiency of Rays Entering the Eye Pupil at Different Points},
  author = {Stiles, W.S. and Crawford, B.H.},
  year = {1933},
  journal = {Proceedings of the Royal Society of London. Series B},
  volume = {112},
  number = {778},
  pages = {428--450}
}

@article{stilesNPLColourmatchingInvestigation1959,
  title = {{{NPL}} Colour-Matching Investigation: {{Final}} Report (1958)},
  author = {Stiles, W.S. and Burch, J.M.},
  year = {1959},
  journal = {Optica Acta},
  volume = {6},
  pages = {1--26}
}

@misc{stockmanColourVisionResearch2001,
  title = {Colour and {{Vision Research Laboratory}}},
  author = {Stockman, Andrew},
  year = {2001},
  journal = {CVRL}
}

@article{stockmanConeFundamentalsCIE2019,
  title = {Cone Fundamentals and {{CIE}} Standards},
  author = {Stockman, Andrew},
  year = {2019},
  month = dec,
  journal = {Current Opinion in Behavioral Sciences},
  volume = {30},
  pages = {87--93}
}

@article{stockmanFormulaeGeneratingStandard2023,
  title = {Formulae for Generating Standard and Individual Human Cone Spectral Sensitivities},
  author = {Stockman, A. and Rider, A. T.},
  year = {2023},
  journal = {Color Research \& Application},
  volume = {48},
  number = {6},
  pages = {818--840}
}

@article{stockmanSpectralSensitivitiesHuman1993,
  title = {Spectral Sensitivities of the Human Cones},
  author = {Stockman, A and MacLeod, D I and Johnson, N E},
  year = {1993},
  month = dec,
  journal = {J. Opt. Soc. Am. A Opt. Image Sci. Vis.},
  volume = {10},
  number = {12},
  pages = {2491--2521},
  langid = {english}
}

@article{stockmanSpectralSensitivitiesMiddle2000,
  title = {Spectral Sensitivities of the Middle- and Long-Wavelength Sensitive Cones Derived from Measurements in Observers of Known Genotype},
  author = {Stockman, A. and Sharpe, L.T.},
  year = {2000},
  journal = {Vision Research},
  volume = {40},
  number = {13},
  pages = {1711--1737}
}

@article{stockmanSpectralSensitivityHuman1999,
  title = {The Spectral Sensitivity of the Human Short-Wavelength Cones},
  author = {Stockman, A. and Sharpe, L. T. and Fach, C. C.},
  year = {1999},
  journal = {Vision Research},
  volume = {39},
  number = {17},
  pages = {2901--2927}
}

@book{strangIntroductionLinearAlgebra2022,
  title = {Introduction to Linear Algebra},
  author = {Strang, Gilbert},
  year = {2022},
  publisher = {SIAM}
}

@inproceedings{subbaraoComputerModelingSimulation1993,
  title = {Computer Modeling and Simulation of Camera Defocus},
  booktitle = {Optics, {{Illumination}}, and {{Image Sensing}} for {{Machine Vision VII}}},
  author = {Subbarao, Murali and Lu, MingChin},
  year = {1993},
  volume = {1822},
  pages = {110--120},
  publisher = {{International Society for Optics and Photonics}}
}

@article{subbaraoImageSensingModel1994,
  title = {Image Sensing Model and Computer Simulation for {{CCD}} Camera Systems},
  author = {Subbarao, Murali and Lu, Ming-Chin},
  year = {1994},
  journal = {Machine Vision and Applications},
  volume = {7},
  number = {4},
  pages = {277--289},
  publisher = {Springer}
}

@inproceedings{subbaraoModelImageSensing1991,
  title = {Model for Image Sensing and Digitization in Machine Vision},
  booktitle = {Optics, {{Illumination}}, and {{Image Sensing}} for {{Machine Vision V}}},
  author = {Subbarao, Murali and Nikzad, Arman},
  year = {1991},
  volume = {1385},
  pages = {70--84},
  publisher = {{International Society for Optics and Photonics}}
}

@inproceedings{sumantan.pattanaikjamesa.ferwerdakennethe.torranceanddonaldgreenbergValidationGlobalIllumination1997,
  title = {Validation of {{Global Illumination Simulations}} through {{CCD Camera Measurements}}},
  booktitle = {Color and {{Imaging Conference}} 1997},
  author = {{Sumanta N. Pattanaik, James A. Ferwerda, Kenneth E. Torrance, and Donald Greenberg}},
  year = {1997},
  pages = {250--53}
}

@article{sunRealtimeFusionNetwork2020,
  title = {Real-Time {{Fusion Network}} for {{RGB-D Semantic Segmentation Incorporating Unexpected Obstacle Detection}} for {{Road-driving Images}}},
  author = {Sun, Lei and Yang, Kailun and Hu, Xinxin and Hu, Weijian and Wang, Kaiwei},
  year = {2020},
  month = feb,
  abstract = {Semantic segmentation has made striking progress due to the success of deep convolutional neural networks. Considering the demands of autonomous driving, real-time semantic segmentation has become a research hotspot these years. However, few real-time RGB-D fusion semantic segmentation studies are carried out despite readily accessible depth information nowadays. In this paper, we propose a real-time fusion semantic segmentation network termed RFNet that effectively exploits complementary cross-modal information. Building on an efficient network architecture, RFNet is capable of running swiftly, which satisfies autonomous vehicles applications. Multi-dataset training is leveraged to incorporate unexpected small obstacle detection, enriching the recognizable classes required to face unforeseen hazards in the real world. A comprehensive set of experiments demonstrates the effectiveness of our framework. On Cityscapes, Our method outperforms previous state-of-the-art semantic segmenters, with excellent accuracy and 22Hz inference speed at the full 2048x1024 resolution, outperforming most existing RGB-D networks.}
}

@inproceedings{sunSeeClearerNight2019,
  title = {See Clearer at Night: Towards Robust Nighttime Semantic Segmentation through Day-Night Image Conversion},
  booktitle = {Artificial {{Intelligence}} and {{Machine Learning}} in {{Defense Applications}}},
  author = {Sun, Lei and Wang, Kaiwei and Yang, Kailun and Xiang, Kaite},
  year = {2019},
  month = sep,
  volume = {11169},
  pages = {77--89},
  publisher = {SPIE},
  langid = {english},
  keywords = {Convolutional Neural Networks,Generative Adversarial Networks,Intelligent Vehicles,Semantic Segmentation}
}

@article{sunSHIFTSyntheticDriving2022,
  title = {{{SHIFT}}: {{A Synthetic Driving Dataset}} for {{Continuous Multi-Task Domain Adaptation}}},
  author = {Sun, Tao and Segu, Mattia and Postels, Janis and Wang, Yuxuan and Van Gool, Luc and Schiele, Bernt and Tombari, Federico and Yu, Fisher},
  year = {2022},
  month = jun
}

@article{sunSPADnetDeepRGBSPAD2020,
  title = {{{SPADnet}}: Deep {{RGB-SPAD}} Sensor Fusion Assisted by Monocular Depth Estimation},
  author = {Sun, Zhanghao and Lindell, David B and Solgaard, Olav and Wetzstein, Gordon},
  year = {2020},
  month = may,
  journal = {Opt. Express, OE},
  volume = {28},
  number = {10},
  pages = {14948--14962},
  publisher = {Optical Society of America},
  abstract = {Single-photon light detection and ranging (LiDAR) techniques use emerging single-photon detectors (SPADs) to push 3D imaging capabilities to unprecedented ranges. However, it remains challenging to robustly estimate scene depth from the noisy and otherwise corrupted measurements recorded by a SPAD. Here, we propose a deep sensor fusion strategy that combines corrupted SPAD data and a conventional 2D image to estimate the depth of a scene. Our primary contribution is a neural network architecture---SPADnet---that uses a monocular depth estimation algorithm together with a SPAD denoising and sensor fusion strategy. This architecture, together with several techniques in network training, achieves state-of-the-art results for RGB-SPAD fusion with simulated and captured data. Moreover, SPADnet is more computationally efficient than previous RGB-SPAD fusion networks.},
  langid = {english},
  keywords = {Light detection,Neural networks,Reconstruction algorithms,Sensors,Three dimensional imaging,Three dimensional sensing}
}

@misc{svs-vistekgmbhRelativeIlluminationMicrolenses,
  title = {Relative Illumination and Microlenses},
  author = {{SVS-Vistek GmbH}},
  abstract = {Sensor based shading with latest CMOS sensors like Sony IMX342 or Canon 120MXSM}
}

@misc{SyntheticDataPerception2024,
  title = {Synthetic Data for Perception {{AI}}},
  year = {2024},
  month = feb,
  publisher = {Anyerse},
  langid = {english}
}

@article{talvalaVeilingGlareHigh2007,
  title = {Veiling Glare in High Dynamic Range Imaging},
  author = {Talvala, Eino-Ville and Adams, Andrew and Horowitz, Mark and Levoy, Marc},
  year = {2007},
  month = jul,
  journal = {ACM Trans. Graph.},
  volume = {26},
  number = {3},
  pages = {37--es},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  keywords = {computational photography,global illumination,HDR imaging,structured occlusion mask,veiling glare}
}

@article{tanNightTimeSceneParsing2021,
  title = {Night-{{Time Scene Parsing With}} a {{Large Real Dataset}}},
  author = {Tan, Xin and Xu, Ke and Cao, Ying and Zhang, Yiheng and Ma, Lizhuang and Lau, Rynson W H},
  year = {2021},
  month = nov,
  journal = {IEEE Trans. Image Process.},
  volume = {30},
  pages = {9085--9098},
  langid = {english}
}

@article{tantaloModelingMTFNoise1996,
  title = {Modeling the {{MTF}} and Noise Characteristics of an Image Chain for a Synthetic Image Generation System},
  author = {Tantalo, Frank},
  year = {1996}
}

@misc{tapiaLICAUCMLampsSpectral,
  title = {{{LICA-UCM}} Lamps Spectral Database},
  author = {Tapia, Carlos and {de Miguel}, Alejandro S{\'a}nchez and Zamorano, Jaime},
  abstract = {Spectra of the lamps that are used for public lighting and ornamental purposes have been obtained with a portable spectrograph around Madrid city. The database is presented in this report along with a description of the procedures.}
}

@misc{TestsImagesImage,
  title = {Tests: {{Images}} -- {{Image Systems Engineering}}},
  shorttitle = {Tests},
  urldate = {2024-09-30},
  langid = {english},
  file = {/Users/wandell/Zotero/storage/DPL6B4RC/image-tests.html}
}

@inproceedings{tewariStateArtNeural2020,
  title = {State of the Art on Neural Rendering},
  booktitle = {Computer {{Graphics Forum}}},
  author = {Tewari, Ayush and Fried, Ohad and Thies, Justus and Sitzmann, Vincent and Lombardi, Stephen and Sunkavalli, Kalyan and {Martin-Brualla}, Ricardo and Simon, Tomas and Saragih, Jason and Nie{\ss}ner, Matthias and others},
  year = {2020},
  volume = {39},
  pages = {701--727},
  publisher = {Wiley Online Library}
}

@misc{the-waymo-teamSimulationCityIntroducing,
  title = {Simulation {{City}}: {{Introducing Waymo}}'s Most Advanced Simulation System yet for Autonomous Driving},
  author = {{The-Waymo-Team}}
}

@article{thibosStatisticalVariationAberration2002,
  title = {Statistical Variation of Aberration Structure and Image Quality in a Normal Population of Healthy Eyes},
  author = {Thibos, L. N. and Hong, X. and Bradley, A. and Cheng, X.},
  year = {2002},
  journal = {Journal of the Optical Society of America A},
  volume = {19},
  number = {12},
  pages = {2329--2348}
}

@article{thibosTheoryMeasurementOcular1990,
  title = {Theory and Measurement of Ocular Chromatic Aberration},
  author = {Thibos, L N and Bradley, A and Still, D L and Zhang, X and Howarth, P A},
  year = {1990},
  journal = {Vision Research},
  volume = {30},
  number = {1},
  pages = {33--49}
}

@inproceedings{tianAutomaticallyDesigningImage2015,
  title = {Automatically Designing an Image Processing Pipeline for a Five-Band Camera Prototype Using the Local, Linear, Learned ({{L3}}) Method},
  booktitle = {Digital {{Photography XI}}},
  author = {Tian, Qiyuan and Blasinski, Henryk and Lansel, Steven and Jiang, Haomiao and Fukunishi, Munenori and Farrell, Joyce E. and Wandell, Brian A.},
  editor = {Sampat, Nitin and Tezaur, Radka and W{\"u}ller, Dietmar},
  year = {2015},
  volume = {9404},
  pages = {18--23},
  publisher = {SPIE},
  doi = {10.1117/12.2083435},
  keywords = {camera calibration and simulation,graphics processing units,image processing pipeline,local linear learned (L3 ),parallel computing},
  annotation = {Backup Publisher: International Society for Optics and Photonics}
}

@inproceedings{tianAutomatingDesignImage2014,
  title = {Automating the Design of Image Processing Pipelines for Novel Color Filter Arrays: Local, Linear, Learned ({{L3}}) Method},
  booktitle = {Digital {{Photography X}}},
  author = {Tian, Qiyuan and Lansel, Steven and Farrell, Joyce E. and Wandell, Brian A.},
  editor = {Sampat, Nitin and Tezaur, Radka and Battiato, Sebastiano and Fowler, Boyd A.},
  year = {2014},
  volume = {9023},
  pages = {189--196},
  publisher = {SPIE},
  doi = {10.1117/12.2042565},
  keywords = {Color filter array,Image processing,Image processing pipeline,Local linear learned (L3),Low light imaging},
  annotation = {Backup Publisher: International Society for Optics and Photonics}
}

@article{tkacikNaturalImagesBirthplace2011,
  title = {Natural Images from the Birthplace of the Human Eye},
  author = {Tkacik, G. and Garrigan, P. and Ratliff, C. and Milcinski, G. and Klein, J. M. and Sterling, P. and Brainard, D. H. and Balasubramanian, V.},
  year = {2011},
  journal = {PLoS ONE},
  volume = {6},
  number = {6:e20409}
}

@inproceedings{toadereProcessSimulationDigital2012,
  title = {Process Simulation in Digital Camera System},
  booktitle = {Optics, {{Photonics}}, and {{Digital Technologies}} for {{Multimedia Applications II}}},
  author = {Toadere, Florin},
  year = {2012},
  volume = {8436},
  pages = {843618},
  publisher = {{International Society for Optics and Photonics}}
}

@article{toadereSimulatingFunctionalityDigital2013,
  title = {Simulating the Functionality of a Digital Camera Pipeline},
  author = {Toadere, Florin},
  year = {2013},
  month = jun,
  journal = {Organ. Ethic.},
  volume = {52},
  number = {10},
  pages = {102005},
  publisher = {SPIE},
  abstract = {The goal of this paper is to simulate the functionality of a digital camera system. The simulations cover the conversion from light to numerical signal, color processing, and rendering. A spectral image processing algorithm is used to simulate the radiometric properties of a digital camera. In the algorithm, we take into consideration the spectral image and the transmittances of the light source, lenses, filters, and the quantum efficiency of a complementary metal-oxide semiconductor (CMOS) image sensor. The optical part is characterized by a multiple convolution between the different point spread functions optical components such as the Cooke triplet, the aperture, the light fall off, and the optical part of the CMOS sensor. The electrical part consists of the Bayer sampling, interpolation, dynamic range, and analog to digital conversion. The reconstruction of the noisy blurred image is performed by blending different light exposed images in order to reduce the noise. Then, the image is filtered, deconvoluted, and sharpened to eliminate the noise and blur. Next, we have the color processing and rendering blocks interpolation, white balancing, color correction, conversion from XYZ color space to LAB color space, and, then, into the RGB color space, the color saturation and contrast.},
  langid = {english},
  keywords = {CMOS sensors,Device simulation,Digital cameras,Digital imaging,Image filtering,Image processing,Image quality,Image sensors,Photons,Point spread functions}
}

@article{tsirikoglouProceduralModelingPhysically2017,
  title = {Procedural Modeling and Physically Based Rendering for Synthetic Data Generation in Automotive Applications},
  author = {Tsirikoglou, Apostolia and Kronander, Joel and Wrenninge, Magnus and Unger, Jonas},
  year = {2017},
  journal = {arXiv preprint arXiv:1710.06270},
  eprint = {1710.06270},
  archiveprefix = {arXiv}
}

@incollection{turnerOriginsColorimetryWhat1996,
  title = {The Origins of Colorimetry: {{What}} Did {{Helmholtz}} and {{Maxwell}} Learn from {{Grassmann}}?},
  booktitle = {Hermann {{G{\"u}nther Gra{\ss}mann}} (1809--1877): {{Visionary Mathematician}}, {{Scientist}} and {{Neohumanist Scholar}}},
  author = {Turner, R Steven},
  year = {1996},
  pages = {71--86},
  publisher = {Springer}
}

@inproceedings{vanbastelaerGlareRemovalAstronomical2023,
  title = {Glare {{Removal}} for {{Astronomical Images}} with {{High Local Dynamic Range}}},
  booktitle = {2023 {{IEEE International Conference}} on {{Computational Photography}} ({{ICCP}})},
  author = {Van Bastelaer, Max-Olivier and Kremer, Heiner and Volchkov, Valentin and Passy, Jean-Claude and Sch{\"o}lkopf, Bernhard},
  year = {2023},
  month = jul,
  pages = {1--11},
  publisher = {IEEE}
}

@article{vangelderOcularPhotoreceptionCircadian2016,
  title = {Ocular {{Photoreception}} for {{Circadian Rhythm Entrainment}} in {{Mammals}}},
  author = {Van Gelder, Russell N and Buhr, Ethan D},
  year = {2016},
  journal = {Annual Review of Vision Science},
  volume = {2}
}

@misc{VerificationValidationStudies,
  title = {Verification and {{Validation Studies}}},
  howpublished = {https://dirsig.cis.rit.edu/docs/new/validation.html}
}

@misc{vistalab.stanford.eduISET3d2022,
  title = {{{ISET3d}}},
  author = {{Vistalab.stanford.edu}},
  year = {2022}
}

@misc{vistalab.stanford.eduISETCam2022,
  title = {{{ISETCam}}},
  author = {{Vistalab.stanford.edu}},
  year = {2022}
}

@misc{vistalab.stanford.eduISETHdrsensor2024,
  title = {{{ISETHdrsensor}}},
  author = {{Vistalab.stanford.edu}},
  year = {2024}
}

@misc{vivoVivoX80Only,
  title = {Vivo {{X80}} Is the Only Vivo Smartphone with a {{Sony IMX866 Sensor}}: {{The World}}'s {{First RGBW Bottom Sensors}}},
  author = {{Vivo}},
  langid = {english}
}

@incollection{vonkriesChromaticAdaptation1902,
  title = {Chromatic Adaptation},
  booktitle = {Sources of {{Color Vision}}},
  author = {{von Kries}, J.},
  year = {1902},
  pages = {109--119},
  publisher = {MIT Press},
  address = {1970, Cambridge, MA}
}

@article{vosDerivationFovealReceptor1971,
  title = {On the Derivation of the Foveal Receptor Primaries},
  author = {Vos, J J and Walraven, P L},
  year = {1971},
  month = aug,
  journal = {Vision Res.},
  volume = {11},
  number = {8},
  pages = {799--818},
  langid = {english}
}

@article{vosImprovedColorFundamentals1990,
  title = {Improved Color Fundamentals Offer a New View on Photometric Additivity},
  author = {Vos, J J and Est{\'e}vez, O and Walraven, P L},
  year = {1990},
  journal = {Vision Res.},
  volume = {30},
  number = {6},
  pages = {937--943},
  langid = {english}
}

@article{vrhelMeasurementAnalysisObject1994,
  title = {Measurement and Analysis of Object Reflectance Spectra},
  author = {Vrhel, M.J. and Gershon, R. and Iwan, L.S.},
  year = {1994},
  journal = {Color Research And Application},
  volume = {19},
  number = {1},
  pages = {4--9}
}

@inproceedings{waldApplyingRayTracing2006,
  title = {Applying {{Ray Tracing}} for {{Virtual Reality}} and {{Industrial Design}}},
  booktitle = {2006 {{IEEE Symposium}} on {{Interactive Ray Tracing}}},
  author = {Wald, Ingo and Dietrich, Andreas and Benthin, Carsten and Efremov, Alexander and Dahmen, Tim and Gunther, Johannes and Havran, Vlastimil and Seidel, Hans-Peter and Slusallek, Philipp},
  year = {2006},
  month = sep,
  pages = {177--185},
  publisher = {ieeexplore.ieee.org}
}

@article{waldChangeRefractivePower1947,
  title = {The {{Change}} in {{Refractive Power}} of the {{Human Eye}} in {{Dim}} and {{Bright Light}}},
  author = {Wald, G. and Griffin, D. R.},
  year = {1947},
  journal = {J. Opt. Soc. Am.},
  volume = {37},
  pages = {321--336}
}

@article{waldRealtimeRayTracing2003,
  title = {Realtime {{Ray Tracing}} and Its Use for {{Interactive Global Illumination}}},
  author = {Wald, I and Purcell, T J and Schmittler, J and Benthin, C and others},
  year = {2003},
  journal = {Eurographics, State of the Art Reports},
  publisher = {Citeseer}
}

@inproceedings{waliaGated2GatedSelfsupervisedDepth2022,
  title = {{{Gated2Gated}}: {{Self-supervised}} Depth Estimation from Gated Images},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Walia, Amanpreet and Walz, Stefanie and Bijelic, Mario and Mannan, Fahim and {Julca-Aguilar}, Frank and Langer, Michael and Ritter, Werner and Heide, Felix},
  year = {2022},
  month = jun,
  pages = {2811--2821},
  publisher = {IEEE}
}

@article{wandellCommonPrinciplesImage2002,
  title = {Common Principles of Image Acquisition Systems and Biological Vision},
  author = {Wandell, B A and El Gamal, A and Girod, B},
  year = {2002},
  month = jan,
  journal = {Proc. IEEE},
  volume = {90},
  number = {1},
  pages = {5--17},
  publisher = {IEEE}
}

@book{wandellFoundationsVision1995,
  title = {Foundations of {{Vision}}},
  author = {Wandell, B. A.},
  year = {1995},
  publisher = {Sinauer},
  address = {Sunderland, MA}
}

@inproceedings{wandellMultipleCaptureSingle1999,
  title = {Multiple {{Capture Single Image Architecture}} with a {{CMOS Sensor}}},
  booktitle = {Proc. {{Chiba Conf}}. {{Multispectral Imaging}}},
  author = {Wandell, B A and Catrysse, P and DiCarlo, Jeffrey M and Yang, D and El Gamal, A},
  year = {1999},
  pages = {1--7},
  address = {Chiba, Japan}
}

@article{wandellVisualEncodingPrinciples2022,
  title = {Visual Encoding: {{Principles}} and Software},
  author = {Wandell, Brian A and Brainard, David H and Cottaris, Nicolas P},
  year = {2022},
  month = jun,
  journal = {Prog. Brain Res.},
  volume = {273},
  number = {1},
  pages = {199--229},
  publisher = {Elsevier B.V.},
  langid = {english},
  keywords = {Computer graphics,Cone fundamentals,Cone mosaic,Display,ipRGC,Irradiance,Lens,Optics,Photoreceptor,Pigment density,Point spread function,Radiance,Retina}
}

@article{wangPseudoLiDARVisualDepth2018,
  title = {Pseudo-{{LiDAR}} from {{Visual Depth Estimation}}: {{Bridging}} the {{Gap}} in {{3D Object Detection}} for {{Autonomous Driving}}},
  author = {Wang, Yan and Chao, Wei-Lun and Garg, Divyansh and Hariharan, Bharath and Campbell, Mark and Weinberger, Kilian Q},
  year = {2018},
  month = dec,
  abstract = {3D object detection is an essential task in autonomous driving. Recent techniques excel with highly accurate detection rates, provided the 3D input data is obtained from precise but expensive LiDAR technology. Approaches based on cheaper monocular or stereo imagery data have, until now, resulted in drastically lower accuracies --- a gap that is commonly attributed to poor image-based depth estimation. However, in this paper we argue that data representation (rather than its quality) accounts for the majority of the difference. Taking the inner workings of convolutional neural networks into consideration, we propose to convert image-based depth maps to pseudo-LiDAR representations --- essentially mimicking LiDAR signal. With this representation we can apply different existing LiDAR-based detection algorithms. On the popular KITTI benchmark, our approach achieves impressive improvements over the existing state-of-the-art in image-based performance --- raising the detection accuracy of objects within 30m range from the previous state-of-the-art of 22\% to an unprecedented 74\%. At the time of submission our algorithm holds the highest entry on the KITTI 3D object detection leaderboard for stereo image based approaches.}
}

@article{wangSelectInformativeSamples2023,
  title = {Select {{Informative Samples}} for {{Night-Time Vehicle Detection Benchmark}} in {{Urban Scenes}}},
  author = {Wang, Xiao and Tu, Xingyue and {Al-Hassani}, Baraa and Lin, Chia-Wen and Xu, Xin},
  year = {2023},
  month = aug,
  journal = {Remote Sensing},
  volume = {15},
  number = {17},
  pages = {4310},
  publisher = {Multidisciplinary Digital Publishing Institute},
  langid = {english}
}

@book{wardHighDynamicRange2005,
  title = {High Dynamic Range Imaging},
  author = {Ward, G and Pattanaik, S and Debevec, P E},
  year = {2005},
  publisher = {Morgan Kaufmann}
}

@article{watsonQUESTBayesianAdaptive1983,
  title = {{{QUEST}}: {{A Bayesian}} Adaptive Psychometric Method},
  author = {Watson, A.B. and Pelli, D.G.},
  year = {1983},
  journal = {Perception and Psychophysics},
  volume = {33},
  number = {2},
  pages = {113--120}
}

@article{watsonQUESTGeneralMultidimensional2017,
  title = {{{QUEST}}+: {{A}} General Multidimensional {{Bayesian}} Adaptive Psychometric Method},
  author = {Watson, A. B.},
  year = {2017},
  journal = {Journal of Vision},
  volume = {17},
  number = {3},
  pages = {10}
}

@article{websterFactorsUnderlyingIndividual1988,
  title = {Factors Underlying Individual Differences in the Color Matches of Normal Observers},
  author = {Webster, M. A. and MacLeod, D. I. A.},
  year = {1988},
  journal = {Journal of the Optical Society of America A},
  volume = {5},
  number = {10},
  pages = {1722--1735}
}

@article{weiNeuralMechanismsMotion2018,
  title = {Neural Mechanisms of Motion Processing in the Mammalian Retina},
  author = {Wei, Wei},
  year = {2018},
  journal = {Annual Review of Vision Science},
  volume = {4},
  pages = {165--192},
  publisher = {Annual Reviews}
}

@article{wengAlloneDriveLargescale2020,
  title = {All-in-One Drive: {{A}} Large-Scale Comprehensive Perception Dataset with High-Density Long-Range Point Clouds},
  author = {Weng, Xinshuo and Man, Yunze and Cheng, Dazhi and Park, Jinhyung and O'Toole, Matthew and Kitani, Kris and Wang, Jianren and Held, David},
  year = {2020},
  journal = {arXiv}
}

@incollection{westheimerEyeOpticalInstrument1986,
  title = {The Eye as an Optical Instrument},
  booktitle = {Handbook of {{Perception}}},
  author = {Westheimer, G.},
  editor = {et {al}, J. Thomas},
  year = {1986},
  pages = {Chapter 4},
  publisher = {Wiley},
  address = {New York}
}

@article{whiteheadMacularPigmentReview2006,
  title = {Macular Pigment. {{A}} Review of Current Knowledge},
  author = {Whitehead, A.J. and Mares, J.A. and Danis, R.P.},
  year = {2006},
  journal = {Archives of Ophthalmology},
  volume = {124},
  number = {7},
  pages = {1038--1045}
}

@phdthesis{whiteValidationRochesterInstitute1996,
  title = {Validation of {{Rochester Institute}} of {{Technology}}'s ({{RIT}}'s) {{Digital Image}} and {{Remote Sensing Generation}} ({{DIRSIG}}) {{Model}}, Reflective Region},
  author = {White, Russell},
  year = {1996},
  abstract = {The performance of RIT's Digital Imaging and Remote Sensing Image Generation (DIRSIG) model is validated. The model is robust enough to treat solar, atmospheric, target/background, and sensor interactions. It operates over the 0.28 - 28 micrometers (Ultraviolet- Long Wavelength Infra-Red) spectral region. However, this study focuses only on the 0.4 - 1.0 micrometer (reflective) region. To validate the model, reference (actual) imagery from an airborne frame sensor is compared to synthetic imagery of the same scene. This study also evaluates DIRSIG's treatment of reflectivity and recommends improvements.},
  school = {Rochester Institute of Technology}
}

@article{whiteValidationRochesterInstitute1996a,
  title = {Validation of {{Rochester Institute}} of {{Technology}}'s ({{RIT}}'s) {{Digital Image}} and {{Remote Sensing Generation}} ({{DIRSIG}}) {{Model}}, Reflective Region},
  author = {White, R},
  year = {1996},
  publisher = {scholarworks.rit.edu}
}

@phdthesis{whiteValidationRochesterInstitute1996b,
  title = {Validation of {{Rochester Institute}} of {{Technology}}'s ({{RIT}}'s) {{Digital Image}} and {{Remote Sensing Generation}} ({{DIRSIG}}) {{Model}}, Reflective Region},
  author = {White, Russell},
  year = {1996},
  abstract = {The performance of RIT's Digital Imaging and Remote Sensing Image Generation (DIRSIG) model is validated. The model is robust enough to treat solar, atmospheric, target/background, and sensor interactions. It operates over the 0.28 - 28 micrometers (Ultraviolet- Long Wavelength Infra-Red) spectral region. However, this study focuses only on the 0.4 - 1.0 micrometer (reflective) region. To validate the model, reference (actual) imagery from an airborne frame sensor is compared to synthetic imagery of the same scene. This study also evaluates DIRSIG's treatment of reflectivity and recommends improvements.},
  school = {Rochester Institute of Technology}
}

@misc{wikipediacontributorsBayerFilter2024,
  title = {Bayer Filter},
  author = {{Wikipedia contributors}},
  year = {2024},
  month = jun,
  journal = {Wikipedia, The Free Encyclopedia},
  publisher = {Wikimedia Foundation, Inc.}
}

@misc{wikipediacontributorsLytro2021,
  title = {Lytro},
  author = {{Wikipedia contributors}},
  year = {2021},
  month = jul,
  journal = {Wikipedia, The Free Encyclopedia}
}

@misc{wikipediacontributorsMultiexposureHDRCapture2024,
  title = {Multi-Exposure {{HDR}} Capture},
  author = {{Wikipedia contributors}},
  year = {2024},
  month = mar,
  journal = {Wikipedia, The Free Encyclopedia}
}

@misc{wikipediaZernikePolynomialsWikipedia,
  title = {Zernike Polynomials --- {{Wikipedia}}, {{The Free Encyclopedia}}},
  author = {{Wikipedia}}
}

@misc{willassen1280x1080MmSplitdiode2015,
  title = {A 1280x1080 {$M$}m {{Split-diode Pixel HDR Sensor}} in 110nm {{BSI CMOS Process}}},
  author = {Willassen, Trygve and Solhusvik, Johannes and Johansson, Robert and Yaghmai, Sohrab and Rhodes, Howard and Manabe, Sohei and Mao, Duli and Lin, Zhiqiang and Yang, Dajiang and Cellek, Orkun and Webster, Eric and Ma, Siguang and Zhang, Bowei},
  year = {2015}
}

@incollection{williamsCostTrichromacySpatial1991,
  title = {The Cost of Trichromacy for Spatial Vision},
  booktitle = {From {{Pigments}} to {{Perception}}},
  author = {Williams, D. R. and Sekiguchi, N. and Haake, W. and Brainard, D. H. and Packer, O.},
  year = {1991},
  pages = {11--22},
  publisher = {Plenum Press},
  address = {New York}
}

@article{williamsFovealTritanopia1981,
  title = {Foveal Tritanopia},
  author = {Williams, D.R. and MacLeod, D.I.A. and Hayhoe, M.M.},
  year = {1981},
  journal = {Vision Research},
  volume = {19},
  number = {9},
  pages = {1341--1356}
}

@article{wrightCharacteristicsTritanopia1952,
  title = {The Characteristics of Tritanopia},
  author = {Wright, W D},
  year = {1952},
  month = aug,
  journal = {J. Opt. Soc. Am.},
  volume = {42},
  number = {8},
  pages = {509--521},
  langid = {english},
  keywords = {COLOR VISION}
}

@article{wrightColourvisionCharacteristicsTwo1935,
  title = {The Colour-Vision Characteristics of Two Trichromats},
  author = {Wright, W D and Pitt, F H G},
  year = {1935},
  month = mar,
  journal = {Proc. Phys. Soc. London},
  volume = {47},
  number = {2},
  pages = {205},
  publisher = {IOP Publishing},
  langid = {english}
}

@book{wrightColourVisionResearch1938,
  title = {Colour {{Vision}}. {{Research}} on {{Normal}} and {{Defective Colour Vision}}},
  author = {Wright, W D},
  year = {1938},
  publisher = {C.V. Mosby},
  langid = {english}
}

@article{wuHowTrainNeural2020,
  title = {How to {{Train Neural Networks}} for {{Flare Removal}}},
  author = {Wu, Yicheng and He, Qiurui and Xue, Tianfan and Garg, Rahul and Chen, Jiawen and Veeraraghavan, Ashok and Barron, Jonathan T},
  year = {2020},
  month = nov,
  journal = {arXiv:2011.12485},
  eprint = {2011.12485},
  archiveprefix = {arXiv}
}

@article{wuIMMVPEfficientDaytime2019,
  title = {{{IMMVP}}: {{An Efficient Daytime}} and {{Nighttime On-Road Object Detector}}},
  author = {Wu, Cheng-En and Chan, Yi-Ming and Chen, Chien-Hung and Chen, Wen-Cheng and Chen, Chu-Song},
  year = {2019},
  month = oct,
  journal = {arXiv:1910.06573},
  eprint = {1910.06573},
  archiveprefix = {arXiv}
}

@misc{wuOneStageDomainAdaptation2023,
  title = {A {{One-Stage Domain Adaptation Network With Image Alignment}} for {{Unsupervised Nighttime Semantic Segmentation}}},
  author = {Wu, Xinyi and Wu, Zhenyao and Ju, Lili and Wang, Song},
  year = {2023},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {45},
  number = {1},
  pages = {58--72}
}

@book{wyszeckiColorScienceConcepts1982,
  title = {Color {{Science}}: {{Concepts}} and {{Methods}}, {{Quantitative Data}} and {{Formulae}}},
  author = {Wyszecki, G. and Stiles, W.S.},
  year = {1982},
  edition = {2nd},
  publisher = {John Wiley \& Sons},
  address = {New York}
}

@article{wyszeckiEvaluationMetamericColors1958,
  title = {Evaluation of Metameric Colors},
  author = {Wyszecki, G.},
  year = {1958},
  journal = {Journal of the Optical Society of America},
  volume = {48},
  number = {7},
  pages = {451--454}
}

@inproceedings{xingEndendLearningJoint2021,
  title = {End-to-End Learning for Joint Image Demosaicing, Denoising and Super-Resolution},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} Conference on Computer Vision and Pattern Recognition},
  author = {Xing, Wenzhu and Egiazarian, Karen},
  year = {2021},
  pages = {3507--3516}
}

@misc{xuCDAdaCurriculumDomain2021,
  title = {{{CDAda}}: {{A Curriculum Domain Adaptation}} for {{Nighttime Semantic Segmentation}}},
  author = {Xu, Qi and Ma, Yinan and Wu, Jing and Long, Chengnian and Huang, Xiaolin},
  year = {2021},
  journal = {2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)}
}

@article{yangMIPI2022Challenge2022,
  title = {{{MIPI}} 2022 {{Challenge}} on {{RGBW Sensor Re-mosaic}}: {{Dataset}} and {{Report}}},
  author = {Yang, Qingyu and Yang, Guang and Jiang, Jun and Li, Chongyi and Feng, Ruicheng and Zhou, Shangchen and Sun, Wenxiu and Zhu, Qingpeng and Loy, Chen Change and Gu, Jinwei},
  year = {2022},
  month = sep,
  journal = {arXiv [cs.CV]}
}

@incollection{yellottBeginningsVisualPerception1984,
  title = {The Beginnings of Visual Perception: The Retinal Image and Its Initial Encoding},
  booktitle = {Handbook of {{Physiology}}: {{The Nervous System}}},
  author = {Yellott, J. I. and Wandell, B. A. and Cornsweet, T. N.},
  editor = {{Darien-Smith}, I.},
  year = {1984},
  volume = {III},
  pages = {257--316},
  publisher = {Easton},
  address = {New York}
}

@article{youngTheoryLightColours1802,
  title = {On the Theory of Light and Colours},
  author = {Young, T.},
  year = {1802},
  journal = {Philosophical Transactions of the Royal Society of London},
  volume = {92},
  pages = {20--71}
}

@article{youPseudoLiDARAccurateDepth2019,
  title = {Pseudo-{{LiDAR}}++: {{Accurate Depth}} for {{3D Object Detection}} in {{Autonomous Driving}}},
  author = {You, Yurong and Wang, Yan and Chao, Wei-Lun and Garg, Divyansh and Pleiss, Geoff and Hariharan, Bharath and Campbell, Mark and Weinberger, Kilian Q},
  year = {2019},
  month = jun,
  abstract = {Detecting objects such as cars and pedestrians in 3D plays an indispensable role in autonomous driving. Existing approaches largely rely on expensive LiDAR sensors for accurate depth information. While recently pseudo-LiDAR has been introduced as a promising alternative, at a much lower cost based solely on stereo images, there is still a notable performance gap. In this paper we provide substantial advances to the pseudo-LiDAR framework through improvements in stereo depth estimation. Concretely, we adapt the stereo network architecture and loss function to be more aligned with accurate depth estimation of faraway objects --- currently the primary weakness of pseudo-LiDAR. Further, we explore the idea to leverage cheaper but extremely sparse LiDAR sensors, which alone provide insufficient information for 3D detection, to de-bias our depth estimation. We propose a depth-propagation algorithm, guided by the initial depth estimates, to diffuse these few exact measurements across the entire depth map. We show on the KITTI object detection benchmark that our combined approach yields substantial improvements in depth estimation and stereo-based 3D object detection --- outperforming the previous state-of-the-art detection accuracy for faraway objects by 40\%. Our code is available at https://github.com/mileyan/Pseudo\_Lidar\_V2.}
}

@article{yuanzhouhancaoExploitingDepthSingle2017,
  title = {Exploiting {{Depth From Single Monocular Images}} for {{Object Detection}} and {{Semantic Segmentation}}},
  author = {{Yuanzhouhan Cao} and {Chunhua Shen} and {Heng Tao Shen}},
  year = {2017},
  month = feb,
  journal = {IEEE Trans. Image Process.},
  volume = {26},
  number = {2},
  pages = {836--846},
  abstract = {Augmenting RGB data with measured depth has been shown to improve the performance of a range of tasks in computer vision, including object detection and semantic segmentation. Although depth sensors such as the Microsoft Kinect have facilitated easy acquisition of such depth information, the vast majority of images used in vision tasks do not contain depth information. In this paper, we show that augmenting RGB images with estimated depth can also improve the accuracy of both object detection and semantic segmentation. Specifically, we first exploit the recent success of depth estimation from monocular images and learn a deep depth estimation model. Then, we learn deep depth features from the estimated depth and combine with RGB features for object detection and semantic segmentation. In addition, we propose an RGB-D semantic segmentation method, which applies a multi-task training scheme: semantic label prediction and depth value regression. We test our methods on several data sets and demonstrate that incorporating information from estimated depth improves the performance of object detection and semantic segmentation remarkably.},
  langid = {english}
}

@article{yuBDD100KDiverseDriving2018,
  title = {{{BDD100K}}: {{A Diverse Driving Video Database}} with {{Scalable Annotation Tooling}}},
  author = {Yu, Fisher and Xian, Wenqi and Chen, Yingying and Liu, Fangchen and Liao, Mike and Madhavan, Vashisht and Darrell, Trevor},
  year = {2018},
  month = may,
  journal = {arXiv [cs.CV]},
  abstract = {Datasets drive vision progress and autonomous driving is a critical vision application, yet existing driving datasets are impoverished in terms of visual content. Driving imagery is becoming plentiful, but annotation is slow and expensive, as annotation tools have not kept pace with the flood of data. Our first contribution is the design and implementation of a scalable annotation system that can provide a comprehensive set of image labels for large-scale driving datasets. Our second contribution is a new driving dataset, facilitated by our tooling, which is an order of magnitude larger than previous efforts, and is comprised of over 100K videos with diverse kinds of annotations including image level tagging, object bounding boxes, drivable areas, lane markings, and full-frame instance segmentation. The dataset possesses geographic, environmental, and weather diversity, which is useful for training models so that they are less likely to be surprised by new conditions. The dataset can be requested at http://bdd-data.berkeley.edu.},
  keywords = {NewAccount}
}

@inproceedings{zamirRestormerEfficientTransformer2022,
  title = {Restormer: {{Efficient}} Transformer for High-Resolution Image Restoration},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} Conference on Computer Vision and Pattern Recognition},
  author = {Zamir, Syed Waqas and Arora, Aditya and Khan, Salman and Hayat, Munawar and Khan, Fahad Shahbaz and Yang, Ming-Hsuan},
  year = {2022},
  pages = {5728--5739}
}

@article{zapataArtificialLightingNight2019,
  title = {Artificial {{Lighting}} at {{Night}} in {{Estuaries}}---{{Implications}} from {{Individuals}} to {{Ecosystems}}},
  author = {Zapata, Martha J and Sullivan, S Ma{\v z}eika P and Gray, Suzanne M},
  year = {2019},
  month = mar,
  journal = {Estuaries Coasts},
  volume = {42},
  number = {2},
  pages = {309--330},
  abstract = {Artificial lighting at night (ALAN) produced by urban, industrial, and roadway lighting, as well as other sources, has dramatically increased in recent decades, especially in coastal environments that support dense human populations. Artificial ``lightscapes'' are characterized by distinct spatial, temporal, and spectral patterns that can alter natural patterns of light and dark with consequences across levels of biological organization. At the individual level, ALAN can elicit a suite of physiological and behavioral responses associated with light-mediated processes such as diel activity patterns and predator-prey interactions. ALAN has also been shown to modify community composition and trophic structure, with implications for ecosystem-level processes including primary productivity, nutrient cycling, and the energetic linkages between aquatic and terrestrial systems. Here, we review the state of the science relative to the impacts of ALAN on estuaries, which is an important step in assessing the long-term sustainability of coastal regions. We first consider how multiple properties of ALAN (e.g., intensity and spectral content) influence the interaction between physiology and behavior of individual estuarine biota (drawing from studies on invertebrates, fishes, and birds). Second, we link individual- to community- and ecosystem-level responses, with a focus on the impacts of ALAN on food webs and implications for estuarine ecosystem functions. Coastal aquatic communities and ecosystems have been identified as a key priority for ALAN research, and a cohesive research framework will be critical for understanding and mitigating ecological consequences.}
}

@inproceedings{zhangDeepSpatialAdaptive2022,
  title = {Deep Spatial Adaptive Network for Real Image Demosaicing},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Zhang, Tao and Fu, Ying and Li, Cheng},
  year = {2022},
  volume = {36},
  pages = {3326--3334}
}

@article{zhangImageReconstructionFramework2021,
  title = {An Image Reconstruction Framework for Characterizing Early Vision},
  author = {Zhang, L. and Cottaris, N. P. and Brainard, D. H.},
  year = {2021},
  journal = {bioRxiv},
  doi = {10.1101/2021.06.02.446829}
}

@misc{zhangPhysicallybasedRenderingIndoor,
  title = {Physically-Based Rendering for Indoor Scene Understanding Using Convolutional Neural Networks. {{CoRR}} Abs/1612.07429 (2016)},
  author = {Zhang, Y and Song, S and Yumer, E and Savva, M and Lee, J and Jin, H and Funkhouser, {\relax TA}}
}

@article{zhangSimulationMachineVision2020,
  title = {Simulation of a Machine Vision System for Reflective Surface Defect Inspection Based on Ray Tracing},
  author = {Zhang, Pengfei and Cao, Pin and Yang, Yongying and Guo, Pan and Chen, Shiwei and Zhang, Danhui},
  year = {2020},
  month = mar,
  journal = {Appl. Opt.},
  volume = {59},
  number = {8},
  pages = {2656--2666},
  abstract = {A complete simulation of a machine vision system aimed at defect inspection on a reflective surface is proposed by ray tracing. The simulated scene is composed of the camera model, surface reflectance property, and light intensity distribution along with their corresponding object geometries. A virtual reflective plane geometry with scratches of various directions and pits of various sizes is built as the sample. Its realistic image is obtained by Monte Carlo ray tracing. Compared to the pinhole camera model, the camera model with a finite aperture emits more rays to deliver physical imaging. The bidirectional reflectance distribution function is applied to describe the surface reflectance property. The illustrated machine vision system captures a number of images while translating the light tubes. Then the image sequence obtained by experiment or simulation is fused to generate a well-contrasted synthetic image for defect detection. A flexible fusion method based on differential images is introduced to enhance the defect contrast on a uniform flawless background. To improve detection efficiency, defect contrast of synthetic images obtained by various fusion methods is evaluated. Influence of total image number, light tube width, and fusion interval is further discussed to optimize the inspection process. Experiments on car painted surfaces have shown that the simulated parameters can instruct the setup of the optical system and detect surface defects efficiently. The proposed simulation is capable of saving great effort in carrying out experimental trials and making improvements on reflective surface defect inspection.},
  langid = {english}
}

@article{zhangStudyPerformanceNovel2007,
  title = {Study of the Performance of a Novel 1 Mm Resolution Dual-Panel {{PET}} Camera Design Dedicated to Breast Cancer Imaging Using {{Monte Carlo}} Simulation},
  author = {Zhang, Jin and Olcott, Peter D and Chinn, Garry and Foudray, Angela M K and Levine, Craig S},
  year = {2007},
  month = feb,
  journal = {Med. Phys.},
  volume = {34},
  number = {2},
  pages = {689--702},
  publisher = {Wiley},
  langid = {english}
}

@article{zhengAdaptiveSparsePolynomial2017,
  title = {Adaptive Sparse Polynomial Regression for Camera Lens Simulation},
  author = {Zheng, Quan and Zheng, Changwen},
  year = {2017},
  journal = {The Visual Computer},
  volume = {33},
  number = {6},
  pages = {715--724},
  publisher = {Springer}
}

@inproceedings{zhengNeuroLensDataDrivenCamera2017,
  title = {{{NeuroLens}}: {{Data-Driven Camera Lens Simulation Using Neural Networks}}},
  booktitle = {Computer {{Graphics Forum}}},
  author = {Zheng, Quan and Zheng, Changwen},
  year = {2017},
  volume = {36},
  pages = {390--401},
  publisher = {Wiley Online Library}
}

@article{zhouImprovingLensFlare2023,
  title = {Improving Lens Flare Removal with General-Purpose Pipeline and Multiple Light Sources Recovery},
  author = {Zhou, Yuyan and Liang, Dong and Chen, Songcan and Huang, Shengjian and Yang, Shuo and Li, Chongyi},
  year = {2023},
  month = aug,
  journal = {ICCV},
  pages = {12923--12933}
}

@inproceedings{zhuUnpairedImageimageTranslation2017,
  title = {Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A},
  year = {2017},
  month = oct,
  pages = {2223--2232},
  publisher = {IEEE},
  address = {Venice}
}

@article{zotero-1039,
  type = {Article}
}

@article{zotero-611,
  type = {Article}
}

@article{zotero-737,
  type = {Article}
}

@article{zotero-857,
  type = {Article}
}

@misc{ZoteroYourPersonal,
  title = {Zotero {\textbar} {{Your}} Personal Research Assistant},
  urldate = {2024-10-25},
  howpublished = {https://www.zotero.org/download/},
  file = {/Users/wandell/Zotero/storage/9WKRMDVV/download.html}
}
